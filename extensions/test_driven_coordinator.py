#!/usr/bin/env python3
"""
æµ‹è¯•é©±åŠ¨åè°ƒå™¨ - å®Œå…¨ç‹¬ç«‹çš„æ‰©å±•åŠŸèƒ½

è¿™æ˜¯ä¸€ä¸ªå®Œå…¨ç‹¬ç«‹çš„æ‰©å±•æ¨¡å—ï¼Œä¸ä¿®æ”¹ä»»ä½•ç°æœ‰ä»£ç 
"""

import asyncio
import json
import re
import time
import logging
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass
from datetime import datetime

# å¯¼å…¥ç°æœ‰æ¡†æ¶ç»„ä»¶ - åªè¯»å–ï¼Œä¸ä¿®æ”¹
from core.centralized_coordinator import CentralizedCoordinator
from core.enums import AgentCapability, AgentStatus
from config.config import FrameworkConfig
from core.context_manager import get_context_manager, FullContextManager
from core.base_agent import TaskMessage


@dataclass
class TestDrivenConfig:
    """æµ‹è¯•é©±åŠ¨é…ç½®"""
    max_iterations: int = 5
    enable_deep_analysis: bool = True
    auto_fix_suggestions: bool = True
    save_iteration_logs: bool = True
    timeout_per_iteration: int = 300  # 5åˆ†é’Ÿ
    enable_persistent_conversation: bool = True  # æ–°å¢ï¼šå¯ç”¨æŒç»­å¯¹è¯
    max_conversation_history: int = 50  # æ–°å¢ï¼šæœ€å¤§å¯¹è¯å†å²é•¿åº¦


class TestDrivenCoordinator:
    """
    æµ‹è¯•é©±åŠ¨åè°ƒå™¨ - æ‰©å±•åŠŸèƒ½
    
    è¿™æ˜¯ä¸€ä¸ªè£…é¥°å™¨æ¨¡å¼çš„å®ç°ï¼ŒåŒ…è£…ç°æœ‰çš„CentralizedCoordinator
    è€Œä¸æ˜¯ç»§æ‰¿æˆ–ä¿®æ”¹å®ƒ
    """
    
    def __init__(self, base_coordinator: CentralizedCoordinator, 
                 config: TestDrivenConfig = None):
        """
        åˆå§‹åŒ–æµ‹è¯•é©±åŠ¨åè°ƒå™¨
        
        Args:
            base_coordinator: ç°æœ‰çš„åè°ƒå™¨å®ä¾‹ï¼ˆä¸ä¼šè¢«ä¿®æ”¹ï¼‰
            config: æµ‹è¯•é©±åŠ¨é…ç½®
        """
        self.base_coordinator = base_coordinator
        self.config = config or TestDrivenConfig()
        self.logger = logging.getLogger(f"{__name__}.TestDrivenCoordinator")
        
        # æ‰©å±•åŠŸèƒ½çŠ¶æ€
        self.test_driven_sessions = {}
        self.iteration_history = {}
        
        # ğŸ”— æŒç»­å¯¹è¯æœºåˆ¶ï¼šæ¯ä¸ªTDDä¼šè¯ä½¿ç”¨åŒä¸€ä¸ªå¯¹è¯ID
        self.persistent_conversation_id = None
        self.current_session_agents = {}  # è®°å½•æ¯ä¸ªä¼šè¯ä¸­ä½¿ç”¨çš„æ™ºèƒ½ä½“
        
        # ğŸ§  å®Œæ•´ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.context_manager: Optional[FullContextManager] = None
        
        # ğŸ¯ æ–°å¢ï¼šå¤šè½®å¯¹è¯å†å²ç®¡ç†
        self.persistent_conversation_history: List[Dict[str, str]] = []
        self.session_conversation_id = None
        self.current_agent_conversation_context = {}  # æ¯ä¸ªæ™ºèƒ½ä½“çš„å¯¹è¯ä¸Šä¸‹æ–‡
        
        # å¯¼å…¥æ‰©å±•è§£æå™¨
        from .enhanced_task_parser import EnhancedTaskParser
        from .test_analyzer import TestAnalyzer
        
        self.task_parser = EnhancedTaskParser()
        self.test_analyzer = TestAnalyzer()
        
        self.logger.info("ğŸ§ª æµ‹è¯•é©±åŠ¨åè°ƒå™¨æ‰©å±•å·²åˆå§‹åŒ–")
        self.logger.info(f"ğŸ”— æŒç»­å¯¹è¯æ¨¡å¼: {'å¯ç”¨' if self.config.enable_persistent_conversation else 'ç¦ç”¨'}")
    
    # ==========================================
    # ğŸ¯ æ–°å¢çš„æµ‹è¯•é©±åŠ¨åŠŸèƒ½ï¼ˆå®Œå…¨ç‹¬ç«‹ï¼‰
    # ==========================================
    
    async def execute_test_driven_task(self, task_description: str,
                                     testbench_path: str = None,
                                     context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œæµ‹è¯•é©±åŠ¨ä»»åŠ¡
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            testbench_path: æµ‹è¯•å°è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ‰§è¡Œç»“æœ
        """
        try:
            self.logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæµ‹è¯•é©±åŠ¨ä»»åŠ¡")
            
            # ğŸ¯ æ–°å¢ï¼šåˆ›å»ºå®éªŒä¼šè¯
            from core.experiment_manager import create_experiment_session
            
            experiment_name = f"tdd_{int(time.time())}"
            experiment_session = create_experiment_session(
                experiment_name=experiment_name,
                task_description=task_description,
                metadata={
                    "testbench_path": testbench_path,
                    "context": context,
                    "coordinator_type": "test_driven"
                }
            )
            
            experiment_id = experiment_session["experiment_id"]
            experiment_file_manager = experiment_session["file_manager"]
            experiment_context_manager = experiment_session["context_manager"]
            
            self.logger.info(f"ğŸ§ª åˆ›å»ºå®éªŒä¼šè¯: {experiment_id}")
            self.logger.info(f"   å·¥ä½œç›®å½•: {experiment_session['workspace_path']}")
            
            # ä½¿ç”¨å®éªŒä¸“ç”¨çš„æ–‡ä»¶ç®¡ç†å™¨å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨
            self.file_manager = experiment_file_manager
            self.context_manager = experiment_context_manager
            
            # è§£ææµ‹è¯•é©±åŠ¨éœ€æ±‚
            enhanced_analysis = await self._parse_test_driven_requirements(
                task_description, testbench_path, context
            )
            
            # éªŒè¯æµ‹è¯•å°ï¼ˆå¦‚æœæä¾›ï¼‰
            if testbench_path:
                validation_result = await self._validate_testbench(testbench_path)
                if not validation_result.get("valid", False):
                    self.logger.warning(f"âš ï¸ æµ‹è¯•å°éªŒè¯å¤±è´¥: {validation_result.get('error', 'unknown error')}")
            
            # æ‰§è¡ŒTDDå¾ªç¯
            session_id = f"tdd_session_{experiment_id}"
            tdd_result = await self._execute_tdd_loop(session_id, enhanced_analysis)
            
            # ğŸ¯ æ–°å¢ï¼šæ›´æ–°å®éªŒçŠ¶æ€
            from core.experiment_manager import get_experiment_manager
            exp_manager = get_experiment_manager()
            
            if tdd_result.get("success", False):
                exp_manager.update_experiment_status(
                    experiment_id, "completed",
                    metadata={
                        "final_result": tdd_result,
                        "iterations": tdd_result.get("total_iterations", 0),
                        "completion_time": datetime.now().isoformat()
                    }
                )
            else:
                exp_manager.update_experiment_status(
                    experiment_id, "failed",
                    metadata={
                        "error": tdd_result.get("error", "unknown error"),
                        "final_result": tdd_result
                    }
                )
            
            # æ·»åŠ å®éªŒä¿¡æ¯åˆ°ç»“æœ
            tdd_result["experiment_id"] = experiment_id
            tdd_result["experiment_workspace"] = experiment_session["workspace_path"]
            
            return tdd_result
            
        except Exception as e:
            self.logger.error(f"âŒ æµ‹è¯•é©±åŠ¨ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _parse_test_driven_requirements(self, task_description: str,
                                            testbench_path: str = None,
                                            context: Dict[str, Any] = None,
                                            force_tdd: bool = False) -> Dict[str, Any]:
        """è§£ææµ‹è¯•é©±åŠ¨éœ€æ±‚"""
        return await self.task_parser.parse_enhanced_task(
            task_description, testbench_path, context, force_tdd
        )
    
    async def _validate_testbench(self, testbench_path: str) -> Dict[str, Any]:
        """éªŒè¯æµ‹è¯•å°æ–‡ä»¶"""
        return await self.test_analyzer.validate_testbench_file(testbench_path)
    
    async def _execute_tdd_loop(self, session_id: str, 
                              enhanced_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡ŒTDDå¾ªç¯ - æ”¯æŒæŒç»­å¯¹è¯
        
        ä¸»è¦æ”¹è¿›ï¼š
        1. ä½¿ç”¨æŒç»­å¯¹è¯æœºåˆ¶ï¼Œé¿å…é‡å¤é€‰æ‹©æ™ºèƒ½ä½“
        2. ä¼ é€’å®Œæ•´çš„å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡
        3. æ™ºèƒ½ä½“èƒ½å¤Ÿè®°ä½ä¹‹å‰çš„æ‰€æœ‰è¿­ä»£
        """
        self.logger.info(f"ğŸ”„ å¼€å§‹TDDå¾ªç¯: {session_id}")
        
        # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
        self.test_driven_sessions[session_id] = {
            "start_time": time.time(),
            "status": "running",
            "iterations": [],
            "current_iteration": 0,
            "success": False,
            "completion_reason": None
        }
        
        # ğŸ¯ æ–°å¢ï¼šè®°å½•ä¼šè¯çº§åˆ«çš„æ™ºèƒ½ä½“é€‰æ‹©
        session_agents = {}
        
        try:
            for iteration in range(1, self.config.max_iterations + 1):
                self.logger.info(f"ğŸ”„ å¼€å§‹ç¬¬ {iteration} æ¬¡è¿­ä»£")
                
                # æ›´æ–°ä¼šè¯çŠ¶æ€
                self.test_driven_sessions[session_id]["current_iteration"] = iteration
                
                # æ‰§è¡Œå•æ¬¡è¿­ä»£
                iteration_result = await self._execute_single_tdd_iteration(
                    session_id, iteration, enhanced_analysis
                )
                
                # è®°å½•è¿­ä»£ç»“æœ
                self.test_driven_sessions[session_id]["iterations"].append(iteration_result)
                
                # ğŸ¯ æ–°å¢ï¼šè®°å½•æ™ºèƒ½ä½“é€‰æ‹©
                if iteration_result.get("agent_id"):
                    session_agents[iteration_result.get("agent_role", "unknown")] = iteration_result["agent_id"]
                
                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if iteration_result.get("success", False):
                    self.logger.info(f"âœ… TDDå¾ªç¯åœ¨ç¬¬ {iteration} æ¬¡è¿­ä»£æˆåŠŸå®Œæˆ")
                    self.test_driven_sessions[session_id].update({
                        "status": "completed",
                        "success": True,
                        "completion_reason": "tests_passed",
                        "total_iterations": iteration
                    })
                    
                    # ğŸ¯ æ–°å¢ï¼šä¿å­˜ä¼šè¯æ™ºèƒ½ä½“ä¿¡æ¯
                    self.current_session_agents.update(session_agents)
                    
                    return {
                        "success": True,
                        "session_id": session_id,
                        "total_iterations": iteration,
                        "final_design": iteration_result.get("generated_files", []),
                        "test_results": iteration_result.get("test_results", {}),
                        "completion_reason": "tests_passed",
                        "conversation_history": self.persistent_conversation_history if self.config.enable_persistent_conversation else []
                    }
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»§ç»­
                if not iteration_result.get("should_continue", True):
                    self.logger.info(f"ğŸ›‘ TDDå¾ªç¯åœ¨ç¬¬ {iteration} æ¬¡è¿­ä»£åœæ­¢")
                    self.test_driven_sessions[session_id].update({
                        "status": "stopped",
                        "success": False,
                        "completion_reason": "manual_stop",
                        "total_iterations": iteration
                    })
                    break
                
                # è¶…æ—¶æ£€æŸ¥
                elapsed_time = time.time() - self.test_driven_sessions[session_id]["start_time"]
                if elapsed_time > self.config.timeout_per_iteration:
                    self.logger.warning(f"â° TDDå¾ªç¯è¶…æ—¶: {elapsed_time:.2f}ç§’")
                    self.test_driven_sessions[session_id].update({
                        "status": "timeout",
                        "success": False,
                        "completion_reason": "timeout",
                        "total_iterations": iteration
                    })
                    break
            
            # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
            self.logger.warning(f"ğŸ”„ TDDå¾ªç¯è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°: {self.config.max_iterations}")
            self.test_driven_sessions[session_id].update({
                "status": "max_iterations_reached",
                "success": False,
                "completion_reason": "max_iterations_reached",
                "total_iterations": self.config.max_iterations
            })
            
            # ğŸ¯ æ–°å¢ï¼šä¿å­˜ä¼šè¯æ™ºèƒ½ä½“ä¿¡æ¯
            self.current_session_agents.update(session_agents)
            
            return {
                "success": False,
                "session_id": session_id,
                "total_iterations": self.config.max_iterations,
                "completion_reason": "max_iterations_reached",
                "partial_results": self.test_driven_sessions[session_id]["iterations"],
                "conversation_history": self.persistent_conversation_history if self.config.enable_persistent_conversation else []
            }
            
        except Exception as e:
            self.logger.error(f"âŒ TDDå¾ªç¯å¼‚å¸¸: {str(e)}")
            self.test_driven_sessions[session_id].update({
                "status": "error",
                "success": False,
                "completion_reason": "error",
                "error": str(e)
            })
            return {
                "success": False,
                "session_id": session_id,
                "error": str(e),
                "completion_reason": "error"
            }
    
    async def _execute_single_tdd_iteration(self, session_id: str, iteration: int,
                                          enhanced_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•æ¬¡TDDè¿­ä»£ - æ”¯æŒæŒç»­å¯¹è¯
        
        æ”¹è¿›ï¼š
        1. ä½¿ç”¨æŒç»­å¯¹è¯æœºåˆ¶
        2. ä¼ é€’å®Œæ•´çš„ä¸Šä¸‹æ–‡å’Œå†å²
        3. æ™ºèƒ½ä½“èƒ½å¤Ÿè®°ä½ä¹‹å‰çš„è®¾è®¡å†³ç­–
        """
        self.logger.info(f"ğŸ”„ æ‰§è¡Œç¬¬ {iteration} æ¬¡TDDè¿­ä»£")
        
        # å¼€å§‹æ–°çš„è¿­ä»£ä¸Šä¸‹æ–‡
        if self.context_manager:
            self.context_manager.start_new_iteration(iteration)
        
        try:
            # 1. è®¾è®¡é˜¶æ®µ
            design_result = await self._execute_design_phase(session_id, iteration, enhanced_analysis)
            
            if not design_result.get("success", False):
                return {
                    "success": False,
                    "error": design_result.get("error", "è®¾è®¡é˜¶æ®µå¤±è´¥"),
                    "iteration": iteration,
                    "should_continue": False
                }
            
            # 2. æµ‹è¯•é˜¶æ®µ
            test_result = await self._execute_test_phase(
                session_id, iteration, design_result, enhanced_analysis,
                design_result.get("generated_files", [])
            )
            
            # 3. åˆ†ææ”¹è¿›
            improvement_analysis = await self._analyze_for_improvement(
                {"design": design_result, "test": test_result}, enhanced_analysis
            )
            
            # 4. å†³å®šæ˜¯å¦ç»§ç»­ - æ”¹è¿›é€»è¾‘
            # ğŸ¯ å…³é”®æ”¹è¿›ï¼šä¸ä»…æ£€æŸ¥æµ‹è¯•é€šè¿‡ï¼Œè¿˜è¦æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®å¤
            needs_fix = test_result.get("needs_fix", False)
            all_tests_passed = test_result.get("all_tests_passed", False)
            
            # å¦‚æœæµ‹è¯•å¤±è´¥æˆ–éœ€è¦ä¿®å¤ï¼Œç»§ç»­è¿­ä»£
            should_continue = not all_tests_passed or needs_fix
            
            # å¦‚æœæœ‰ä»¿çœŸé”™è¯¯ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
            if test_result.get("simulation_result") and not test_result["simulation_result"].get("success", False):
                if "simulation_errors" not in enhanced_analysis:
                    enhanced_analysis["simulation_errors"] = []
                
                error_info = {
                    "iteration": iteration,
                    "error": test_result["simulation_result"].get("error", "æœªçŸ¥é”™è¯¯"),
                    "compilation_output": test_result["simulation_result"].get("compilation_output", ""),
                    "command": test_result["simulation_result"].get("command", ""),
                    "stage": test_result["simulation_result"].get("stage", "unknown"),
                    "return_code": test_result["simulation_result"].get("return_code", -1),
                    "timestamp": time.time()
                }
                
                enhanced_analysis["simulation_errors"].append(error_info)
                self.logger.info(f"ğŸ“ è®°å½•è¿­ä»£{iteration}çš„ä»¿çœŸé”™è¯¯: {error_info['error'][:100]}...")
            
            return {
                "success": all_tests_passed and not needs_fix,
                "iteration": iteration,
                "design_result": design_result,
                "test_result": test_result,
                "improvement_analysis": improvement_analysis,
                "should_continue": should_continue,
                "needs_fix": needs_fix,
                "agent_id": design_result.get("agent_id"),
                "agent_role": design_result.get("agent_role", "verilog_designer"),
                "generated_files": design_result.get("generated_files", [])
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ç¬¬ {iteration} æ¬¡è¿­ä»£å¼‚å¸¸: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "iteration": iteration,
                "should_continue": False
            }
    
    async def _execute_design_phase(self, session_id: str, iteration: int,
                                  enhanced_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œè®¾è®¡é˜¶æ®µ - æ”¯æŒæŒç»­å¯¹è¯
        
        æ”¹è¿›ï¼š
        1. ä½¿ç”¨æŒç»­å¯¹è¯æœºåˆ¶ï¼Œé¿å…é‡å¤é€‰æ‹©æ™ºèƒ½ä½“
        2. ä¼ é€’å®Œæ•´çš„å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡
        3. æ™ºèƒ½ä½“èƒ½å¤Ÿè®°ä½ä¹‹å‰çš„è®¾è®¡å†³ç­–
        """
        self.logger.info(f"ğŸ¨ æ‰§è¡Œè®¾è®¡é˜¶æ®µ: è¿­ä»£ {iteration}")
        
        # ğŸ¯ æ–°å¢ï¼šæ„å»ºåŒ…å«å®Œæ•´å†å²çš„ä»»åŠ¡
        task = await self._build_design_task(enhanced_analysis, iteration)
        
        # ğŸ¯ æ–°å¢ï¼šä½¿ç”¨æŒç»­å¯¹è¯æœºåˆ¶
        if self.config.enable_persistent_conversation:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®¾è®¡æ™ºèƒ½ä½“
            design_agent_id = self.current_session_agents.get("verilog_designer")
            
            if design_agent_id:
                # ä½¿ç”¨æŒç»­å¯¹è¯
                self.logger.info(f"ğŸ”— ä½¿ç”¨æŒç»­å¯¹è¯æ™ºèƒ½ä½“: {design_agent_id}")
                result = await self._execute_with_persistent_conversation(
                    task, design_agent_id, "verilog_designer", iteration
                )
            else:
                # é¦–æ¬¡é€‰æ‹©æ™ºèƒ½ä½“
                self.logger.info("ğŸ” é¦–æ¬¡é€‰æ‹©è®¾è®¡æ™ºèƒ½ä½“")
                result = await self._execute_with_agent_selection(
                    task, "verilog_designer", iteration
                )
                # è®°å½•é€‰æ‹©çš„æ™ºèƒ½ä½“
                if result.get("success") and result.get("agent_id"):
                    self.current_session_agents["verilog_designer"] = result["agent_id"]
        else:
            # å›é€€åˆ°æ ‡å‡†æµç¨‹
            result = await self._execute_with_agent_selection(task, "verilog_designer", iteration)
        
        # ğŸ¯ ä¿®å¤ï¼šæå–æ–‡ä»¶å¼•ç”¨
        design_files = self._extract_file_references(result)
        
        return {
            "success": result.get("success", False),
            "design_result": result,
            "generated_files": design_files,
            "agent_id": result.get("agent_id"),
            "agent_role": "verilog_designer"
        }
    
    async def _execute_test_phase(self, session_id: str, iteration: int,
                                design_result: Dict[str, Any],
                                enhanced_analysis: Dict[str, Any],
                                design_files: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œæµ‹è¯•é˜¶æ®µ - æ”¯æŒæŒç»­å¯¹è¯
        
        æ”¹è¿›ï¼š
        1. å¼ºåˆ¶ç”Ÿæˆæµ‹è¯•å°
        2. å¼ºåˆ¶è¿è¡Œä»¿çœŸéªŒè¯
        3. ç¡®ä¿çœŸæ­£çš„TDDæµç¨‹
        """
        self.logger.info(f"ğŸ§ª æ‰§è¡Œæµ‹è¯•é˜¶æ®µ: è¿­ä»£ {iteration}")
        
        # è·å–è®¾è®¡æ–‡ä»¶
        if not design_files:
            design_files = design_result.get("generated_files", [])
        
        # å¦‚æœdesign_filesä¸ºç©ºï¼Œå°è¯•ä»å®éªŒç®¡ç†å™¨è·å–
        if not design_files:
            try:
                from core.experiment_manager import get_experiment_manager
                exp_manager = get_experiment_manager()
                if exp_manager and exp_manager.current_experiment_path:
                    # æ‰«ædesignsç›®å½•
                    designs_dir = exp_manager.current_experiment_path / "designs"
                    if designs_dir.exists():
                        for file_path in designs_dir.glob("*.v"):
                            design_files.append({
                                "path": str(file_path),
                                "filename": file_path.name,
                                "type": "verilog"
                            })
                        self.logger.info(f"ğŸ” ä»å®éªŒç®¡ç†å™¨è·å–åˆ° {len(design_files)} ä¸ªè®¾è®¡æ–‡ä»¶")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ä»å®éªŒç®¡ç†å™¨è·å–æ–‡ä»¶å¤±è´¥: {e}")
        
        # ğŸ¯ å¼ºåˆ¶æµ‹è¯•å°ç”Ÿæˆå’Œä»¿çœŸéªŒè¯
        test_result = await self._execute_comprehensive_testing(
            session_id, iteration, design_files, enhanced_analysis
        )
        
        return test_result
    
    async def _execute_comprehensive_testing(self, session_id: str, iteration: int,
                                           design_files: List[Dict[str, Any]],
                                           enhanced_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå…¨é¢çš„æµ‹è¯•éªŒè¯æµç¨‹
        
        å¼ºåˆ¶æ­¥éª¤ï¼š
        1. ç”Ÿæˆæµ‹è¯•å°
        2. è¿è¡Œä»¿çœŸ
        3. åˆ†æç»“æœ
        """
        self.logger.info(f"ğŸ§ª å¼€å§‹å…¨é¢æµ‹è¯•éªŒè¯æµç¨‹")
        
        try:
            # 1. å¼ºåˆ¶ç”Ÿæˆæµ‹è¯•å°
            testbench_result = await self._force_generate_testbench(
                session_id, iteration, design_files, enhanced_analysis
            )
            
            if not testbench_result.get("success", False):
                return {
                    "success": False,
                    "all_tests_passed": False,
                    "error": f"æµ‹è¯•å°ç”Ÿæˆå¤±è´¥: {testbench_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                    "stage": "testbench_generation"
                }
            
            # 2. å¼ºåˆ¶è¿è¡Œä»¿çœŸ
            simulation_result = await self._force_run_simulation(
                session_id, iteration, design_files, testbench_result, enhanced_analysis
            )
            
            # ğŸ¯ å…³é”®æ”¹è¿›ï¼šå¦‚æœä»¿çœŸå¤±è´¥ï¼Œä¸è¦ç›´æ¥è¿”å›ï¼Œè€Œæ˜¯ç»§ç»­åˆ†æé”™è¯¯
            if not simulation_result.get("success", False):
                self.logger.warning(f"âš ï¸ ä»¿çœŸå¤±è´¥ï¼Œä½†ç»§ç»­åˆ†æé”™è¯¯: {simulation_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
                # 3. åˆ†æä»¿çœŸå¤±è´¥åŸå› 
                analysis_result = await self._analyze_simulation_results(
                    session_id, iteration, simulation_result, enhanced_analysis
                )
                
                return {
                    "success": False,
                    "all_tests_passed": False,
                    "error": f"ä»¿çœŸè¿è¡Œå¤±è´¥: {simulation_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                    "stage": "simulation_failed",
                    "testbench_result": testbench_result,
                    "simulation_result": simulation_result,
                    "analysis_result": analysis_result,
                    "needs_fix": True  # æ ‡è®°éœ€è¦ä¿®å¤
                }
            
            # 3. åˆ†æä»¿çœŸç»“æœ
            analysis_result = await self._analyze_simulation_results(
                session_id, iteration, simulation_result, enhanced_analysis
            )
            
            # 4. ç»¼åˆç»“æœ
            all_tests_passed = simulation_result.get("all_tests_passed", False)
            
            return {
                "success": True,
                "all_tests_passed": all_tests_passed,
                "stage": "complete",
                "testbench_result": testbench_result,
                "simulation_result": simulation_result,
                "analysis_result": analysis_result,
                "test_summary": simulation_result.get("test_summary", "æ— æµ‹è¯•æ‘˜è¦"),
                "return_code": simulation_result.get("return_code", -1)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ å…¨é¢æµ‹è¯•éªŒè¯å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "all_tests_passed": False,
                "error": str(e),
                "stage": "error"
            }
    
    async def _force_generate_testbench(self, session_id: str, iteration: int,
                                       design_files: List[Dict[str, Any]],
                                       enhanced_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """å¼ºåˆ¶ç”Ÿæˆæµ‹è¯•å° - å¢å¼ºç«¯å£ä¸€è‡´æ€§æ£€æŸ¥"""
        try:
            self.logger.info("ğŸ§ª å¼€å§‹å¼ºåˆ¶ç”Ÿæˆæµ‹è¯•å°")
            
            # ğŸ¯ æ–°å¢ï¼šè·å–è®¾è®¡æ–‡ä»¶çš„ç«¯å£ä¿¡æ¯
            design_port_info = None
            design_content = ""
            module_name = ""
            
            for design_file in design_files:
                if isinstance(design_file, dict) and 'file_path' in design_file:
                    try:
                        with open(design_file['file_path'], 'r', encoding='utf-8') as f:
                            design_content = f.read()
                            module_name = self._extract_module_name(design_content)
                            
                            # è·å–ç«¯å£ä¿¡æ¯
                            from core.file_manager import get_file_manager
                            file_manager = get_file_manager()
                            design_port_info = file_manager.get_design_port_info(module_name)
                            
                            if design_port_info:
                                self.logger.info(f"ğŸ¯ è·å–åˆ°è®¾è®¡ç«¯å£ä¿¡æ¯: {module_name} - {design_port_info['port_count']} ä¸ªç«¯å£")
                                break
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ è¯»å–è®¾è®¡æ–‡ä»¶å¤±è´¥: {str(e)}")
            
            # æ„å»ºå¢å¼ºçš„æµ‹è¯•å°ç”Ÿæˆä»»åŠ¡
            enhanced_task = self._build_enhanced_testbench_task(
                design_files, enhanced_analysis, design_port_info, module_name
            )
            
            # æ‰§è¡Œæµ‹è¯•å°ç”Ÿæˆ
            testbench_result = await self._execute_with_agent_selection(
                enhanced_task, "code_reviewer", iteration
            )
            
            # ğŸ¯ æ–°å¢ï¼šéªŒè¯ç”Ÿæˆçš„æµ‹è¯•å°ç«¯å£ä¸€è‡´æ€§
            if testbench_result.get("success", False):
                testbench_file = self._get_testbench_file(testbench_result)
                if testbench_file and design_port_info:
                    validation_result = self._validate_testbench_ports(
                        testbench_file, design_port_info, module_name
                    )
                    
                    if not validation_result["valid"]:
                        self.logger.warning(f"âš ï¸ æµ‹è¯•å°ç«¯å£ä¸ä¸€è‡´: {validation_result}")
                        # å°è¯•è‡ªåŠ¨ä¿®å¤
                        fixed_testbench = self._auto_fix_testbench_ports(
                            testbench_file, design_port_info, module_name
                        )
                        if fixed_testbench:
                            # ä¿å­˜ä¿®å¤åçš„æµ‹è¯•å°
                            fixed_file_path = testbench_file.replace('.v', '_fixed.v')
                            with open(fixed_file_path, 'w', encoding='utf-8') as f:
                                f.write(fixed_testbench)
                            self.logger.info(f"ğŸ”§ è‡ªåŠ¨ä¿®å¤æµ‹è¯•å°ç«¯å£: {fixed_file_path}")
                            
                            # æ›´æ–°ç»“æœ
                            testbench_result["fixed_testbench_file"] = fixed_file_path
                            testbench_result["port_validation"] = validation_result
            
            return testbench_result
            
        except Exception as e:
            self.logger.error(f"âŒ å¼ºåˆ¶ç”Ÿæˆæµ‹è¯•å°å¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _build_enhanced_testbench_task(self, design_files: List[Dict[str, Any]], 
                                      enhanced_analysis: Dict[str, Any],
                                      design_port_info: Dict[str, Any],
                                      module_name: str) -> str:
        """æ„å»ºå¢å¼ºçš„æµ‹è¯•å°ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…å«ç«¯å£ä¿¡æ¯"""
        design_file = self._get_design_file(design_files)
        if not design_file:
            return "âŒ æœªæ‰¾åˆ°è®¾è®¡æ–‡ä»¶"
        
        try:
            with open(design_file, 'r', encoding='utf-8') as f:
                design_content = f.read()
        except Exception as e:
            return f"âŒ è¯»å–è®¾è®¡æ–‡ä»¶å¤±è´¥: {str(e)}"
        
        # æ„å»ºåŒ…å«ç«¯å£ä¿¡æ¯çš„ä»»åŠ¡
        port_info_text = ""
        if design_port_info:
            port_info_text = f"""
**ç«¯å£ä¿¡æ¯ï¼ˆå¿…é¡»ä¸¥æ ¼åŒ¹é…ï¼‰**ï¼š
æ¨¡å—å: {module_name}
ç«¯å£åˆ—è¡¨:
"""
            for port in design_port_info.get("ports", []):
                port_info_text += f"- {port['direction']} [{port['width']-1}:0] {port['name']}\n"
        
        task = f"""
ğŸ§ª å¼ºåˆ¶æµ‹è¯•å°ç”Ÿæˆä»»åŠ¡

ä½ å¿…é¡»ä¸ºä»¥ä¸‹è®¾è®¡ç”Ÿæˆæµ‹è¯•å°æ–‡ä»¶ï¼š

- æ–‡ä»¶å: {Path(design_file).name}
  è·¯å¾„: {design_file}

{port_info_text}
**é‡è¦è¦æ±‚**ï¼š
1. å¿…é¡»ä½¿ç”¨ generate_testbench å·¥å…·ç”Ÿæˆæµ‹è¯•å°
2. å¿…é¡»åŒ…å«æ‰€æœ‰åŠŸèƒ½çš„æµ‹è¯•ç”¨ä¾‹
3. å¿…é¡»åŒ…å«è¾¹ç•Œæ¡ä»¶æµ‹è¯•
4. å¿…é¡»ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•å°æ–‡ä»¶
5. å¿…é¡»ä¿å­˜æµ‹è¯•å°æ–‡ä»¶åˆ°å®éªŒç›®å½•
6. **ç«¯å£è¿æ¥å¿…é¡»ä¸è®¾è®¡æ–‡ä»¶å®Œå…¨ä¸€è‡´**

è¯·ç«‹å³æ‰§è¡Œæµ‹è¯•å°ç”Ÿæˆï¼Œä¸è¦è·³è¿‡æ­¤æ­¥éª¤ã€‚
"""
        
        return task
    
    def _validate_testbench_ports(self, testbench_file: str, design_port_info: Dict[str, Any], 
                                 module_name: str) -> Dict[str, Any]:
        """éªŒè¯æµ‹è¯•å°ç«¯å£ä¸è®¾è®¡ç«¯å£çš„ä¸€è‡´æ€§"""
        try:
            with open(testbench_file, 'r', encoding='utf-8') as f:
                testbench_content = f.read()
            
            from core.file_manager import get_file_manager
            file_manager = get_file_manager()
            
            return file_manager.validate_testbench_ports(testbench_content, module_name)
            
        except Exception as e:
            return {"valid": False, "error": f"éªŒè¯å¤±è´¥: {str(e)}"}
    
    def _auto_fix_testbench_ports(self, testbench_file: str, design_port_info: Dict[str, Any], 
                                 module_name: str) -> Optional[str]:
        """è‡ªåŠ¨ä¿®å¤æµ‹è¯•å°ç«¯å£ä¸åŒ¹é…é—®é¢˜"""
        try:
            with open(testbench_file, 'r', encoding='utf-8') as f:
                testbench_content = f.read()
            
            import re
            
            # æŸ¥æ‰¾æ¨¡å—å®ä¾‹åŒ–
            instance_pattern = rf'{module_name}\s+\w+\s*\(([^)]+)\);'
            match = re.search(instance_pattern, testbench_content, re.DOTALL)
            
            if not match:
                return None
            
            instance_ports = match.group(1)
            
            # æ„å»ºæ­£ç¡®çš„ç«¯å£è¿æ¥
            correct_connections = []
            for port in design_port_info.get("ports", []):
                port_name = port["name"]
                # æŸ¥æ‰¾ç°æœ‰çš„è¿æ¥
                port_pattern = rf'\.{port_name}\s*\(\s*(\w+)\s*\)'
                port_match = re.search(port_pattern, instance_ports)
                
                if port_match:
                    signal_name = port_match.group(1)
                    correct_connections.append(f".{port_name}({signal_name})")
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¿æ¥ï¼Œä½¿ç”¨é»˜è®¤ä¿¡å·å
                    default_signal = f"{port_name}_signal"
                    correct_connections.append(f".{port_name}({default_signal})")
            
            # æ›¿æ¢ç«¯å£è¿æ¥
            new_instance_ports = ",\n        ".join(correct_connections)
            new_instance = f"{module_name} uut (\n        {new_instance_ports}\n    );"
            
            # æ›¿æ¢æ•´ä¸ªå®ä¾‹åŒ–
            fixed_content = re.sub(instance_pattern + r';', new_instance, testbench_content, flags=re.DOTALL)
            
            return fixed_content
            
        except Exception as e:
            self.logger.error(f"âŒ è‡ªåŠ¨ä¿®å¤æµ‹è¯•å°ç«¯å£å¤±è´¥: {str(e)}")
            return None
    
    async def _force_run_simulation(self, session_id: str, iteration: int,
                                   design_files: List[Dict[str, Any]],
                                   testbench_result: Dict[str, Any],
                                   enhanced_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¼ºåˆ¶è¿è¡Œä»¿çœŸ - æ™ºèƒ½å‚æ•°å¤„ç†
        
        ç¡®ä¿ä»¿çœŸè¢«è¿è¡Œï¼Œå³ä½¿æ™ºèƒ½ä½“æ²¡æœ‰ä¸»åŠ¨è¿è¡Œ
        """
        self.logger.info(f"ğŸ”§ å¼ºåˆ¶è¿è¡Œä»¿çœŸ - è¿­ä»£ {iteration}")
        
        # è·å–æµ‹è¯•å°æ–‡ä»¶
        testbench_file = self._get_testbench_file(testbench_result)
        if not testbench_file:
            return {
                "success": False,
                "error": "æ— æ³•æ‰¾åˆ°æµ‹è¯•å°æ–‡ä»¶",
                "all_tests_passed": False
            }
        
        # è·å–è®¾è®¡æ–‡ä»¶
        design_file = self._get_design_file(design_files)
        if not design_file:
            return {
                "success": False,
                "error": "æ— æ³•æ‰¾åˆ°è®¾è®¡æ–‡ä»¶",
                "all_tests_passed": False
            }
        
        # ğŸ§  æ™ºèƒ½å‚æ•°å¤„ç†ç­–ç•¥
        # 1. é¦–å…ˆå°è¯•ä½¿ç”¨æ–‡ä»¶è·¯å¾„å‚æ•°
        # 2. å¦‚æœå¤±è´¥ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ä»£ç å†…å®¹å‚æ•°
        # 3. å¦‚æœä»£ç å†…å®¹ä¹Ÿæ²¡æœ‰ï¼Œä»æ–‡ä»¶ç®¡ç†å™¨è·å–
        
        # å°è¯•è¯»å–æ–‡ä»¶å†…å®¹ä½œä¸ºå¤‡ç”¨
        design_code = None
        testbench_code = None
        
        try:
            # è¯»å–è®¾è®¡æ–‡ä»¶å†…å®¹
            if design_file and Path(design_file).exists():
                with open(design_file, 'r', encoding='utf-8') as f:
                    design_code = f.read()
                    self.logger.info(f"ğŸ“„ è¯»å–è®¾è®¡æ–‡ä»¶å†…å®¹: {len(design_code)} å­—ç¬¦")
            
            # è¯»å–æµ‹è¯•å°æ–‡ä»¶å†…å®¹
            if testbench_file and Path(testbench_file).exists():
                with open(testbench_file, 'r', encoding='utf-8') as f:
                    testbench_code = f.read()
                    self.logger.info(f"ğŸ“„ è¯»å–æµ‹è¯•å°æ–‡ä»¶å†…å®¹: {len(testbench_code)} å­—ç¬¦")
                    
        except Exception as e:
            self.logger.warning(f"âš ï¸ è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {str(e)}")
        
        # æ„å»ºæ™ºèƒ½ä»¿çœŸä»»åŠ¡
        task = self._build_smart_simulation_task(design_file, testbench_file, design_code, testbench_code)
        
        # ä½¿ç”¨æµ‹è¯•æ™ºèƒ½ä½“è¿è¡Œä»¿çœŸ
        if self.config.enable_persistent_conversation:
            test_agent_id = self.current_session_agents.get("code_reviewer")
            
            if test_agent_id:
                result = await self._execute_with_persistent_conversation(
                    task, test_agent_id, "code_reviewer", iteration
                )
            else:
                result = await self._execute_with_agent_selection(
                    task, "code_reviewer", iteration
                )
        else:
            result = await self._execute_with_agent_selection(task, "code_reviewer", iteration)
        
        # å¦‚æœæ™ºèƒ½ä½“æ²¡æœ‰è¿è¡Œä»¿çœŸï¼Œå¼ºåˆ¶è¿è¡Œ
        if not result.get("success", False) or not self._has_simulation_run():
            self.logger.warning("âš ï¸ æ™ºèƒ½ä½“æœªè¿è¡Œä»¿çœŸï¼Œå¼ºåˆ¶è¿è¡Œä»¿çœŸ")
            result = await self._run_fallback_simulation(design_file, testbench_file)
        
        # ğŸ¯ å…³é”®æ”¹è¿›ï¼šå¦‚æœä»¿çœŸå¤±è´¥ï¼Œç«‹å³å°†é”™è¯¯ä¿¡æ¯æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
        if not result.get("success", False):
            self.logger.info("ğŸ”§ ä»¿çœŸå¤±è´¥ï¼Œå°†é”™è¯¯ä¿¡æ¯æ·»åŠ åˆ°ä¸Šä¸‹æ–‡")
            
            # å°†é”™è¯¯ä¿¡æ¯æ·»åŠ åˆ°å¢å¼ºåˆ†æä¸­
            if "simulation_errors" not in enhanced_analysis:
                enhanced_analysis["simulation_errors"] = []
            
            error_info = {
                "iteration": iteration,
                "error": result.get("error", "æœªçŸ¥é”™è¯¯"),
                "compilation_output": result.get("compilation_output", ""),
                "command": result.get("command", ""),
                "stage": result.get("stage", "unknown"),
                "return_code": result.get("return_code", -1),
                "timestamp": time.time()
            }
            
            enhanced_analysis["simulation_errors"].append(error_info)
            
            # è®°å½•åˆ°æ—¥å¿—
            self.logger.info(f"ğŸ“ è®°å½•ä»¿çœŸé”™è¯¯: {error_info['error'][:100]}...")
        
        return result
    
    def _build_forced_testbench_task(self, design_files: List[Dict[str, Any]],
                                    enhanced_analysis: Dict[str, Any]) -> str:
        """
        æ„å»ºå¼ºåˆ¶æµ‹è¯•å°ç”Ÿæˆä»»åŠ¡
        """
        # æ„å»ºæ–‡ä»¶åˆ—è¡¨
        if design_files:
            file_list = []
            for f in design_files:
                if isinstance(f, dict):
                    file_path = f.get('path', f.get('filename', 'unknown'))
                    file_name = f.get('filename', Path(file_path).name if file_path != 'unknown' else 'unknown')
                    file_list.append(f"- æ–‡ä»¶å: {file_name}")
                    file_list.append(f"  è·¯å¾„: {file_path}")
                    file_list.append("")
                else:
                    file_list.append(f"- {str(f)}")
            
            files_section = "\n".join(file_list)
        else:
            files_section = "è®¾è®¡æ–‡ä»¶: æ— ï¼ˆéœ€è¦å…ˆç”Ÿæˆè®¾è®¡æ–‡ä»¶ï¼‰"
        
        return f"""
ğŸ§ª å¼ºåˆ¶æµ‹è¯•å°ç”Ÿæˆä»»åŠ¡

ä½ å¿…é¡»ä¸ºä»¥ä¸‹è®¾è®¡ç”Ÿæˆæµ‹è¯•å°æ–‡ä»¶ï¼š

{files_section}

å¼ºåˆ¶è¦æ±‚ï¼š
1. å¿…é¡»ä½¿ç”¨ generate_testbench å·¥å…·ç”Ÿæˆæµ‹è¯•å°
2. å¿…é¡»åŒ…å«æ‰€æœ‰åŠŸèƒ½çš„æµ‹è¯•ç”¨ä¾‹
3. å¿…é¡»åŒ…å«è¾¹ç•Œæ¡ä»¶æµ‹è¯•
4. å¿…é¡»ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•å°æ–‡ä»¶
5. å¿…é¡»ä¿å­˜æµ‹è¯•å°æ–‡ä»¶åˆ°å®éªŒç›®å½•

è¯·ç«‹å³æ‰§è¡Œæµ‹è¯•å°ç”Ÿæˆï¼Œä¸è¦è·³è¿‡æ­¤æ­¥éª¤ã€‚
"""
    
    def _build_forced_simulation_task(self, design_file: str, testbench_file: str) -> str:
        """
        æ„å»ºå¼ºåˆ¶ä»¿çœŸä»»åŠ¡
        """
        return f"""
ğŸ§ª å¼ºåˆ¶ä»¿çœŸè¿è¡Œä»»åŠ¡

ä½ å¿…é¡»è¿è¡Œä»¿çœŸéªŒè¯ä»¥ä¸‹æ–‡ä»¶ï¼š

è®¾è®¡æ–‡ä»¶: {design_file}
æµ‹è¯•å°æ–‡ä»¶: {testbench_file}

å¼ºåˆ¶è¦æ±‚ï¼š
1. å¿…é¡»ä½¿ç”¨ run_simulation å·¥å…·è¿è¡Œä»¿çœŸ
2. å¿…é¡»ç¼–è¯‘è®¾è®¡æ–‡ä»¶å’Œæµ‹è¯•å°
3. å¿…é¡»æ‰§è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
4. å¿…é¡»åˆ†æä»¿çœŸç»“æœ
5. å¿…é¡»æä¾›è¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š

è¯·ç«‹å³æ‰§è¡Œä»¿çœŸï¼Œä¸è¦è·³è¿‡æ­¤æ­¥éª¤ã€‚
"""

    def _build_smart_simulation_task(self, design_file: str, testbench_file: str, 
                                   design_code: str = None, testbench_code: str = None) -> str:
        """
        æ„å»ºæ™ºèƒ½ä»¿çœŸä»»åŠ¡ - æ”¯æŒå¤šç§å‚æ•°æ ¼å¼
        """
        task_lines = [
            "ğŸ§ª æ™ºèƒ½ä»¿çœŸè¿è¡Œä»»åŠ¡",
            "",
            "ä½ å¿…é¡»è¿è¡Œä»¿çœŸéªŒè¯ä»¥ä¸‹è®¾è®¡ï¼š",
            ""
        ]
        
        # æ·»åŠ æ–‡ä»¶è·¯å¾„ä¿¡æ¯
        if design_file:
            task_lines.append(f"è®¾è®¡æ–‡ä»¶: {design_file}")
        if testbench_file:
            task_lines.append(f"æµ‹è¯•å°æ–‡ä»¶: {testbench_file}")
        
        # æ·»åŠ ä»£ç å†…å®¹ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if design_code:
            task_lines.extend([
                "",
                "è®¾è®¡ä»£ç å†…å®¹ï¼ˆå·²æä¾›ï¼‰:",
                "```verilog",
                design_code[:500] + "..." if len(design_code) > 500 else design_code,
                "```"
            ])
        
        if testbench_code:
            task_lines.extend([
                "",
                "æµ‹è¯•å°ä»£ç å†…å®¹ï¼ˆå·²æä¾›ï¼‰:",
                "```verilog",
                testbench_code[:500] + "..." if len(testbench_code) > 500 else testbench_code,
                "```"
            ])
        
        task_lines.extend([
            "",
            "ğŸ§  æ™ºèƒ½å‚æ•°å¤„ç†ç­–ç•¥ï¼š",
            "1. ä¼˜å…ˆä½¿ç”¨æ–‡ä»¶è·¯å¾„å‚æ•°ï¼ˆmodule_file, testbench_fileï¼‰",
            "2. å¦‚æœæ–‡ä»¶è·¯å¾„å‚æ•°å¤±è´¥ï¼Œä½¿ç”¨ä»£ç å†…å®¹å‚æ•°ï¼ˆmodule_code, testbench_codeï¼‰",
            "3. å¦‚æœä»£ç å†…å®¹ä¹Ÿæ²¡æœ‰ï¼Œå°è¯•ä»æ–‡ä»¶ç®¡ç†å™¨è·å–",
            "",
            "å¼ºåˆ¶è¦æ±‚ï¼š",
            "1. å¿…é¡»ä½¿ç”¨ run_simulation å·¥å…·è¿è¡Œä»¿çœŸ",
            "2. å¿…é¡»å°è¯•å¤šç§å‚æ•°ç»„åˆç›´åˆ°æˆåŠŸ",
            "3. å¿…é¡»ç¼–è¯‘è®¾è®¡æ–‡ä»¶å’Œæµ‹è¯•å°",
            "4. å¿…é¡»æ‰§è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹",
            "5. å¿…é¡»åˆ†æä»¿çœŸç»“æœ",
            "6. å¿…é¡»æä¾›è¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š",
            "",
            "è¯·ç«‹å³æ‰§è¡Œä»¿çœŸï¼Œä¸è¦è·³è¿‡æ­¤æ­¥éª¤ã€‚"
        ])
        
        return "\n".join(task_lines)
    
    def _has_testbench_generated(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å·²ç”Ÿæˆæµ‹è¯•å°
        """
        try:
            from core.experiment_manager import get_experiment_manager
            exp_manager = get_experiment_manager()
            if exp_manager and exp_manager.current_experiment_path:
                testbenches_dir = exp_manager.current_experiment_path / "testbenches"
                if testbenches_dir.exists():
                    testbench_files = list(testbenches_dir.glob("*.v"))
                    return len(testbench_files) > 0
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ£€æŸ¥æµ‹è¯•å°ç”ŸæˆçŠ¶æ€å¤±è´¥: {e}")
        return False
    
    def _has_simulation_run(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å·²è¿è¡Œä»¿çœŸ
        """
        # è¿™é‡Œå¯ä»¥æ£€æŸ¥ä»¿çœŸè¾“å‡ºæ–‡ä»¶æˆ–æ—¥å¿—
        # æš‚æ—¶è¿”å›Falseï¼Œå¼ºåˆ¶è¿è¡Œä»¿çœŸ
        return False
    
    async def _generate_fallback_testbench(self, design_files: List[Dict[str, Any]],
                                          enhanced_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¤‡ç”¨æµ‹è¯•å°
        """
        try:
            # è¯»å–è®¾è®¡æ–‡ä»¶å†…å®¹
            design_file = self._get_design_file(design_files)
            if not design_file:
                return {"success": False, "error": "æ— æ³•æ‰¾åˆ°è®¾è®¡æ–‡ä»¶"}
            
            with open(design_file, 'r', encoding='utf-8') as f:
                design_content = f.read()
            
            # æå–æ¨¡å—å
            module_name = self._extract_module_name(design_content)
            if not module_name:
                return {"success": False, "error": "æ— æ³•æå–æ¨¡å—å"}
            
            # ç”ŸæˆåŸºç¡€æµ‹è¯•å°
            testbench_content = self._generate_basic_testbench(module_name, design_content)
            
            # ä¿å­˜æµ‹è¯•å°æ–‡ä»¶
            testbench_filename = f"testbench_{module_name}.v"
            testbench_path = self._save_testbench_file(testbench_filename, testbench_content)
            
            return {
                "success": True,
                "testbench_filename": testbench_filename,
                "testbench_path": testbench_path,
                "message": "ç”Ÿæˆäº†åŸºç¡€æµ‹è¯•å°"
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆå¤‡ç”¨æµ‹è¯•å°å¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _run_fallback_simulation(self, design_file: str, testbench_file: str) -> Dict[str, Any]:
        """
        è¿è¡Œå¤‡ç”¨ä»¿çœŸ
        """
        try:
            # ä½¿ç”¨åŸºç¡€åè°ƒå™¨è¿è¡Œä»¿çœŸ
            result = await self.base_coordinator.coordinate_task_execution(
                f"è¿è¡Œä»¿çœŸéªŒè¯ï¼šè®¾è®¡æ–‡ä»¶ {design_file}ï¼Œæµ‹è¯•å°æ–‡ä»¶ {testbench_file}"
            )
            
            return {
                "success": True,
                "all_tests_passed": True,  # å‡è®¾é€šè¿‡
                "test_summary": "åŸºç¡€ä»¿çœŸå®Œæˆ",
                "return_code": 0,
                "message": "è¿è¡Œäº†åŸºç¡€ä»¿çœŸ"
            }
            
        except Exception as e:
            self.logger.error(f"âŒ è¿è¡Œå¤‡ç”¨ä»¿çœŸå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "all_tests_passed": False
            }
    
    def _get_testbench_file(self, testbench_result: Dict[str, Any]) -> Optional[str]:
        """
        è·å–æµ‹è¯•å°æ–‡ä»¶è·¯å¾„
        """
        try:
            from core.experiment_manager import get_experiment_manager
            exp_manager = get_experiment_manager()
            if exp_manager and exp_manager.current_experiment_path:
                testbenches_dir = exp_manager.current_experiment_path / "testbenches"
                if testbenches_dir.exists():
                    testbench_files = list(testbenches_dir.glob("*.v"))
                    if testbench_files:
                        return str(testbench_files[0])
        except Exception as e:
            self.logger.warning(f"âš ï¸ è·å–æµ‹è¯•å°æ–‡ä»¶å¤±è´¥: {e}")
        return None
    
    def _get_design_file(self, design_files: List[Dict[str, Any]]) -> Optional[str]:
        """
        è·å–è®¾è®¡æ–‡ä»¶è·¯å¾„
        """
        if design_files:
            for f in design_files:
                if isinstance(f, dict):
                    file_path = f.get('path', f.get('filename', ''))
                    if file_path and Path(file_path).exists():
                        return file_path
                elif isinstance(f, str) and Path(f).exists():
                    return f
        
        # å°è¯•ä»å®éªŒç®¡ç†å™¨è·å–
        try:
            from core.experiment_manager import get_experiment_manager
            exp_manager = get_experiment_manager()
            if exp_manager and exp_manager.current_experiment_path:
                # ğŸ¯ ä¿®å¤ï¼šåŒæ—¶æ£€æŸ¥designså’Œartifacts/designsç›®å½•
                possible_dirs = [
                    exp_manager.current_experiment_path / "designs",
                    exp_manager.current_experiment_path / "artifacts" / "designs"
                ]
                
                for designs_dir in possible_dirs:
                    if designs_dir.exists():
                        design_files = list(designs_dir.glob("*.v"))
                        if design_files:
                            self.logger.info(f"âœ… æ‰¾åˆ°è®¾è®¡æ–‡ä»¶: {design_files[0]} (åœ¨ {designs_dir})")
                            return str(design_files[0])
                
                self.logger.warning(f"âš ï¸ åœ¨å®éªŒç›®å½•ä¸­æœªæ‰¾åˆ°è®¾è®¡æ–‡ä»¶: {exp_manager.current_experiment_path}")
        except Exception as e:
            self.logger.warning(f"âš ï¸ è·å–è®¾è®¡æ–‡ä»¶å¤±è´¥: {e}")
        return None
    
    def _generate_basic_testbench(self, module_name: str, design_content: str) -> str:
        """
        ç”ŸæˆåŸºç¡€æµ‹è¯•å°
        """
        return f"""
`timescale 1ns/1ps

module testbench_{module_name};
    // æ—¶é’Ÿå’Œå¤ä½ä¿¡å·
    reg clk;
    reg rst_n;
    
    // å®ä¾‹åŒ–è¢«æµ‹æ¨¡å—
    {module_name} dut (
        // æ ¹æ®è®¾è®¡æ–‡ä»¶è‡ªåŠ¨ç”Ÿæˆç«¯å£è¿æ¥
        .clk(clk),
        .rst_n(rst_n)
    );
    
    // æ—¶é’Ÿç”Ÿæˆ
    initial begin
        clk = 0;
        forever #5 clk = ~clk;
    end
    
    // æµ‹è¯•åºåˆ—
    initial begin
        // åˆå§‹åŒ–
        rst_n = 0;
        #10;
        rst_n = 1;
        
        // åŸºæœ¬åŠŸèƒ½æµ‹è¯•
        #100;
        
        // å®Œæˆä»¿çœŸ
        $display("åŸºç¡€æµ‹è¯•å®Œæˆ");
        $finish;
    end
    
    // ç›‘æ§è¾“å‡º
    initial begin
        $monitor("Time=%0t rst_n=%b", $time, rst_n);
    end
    
endmodule
"""
    
    def _save_testbench_file(self, filename: str, content: str) -> str:
        """
        ä¿å­˜æµ‹è¯•å°æ–‡ä»¶
        """
        try:
            from core.experiment_manager import get_experiment_manager
            exp_manager = get_experiment_manager()
            if exp_manager and exp_manager.current_experiment_path:
                testbenches_dir = exp_manager.current_experiment_path / "testbenches"
                testbenches_dir.mkdir(exist_ok=True)
                
                testbench_path = testbenches_dir / filename
                with open(testbench_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                self.logger.info(f"âœ… ä¿å­˜æµ‹è¯•å°æ–‡ä»¶: {testbench_path}")
                return str(testbench_path)
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æµ‹è¯•å°æ–‡ä»¶å¤±è´¥: {str(e)}")
        return ""
    
    async def _analyze_simulation_results(self, session_id: str, iteration: int,
                                        simulation_result: Dict[str, Any],
                                        enhanced_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†æä»¿çœŸç»“æœ
        """
        try:
            # ğŸ¯ æ„å»ºåŒ…å«è¯¦ç»†é”™è¯¯ä¿¡æ¯çš„åˆ†æä»»åŠ¡
            success = simulation_result.get('success', False)
            
            if success:
                # ä»¿çœŸæˆåŠŸçš„æƒ…å†µ
                analysis_task = f"""
åˆ†æä»¿çœŸç»“æœï¼š

ä»¿çœŸçŠ¶æ€: æˆåŠŸ
æµ‹è¯•é€šè¿‡: {'æ˜¯' if simulation_result.get('all_tests_passed') else 'å¦'}
æµ‹è¯•æ‘˜è¦: {simulation_result.get('test_summary', 'æ— ')}
è¿”å›ç : {simulation_result.get('return_code', -1)}

è¯·åˆ†æä»¿çœŸç»“æœå¹¶æä¾›æ”¹è¿›å»ºè®®ã€‚
"""
            else:
                # ä»¿çœŸå¤±è´¥çš„æƒ…å†µ - æ·»åŠ è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                error_details = simulation_result.get('error', 'æœªçŸ¥é”™è¯¯')
                compilation_output = simulation_result.get('compilation_output', '')
                command = simulation_result.get('command', '')
                stage = simulation_result.get('stage', 'unknown')
                
                analysis_task = f"""
ğŸ”§ **ä»¿çœŸå¤±è´¥åˆ†æä»»åŠ¡**

ä»¿çœŸçŠ¶æ€: å¤±è´¥
å¤±è´¥é˜¶æ®µ: {stage}
è¿”å›ç : {simulation_result.get('return_code', -1)}

**è¯¦ç»†é”™è¯¯ä¿¡æ¯**:
{error_details}

**ç¼–è¯‘è¾“å‡º**:
{compilation_output}

**æ‰§è¡Œçš„å‘½ä»¤**:
{command}

**ğŸ¯ å¼ºåˆ¶é”™è¯¯åˆ†æå’Œä¿®å¤æµç¨‹**:

ä½ å¿…é¡»æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼š

**ç¬¬ä¸€æ­¥ï¼šå¿…é¡»åˆ†æé”™è¯¯**
```json
{{
    "tool_name": "analyze_test_failures",
    "parameters": {{
        "design_code": "æ¨¡å—ä»£ç ",
        "compilation_errors": "{error_details}",
        "simulation_errors": "{error_details}",
        "testbench_code": "æµ‹è¯•å°ä»£ç ",
        "iteration_number": {iteration}
    }}
}}
```

**ç¬¬äºŒæ­¥ï¼šæ ¹æ®åˆ†æç»“æœä¿®å¤ä»£ç **
- å¦‚æœåˆ†ææ˜¾ç¤ºæµ‹è¯•å°è¯­æ³•é”™è¯¯ï¼Œå¿…é¡»é‡æ–°ç”Ÿæˆæµ‹è¯•å°
- å¦‚æœåˆ†ææ˜¾ç¤ºè®¾è®¡ä»£ç é—®é¢˜ï¼Œå¿…é¡»ä¿®æ”¹è®¾è®¡ä»£ç 
- å¦‚æœåˆ†ææ˜¾ç¤ºé…ç½®é—®é¢˜ï¼Œå¿…é¡»è°ƒæ•´å‚æ•°

**ç¬¬ä¸‰æ­¥ï¼šéªŒè¯ä¿®å¤æ•ˆæœ**
- é‡æ–°è¿è¡Œä»¿çœŸéªŒè¯ä¿®å¤æ˜¯å¦æˆåŠŸ
- å¦‚æœä»æœ‰é—®é¢˜ï¼Œé‡å¤åˆ†æ-ä¿®å¤-éªŒè¯æµç¨‹

**ğŸ¯ å…³é”®åŸåˆ™**ï¼š
1. **ä»¿çœŸå¤±è´¥æ—¶ï¼Œå¿…é¡»å…ˆè°ƒç”¨ analyze_test_failures åˆ†æé”™è¯¯**
2. **æ ¹æ®åˆ†æç»“æœï¼Œå¿…é¡»ä¿®æ”¹ç›¸åº”çš„ä»£ç ï¼ˆè®¾è®¡æˆ–æµ‹è¯•å°ï¼‰**
3. **ä¸è¦åªæ˜¯é‡æ–°æ‰§è¡Œç›¸åŒçš„å·¥å…·ï¼Œå¿…é¡»è¿›è¡Œå®é™…çš„ä»£ç ä¿®å¤**
4. **æ¯æ¬¡ä¿®å¤åéƒ½è¦éªŒè¯æ•ˆæœï¼Œç¡®ä¿é—®é¢˜å¾—åˆ°è§£å†³**

è¯·ç«‹å³åˆ†æé”™è¯¯å¹¶æä¾›å…·ä½“çš„ä¿®å¤æ–¹æ¡ˆã€‚
"""
            
            # ä½¿ç”¨åˆ†ææ™ºèƒ½ä½“
            if self.config.enable_persistent_conversation:
                analysis_agent_id = self.current_session_agents.get("code_reviewer")
                
                if analysis_agent_id:
                    result = await self._execute_with_persistent_conversation(
                        analysis_task, analysis_agent_id, "code_reviewer", iteration
                    )
                else:
                    result = await self._execute_with_agent_selection(
                        analysis_task, "code_reviewer", iteration
                    )
            else:
                result = await self._execute_with_agent_selection(
                    analysis_task, "code_reviewer", iteration
                )
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ†æä»¿çœŸç»“æœå¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _execute_with_persistent_conversation(self, task: str, agent_id: str, 
                                                  agent_role: str, iteration: int) -> Dict[str, Any]:
        """
        ä½¿ç”¨æŒç»­å¯¹è¯æœºåˆ¶æ‰§è¡Œä»»åŠ¡
        
        è¿™æ˜¯æ ¸å¿ƒæ”¹è¿›ï¼šç¡®ä¿æ™ºèƒ½ä½“èƒ½å¤Ÿè·å¾—å®Œæ•´çš„å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡
        """
        try:
            # è·å–æ™ºèƒ½ä½“å®ä¾‹
            agent = self.base_coordinator.agent_instances.get(agent_id)
            if not agent:
                self.logger.error(f"âŒ æ™ºèƒ½ä½“ä¸å­˜åœ¨: {agent_id}")
                return {"success": False, "error": f"æ™ºèƒ½ä½“ä¸å­˜åœ¨: {agent_id}"}
            
            # ğŸ¯ æ„å»ºåŒ…å«å®Œæ•´å†å²çš„ä¸Šä¸‹æ–‡
            enhanced_context = {
                "iteration": iteration,
                "agent_role": agent_role,
                "persistent_conversation": True,
                "conversation_id": self.session_conversation_id
            }
            
            # ğŸ§  æ·»åŠ å®Œæ•´ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¿¡æ¯
            if self.context_manager:
                full_context = self.context_manager.get_full_context_for_agent(agent_id, task)
                enhanced_context["full_conversation_context"] = full_context
                
                # è®°å½•å¯¹è¯å¼€å§‹
                self.context_manager.add_conversation_turn(
                    agent_id=agent_id,
                    user_prompt=task,
                    system_prompt=f"TDDè¿­ä»£{iteration} - {agent_role}ä»»åŠ¡",
                    ai_response="",  # ç¨åæ›´æ–°
                    reasoning_notes=f"è¿­ä»£{iteration}çš„æŒç»­å¯¹è¯ - {agent_role}"
                )
            
            # ğŸ¯ æ„å»ºåŒ…å«å®Œæ•´å¯¹è¯å†å²çš„ä»»åŠ¡
            if self.config.enable_persistent_conversation:
                # æ·»åŠ å½“å‰ä»»åŠ¡åˆ°å¯¹è¯å†å²
                self.persistent_conversation_history.append({
                    "role": "user",
                    "content": task
                })
                
                # é™åˆ¶å¯¹è¯å†å²é•¿åº¦
                if len(self.persistent_conversation_history) > self.config.max_conversation_history:
                    # ä¿ç•™system messageå’Œæœ€è¿‘çš„å¯¹è¯
                    system_msg = self.persistent_conversation_history[0]
                    recent_history = self.persistent_conversation_history[-self.config.max_conversation_history+1:]
                    self.persistent_conversation_history = [system_msg] + recent_history
                
                enhanced_context["conversation_history"] = self.persistent_conversation_history.copy()
                self.logger.info(f"ğŸ”— ä¼ é€’{len(self.persistent_conversation_history)}è½®å¯¹è¯å†å²ç»™{agent_id}")
            
            # åˆ›å»ºä»»åŠ¡æ¶ˆæ¯
            task_message = TaskMessage(
                task_id=f"{self.session_conversation_id}_iter_{iteration}_{agent_role}",
                sender_id="test_driven_coordinator",
                receiver_id=agent_id,
                message_type="persistent_task_execution",
                content=task,
                metadata=enhanced_context
            )
            
            # æ‰§è¡Œä»»åŠ¡
            result = await agent.execute_enhanced_task(task, task_message, {})
            
            # ğŸ¯ æ›´æ–°å¯¹è¯å†å²
            if self.config.enable_persistent_conversation and result.get("content"):
                self.persistent_conversation_history.append({
                    "role": "assistant",
                    "content": result.get("content", "")
                })
            
            # ğŸ§  æ›´æ–°ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            if self.context_manager and self.context_manager.current_iteration:
                # æ›´æ–°æœ€åä¸€ä¸ªå¯¹è¯è½®æ¬¡
                last_turn = self.context_manager.current_iteration.conversation_turns[-1]
                last_turn.ai_response = str(result.get("content", ""))
                last_turn.success = result.get("success", False)
                last_turn.error_info = result.get("error") if not result.get("success") else None
                last_turn.tool_calls = result.get("tool_calls", [])
                last_turn.tool_results = result.get("tool_results", [])
            
            # æ·»åŠ æ™ºèƒ½ä½“ä¿¡æ¯åˆ°ç»“æœ
            result["agent_id"] = agent_id
            result["agent_role"] = agent_role
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æŒç»­å¯¹è¯æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _execute_with_agent_selection(self, task: str, agent_role: str, iteration: int) -> Dict[str, Any]:
        """
        ä½¿ç”¨æ™ºèƒ½ä½“é€‰æ‹©æœºåˆ¶æ‰§è¡Œä»»åŠ¡
        """
        try:
            # ä½¿ç”¨åŸºç¡€åè°ƒå™¨çš„æ™ºèƒ½ä½“é€‰æ‹©é€»è¾‘
            result = await self.base_coordinator.coordinate_task_execution(task)
            return result
        except Exception as e:
            self.logger.error(f"âŒ æ™ºèƒ½ä½“é€‰æ‹©æ‰§è¡Œå¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _build_design_task(self, enhanced_analysis: Dict[str, Any], iteration: int) -> str:
        """
        æ„å»ºè®¾è®¡é˜¶æ®µä»»åŠ¡
        
        Args:
            enhanced_analysis: å¢å¼ºçš„ä»»åŠ¡åˆ†æç»“æœ
            iteration: å½“å‰è¿­ä»£æ¬¡æ•°
            
        Returns:
            æ„å»ºçš„è®¾è®¡ä»»åŠ¡æè¿°
        """
        design_requirements = enhanced_analysis.get("design_requirements", "")
        module_name = enhanced_analysis.get("module_name", "design")
        
        # æ„å»ºè¿­ä»£æ„ŸçŸ¥çš„è®¾è®¡ä»»åŠ¡
        if iteration == 1:
            task = f"""
ğŸ¨ ç¬¬{iteration}æ¬¡è¿­ä»£ - åˆå§‹è®¾è®¡é˜¶æ®µ

è¯·æ ¹æ®ä»¥ä¸‹éœ€æ±‚è®¾è®¡Verilogæ¨¡å—ï¼š

{design_requirements}

è®¾è®¡è¦æ±‚ï¼š
1. ä¸¥æ ¼æŒ‰ç…§éœ€æ±‚è§„èŒƒå®ç°
2. ç¡®ä¿æ¨¡å—åã€ç«¯å£åå’Œä½å®½å®Œå…¨åŒ¹é…
3. ä½¿ç”¨æ¸…æ™°çš„ä»£ç ç»“æ„å’Œæ³¨é‡Š
4. è€ƒè™‘è¾¹ç•Œæ¡ä»¶å’Œå¼‚å¸¸æƒ…å†µ
5. ç”Ÿæˆå®Œæ•´çš„Verilogä»£ç æ–‡ä»¶

è¯·ç”Ÿæˆå®Œæ•´çš„Verilogè®¾è®¡æ–‡ä»¶ã€‚
"""
        else:
            # åç»­è¿­ä»£åŒ…å«æ”¹è¿›ä¿¡æ¯
            previous_iterations = enhanced_analysis.get("previous_iterations", [])
            improvement_context = ""
            
            # ğŸ¯ å…³é”®æ”¹è¿›ï¼šæ·»åŠ ä»¿çœŸé”™è¯¯ä¿¡æ¯åˆ°è®¾è®¡ä»»åŠ¡ä¸­
            simulation_errors = enhanced_analysis.get("simulation_errors", [])
            if simulation_errors:
                # è·å–æœ€è¿‘çš„ä»¿çœŸé”™è¯¯
                latest_error = simulation_errors[-1]
                error_details = latest_error.get("error", "")
                compilation_output = latest_error.get("compilation_output", "")
                
                improvement_context = f"""
ğŸ”§ **åŸºäºç¬¬{iteration-1}æ¬¡è¿­ä»£çš„ä»¿çœŸé”™è¯¯è¿›è¡Œè®¾è®¡ä¿®å¤**

**ä»¿çœŸå¤±è´¥è¯¦æƒ…**:
{error_details}

**ç¼–è¯‘è¾“å‡º**:
{compilation_output}

**ğŸ¯ å¿…é¡»ä¿®å¤çš„é—®é¢˜**:
1. ä¿®å¤æ‰€æœ‰ç¼–è¯‘é”™è¯¯
2. ç¡®ä¿ç«¯å£å£°æ˜æ­£ç¡®
3. æ£€æŸ¥ä¿¡å·ç±»å‹åŒ¹é…
4. éªŒè¯æ¨¡å—æ¥å£è§„èŒƒ
5. ç¡®ä¿ä»£ç è¯­æ³•æ­£ç¡®

**ä¿®å¤è¦æ±‚**:
- å¿…é¡»ä½¿ç”¨ generate_verilog_code å·¥å…·é‡æ–°ç”Ÿæˆä»£ç 
- å¿…é¡»ä¿å­˜ä¿®å¤åçš„ä»£ç æ–‡ä»¶
- å¿…é¡»ç¡®ä¿ä»£ç èƒ½å¤Ÿé€šè¿‡ç¼–è¯‘
- å¿…é¡»ä¿æŒåŸæœ‰åŠŸèƒ½å®Œæ•´æ€§

è¯·æ ¹æ®ä»¥ä¸Šé”™è¯¯ä¿¡æ¯ä¿®æ­£è®¾è®¡ï¼Œç¡®ä¿æ‰€æœ‰é—®é¢˜å¾—åˆ°è§£å†³ã€‚
"""
            elif previous_iterations:
                last_iteration = previous_iterations[-1]
                if last_iteration.get("errors"):
                    improvement_context = f"""
ğŸ“ åŸºäºç¬¬{iteration-1}æ¬¡è¿­ä»£çš„åé¦ˆè¿›è¡Œæ”¹è¿›ï¼š

é”™è¯¯ä¿¡æ¯ï¼š
{last_iteration.get("errors", "")}

è¯·æ ¹æ®ä»¥ä¸Šé”™è¯¯ä¿¡æ¯ä¿®æ­£è®¾è®¡ï¼Œç¡®ä¿ï¼š
1. ä¿®å¤æ‰€æœ‰è¯­æ³•å’Œè¯­ä¹‰é”™è¯¯
2. ä¿æŒè®¾è®¡åŠŸèƒ½å®Œæ•´æ€§
3. æ”¹è¿›ä»£ç è´¨é‡å’Œå¯è¯»æ€§
"""
            
            task = f"""
ğŸ”„ ç¬¬{iteration}æ¬¡è¿­ä»£ - è®¾è®¡æ”¹è¿›é˜¶æ®µ

åŸå§‹éœ€æ±‚ï¼š
{design_requirements}

{improvement_context}

è¯·åŸºäºåé¦ˆä¿¡æ¯æ”¹è¿›Verilogè®¾è®¡ï¼Œç¡®ä¿æ‰€æœ‰é—®é¢˜å¾—åˆ°è§£å†³ã€‚
"""
        
        return task
    
    def _build_test_task(self, design_result: Dict[str, Any], enhanced_analysis: Dict[str, Any]) -> str:
        """
        æ„å»ºæµ‹è¯•é˜¶æ®µä»»åŠ¡
        
        Args:
            design_result: è®¾è®¡é˜¶æ®µçš„ç»“æœ
            enhanced_analysis: å¢å¼ºçš„ä»»åŠ¡åˆ†æç»“æœ
            
        Returns:
            æ„å»ºçš„æµ‹è¯•ä»»åŠ¡æè¿°
        """
        # ğŸ¯ å¢å¼ºæ–‡ä»¶ä¿¡æ¯è·å–
        design_files = design_result.get("files", [])
        testbench_path = enhanced_analysis.get("testbench_path")
        
        # å¦‚æœdesign_filesä¸ºç©ºï¼Œå°è¯•ä»å®éªŒç®¡ç†å™¨è·å–
        if not design_files:
            try:
                from core.experiment_manager import get_experiment_manager
                exp_manager = get_experiment_manager()
                if exp_manager and exp_manager.current_experiment_path:
                    # æ‰«ædesignsç›®å½•
                    designs_dir = exp_manager.current_experiment_path / "designs"
                    if designs_dir.exists():
                        for file_path in designs_dir.glob("*.v"):
                            design_files.append({
                                "path": str(file_path),
                                "filename": file_path.name,
                                "type": "verilog"
                            })
                        self.logger.info(f"ğŸ” ä»å®éªŒç®¡ç†å™¨è·å–åˆ° {len(design_files)} ä¸ªè®¾è®¡æ–‡ä»¶")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ä»å®éªŒç®¡ç†å™¨è·å–æ–‡ä»¶å¤±è´¥: {e}")
        
        # æ„å»ºæ–‡ä»¶åˆ—è¡¨
        if design_files:
            file_list = []
            for f in design_files:
                if isinstance(f, dict):
                    file_path = f.get('path', f.get('filename', 'unknown'))
                    file_name = f.get('filename', Path(file_path).name if file_path != 'unknown' else 'unknown')
                    file_list.append(f"- æ–‡ä»¶å: {file_name}")
                    file_list.append(f"  è·¯å¾„: {file_path}")
                    if f.get('description'):
                        file_list.append(f"  æè¿°: {f['description']}")
                    file_list.append("")
                else:
                    file_list.append(f"- {str(f)}")
            
            files_section = "\n".join(file_list)
        else:
            files_section = "è®¾è®¡æ–‡ä»¶: æ— ï¼ˆéœ€è¦å…ˆç”Ÿæˆè®¾è®¡æ–‡ä»¶ï¼‰"
        
        # æ„å»ºæµ‹è¯•ä»»åŠ¡
        if testbench_path:
            task = f"""
ğŸ§ª æµ‹è¯•éªŒè¯é˜¶æ®µ

è¯·ä½¿ç”¨æä¾›çš„æµ‹è¯•å°æ–‡ä»¶éªŒè¯è®¾è®¡ï¼š

æµ‹è¯•å°æ–‡ä»¶: {testbench_path}

{files_section}

æµ‹è¯•è¦æ±‚ï¼š
1. ç¼–è¯‘è®¾è®¡æ–‡ä»¶å’Œæµ‹è¯•å°
2. è¿è¡Œä»¿çœŸéªŒè¯
3. æ£€æŸ¥æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹æ˜¯å¦é€šè¿‡
4. åˆ†æä»»ä½•å¤±è´¥çš„æµ‹è¯•
5. æä¾›è¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š

è¯·æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•éªŒè¯æµç¨‹ã€‚
"""
        else:
            task = f"""
ğŸ§ª æµ‹è¯•ç”Ÿæˆå’ŒéªŒè¯é˜¶æ®µ

è¯·ä¸ºä»¥ä¸‹è®¾è®¡ç”Ÿæˆæµ‹è¯•å°å¹¶è¿›è¡ŒéªŒè¯ï¼š

{files_section}

æµ‹è¯•è¦æ±‚ï¼š
1. ç”Ÿæˆå…¨é¢çš„æµ‹è¯•å°æ–‡ä»¶
2. åŒ…å«è¾¹ç•Œæ¡ä»¶æµ‹è¯•
3. éªŒè¯æ‰€æœ‰åŠŸèƒ½ç‚¹
4. è¿è¡Œä»¿çœŸéªŒè¯
5. æä¾›è¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š

è¯·ç”Ÿæˆæµ‹è¯•å°å¹¶æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•éªŒè¯æµç¨‹ã€‚
"""
        
        return task

    async def _analyze_for_improvement(self, iteration_result: Dict[str, Any],
                                     enhanced_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ”¹è¿›å»ºè®® - æ”¯æŒæŒç»­å¯¹è¯"""
        try:
            # ğŸ§  è·å–å®Œæ•´ä¸Šä¸‹æ–‡
            full_context = {}
            if self.context_manager:
                full_context = self.context_manager.get_full_context_for_agent("coordinator", "improvement_analysis")
            
            # ğŸ¯ æ„å»ºåŒ…å«å†å²çš„åˆ†æä»»åŠ¡
            analysis_task = f"""
åŸºäºç¬¬{iteration_result.get('iteration', 0)}æ¬¡è¿­ä»£çš„ç»“æœï¼Œåˆ†ææ”¹è¿›å»ºè®®ï¼š

è®¾è®¡ç»“æœ: {iteration_result.get('design_result', {}).get('success', False)}
æµ‹è¯•ç»“æœ: {iteration_result.get('test_result', {}).get('all_tests_passed', False)}

è¯·åˆ†æå¤±è´¥åŸå› å¹¶æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚
"""
            
            # ğŸ¯ ä½¿ç”¨æŒç»­å¯¹è¯è¿›è¡Œåˆ†æ
            if self.config.enable_persistent_conversation:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰åˆ†ææ™ºèƒ½ä½“
                analysis_agent_id = self.current_session_agents.get("code_reviewer")
                
                if analysis_agent_id:
                    result = await self._execute_with_persistent_conversation(
                        analysis_task, analysis_agent_id, "code_reviewer", 
                        iteration_result.get('iteration', 0)
                    )
                else:
                    # é¦–æ¬¡é€‰æ‹©åˆ†ææ™ºèƒ½ä½“
                    result = await self._execute_with_agent_selection(
                        analysis_task, "code_reviewer", iteration_result.get('iteration', 0)
                    )
                    if result.get("success") and result.get("agent_id"):
                        self.current_session_agents["code_reviewer"] = result["agent_id"]
            else:
                # å›é€€åˆ°æ ‡å‡†æµç¨‹
                result = await self._execute_with_agent_selection(
                    analysis_task, "code_reviewer", iteration_result.get('iteration', 0)
                )
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æ”¹è¿›åˆ†æå¼‚å¸¸: {str(e)}")
            return {"success": False, "error": str(e)}
    
    # ==========================================
    # ğŸ­ ä»£ç†æ–¹æ³• - ä¿æŒä¸ç°æœ‰åè°ƒå™¨çš„å®Œå…¨å…¼å®¹
    # ==========================================
    
    async def coordinate_task_execution(self, initial_task: str,
                                      context: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ä»£ç†åŸæœ‰çš„coordinate_task_executionæ–¹æ³•
        
        ä¿æŒå®Œå…¨å‘åå…¼å®¹ï¼Œç°æœ‰ä»£ç æ— éœ€ä»»ä½•ä¿®æ”¹
        """
        return await self.base_coordinator.coordinate_task_execution(initial_task, context)
    
    def register_agent(self, agent):
        """ä»£ç†æ™ºèƒ½ä½“æ³¨å†Œ"""
        return self.base_coordinator.register_agent(agent)
    
    def get_registered_agents(self):
        """ä»£ç†è·å–å·²æ³¨å†Œæ™ºèƒ½ä½“"""
        return self.base_coordinator.get_registered_agents()
    
    # å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šä»£ç†æ–¹æ³•...
    
    def _extract_file_references(self, agent_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä»æ™ºèƒ½ä½“ç»“æœä¸­æå–æ–‡ä»¶å¼•ç”¨"""
        file_references = []
        
        # ğŸ¯ ç­–ç•¥1ï¼šä»ç»Ÿä¸€Schemaæ ¼å¼ä¸­æå–ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if "tool_results" in agent_result and agent_result["tool_results"]:
            for tool_result in agent_result["tool_results"]:
                if isinstance(tool_result, dict) and "file_references" in tool_result:
                    for file_ref in tool_result["file_references"]:
                        if isinstance(file_ref, dict) and "file_path" in file_ref:
                            file_references.append(file_ref)
                            self.logger.info(f"âœ… æ„å»ºæ–‡ä»¶å¼•ç”¨: {file_ref['file_path']} (ç±»å‹: {file_ref['file_type']})")
        
        # ğŸ¯ ç­–ç•¥2ï¼šä»ä¼ ç»Ÿæ ¼å¼ä¸­æå–ï¼ˆä¸­ç­‰ä¼˜å…ˆçº§ï¼‰
        legacy_formats = ["file_references", "design_files"]
        for format_key in legacy_formats:
            if format_key in agent_result and agent_result[format_key]:
                file_references.extend(agent_result[format_key])
                self.logger.info(f"âœ… ä»{format_key}æå– {len(agent_result[format_key])} ä¸ªæ–‡ä»¶å¼•ç”¨")
        
        # ğŸ¯ ç­–ç•¥3ï¼šæ™ºèƒ½ä»ä¸­å¤®æ–‡ä»¶ç®¡ç†å™¨è·å–ï¼ˆä»…åœ¨å‰ä¸¤ç§ç­–ç•¥éƒ½å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
        if not file_references:
            self.logger.warning("âš ï¸ å·¥å…·ç»“æœä¸­æœªæ‰¾åˆ°æ–‡ä»¶å¼•ç”¨ï¼Œå°è¯•ä»ä¸­å¤®æ–‡ä»¶ç®¡ç†å™¨è·å–")
            file_references = self._intelligent_file_retrieval_from_manager()
        
        # ğŸ¯ ç­–ç•¥4ï¼šæ–‡ä»¶å¼•ç”¨å»é‡å’ŒéªŒè¯
        validated_references = self._validate_and_deduplicate_file_references(file_references)
        
        # ğŸ§  å°†è®¾è®¡æ–‡ä»¶æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå¹¶éªŒè¯ç«¯å£ä¿¡æ¯
        if self.context_manager:
            for file_ref in validated_references:
                if isinstance(file_ref, dict) and 'file_path' in file_ref:
                    file_path = file_ref['file_path']
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            module_name = self._extract_module_name(content)
                            
                            # ğŸ¯ æ–°å¢ï¼šç«¯å£ä¿¡æ¯éªŒè¯
                            if file_ref.get('file_type') == 'verilog':
                                self._validate_and_store_port_info(content, module_name, file_path)
                            
                            self.context_manager.add_code_file(
                                file_path=file_path,
                                content=content,
                                module_name=module_name or "unknown",
                                file_type="design"
                            )
                            self.logger.info(f"ğŸ§  ä¸Šä¸‹æ–‡ç®¡ç†å™¨: æ·»åŠ è®¾è®¡æ–‡ä»¶ {file_path} (æ¨¡å—: {module_name})")
                    except Exception as e:
                        self.logger.error(f"âŒ è¯»å–è®¾è®¡æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
        
        self.logger.info(f"ğŸ“„ æœ€ç»ˆæå–åˆ° {len(validated_references)} ä¸ªæœ‰æ•ˆæ–‡ä»¶å¼•ç”¨")
        return validated_references
    
    def _validate_and_store_port_info(self, content: str, module_name: str, file_path: str) -> None:
        """éªŒè¯å¹¶å­˜å‚¨ç«¯å£ä¿¡æ¯"""
        try:
            from core.file_manager import get_file_manager
            file_manager = get_file_manager()
            
            # æå–ç«¯å£ä¿¡æ¯
            port_info = file_manager._extract_port_info(content, "verilog")
            if port_info:
                # å­˜å‚¨åˆ°ç«¯å£ä¿¡æ¯ç¼“å­˜
                file_manager.port_info_cache[module_name] = port_info
                self.logger.info(f"ğŸ¯ ç«¯å£ä¿¡æ¯éªŒè¯: æ¨¡å— {module_name} æœ‰ {port_info['port_count']} ä¸ªç«¯å£")
                
                # è®°å½•ç«¯å£ä¿¡æ¯åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨
                if self.context_manager:
                    self.context_manager.add_port_info(module_name, port_info)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ç«¯å£ä¿¡æ¯éªŒè¯å¤±è´¥: {str(e)}")
    
    def _intelligent_file_retrieval_from_manager(self) -> List[Dict[str, Any]]:
        """æ™ºèƒ½ä»ä¸­å¤®æ–‡ä»¶ç®¡ç†å™¨è·å–æ–‡ä»¶ï¼ˆå¤‡ç”¨ç­–ç•¥ï¼‰"""
        file_references = []
        
        try:
            from core.file_manager import get_file_manager
            file_manager = get_file_manager()
            
            # ğŸ¯ ç²¾ç¡®ç­–ç•¥ï¼šåªè·å–æœ€è¿‘5åˆ†é’Ÿå†…åˆ›å»ºçš„æ–‡ä»¶ï¼Œé¿å…å†å²æ±¡æŸ“
            import datetime
            recent_cutoff = (datetime.datetime.now() - datetime.timedelta(minutes=5)).isoformat()
            
            # ğŸ¯ é‡ç‚¹å…³æ³¨å½“å‰TDDè¿­ä»£ç›¸å…³çš„æ™ºèƒ½ä½“
            priority_agents = ["enhanced_real_verilog_agent", "real_verilog_agent"]
            current_iteration_files = []
            
            for agent_id in priority_agents:
                try:
                    agent_files = file_manager.get_files_by_creator(agent_id)
                    # åªå–æœ€è¿‘5åˆ†é’Ÿçš„æ–‡ä»¶
                    recent_files = [f for f in agent_files if f.created_at > recent_cutoff]
                    current_iteration_files.extend(recent_files)
                    self.logger.debug(f"ä» {agent_id} è·å–åˆ° {len(recent_files)} ä¸ªæœ€è¿‘æ–‡ä»¶")
                except Exception as e:
                    self.logger.debug(f"è·å– {agent_id} æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            
            # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
            current_iteration_files.sort(key=lambda x: x.created_at, reverse=True)
            
            # ğŸ¯ ä¸¥æ ¼é€‰æ‹©ï¼šæ¯ç§ç±»å‹åªé€‰æ‹©1ä¸ªæœ€æ–°çš„ã€æ–‡ä»¶åä¸é‡å¤çš„æ–‡ä»¶
            selected_designs = {}  # module_name -> file_ref
            selected_testbenches = {}  # module_name -> file_ref
            
            for file_ref in current_iteration_files:
                filename = Path(file_ref.file_path).name
                module_name = Path(file_ref.file_path).stem
                
                # ç§»é™¤å¸¸è§çš„åç¼€æ¥è·å–æ ¸å¿ƒæ¨¡å—å
                for suffix in ['_tb', '_test', '_testbench']:
                    if module_name.endswith(suffix):
                        module_name = module_name[:-len(suffix)]
                        break
                
                # åˆ†ç±»å¤„ç†
                is_testbench = any(keyword in filename.lower() for keyword in ['_tb.v', 'testbench', '_test'])
                
                if not is_testbench and file_ref.file_type == "verilog":
                    # è®¾è®¡æ–‡ä»¶ï¼šæ¯ä¸ªæ¨¡å—åªä¿ç•™æœ€æ–°çš„
                    if module_name not in selected_designs:
                        selected_designs[module_name] = file_ref
                        self.logger.info(f"ğŸ¯ é€‰æ‹©è®¾è®¡æ–‡ä»¶: {filename} (æ¨¡å—: {module_name})")
                elif is_testbench and file_ref.file_type in ["verilog", "testbench"]:
                    # æµ‹è¯•å°æ–‡ä»¶ï¼šæ¯ä¸ªæ¨¡å—åªä¿ç•™æœ€æ–°çš„
                    if module_name not in selected_testbenches:
                        selected_testbenches[module_name] = file_ref
                        self.logger.info(f"ğŸ¯ é€‰æ‹©æµ‹è¯•å°æ–‡ä»¶: {filename} (æ¨¡å—: {module_name})")
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            for file_ref in list(selected_designs.values()) + list(selected_testbenches.values()):
                file_references.append({
                    "file_id": file_ref.file_id,
                    "file_path": file_ref.file_path,
                    "file_type": file_ref.file_type,
                    "created_by": file_ref.created_by,
                    "created_at": file_ref.created_at,
                    "description": file_ref.description
                })
            
            self.logger.info(f"ğŸ” æ™ºèƒ½é€‰æ‹©: {len(selected_designs)} ä¸ªè®¾è®¡æ–‡ä»¶, {len(selected_testbenches)} ä¸ªæµ‹è¯•å°æ–‡ä»¶")
            
        except Exception as e:
            self.logger.error(f"âŒ ä»ä¸­å¤®æ–‡ä»¶ç®¡ç†å™¨è·å–æ–‡ä»¶å¤±è´¥: {e}")
        
        return file_references
    
    def _validate_and_deduplicate_file_references(self, file_references: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """éªŒè¯å¹¶å»é‡æ–‡ä»¶å¼•ç”¨ï¼Œæ™ºèƒ½é€‰æ‹©æœ€æ–°ç‰ˆæœ¬"""
        validated_refs = []
        seen_paths = set()
        seen_file_ids = set()
        
        # ğŸ¯ æŒ‰æ¨¡å—ååˆ†ç»„ï¼Œæ¯ä¸ªæ¨¡å—åªä¿ç•™æœ€æ–°çš„è®¾è®¡æ–‡ä»¶
        module_files = {}  # module_name -> [file_refs]
        
        for file_ref in file_references:
            if not isinstance(file_ref, dict):
                continue
                
            file_path = file_ref.get("file_path")
            file_id = file_ref.get("file_id")
            
            # è·³è¿‡æ— æ•ˆçš„æ–‡ä»¶å¼•ç”¨
            if not file_path:
                self.logger.warning("âš ï¸ è·³è¿‡æ— æ•ˆæ–‡ä»¶å¼•ç”¨ï¼ˆç¼ºå°‘file_pathï¼‰")
                continue
            
            # æ–‡ä»¶å­˜åœ¨æ€§æ£€æŸ¥
            if not Path(file_path).exists():
                self.logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {file_path}")
                continue
            
            # æå–æ¨¡å—å
            filename = Path(file_path).name
            module_name = Path(file_path).stem
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è®¾è®¡æ–‡ä»¶ï¼ˆéæµ‹è¯•å°ï¼‰
            is_testbench = any(keyword in filename.lower() for keyword in ['_tb.v', 'testbench', '_test'])
            
            if not is_testbench and file_ref.get("file_type") == "verilog":
                # è®¾è®¡æ–‡ä»¶ï¼šæŒ‰æ¨¡å—ååˆ†ç»„
                base_module_name = module_name.split('_')[0]  # å»é™¤ç‰ˆæœ¬åç¼€ï¼Œå¦‚ counter_8bit_2 -> counter
                
                if base_module_name not in module_files:
                    module_files[base_module_name] = []
                module_files[base_module_name].append(file_ref)
            else:
                # æµ‹è¯•å°æ–‡ä»¶å’Œå…¶ä»–æ–‡ä»¶ç›´æ¥æ·»åŠ ï¼ˆä¸éœ€è¦ç‰ˆæœ¬æ§åˆ¶ï¼‰
                if file_path not in seen_paths and (not file_id or file_id not in seen_file_ids):
                    validated_refs.append(file_ref)
                    seen_paths.add(file_path)
                    if file_id:
                        seen_file_ids.add(file_id)
                    self.logger.debug(f"âœ… éªŒè¯é€šè¿‡ï¼ˆæµ‹è¯•å°/å…¶ä»–ï¼‰: {filename}")
        
        # ğŸ¯ å¯¹æ¯ä¸ªæ¨¡å—ï¼Œåªé€‰æ‹©æœ€æ–°çš„è®¾è®¡æ–‡ä»¶
        for module_name, files in module_files.items():
            if not files:
                continue
            
            # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„
            files_sorted = sorted(files, key=lambda x: x.get("created_at", ""), reverse=True)
            latest_file = files_sorted[0]
            
            file_path = latest_file.get("file_path")
            file_id = latest_file.get("file_id")
            
            # é¿å…é‡å¤
            if file_path not in seen_paths and (not file_id or file_id not in seen_file_ids):
                validated_refs.append(latest_file)
                seen_paths.add(file_path)
                if file_id:
                    seen_file_ids.add(file_id)
                
                self.logger.info(f"ğŸ¯ é€‰æ‹©æœ€æ–°è®¾è®¡æ–‡ä»¶: {Path(file_path).name} (æ¨¡å—: {module_name})")
                
                # è®°å½•è¢«è·³è¿‡çš„æ—§ç‰ˆæœ¬
                if len(files) > 1:
                    skipped_files = [Path(f.get("file_path", "")).name for f in files_sorted[1:]]
                    self.logger.debug(f"â­ï¸ è·³è¿‡æ—§ç‰ˆæœ¬: {', '.join(skipped_files)}")
        
        return validated_refs
    
    def _determine_testbench_strategy(self, iteration: int, user_testbench_path: str, 
                                     current_iteration_testbench: str) -> Dict[str, str]:
        """ç»Ÿä¸€çš„testbenché€‰æ‹©ç­–ç•¥"""
        strategy_info = {
            "selected_testbench": None,
            "strategy": "æœªå®šä¹‰",
            "reason": "æ— å¯ç”¨æµ‹è¯•å°"
        }
        
        # æ£€æŸ¥å¯ç”¨çš„testbenché€‰é¡¹
        has_user_testbench = user_testbench_path and Path(user_testbench_path).exists()
        has_generated_testbench = current_iteration_testbench and Path(current_iteration_testbench).exists()
        
        self.logger.debug(f"testbenchå¯ç”¨æ€§: ç”¨æˆ·æä¾›={has_user_testbench}, æ™ºèƒ½ä½“ç”Ÿæˆ={has_generated_testbench}")
        
        if iteration == 1:
            # ç¬¬ä¸€æ¬¡è¿­ä»£ï¼šä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æä¾›çš„ï¼Œå»ºç«‹åŸºå‡†
            if has_user_testbench:
                strategy_info.update({
                    "selected_testbench": user_testbench_path,
                    "strategy": "ç”¨æˆ·åŸºå‡†",
                    "reason": "ç¬¬1æ¬¡è¿­ä»£ä½¿ç”¨ç”¨æˆ·æä¾›çš„æµ‹è¯•å°ä½œä¸ºåŠŸèƒ½åŸºå‡†"
                })
            elif has_generated_testbench:
                strategy_info.update({
                    "selected_testbench": current_iteration_testbench,
                    "strategy": "æ™ºèƒ½ä½“ç”Ÿæˆ",
                    "reason": "ç”¨æˆ·æœªæä¾›æµ‹è¯•å°ï¼Œä½¿ç”¨æ™ºèƒ½ä½“ç”Ÿæˆçš„æµ‹è¯•å°"
                })
        else:
            # åç»­è¿­ä»£ï¼šğŸ¯ ä¼˜å…ˆä½¿ç”¨æ™ºèƒ½ä½“ç”Ÿæˆçš„æµ‹è¯•å°
            if has_generated_testbench:
                # âœ¨ æ™ºèƒ½ä½“ç”Ÿæˆäº†æµ‹è¯•å°ï¼Œä¼˜å…ˆä½¿ç”¨ï¼ˆæ¨åŠ¨TDDå¾ªç¯ï¼‰
                strategy_info.update({
                    "selected_testbench": current_iteration_testbench,
                    "strategy": "æ™ºèƒ½ä½“ä¼˜åŒ–",
                    "reason": f"ç¬¬{iteration}æ¬¡è¿­ä»£ï¼Œä¼˜å…ˆä½¿ç”¨æ™ºèƒ½ä½“ç”Ÿæˆçš„æœ€æ–°æµ‹è¯•å°æ¨åŠ¨TDDå¾ªç¯"
                })
            elif has_user_testbench:
                # åªæœ‰ç”¨æˆ·æµ‹è¯•å°å¯ç”¨æ—¶æ‰ä½¿ç”¨
                strategy_info.update({
                    "selected_testbench": user_testbench_path,
                    "strategy": "ç”¨æˆ·å¤‡ç”¨",
                    "reason": f"ç¬¬{iteration}æ¬¡è¿­ä»£ï¼Œæ™ºèƒ½ä½“æœªç”Ÿæˆæµ‹è¯•å°ï¼Œä½¿ç”¨ç”¨æˆ·æµ‹è¯•å°"
                })
        
        return strategy_info
    
    def _should_use_generated_testbench(self, iteration: int) -> bool:
        """å†³å®šæ˜¯å¦åº”è¯¥ä½¿ç”¨æ™ºèƒ½ä½“ç”Ÿæˆçš„æµ‹è¯•å°"""
        # âœ¨ ç§¯æç­–ç•¥ï¼šä»ç¬¬2æ¬¡è¿­ä»£å¼€å§‹ï¼Œä¼˜å…ˆä½¿ç”¨æ™ºèƒ½ä½“ç”Ÿæˆçš„æµ‹è¯•å°
        # è¿™ç¡®ä¿äº† TDD å¾ªç¯ä¸­æµ‹è¯•å°å¯ä»¥éšç€è®¾è®¡ä¸€èµ·è¿­ä»£æ”¹è¿›
        # ç¬¬1æ¬¡è¿­ä»£å»ºç«‹åŸºå‡†ï¼Œç¬¬2æ¬¡åŠä»¥åè¿­ä»£ä¼˜åŒ–
        return iteration >= 2
    
    def _analyze_code_violations(self, error_text: str, enhanced_analysis: Dict[str, Any]) -> str:
        """åˆ†æä»£ç è¿è§„é—®é¢˜"""
        analysis = ""
        
        # æ£€æŸ¥SystemVerilogè¯­æ³•è¿è§„
        if "logic" in error_text.lower():
            analysis += "ğŸš¨ **SystemVerilogè¯­æ³•è¿è§„**:\n"
            analysis += "- æ£€æµ‹åˆ° `logic` ç±»å‹ï¼Œiverilogä¸æ”¯æŒ\n"
            analysis += "- å¿…é¡»æ”¹ç”¨ `wire` æˆ– `reg`\n\n"
        
        # æ£€æŸ¥æ—¶åºé€»è¾‘è¿è§„
        last_code = enhanced_analysis.get("last_generated_code", "")
        if last_code:
            if "posedge" in last_code and ("clk" in last_code or "rst" in last_code):
                analysis += "ğŸš¨ **æ—¶åºé€»è¾‘è¿è§„**:\n"
                analysis += "- æ£€æµ‹åˆ°æ—¶é’Ÿ/å¤ä½ä¿¡å·ï¼Œè¿åçº¯ç»„åˆé€»è¾‘è¦æ±‚\n"
                analysis += "- å¿…é¡»ç§»é™¤ clkã€rst ä¿¡å·\n"
                analysis += "- å°† always @(posedge clk) æ”¹ä¸º always @(*)\n\n"
        
        # æ£€æŸ¥ç«¯å£åŒ¹é…é—®é¢˜
        if "not a port" in error_text:
            import re
            port_match = re.search(r"port\s+[`'\"]*(\w+)[`'\"]*\s+is not a port", error_text)
            if port_match:
                missing_port = port_match.group(1)
                analysis += f"ğŸš¨ **ç«¯å£åŒ¹é…é”™è¯¯**:\n"
                analysis += f"- æµ‹è¯•å°æœŸæœ›ç«¯å£: `{missing_port}`\n"
                analysis += f"- å½“å‰æ¨¡å—ç¼ºå°‘æ­¤ç«¯å£\n"
                analysis += f"- **ç«‹å³ä¿®å¤**: åœ¨æ¨¡å—ä¸­æ·»åŠ  `{missing_port}` ç«¯å£\n\n"
        
        return analysis
    
    def _analyze_test_failures(self, test_results: str, enhanced_analysis: Dict[str, Any]) -> str:
        """æ·±åº¦åˆ†ææµ‹è¯•å¤±è´¥åŸå› """
        analysis = ""
        
        # è§£ææµ‹è¯•ç»“æœ
        import re
        
        # æŸ¥æ‰¾å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹
        test_cases = re.findall(r"Test Case (\d+).*?Failed.*?Expected.*?got\s+(.*?)(?=\n|$)", test_results, re.DOTALL)
        
        for case_num, details in test_cases:
            analysis += f"ğŸ“Š **æµ‹è¯•ç”¨ä¾‹ {case_num} å¤±è´¥åˆ†æ**:\n"
            
            # æå–å…·ä½“æ•°å€¼
            values = re.findall(r"(\w+)=([0-9a-fA-F]+)", details)
            if values:
                analysis += "- å®é™…ç»“æœ: " + ", ".join([f"{k}={v}" for k, v in values]) + "\n"
            
            # ç‰¹å®šé”™è¯¯æ¨¡å¼åˆ†æ
            if "overflow" in details and "overflow=0" in details:
                analysis += "ğŸ” **æº¢å‡ºæ£€æµ‹é€»è¾‘é”™è¯¯**:\n"
                analysis += "- æº¢å‡ºæ£€æµ‹è¾“å‡ºä¸º0ï¼Œä½†åº”è¯¥ä¸º1\n"
                analysis += "- æ£€æŸ¥å…¬å¼: `overflow = (a[15] == b[15]) && (a[15] != sum[15])`\n"
                analysis += "- éªŒè¯: ä¸¤ä¸ªåŒå·æ•°ç›¸åŠ ç»“æœå˜å·æ—¶åº”è¯¥æº¢å‡º\n\n"
            
            if "cout" in details:
                analysis += "ğŸ” **è¿›ä½é€»è¾‘æ£€æŸ¥**:\n"
                analysis += "- éªŒè¯è¿›ä½è®¡ç®—: æœ€é«˜ä½æ˜¯å¦æ­£ç¡®ä¼ æ’­\n"
                analysis += "- æ£€æŸ¥: `cout = (a[15] & b[15]) | ((a[15] ^ b[15]) & carry[14])`\n\n"
        
        return analysis
    
    async def _llm_analyze_failures(self, error_text: str, enhanced_analysis: Dict[str, Any], iteration: int) -> str:
        """ä½¿ç”¨LLMæ™ºèƒ½åˆ†æç¼–è¯‘å¤±è´¥åŸå› """
        if not error_text.strip():
            return ""
        
        # å‡†å¤‡LLMåˆ†æçš„context
        last_code = enhanced_analysis.get("last_generated_code", "")
        design_requirements = enhanced_analysis.get("design_requirements", "")
        
        analysis_prompt = f"""ä½œä¸ºVerilogä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹ç¼–è¯‘é”™è¯¯å¹¶æä¾›ç²¾å‡†çš„ä¿®å¤æŒ‡å¯¼ï¼š

ğŸ¯ **è®¾è®¡è¦æ±‚**:
{design_requirements}

ğŸ“„ **ç”Ÿæˆçš„ä»£ç **:
```verilog
{last_code[:1000] if last_code else "ä»£ç ä¸å¯ç”¨"}
```

âŒ **ç¼–è¯‘é”™è¯¯**:
```
{error_text}
```

ğŸ” **è¿­ä»£ä¿¡æ¯**: ç¬¬{iteration}æ¬¡è¿­ä»£

è¯·æä¾›ä»¥ä¸‹åˆ†æï¼š
1. **æ ¹æœ¬åŸå› **: é”™è¯¯çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ
2. **è¿åçš„çº¦æŸ**: è¿åäº†å“ªäº›è®¾è®¡è¦æ±‚ï¼Ÿ
3. **å…·ä½“ä¿®å¤**: æä¾›3-5ä¸ªå…·ä½“çš„ä¿®å¤æ­¥éª¤
4. **éªŒè¯æ–¹æ³•**: å¦‚ä½•éªŒè¯ä¿®å¤æ˜¯å¦æ­£ç¡®ï¼Ÿ

æ ¼å¼è¦æ±‚ï¼š
- ä½¿ç”¨markdownæ ¼å¼
- é‡ç‚¹å†…å®¹ç”¨**åŠ ç²—**
- ä¿®å¤æ­¥éª¤è¦å…·ä½“å¯æ“ä½œ
- é¿å…ä¸€èˆ¬æ€§å»ºè®®ï¼Œè¦é’ˆå¯¹å…·ä½“é”™è¯¯"""

        try:
            # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
            from core.llm_integration.enhanced_llm_client import EnhancedLLMClient
            from config.config import FrameworkConfig
            
            config = FrameworkConfig.from_env()
            llm_client = EnhancedLLMClient(config.llm)
            
            analysis_result = await llm_client.send_prompt(
                prompt=analysis_prompt,
                system_prompt="ä½ æ˜¯èµ„æ·±çš„Verilogè®¾è®¡ä¸“å®¶ï¼Œä¸“é—¨åˆ†æå’Œä¿®å¤ç¡¬ä»¶æè¿°è¯­è¨€çš„é—®é¢˜ã€‚",
                temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿åˆ†æå‡†ç¡®æ€§
                max_tokens=1000
            )
            
            return f"ğŸ¤– **LLMæ™ºèƒ½å¤±è´¥åˆ†æ**:\n{analysis_result}\n\n"
            
        except Exception as e:
            self.logger.error(f"âŒ LLMå¤±è´¥åˆ†æå¼‚å¸¸: {str(e)}")
            # é™çº§åˆ°ç®€åŒ–çš„è§„åˆ™åˆ†æ
            return self._analyze_code_violations(error_text, enhanced_analysis)
    
    async def _llm_analyze_test_failures(self, test_results: str, enhanced_analysis: Dict[str, Any], iteration: int) -> str:
        """ä½¿ç”¨LLMæ™ºèƒ½åˆ†ææµ‹è¯•å¤±è´¥åŸå› """
        if not test_results.strip():
            return ""
        
        last_code = enhanced_analysis.get("last_generated_code", "")
        design_requirements = enhanced_analysis.get("design_requirements", "")
        
        analysis_prompt = f"""ä½œä¸ºæ•°å­—ç”µè·¯éªŒè¯ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹æµ‹è¯•å¤±è´¥åŸå› ï¼š

ğŸ¯ **è®¾è®¡è¦æ±‚**:
{design_requirements}

ğŸ“„ **è¢«æµ‹ä»£ç **:
```verilog
{last_code[:1000] if last_code else "ä»£ç ä¸å¯ç”¨"}
```

ğŸ§ª **æµ‹è¯•ç»“æœ**:
```
{test_results}
```

ğŸ” **è¿­ä»£ä¿¡æ¯**: ç¬¬{iteration}æ¬¡è¿­ä»£

è¯·æä¾›ä»¥ä¸‹åˆ†æï¼š
1. **å¤±è´¥æ¨¡å¼**: æµ‹è¯•å¤±è´¥çš„å…·ä½“æ¨¡å¼æ˜¯ä»€ä¹ˆï¼Ÿ
2. **æ•°å­¦åˆ†æ**: ä»æ•°å­¦è§’åº¦åˆ†æä¸ºä»€ä¹ˆä¼šå¤±è´¥ï¼Ÿ
3. **é€»è¾‘é”™è¯¯**: æŒ‡å‡ºå…·ä½“çš„é€»è¾‘å®ç°é”™è¯¯
4. **ä¿®å¤å»ºè®®**: æä¾›å…·ä½“çš„ä»£ç ä¿®æ”¹å»ºè®®
5. **éªŒè¯ç­–ç•¥**: å¦‚ä½•éªŒè¯ä¿®å¤åçš„æ­£ç¡®æ€§ï¼Ÿ

ç‰¹åˆ«å…³æ³¨ï¼š
- æº¢å‡ºæ£€æµ‹é€»è¾‘çš„æ•°å­¦æ­£ç¡®æ€§
- è¿›ä½ä¼ æ’­çš„å®ç°æ–¹å¼
- è¾¹ç•Œæ¡ä»¶çš„å¤„ç†

æ ¼å¼è¦æ±‚ï¼š
- ä½¿ç”¨markdownæ ¼å¼
- åŒ…å«å…·ä½“çš„æ•°å€¼ä¾‹å­
- æä¾›å¯æ“ä½œçš„ä¿®å¤ä»£ç ç‰‡æ®µ"""

        try:
            from core.llm_integration.enhanced_llm_client import EnhancedLLMClient
            from config.config import FrameworkConfig
            
            config = FrameworkConfig.from_env()
            llm_client = EnhancedLLMClient(config.llm)
            
            analysis_result = await llm_client.send_prompt(
                prompt=analysis_prompt,
                system_prompt="ä½ æ˜¯èµ„æ·±çš„æ•°å­—ç”µè·¯éªŒè¯ä¸“å®¶ï¼Œä¸“é—¨åˆ†ææµ‹è¯•å¤±è´¥åŸå› å¹¶æä¾›ç²¾ç¡®çš„ä¿®å¤æ–¹æ¡ˆã€‚",
                temperature=0.1,
                max_tokens=1000
            )
            
            return f"ğŸ¤– **LLMæ™ºèƒ½æµ‹è¯•åˆ†æ**:\n{analysis_result}\n\n"
            
        except Exception as e:
            self.logger.error(f"âŒ LLMæµ‹è¯•åˆ†æå¼‚å¸¸: {str(e)}")
            # é™çº§åˆ°ç®€åŒ–çš„è§„åˆ™åˆ†æ
            return self._analyze_test_failures(test_results, enhanced_analysis)
    
    async def _build_detailed_error_context(self, error_text: str, enhanced_analysis: Dict[str, Any]) -> str:
        """æ„å»ºè¯¦ç»†çš„é”™è¯¯ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context = "ğŸ” **è¯¦ç»†é”™è¯¯ä¸Šä¸‹æ–‡åˆ†æ**:\n"
        
        # è§£æé”™è¯¯ä¿¡æ¯ï¼Œæå–æ–‡ä»¶åå’Œè¡Œå·
        import re
        error_matches = re.findall(r'([^:\s]+\.s?v):(\d+):\s*(.+)', error_text)
        
        last_code = enhanced_analysis.get("last_generated_code", "")
        
        for file_path, line_num, error_msg in error_matches:
            context += f"\nğŸ“ **é”™è¯¯ä½ç½®**: {file_path}:{line_num}\n"
            context += f"ğŸ“ **é”™è¯¯æè¿°**: {error_msg}\n"
            
            if last_code:
                try:
                    lines = last_code.split('\n')
                    line_idx = int(line_num) - 1
                    
                    if 0 <= line_idx < len(lines):
                        # æ˜¾ç¤ºé”™è¯¯è¡ŒåŠå…¶ä¸Šä¸‹æ–‡
                        context += f"\nğŸ’¡ **é”™è¯¯ä»£ç ä¸Šä¸‹æ–‡**:\n```verilog\n"
                        
                        # æ˜¾ç¤ºå‰å3è¡Œä»£ç 
                        start = max(0, line_idx - 3)
                        end = min(len(lines), line_idx + 4)
                        
                        for i in range(start, end):
                            marker = ">>> " if i == line_idx else "    "
                            context += f"{marker}{i+1:3d}: {lines[i]}\n"
                        
                        context += "```\n"
                        
                        # åˆ†æå…·ä½“é”™è¯¯
                        error_line = lines[line_idx].strip()
                        context += f"\nğŸ” **é—®é¢˜ä»£ç **: `{error_line}`\n"
                        
                        # åŸºäºé”™è¯¯ç±»å‹æä¾›å…·ä½“åˆ†æ
                        if "syntax error" in error_msg.lower():
                            context += await self._analyze_syntax_error(error_line, error_msg)
                        elif "not a port" in error_msg.lower():
                            context += await self._analyze_port_error(error_line, error_msg, last_code)
                        
                except Exception as e:
                    self.logger.error(f"âŒ æ„å»ºé”™è¯¯ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")
        
        return context + "\n"
    
    async def _analyze_syntax_error(self, error_line: str, error_msg: str) -> str:
        """åˆ†æè¯­æ³•é”™è¯¯"""
        analysis = "\nğŸš¨ **è¯­æ³•é”™è¯¯åˆ†æ**:\n"
        
        # æ£€æŸ¥å¸¸è§è¯­æ³•é—®é¢˜
        if "for (" in error_line and "int " in error_line:
            analysis += "- æ£€æµ‹åˆ°SystemVerilog `for (int i...)` è¯­æ³•\n"
            analysis += "- iverilogä¸æ”¯æŒæ­¤è¯­æ³•ï¼Œéœ€è¦æ”¹ä¸ºVerilog-2001è¯­æ³•\n"
            analysis += "- **ä¿®å¤å»ºè®®**: åœ¨æ¨¡å—é¡¶éƒ¨å£°æ˜ `integer i;`ï¼Œç„¶åä½¿ç”¨ `for (i = 0; ...)`\n"
        elif "logic" in error_line:
            analysis += "- æ£€æµ‹åˆ°SystemVerilog `logic` ç±»å‹\n"
            analysis += "- iverilogä¸æ”¯æŒæ­¤ç±»å‹ï¼Œéœ€è¦æ”¹ä¸º `wire` æˆ– `reg`\n"
            analysis += "- **ä¿®å¤å»ºè®®**: å°† `logic` æ”¹ä¸º `wire`ï¼ˆç»„åˆé€»è¾‘ï¼‰æˆ– `reg`ï¼ˆæ—¶åºé€»è¾‘ï¼‰\n"
        elif "assert" in error_line:
            analysis += "- æ£€æµ‹åˆ°SystemVerilog `assert` è¯­å¥\n"
            analysis += "- iverilogä¸æ”¯æŒæ­¤åŠŸèƒ½ï¼Œéœ€è¦åˆ é™¤æˆ–æ”¹ä¸ºæ™®é€šéªŒè¯\n"
            analysis += "- **ä¿®å¤å»ºè®®**: åˆ é™¤assertè¯­å¥æˆ–æ”¹ä¸ºifè¯­å¥è¿›è¡Œæ£€æŸ¥\n"
        else:
            analysis += f"- è¯­æ³•é”™è¯¯: {error_msg}\n"
            analysis += "- **å»ºè®®**: æ£€æŸ¥æ‹¬å·åŒ¹é…ã€åˆ†å·ã€å…³é”®å­—æ‹¼å†™\n"
        
        return analysis
    
    async def _analyze_port_error(self, error_line: str, error_msg: str, full_code: str) -> str:
        """åˆ†æç«¯å£åŒ¹é…é”™è¯¯"""
        analysis = "\nğŸ”Œ **ç«¯å£åŒ¹é…é”™è¯¯åˆ†æ**:\n"
        
        # æå–é”™è¯¯çš„ç«¯å£å
        import re
        port_match = re.search(r"port\s+[`'\"]*(\w+)[`'\"]*\s+is not a port", error_msg)
        
        if port_match:
            missing_port = port_match.group(1)
            analysis += f"- æµ‹è¯•å°æœŸæœ›ç«¯å£: `{missing_port}`\n"
            
            # åˆ†ææ¨¡å—å®é™…ç«¯å£
            module_match = re.search(r'module\s+\w+\s*\([^)]*\)', full_code, re.DOTALL)
            if module_match:
                module_ports = module_match.group(0)
                analysis += f"- å½“å‰æ¨¡å—ç«¯å£: {module_ports}\n"
            
            analysis += f"- **ä¿®å¤å»ºè®®**: åœ¨æ¨¡å—ç«¯å£åˆ—è¡¨ä¸­æ·»åŠ  `{missing_port}` ç«¯å£\n"
            
            # åˆ†æå¯èƒ½çš„åŸå› 
            if missing_port in ["clk", "rst", "rst_n"]:
                analysis += f"- **é—®é¢˜åŸå› **: è®¾è®¡è¦æ±‚çº¯ç»„åˆé€»è¾‘ï¼Œä½†æ¨¡å—ä»åŒ…å«æ—¶é’Ÿ/å¤ä½ä¿¡å·\n"
                analysis += f"- **è§£å†³æ–¹æ¡ˆ**: å®Œå…¨ç§»é™¤æ—¶é’Ÿå’Œå¤ä½ç›¸å…³çš„ç«¯å£å’Œé€»è¾‘\n"
        
        return analysis
    
    async def _build_test_failure_context(self, test_results: str, enhanced_analysis: Dict[str, Any]) -> str:
        """æ„å»ºè¯¦ç»†çš„æµ‹è¯•å¤±è´¥ä¸Šä¸‹æ–‡"""
        context = "ğŸ§ª **è¯¦ç»†æµ‹è¯•å¤±è´¥ä¸Šä¸‹æ–‡**:\n"
        
        # è§£ææµ‹è¯•ç»“æœï¼Œæå–å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹
        import re
        
        # æŸ¥æ‰¾å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹
        test_case_matches = re.findall(r'Test[^\n]*?failed[^\n]*', test_results, re.IGNORECASE)
        
        if test_case_matches:
            context += f"\nğŸ“Š **å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹** ({len(test_case_matches)}ä¸ª):\n"
            
            for i, test_case in enumerate(test_case_matches, 1):
                context += f"\nğŸ” **å¤±è´¥ç”¨ä¾‹ {i}**: {test_case}\n"
                
                # æå–å…·ä½“çš„è¾“å…¥è¾“å‡ºå€¼
                values = re.findall(r'(\w+)=([0-9a-fA-FxX]+)', test_case)
                if values:
                    context += "ğŸ“‹ **å®é™…ç»“æœ**:\n"
                    for var, val in values:
                        context += f"  - {var} = {val}\n"
        
        # æŸ¥æ‰¾æœŸæœ›å€¼ä¿¡æ¯
        expected_matches = re.findall(r'Expected[^\n]*', test_results, re.IGNORECASE)
        if expected_matches:
            context += f"\nâœ… **æœŸæœ›ç»“æœ**:\n"
            for expected in expected_matches:
                context += f"  - {expected}\n"
        
        # åˆ†æå…·ä½“çš„æ•°å€¼å·®å¼‚
        last_code = enhanced_analysis.get("last_generated_code", "")
        if last_code:
            context += await self._analyze_test_value_differences(test_results, last_code)
        
        return context + "\n"
    
    async def _analyze_test_value_differences(self, test_results: str, dut_code: str) -> str:
        """åˆ†ææµ‹è¯•å€¼å·®å¼‚"""
        analysis = "\nğŸ” **æ•°å€¼å·®å¼‚åˆ†æ**:\n"
        
        # ç‰¹å®šæµ‹è¯•ç”¨ä¾‹çš„æ·±åº¦åˆ†æ
        if "overflow=0" in test_results and "expected" in test_results.lower():
            analysis += "\nğŸš¨ **æº¢å‡ºæ£€æµ‹é”™è¯¯åˆ†æ**:\n"
            
            # æå–æº¢å‡ºæ£€æµ‹é€»è¾‘
            overflow_match = re.search(r'(assign\s+overflow[^;]+;)', dut_code, re.IGNORECASE)
            if overflow_match:
                overflow_logic = overflow_match.group(1)
                analysis += f"ğŸ“‹ **å½“å‰æº¢å‡ºé€»è¾‘**: `{overflow_logic}`\n"
                
                # åˆ†æå¸¸è§çš„æº¢å‡ºæ£€æµ‹é”™è¯¯
                if "(a[15] == b[15]) && (a[15] != sum[15])" in overflow_logic:
                    analysis += "âœ… æº¢å‡ºæ£€æµ‹å…¬å¼æ­£ç¡®\n"
                    analysis += "âš ï¸ å¯èƒ½é—®é¢˜ï¼šsumçš„è®¡ç®—æ—¶åºæˆ–ä½ç½®\n"
                    analysis += "ğŸ” **å»ºè®®æ£€æŸ¥**: sumè®¡ç®—æ˜¯å¦åœ¨overflowæ£€æµ‹ä¹‹å‰å®Œæˆ\n"
                else:
                    analysis += "âŒ æº¢å‡ºæ£€æµ‹å…¬å¼å¯èƒ½æœ‰è¯¯\n"
                    analysis += "âœ… **æ­£ç¡®å…¬å¼**: `overflow = (a[15] == b[15]) && (a[15] != sum[15])`\n"
        
        # è¿›ä½é€»è¾‘åˆ†æ
        if "cout=" in test_results:
            analysis += "\nğŸ”— **è¿›ä½é€»è¾‘åˆ†æ**:\n"
            cout_match = re.search(r'(assign\s+cout[^;]+;)', dut_code, re.IGNORECASE)
            if cout_match:
                cout_logic = cout_match.group(1)
                analysis += f"ğŸ“‹ **å½“å‰è¿›ä½é€»è¾‘**: `{cout_logic}`\n"
        
        return analysis
    
    def _parse_compilation_errors(self, stderr_content: str) -> List[Dict[str, Any]]:
        """è§£æç¼–è¯‘é”™è¯¯ä¿¡æ¯"""
        errors = []
        if not stderr_content:
            return errors
        
        # å¸¸è§çš„ç¼–è¯‘é”™è¯¯æ¨¡å¼
        error_patterns = [
            r'(.+?):(\d+):\s*error:\s*(.+)',  # file:line: error: message
            r'(.+?):(\d+):\s*(.+error.+)',   # file:line: error message
            r'Error.*?in\s+(.+?)\s+at\s+line\s+(\d+)',  # Error in file at line
        ]
        
        lines = stderr_content.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            for pattern in error_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    if len(match.groups()) >= 3:
                        file_path = match.group(1).strip()
                        line_number = match.group(2).strip()
                        message = match.group(3).strip()
                    else:
                        file_path = match.group(1).strip() if len(match.groups()) >= 1 else "unknown"
                        line_number = match.group(2).strip() if len(match.groups()) >= 2 else "0"
                        message = line
                    
                    errors.append({
                        "file": file_path,
                        "line": line_number,
                        "message": message,
                        "type": "compilation_error",
                        "raw_line": line
                    })
                    break
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šæ¨¡å¼ï¼Œä½†åŒ…å«é”™è¯¯å…³é”®è¯ï¼Œæ·»åŠ ä¸ºé€šç”¨é”™è¯¯
        if not errors and any(keyword in stderr_content.lower() for keyword in ['error', 'failed', 'cannot']):
            errors.append({
                "file": "unknown",
                "line": "0",
                "message": stderr_content.strip(),
                "type": "general_error",
                "raw_line": stderr_content.strip()
            })
        
        return errors
    
    def _extract_module_name(self, verilog_content: str) -> Optional[str]:
        """ä»Verilogä»£ç ä¸­æå–æ¨¡å—å"""
        if not verilog_content:
            return None
        
        # æŸ¥æ‰¾ module å£°æ˜
        module_pattern = r'module\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*[#(]'
        match = re.search(module_pattern, verilog_content)
        if match:
            return match.group(1)
        
        # ç®€åŒ–ç‰ˆæ¨¡å¼
        simple_pattern = r'module\s+([a-zA-Z_][a-zA-Z0-9_]*)'
        match = re.search(simple_pattern, verilog_content)
        if match:
            return match.group(1)
        
        return None

    async def _continue_persistent_conversation(self, task: str, context: Dict[str, Any], iteration: int) -> Dict[str, Any]:
        """ç»§ç»­æŒç»­å¯¹è¯ï¼Œä¸é‡æ–°é€‰æ‹©æ™ºèƒ½ä½“ - ä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        try:
            # è·å–ä¹‹å‰ä½¿ç”¨çš„æ™ºèƒ½ä½“
            if not hasattr(self, 'current_session_agents') or 'verilog_designer' not in self.current_session_agents:
                # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œå›é€€åˆ°æ ‡å‡†æµç¨‹
                self.logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æŒç»­å¯¹è¯çš„æ™ºèƒ½ä½“ï¼Œå›é€€åˆ°æ ‡å‡†æµç¨‹")
                return await self.base_coordinator.coordinate_task_execution(task, context)
            
            design_agent_id = self.current_session_agents['verilog_designer']
            self.logger.info(f"ğŸ”— å‘æŒç»­å¯¹è¯æ™ºèƒ½ä½“å‘é€ä»»åŠ¡: {design_agent_id}")
            
            # ç›´æ¥å‘æ™ºèƒ½ä½“å‘é€ä»»åŠ¡ï¼Œä¿æŒå¯¹è¯ä¸Šä¸‹æ–‡
            design_agent = self.base_coordinator.agent_instances.get(design_agent_id)
            if not design_agent:
                self.logger.error(f"âŒ æ™ºèƒ½ä½“ä¸å­˜åœ¨: {design_agent_id}")
                return {"success": False, "error": f"æ™ºèƒ½ä½“ä¸å­˜åœ¨: {design_agent_id}"}
            
            # ğŸ§  ä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ„å»ºå…¨é‡ä¸Šä¸‹æ–‡
            enhanced_context = context.copy()
            if self.context_manager:
                full_context = self.context_manager.get_full_context_for_agent(design_agent_id, task)
                enhanced_context["full_conversation_context"] = full_context
                self.logger.info(f"ğŸ§  ä¼ é€’å®Œæ•´ä¸Šä¸‹æ–‡ç»™{design_agent_id}: "
                               f"{len(full_context.get('complete_conversation_history', []))}è½®å¯¹è¯å†å²")
            
            # ğŸ§  è®°å½•å¯¹è¯å¼€å§‹ï¼ˆAIæ¥æ”¶åˆ°çš„promptï¼‰
            if self.context_manager:
                self.context_manager.add_conversation_turn(
                    agent_id=design_agent_id,
                    user_prompt=task,
                    system_prompt="TDDè¿­ä»£è®¾è®¡ä»»åŠ¡",
                    ai_response="",  # ç¨åæ›´æ–°
                    reasoning_notes=f"è¿­ä»£{iteration}çš„æŒç»­å¯¹è¯"
                )
            
            # åˆ›å»ºä»»åŠ¡æ¶ˆæ¯
            task_message = TaskMessage(
                task_id=f"{self.persistent_conversation_id}_iter_{iteration}",
                sender_id="test_driven_coordinator",
                receiver_id=design_agent_id,
                message_type="task_execution",
                content=task,
                metadata=enhanced_context
            )
            
            # ç›´æ¥è°ƒç”¨æ™ºèƒ½ä½“çš„å¢å¼ºä»»åŠ¡æ‰§è¡Œæ–¹æ³•
            result = await design_agent.execute_enhanced_task(task, task_message, {})
            
            # ğŸ§  è®°å½•å¯¹è¯ç»“æœï¼ˆAIçš„å®Œæ•´å“åº”ï¼‰
            if self.context_manager and self.context_manager.current_iteration:
                # æ›´æ–°æœ€åä¸€ä¸ªå¯¹è¯è½®æ¬¡çš„AIå“åº”
                last_turn = self.context_manager.current_iteration.conversation_turns[-1]
                last_turn.ai_response = str(result.get("content", ""))
                last_turn.success = result.get("success", False)
                last_turn.error_info = result.get("error") if not result.get("success") else None
                last_turn.tool_calls = result.get("tool_calls", [])
                last_turn.tool_results = result.get("tool_results", [])
            
            # è®°å½•æœ¬æ¬¡è¿­ä»£çš„ç»“æœ
            if result.get("success", False):
                self.logger.info(f"âœ… æŒç»­å¯¹è¯ä»»åŠ¡å®Œæˆ: è¿­ä»£ {iteration}")
            else:
                self.logger.error(f"âŒ æŒç»­å¯¹è¯ä»»åŠ¡å¤±è´¥: {result.get('error', 'Unknown error')}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ æŒç»­å¯¹è¯å¼‚å¸¸: {str(e)}")
            # å‡ºé”™æ—¶å›é€€åˆ°æ ‡å‡†æµç¨‹
            return await self.base_coordinator.coordinate_task_execution(task, context)
    


# ==========================================
# ğŸ­ å·¥å‚å‡½æ•° - ä¾¿äºé›†æˆ
# ==========================================

def create_test_driven_coordinator(base_coordinator: CentralizedCoordinator,
                                 config: TestDrivenConfig = None) -> TestDrivenCoordinator:
    """
    åˆ›å»ºæµ‹è¯•é©±åŠ¨åè°ƒå™¨çš„å·¥å‚å‡½æ•°
    
    Usage:
        # ç°æœ‰ä»£ç 
        coordinator = CentralizedCoordinator(config)
        
        # å‡çº§ä¸ºæµ‹è¯•é©±åŠ¨åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰
        enhanced_coordinator = create_test_driven_coordinator(coordinator)
        
        # ç°æœ‰åŠŸèƒ½å®Œå…¨ä¸å˜
        result = await enhanced_coordinator.coordinate_task_execution(task)
        
        # æ–°å¢æµ‹è¯•é©±åŠ¨åŠŸèƒ½
        tdd_result = await enhanced_coordinator.execute_test_driven_task(task, testbench_path)
    """
    return TestDrivenCoordinator(base_coordinator, config)