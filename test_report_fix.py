#!/usr/bin/env python3
"""
æµ‹è¯•æŠ¥å‘Šä¿®å¤è„šæœ¬
================

éªŒè¯ä¿®å¤åçš„å®éªŒæŠ¥å‘Šç³»ç»Ÿæ˜¯å¦èƒ½æ­£ç¡®ä¿å­˜æ‰€æœ‰æ•°æ®ã€‚
"""

import asyncio
import json
import time
from pathlib import Path

from core.llm_coordinator_agent import LLMCoordinatorAgent, TaskContext
from config.config import FrameworkConfig
from agents.enhanced_real_verilog_agent import EnhancedRealVerilogAgent
from agents.enhanced_real_code_reviewer import EnhancedRealCodeReviewAgent


async def test_report_fix():
    """æµ‹è¯•æŠ¥å‘Šä¿®å¤"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•æŠ¥å‘Šä¿®å¤...")

    # åˆ›å»ºé…ç½®
    config = FrameworkConfig()
    config.enable_detailed_logging = True

    # åˆ›å»ºåè°ƒæ™ºèƒ½ä½“
    coordinator = LLMCoordinatorAgent(config)

    # æ³¨å†Œæ™ºèƒ½ä½“
    verilog_agent = EnhancedRealVerilogAgent()
    reviewer_agent = EnhancedRealCodeReviewAgent()

    await coordinator.register_agent(verilog_agent)
    await coordinator.register_agent(reviewer_agent)

    # åˆ›å»ºæ¨¡æ‹Ÿçš„TaskContext
    task_context = TaskContext(
        task_id="test_task_001",
        original_request="è®¾è®¡ä¸€ä¸ªç®€å•çš„2ä½è®¡æ•°å™¨"
    )

    # æ¨¡æ‹Ÿæ•°æ®æ”¶é›†
    print("ğŸ“Š æ¨¡æ‹Ÿæ•°æ®æ”¶é›†...")
    
    # æ·»åŠ å·¥å…·æ‰§è¡Œè®°å½•
    task_context.add_tool_execution(
        tool_name="identify_task_type",
        parameters={"user_request": "è®¾è®¡è®¡æ•°å™¨"},
        agent_id="llm_coordinator_agent",
        success=True,
        result="ä»»åŠ¡ç±»å‹: design",
        execution_time=2.5
    )
    
    task_context.add_tool_execution(
        tool_name="assign_task_to_agent",
        parameters={"agent_id": "enhanced_real_verilog_agent"},
        agent_id="llm_coordinator_agent",
        success=True,
        result="ä»»åŠ¡åˆ†é…æˆåŠŸ",
        execution_time=1.8
    )

    # æ·»åŠ æ–‡ä»¶æ“ä½œè®°å½•
    task_context.add_file_operation(
        operation_type="create",
        file_path="/test/counter.v",
        agent_id="enhanced_real_verilog_agent",
        success=True,
        file_size=1024
    )

    # æ·»åŠ å·¥ä½œæµé˜¶æ®µè®°å½•
    task_context.add_workflow_stage(
        stage_name="task_analysis",
        description="åˆ†æä»»åŠ¡éœ€æ±‚",
        agent_id="llm_coordinator_agent",
        duration=3.2,
        success=True
    )

    # æ·»åŠ LLMå¯¹è¯è®°å½•
    task_context.add_llm_conversation(
        agent_id="llm_coordinator_agent",
        conversation_id="test_conv_001",
        system_prompt="ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åè°ƒè€…...",
        user_message="è¯·è®¾è®¡ä¸€ä¸ª2ä½è®¡æ•°å™¨",
        assistant_response="æˆ‘å°†åˆ†æè¿™ä¸ªä»»åŠ¡å¹¶åˆ†é…ç»™åˆé€‚çš„æ™ºèƒ½ä½“...",
        model_name="claude-3.5-sonnet",
        duration=2.5,
        success=True,
        is_first_call=True,
        total_tokens=150
    )

    # æµ‹è¯•æ•°æ®æ”¶é›†æ‘˜è¦
    print("ğŸ“‹ æµ‹è¯•æ•°æ®æ”¶é›†æ‘˜è¦...")
    summary = task_context.get_data_collection_summary()
    
    print(f"å·¥å…·è°ƒç”¨: {summary['tool_executions']['total']} æ¬¡")
    print(f"æ–‡ä»¶æ“ä½œ: {summary['file_operations']['total']} æ¬¡")
    print(f"å·¥ä½œæµé˜¶æ®µ: {summary['workflow_stages']['total']} ä¸ª")
    print(f"LLMå¯¹è¯: {summary['llm_conversations']['total']} æ¬¡")

    # æµ‹è¯•æœ€ç»ˆç»“æœæ”¶é›†
    print("ğŸ” æµ‹è¯•æœ€ç»ˆç»“æœæ”¶é›†...")
    final_result = coordinator._collect_final_result(
        task_context=task_context,
        coordination_result="ä»»åŠ¡å®Œæˆ"
    )

    # æ£€æŸ¥ç»“æœç»“æ„
    print("\nğŸ“Š æ£€æŸ¥ç»“æœç»“æ„...")
    
    # åŸºæœ¬å­—æ®µæ£€æŸ¥
    assert "success" in final_result, "ç¼ºå°‘successå­—æ®µ"
    assert "task_context" in final_result, "ç¼ºå°‘task_contextå­—æ®µ"
    
    # TaskContextå­—æ®µæ£€æŸ¥
    task_context_data = final_result.get("task_context", {})
    
    # æ£€æŸ¥æ•°æ®æ”¶é›†å­—æ®µ
    data_fields = [
        "tool_executions", "agent_interactions", "performance_metrics",
        "workflow_stages", "file_operations", "execution_timeline",
        "llm_conversations", "data_collection_summary"
    ]
    
    for field in data_fields:
        assert field in task_context_data, f"ç¼ºå°‘{field}å­—æ®µ"
        print(f"  âœ… {field}: {len(task_context_data[field]) if isinstance(task_context_data[field], list) else type(task_context_data[field])}")

    # ä¿å­˜æµ‹è¯•ç»“æœ
    output_dir = Path("test_output")
    output_dir.mkdir(exist_ok=True)

    result_file = output_dir / "report_fix_test_result.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(final_result, f, ensure_ascii=False, indent=2, default=str)

    print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {result_file}")

    # æ˜¾ç¤ºæ‘˜è¦ç»Ÿè®¡
    print("\nğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")
    print(f"  å·¥å…·è°ƒç”¨: {len(task_context_data.get('tool_executions', []))} æ¬¡")
    print(f"  æ–‡ä»¶æ“ä½œ: {len(task_context_data.get('file_operations', []))} æ¬¡")
    print(f"  å·¥ä½œæµé˜¶æ®µ: {len(task_context_data.get('workflow_stages', []))} ä¸ª")
    print(f"  LLMå¯¹è¯: {len(task_context_data.get('llm_conversations', []))} æ¬¡")
    print(f"  æ‰§è¡Œäº‹ä»¶: {len(task_context_data.get('execution_timeline', []))} ä¸ª")

    return final_result


async def main():
    """ä¸»å‡½æ•°"""
    try:
        result = await test_report_fix()
        if result:
            print("\nğŸ‰ æŠ¥å‘Šä¿®å¤éªŒè¯æˆåŠŸï¼")
            print("âœ… æ‰€æœ‰æ•°æ®æ”¶é›†å­—æ®µæ­£å¸¸")
            print("âœ… æ²¡æœ‰å‡ºç°å­—æ®µç¼ºå¤±é”™è¯¯")
            print("âœ… å®éªŒæŠ¥å‘Šç»“æ„å®Œæ•´")
        else:
            print("\nğŸ’¥ æŠ¥å‘Šä¿®å¤éªŒè¯å¤±è´¥ï¼")
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main()) 