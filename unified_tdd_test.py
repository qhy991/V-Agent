#!/usr/bin/env python3
"""
ğŸ§ª ç»Ÿä¸€æµ‹è¯•é©±åŠ¨å¼€å‘(TDD)å…¥å£
==================================================

è¿™ä¸ªè„šæœ¬æä¾›äº†ä¸€ä¸ªå®Œæ•´ã€æ˜“ç”¨çš„TDDæµ‹è¯•å…¥å£ï¼Œæ”¯æŒï¼š
âœ… å¤šè½®è¿­ä»£ç»“æœå®Œæ•´ä¿å­˜
âœ… é…ç½®åŒ–çš„å®éªŒå‚æ•°
âœ… è¯¦ç»†çš„è¿›åº¦è·Ÿè¸ªå’Œç»“æœåˆ†æ
âœ… é€šç”¨çš„æµ‹è¯•å°æ¨¡æ¿æ”¯æŒ

ä½¿ç”¨æ–¹æ³•:
    python unified_tdd_test.py --design alu --iterations 5
    python unified_tdd_test.py --design counter --testbench /path/to/tb.v
    python unified_tdd_test.py --design custom --requirements "è®¾è®¡éœ€æ±‚æ–‡æœ¬"
"""

import asyncio
import sys
import argparse
import json
import time
from pathlib import Path
from typing import Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config.config import FrameworkConfig
from core.centralized_coordinator import CentralizedCoordinator
from agents.real_verilog_agent import RealVerilogDesignAgent
from agents.real_code_reviewer import RealCodeReviewAgent
from extensions import create_test_driven_coordinator, TestDrivenConfig


class UnifiedTDDTest:
    """ç»Ÿä¸€çš„æµ‹è¯•é©±åŠ¨å¼€å‘æµ‹è¯•å…¥å£"""
    
    # é¢„å®šä¹‰çš„è®¾è®¡æ¨¡æ¿
    DESIGN_TEMPLATES = {
        "alu": {
            "description": """
è®¾è®¡ä¸€ä¸ª32ä½ç®—æœ¯é€»è¾‘å•å…ƒ(ALU)ï¼Œæ”¯æŒä»¥ä¸‹æ“ä½œï¼š
- ç®—æœ¯è¿ç®—ï¼šåŠ æ³•(ADD)ã€å‡æ³•(SUB)
- é€»è¾‘è¿ç®—ï¼šä¸(AND)ã€æˆ–(OR)ã€å¼‚æˆ–(XOR)ã€é(NOT)
- æ¯”è¾ƒè¿ç®—ï¼šç­‰äº(EQ)ã€å°äº(LT)ã€å¤§äº(GT)

æ¨¡å—æ¥å£ï¼š
```verilog
module alu_32bit (
    input  [31:0] a,        // æ“ä½œæ•°A
    input  [31:0] b,        // æ“ä½œæ•°B
    input  [3:0]  op,       // æ“ä½œç 
    output [31:0] result,   // ç»“æœ
    output        zero,     // é›¶æ ‡å¿—
    output        overflow  // æº¢å‡ºæ ‡å¿—
);
```
            """,
            "testbench": "test_cases/alu_testbench.v",
            "complexity": "standard"
        },
        
        "counter": {
            "description": """
è®¾è®¡ä¸€ä¸ª8ä½è®¡æ•°å™¨ï¼Œå…·æœ‰ä»¥ä¸‹åŠŸèƒ½ï¼š
- åŒæ­¥æ—¶é’Ÿï¼Œå¼‚æ­¥å¤ä½
- å¯æ§åˆ¶çš„è®¡æ•°ä½¿èƒ½
- å¯è®¾ç½®çš„è®¡æ•°æ¨¡å¼(ä¸Šè®¡æ•°/ä¸‹è®¡æ•°)
- è®¡æ•°å€¼è¾“å‡ºå’Œæº¢å‡ºæ£€æµ‹

æ¨¡å—æ¥å£ï¼š
```verilog
module counter_8bit (
    input        clk,       // æ—¶é’Ÿ
    input        rst_n,     // å¼‚æ­¥å¤ä½
    input        enable,    // è®¡æ•°ä½¿èƒ½
    input        up_down,   // è®¡æ•°æ–¹å‘(1:ä¸Šè®¡æ•°, 0:ä¸‹è®¡æ•°)
    output [7:0] count,     // è®¡æ•°å€¼
    output       overflow   // æº¢å‡ºæ ‡å¿—
);
```
            """,
            "testbench": None,  # éœ€è¦ç”¨æˆ·æä¾›æˆ–ç”Ÿæˆ
            "complexity": "simple"
        },
        
        "simple_adder": {
            "description": """
è®¾è®¡ä¸€ä¸ªç®€å•çš„8ä½åŠ æ³•å™¨ï¼Œæ”¯æŒåŸºæœ¬çš„äºŒè¿›åˆ¶åŠ æ³•è¿ç®—ã€‚

æ¨¡å—æ¥å£ï¼š
```verilog
module simple_8bit_adder (
    input  [7:0] a,         // ç¬¬ä¸€ä¸ª8ä½æ“ä½œæ•°
    input  [7:0] b,         // ç¬¬äºŒä¸ª8ä½æ“ä½œæ•°
    input        cin,       // è¾“å…¥è¿›ä½
    output [7:0] sum,       // 8ä½å’Œ
    output       cout       // è¾“å‡ºè¿›ä½
);
```

ğŸ¯ åŠŸèƒ½è¦æ±‚ï¼š
1. å®ç°8ä½äºŒè¿›åˆ¶åŠ æ³•è¿ç®—ï¼šsum = a + b + cin
2. æ­£ç¡®è®¡ç®—è¾“å‡ºè¿›ä½ï¼šcout
3. æ”¯æŒæ‰€æœ‰å¯èƒ½çš„è¾“å…¥ç»„åˆï¼ˆ0åˆ°255ï¼‰
4. å¤„ç†è¿›ä½ä¼ æ’­

ğŸ’¡ è®¾è®¡æç¤ºï¼š
- å¯ä»¥ä½¿ç”¨ç®€å•çš„è¡Œæ³¢è¿›ä½é“¾
- ç¡®ä¿æ‰€æœ‰è¾¹ç•Œæ¡ä»¶æ­£ç¡®å¤„ç†
- ä»£ç è¦ç®€æ´æ¸…æ™°ï¼Œæ˜“äºç†è§£
            """,
            "testbench": "test_cases/simple_8bit_adder_tb.v",
            "complexity": "simple"
        },
        
        "adder": {
            "description": """
è®¾è®¡ä¸€ä¸ª16ä½è¶…å‰è¿›ä½åŠ æ³•å™¨ï¼ˆCarry Lookahead Adderï¼‰ï¼Œå®ç°é«˜æ•ˆçš„å¹¶è¡ŒåŠ æ³•è¿ç®—ã€‚

æ¨¡å—æ¥å£ï¼š
```verilog
module carry_lookahead_adder_16bit (
    input  [15:0] a,        // ç¬¬ä¸€ä¸ª16ä½æ“ä½œæ•°
    input  [15:0] b,        // ç¬¬äºŒä¸ª16ä½æ“ä½œæ•°  
    input         cin,      // è¾“å…¥è¿›ä½
    output [15:0] sum,      // 16ä½å’Œ
    output        cout      // è¾“å‡ºè¿›ä½
);
```

ğŸ¯ åŠŸèƒ½è¦æ±‚ï¼š
1. å®ç°16ä½äºŒè¿›åˆ¶åŠ æ³•è¿ç®—ï¼šsum = a + b + cin
2. æ­£ç¡®è®¡ç®—è¾“å‡ºè¿›ä½ï¼šcout
3. ä½¿ç”¨è¶…å‰è¿›ä½æŠ€æœ¯æé«˜æ€§èƒ½ï¼Œè€Œä¸æ˜¯ç®€å•çš„è¡Œæ³¢è¿›ä½
4. æ”¯æŒæ‰€æœ‰å¯èƒ½çš„è¾“å…¥ç»„åˆ

ğŸ“Š è¶…å‰è¿›ä½åŠ æ³•å™¨è®¾è®¡è¦ç‚¹ï¼š
1. **è¿›ä½ç”Ÿæˆ (Generate)**: Gi = Ai & Bi
2. **è¿›ä½ä¼ æ’­ (Propagate)**: Pi = Ai ^ Bi
3. **è¶…å‰è¿›ä½è®¡ç®—**: 
   - C1 = G0 + P0Ã—C0
   - C2 = G1 + P1Ã—G0 + P1Ã—P0Ã—C0
   - C3 = G2 + P2Ã—G1 + P2Ã—P1Ã—G0 + P2Ã—P1Ã—P0Ã—C0
   - ...
4. **æ±‚å’Œ**: Si = Pi ^ Ci
            """,
            "testbench": "test_cases/carry_lookahead_adder_tb.v",
            "complexity": "advanced"
        }
    }
    
    # é¢„å®šä¹‰çš„å®éªŒé…ç½®
    EXPERIMENT_CONFIGS = {
        "quick": {"max_iterations": 3, "timeout_per_iteration": 120, "deep_analysis": False},
        "standard": {"max_iterations": 5, "timeout_per_iteration": 300, "deep_analysis": True},
        "thorough": {"max_iterations": 8, "timeout_per_iteration": 600, "deep_analysis": True},
        "debug": {"max_iterations": 10, "timeout_per_iteration": 900, "deep_analysis": True}
    }
    
    def __init__(self, design_type: str = "alu", 
                 config_profile: str = "standard",
                 custom_config: Dict[str, Any] = None,
                 testbench_path: str = None,
                 custom_requirements: str = None):
        """åˆå§‹åŒ–ç»Ÿä¸€TDDæµ‹è¯•"""
        self.design_type = design_type
        self.config_profile = config_profile
        self.testbench_path = testbench_path
        self.custom_requirements = custom_requirements
        
        # å®éªŒé…ç½®
        base_config = self.EXPERIMENT_CONFIGS.get(config_profile, self.EXPERIMENT_CONFIGS["standard"])
        if custom_config:
            base_config.update(custom_config)
        self.experiment_config = base_config
        
        # ç”Ÿæˆå®éªŒID
        self.experiment_id = f"unified_tdd_{design_type}_{int(time.time())}"
        
        print(f"ğŸ§ª ç»Ÿä¸€TDDæµ‹è¯•åˆå§‹åŒ–")
        print(f"   è®¾è®¡ç±»å‹: {design_type}")
        print(f"   é…ç½®æ¡£æ¡ˆ: {config_profile}")
        print(f"   å®éªŒID: {self.experiment_id}")
    
    def get_design_requirements(self) -> str:
        """è·å–è®¾è®¡éœ€æ±‚"""
        if self.custom_requirements:
            return self.custom_requirements
        
        template = self.DESIGN_TEMPLATES.get(self.design_type)
        if not template:
            raise ValueError(f"æœªçŸ¥çš„è®¾è®¡ç±»å‹: {self.design_type}")
        
        return template["description"]
    
    def get_testbench_path(self) -> Optional[str]:
        """è·å–æµ‹è¯•å°è·¯å¾„"""
        if self.testbench_path:
            return self.testbench_path
        
        template = self.DESIGN_TEMPLATES.get(self.design_type)
        if template and template.get("testbench"):
            tb_path = project_root / template["testbench"]
            if tb_path.exists():
                return str(tb_path)
        
        return None
    
    async def setup_framework(self):
        """è®¾ç½®TDDæ¡†æ¶"""
        print("ğŸ—ï¸ åˆå§‹åŒ–TDDæ¡†æ¶...")
        
        # 1. åˆ›å»ºæ ‡å‡†ç»„ä»¶
        config = FrameworkConfig.from_env()
        self.coordinator = CentralizedCoordinator(config)
        
        # 2. æ³¨å†Œæ™ºèƒ½ä½“
        self.verilog_agent = RealVerilogDesignAgent(config)
        self.reviewer_agent = RealCodeReviewAgent(config)
        
        self.coordinator.register_agent(self.verilog_agent)
        self.coordinator.register_agent(self.reviewer_agent)
        
        print("   âœ… æ ‡å‡†åè°ƒå™¨å’Œæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
        
        # 3. åˆ›å»ºæµ‹è¯•é©±åŠ¨é…ç½®
        tdd_config = TestDrivenConfig(
            max_iterations=self.experiment_config.get("max_iterations", 5),
            enable_deep_analysis=self.experiment_config.get("deep_analysis", True),
            timeout_per_iteration=self.experiment_config.get("timeout_per_iteration", 300),
            save_iteration_logs=True
        )
        
        # 4. å‡çº§ä¸ºæµ‹è¯•é©±åŠ¨åè°ƒå™¨
        self.tdd_coordinator = create_test_driven_coordinator(self.coordinator, tdd_config)
        
        print("   âœ… æµ‹è¯•é©±åŠ¨æ‰©å±•å¯ç”¨æˆåŠŸ")
        
        return True
    
    async def run_experiment(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„TDDå®éªŒ"""
        experiment_start_time = time.time()
        
        print("=" * 80)
        print(f"ğŸš€ å¼€å§‹ç»Ÿä¸€TDDå®éªŒ: {self.design_type.upper()}")
        print("=" * 80)
        
        try:
            # 1. è®¾ç½®æ¡†æ¶
            await self.setup_framework()
            
            # 2. è·å–è®¾è®¡éœ€æ±‚å’Œæµ‹è¯•å°
            design_requirements = self.get_design_requirements()
            testbench_path = self.get_testbench_path()
            
            print(f"ğŸ“‹ è®¾è®¡éœ€æ±‚å·²å‡†å¤‡")
            if testbench_path:
                print(f"ğŸ¯ æµ‹è¯•å°: {Path(testbench_path).name}")
            else:
                print("ğŸ¯ æµ‹è¯•å°: å°†ç”±AIç”Ÿæˆ")
            
            print(f"âš™ï¸ é…ç½®: {self.config_profile} ({self.experiment_config})")
            
            # 3. æ‰§è¡Œæµ‹è¯•é©±åŠ¨ä»»åŠ¡
            print(f"ğŸ”„ å¯åŠ¨æµ‹è¯•é©±åŠ¨å¼€å‘å¾ªç¯...")
            print(f"   æœ€å¤§è¿­ä»£æ¬¡æ•°: {self.experiment_config.get('max_iterations', 5)}")
            print(f"   æ¯æ¬¡è¿­ä»£è¶…æ—¶: {self.experiment_config.get('timeout_per_iteration', 300)}ç§’")
            
            result = await self.tdd_coordinator.execute_test_driven_task(
                task_description=design_requirements,
                testbench_path=testbench_path
            )
            
            # 4. åˆ†æç»“æœ
            experiment_duration = time.time() - experiment_start_time
            analysis = await self._analyze_experiment_result(result, experiment_duration)
            
            # 5. ä¿å­˜å®éªŒæŠ¥å‘Š
            await self._save_experiment_report(analysis)
            
            return analysis
            
        except Exception as e:
            print(f"âŒ å®éªŒæ‰§è¡Œå¼‚å¸¸: {str(e)}")
            error_result = {
                "success": False,
                "error": str(e),
                "experiment_id": self.experiment_id,
                "duration": time.time() - experiment_start_time
            }
            await self._save_experiment_report(error_result)
            return error_result
    
    async def _analyze_experiment_result(self, result: Dict[str, Any], duration: float) -> Dict[str, Any]:
        """åˆ†æå®éªŒç»“æœ"""
        print("=" * 80)
        print("ğŸ“Š å®éªŒç»“æœåˆ†æ")
        print("=" * 80)
        
        analysis = {
            "experiment_id": self.experiment_id,
            "design_type": self.design_type,
            "config_profile": self.config_profile,
            "success": result.get("success", False),
            "total_duration": duration,
            "timestamp": time.time(),
            "detailed_result": result
        }
        
        if result.get("success"):
            print("ğŸ‰ å®éªŒæˆåŠŸå®Œæˆï¼")
            
            iterations = result.get("total_iterations", 0)
            final_design = result.get("final_design", [])
            
            print(f"   ğŸ“ˆ æ€»è¿­ä»£æ¬¡æ•°: {iterations}")
            print(f"   â±ï¸ æ€»è€—æ—¶: {duration:.2f} ç§’")
            print(f"   ğŸ“ æœ€ç»ˆè®¾è®¡æ–‡ä»¶: {len(final_design)} ä¸ª")
            
            analysis["summary"] = {
                "iterations_used": iterations,
                "efficiency": f"æˆåŠŸç‡: 100%",
                "files_generated": len(final_design),
                "completion_reason": result.get("completion_reason", "tests_passed"),
                "average_iteration_time": duration / max(iterations, 1)
            }
            
            # æ˜¾ç¤ºè®¾è®¡æ–‡ä»¶ä¿¡æ¯
            if final_design:
                print(f"ğŸ“„ ç”Ÿæˆçš„è®¾è®¡æ–‡ä»¶:")
                for i, file_info in enumerate(final_design, 1):
                    if isinstance(file_info, dict):
                        file_path = file_info.get('path', str(file_info))
                    else:
                        file_path = str(file_info)
                    print(f"   {i}. {Path(file_path).name}")
            
        else:
            print("âŒ å®éªŒæœªèƒ½å®Œæˆ")
            
            iterations = result.get("total_iterations", 0)
            error = result.get("error", "æœªçŸ¥é”™è¯¯")
            
            print(f"   ğŸ“ˆ å·²ç”¨è¿­ä»£æ¬¡æ•°: {iterations}")
            print(f"   â±ï¸ æ€»è€—æ—¶: {duration:.2f} ç§’")
            print(f"   âŒ å¤±è´¥åŸå› : {error}")
            
            analysis["summary"] = {
                "iterations_used": iterations,
                "completion_reason": result.get("completion_reason", "failed"),
                "error": error,
                "partial_progress": iterations > 0
            }
            
            # åˆ†æéƒ¨åˆ†ç»“æœ
            partial_results = result.get("partial_results", [])
            if partial_results:
                print(f"ğŸ” è¿­ä»£å†å²åˆ†æ:")
                for i, iteration in enumerate(partial_results, 1):
                    iter_result = iteration.get("result", {})
                    success = iter_result.get("all_tests_passed", False)
                    print(f"   ç¬¬{i}æ¬¡è¿­ä»£: {'âœ… é€šè¿‡' if success else 'âŒ å¤±è´¥'}")
        
        # æ˜¾ç¤ºä¼šè¯ä¿¡æ¯
        session_id = result.get("session_id")
        if session_id:
            session_info = self.tdd_coordinator.get_session_info(session_id)
            if session_info:
                print(f"ğŸ“‹ ä¼šè¯è¯¦æƒ…:")
                print(f"   ä¼šè¯ID: {session_id}")
                print(f"   çŠ¶æ€: {session_info.get('status', 'unknown')}")
        
        print("=" * 80)
        return analysis
    
    async def _save_experiment_report(self, analysis: Dict[str, Any]):
        """ä¿å­˜å®éªŒæŠ¥å‘Š"""
        report_path = project_root / f"unified_tdd_report_{self.experiment_id}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ’¾ å®éªŒæŠ¥å‘Šå·²ä¿å­˜: {report_path.name}")


def create_argument_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='ç»Ÿä¸€æµ‹è¯•é©±åŠ¨å¼€å‘(TDD)æµ‹è¯•å…¥å£',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä½¿ç”¨é¢„å®šä¹‰çš„ALUæ¨¡æ¿ï¼Œæ ‡å‡†é…ç½®
  python unified_tdd_test.py --design alu
  
  # ä½¿ç”¨è¶…å‰è¿›ä½åŠ æ³•å™¨æ¨¡æ¿ï¼Œå¿«é€Ÿæµ‹è¯•
  python unified_tdd_test.py --design adder --config quick
  
  # è‡ªå®šä¹‰è®¾è®¡éœ€æ±‚
  python unified_tdd_test.py --design custom --requirements "è®¾è®¡ä¸€ä¸ªUARTæ¨¡å—" --testbench uart_tb.v
  
  # è°ƒè¯•æ¨¡å¼ï¼Œæ›´å¤šè¿­ä»£æ¬¡æ•°
  python unified_tdd_test.py --design alu --config debug --iterations 12
        """
    )
    
    parser.add_argument('--design', '-d', 
                       choices=['alu', 'counter', 'simple_adder', 'adder', 'custom'],
                       default='simple_adder',
                       help='è®¾è®¡ç±»å‹ (é»˜è®¤: simple_adder)')
    
    parser.add_argument('--config', '-c',
                       choices=['quick', 'standard', 'thorough', 'debug'],
                       default='standard',
                       help='é…ç½®æ¡£æ¡ˆ (é»˜è®¤: standard)')
    
    parser.add_argument('--testbench', '-t',
                       help='æµ‹è¯•å°æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--requirements', '-r',
                       help='è‡ªå®šä¹‰è®¾è®¡éœ€æ±‚æ–‡æœ¬')
    
    parser.add_argument('--iterations', '-i',
                       type=int,
                       help='æœ€å¤§è¿­ä»£æ¬¡æ•° (è¦†ç›–é…ç½®æ¡£æ¡ˆ)')
    
    parser.add_argument('--timeout',
                       type=int,
                       help='æ¯æ¬¡è¿­ä»£è¶…æ—¶ç§’æ•° (è¦†ç›–é…ç½®æ¡£æ¡ˆ)')
    
    parser.add_argument('--no-deep-analysis',
                       action='store_true',
                       help='ç¦ç”¨æ·±åº¦åˆ†æ')
    
    return parser


async def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = create_argument_parser()
    args = parser.parse_args()
    
    print("ğŸ§ª ç»Ÿä¸€æµ‹è¯•é©±åŠ¨å¼€å‘(TDD)æµ‹è¯•å…¥å£")
    print("=" * 50)
    
    # æ„å»ºè‡ªå®šä¹‰é…ç½®
    custom_config = {}
    if args.iterations:
        custom_config['max_iterations'] = args.iterations
    if args.timeout:
        custom_config['timeout_per_iteration'] = args.timeout
    if args.no_deep_analysis:
        custom_config['deep_analysis'] = False
    
    # åˆ›å»ºå¹¶è¿è¡Œå®éªŒ
    experiment = UnifiedTDDTest(
        design_type=args.design,
        config_profile=args.config,
        custom_config=custom_config if custom_config else None,
        testbench_path=args.testbench,
        custom_requirements=args.requirements
    )
    
    try:
        result = await experiment.run_experiment()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print(f"ğŸ å®éªŒå®Œæˆ")
        if result["success"]:
            print("âœ… è®¾è®¡æˆåŠŸå®Œæˆå¹¶é€šè¿‡æ‰€æœ‰æµ‹è¯•ï¼")
            print("ğŸ¯ æµ‹è¯•é©±åŠ¨å¼€å‘åŠŸèƒ½éªŒè¯æˆåŠŸ")
        else:
            print("âŒ è®¾è®¡æœªèƒ½é€šè¿‡æ‰€æœ‰æµ‹è¯•")
            print("ğŸ” å¯ä»¥æŸ¥çœ‹æ—¥å¿—åˆ†æè¿­ä»£æ”¹è¿›è¿‡ç¨‹")
            print(f"ğŸ“Š å®éªŒæŠ¥å‘Š: unified_tdd_report_{experiment.experiment_id}.json")
        
        return result["success"]
        
    except Exception as e:
        print(f"ğŸ’¥ å®éªŒæ‰§è¡Œå¼‚å¸¸: {str(e)}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)