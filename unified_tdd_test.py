#!/usr/bin/env python3
"""
ğŸ§ª ç»Ÿä¸€æµ‹è¯•é©±åŠ¨å¼€å‘(TDD)å…¥å£
==================================================

è¿™ä¸ªè„šæœ¬æä¾›äº†ä¸€ä¸ªå®Œæ•´ã€æ˜“ç”¨çš„TDDæµ‹è¯•å…¥å£ï¼Œæ”¯æŒï¼š
âœ… å¤šè½®è¿­ä»£ç»“æœå®Œæ•´ä¿å­˜
âœ… é…ç½®åŒ–çš„å®éªŒå‚æ•°
âœ… è¯¦ç»†çš„è¿›åº¦è·Ÿè¸ªå’Œç»“æœåˆ†æ
âœ… é€šç”¨çš„æµ‹è¯•å°æ¨¡æ¿æ”¯æŒ

ä½¿ç”¨æ–¹æ³•:
    python unified_tdd_test.py --design alu --iterations 5
    python unified_tdd_test.py --design counter --testbench /path/to/tb.v
    python unified_tdd_test.py --design custom --requirements "è®¾è®¡éœ€æ±‚æ–‡æœ¬"
"""

import asyncio
import sys
import argparse
import json
import time
from pathlib import Path
from typing import Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config.config import FrameworkConfig
from core.enhanced_centralized_coordinator import EnhancedCentralizedCoordinator
from agents.enhanced_real_verilog_agent import EnhancedRealVerilogAgent
from agents.enhanced_real_code_reviewer import EnhancedRealCodeReviewAgent
from extensions import create_test_driven_coordinator, TestDrivenConfig


class UnifiedTDDTest:
    """ç»Ÿä¸€çš„æµ‹è¯•é©±åŠ¨å¼€å‘æµ‹è¯•å…¥å£"""
    
    # é¢„å®šä¹‰çš„è®¾è®¡æ¨¡æ¿
    DESIGN_TEMPLATES = {
        "alu": {
            "description": """
è®¾è®¡ä¸€ä¸ª32ä½ç®—æœ¯é€»è¾‘å•å…ƒ(ALU)ï¼Œæ”¯æŒä»¥ä¸‹æ“ä½œï¼š
- ç®—æœ¯è¿ç®—ï¼šåŠ æ³•(ADD)ã€å‡æ³•(SUB)
- é€»è¾‘è¿ç®—ï¼šä¸(AND)ã€æˆ–(OR)ã€å¼‚æˆ–(XOR)ã€é(NOT)
- æ¯”è¾ƒè¿ç®—ï¼šç­‰äº(EQ)ã€å°äº(LT)ã€å¤§äº(GT)

æ¨¡å—æ¥å£ï¼š
```verilog
module alu_32bit (
    input  [31:0] a,        // æ“ä½œæ•°A
    input  [31:0] b,        // æ“ä½œæ•°B
    input  [3:0]  op,       // æ“ä½œç 
    output [31:0] result,   // ç»“æœ
    output        zero,     // é›¶æ ‡å¿—
    output        overflow  // æº¢å‡ºæ ‡å¿—
);
```
            """,
            "testbench": "test_cases/alu_testbench.v",
            "complexity": "standard"
        },
        
        "counter": {
            "description": """
è®¾è®¡ä¸€ä¸ª8ä½è®¡æ•°å™¨æ¨¡å—counter_8bitï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ¥å£è§„èŒƒå®ç°ï¼š

**å…³é”®è¦æ±‚ - æ¥å£å¿…é¡»å®Œå…¨åŒ¹é…**:
```verilog
module counter_8bit (
    input        clk,       // æ—¶é’Ÿ
    input        rst_n,     // å¼‚æ­¥å¤ä½ï¼ˆä½ç”µå¹³æœ‰æ•ˆï¼‰ - æ³¨æ„æ˜¯rst_nä¸æ˜¯rstï¼
    input        enable,    // è®¡æ•°ä½¿èƒ½
    input        up_down,   // è®¡æ•°æ–¹å‘(1:ä¸Šè®¡æ•°, 0:ä¸‹è®¡æ•°)
    output [7:0] count,     // è®¡æ•°å€¼
    output       overflow   // æº¢å‡ºæ ‡å¿—
);
```

**åŠŸèƒ½è¦æ±‚**:
- å¼‚æ­¥å¤ä½ï¼šå½“rst_nä¸ºä½ç”µå¹³æ—¶ï¼Œcount=0, overflow=0
- åŒæ­¥è®¡æ•°ï¼šåœ¨æ—¶é’Ÿä¸Šå‡æ²¿è¿›è¡Œè®¡æ•°
- ä½¿èƒ½æ§åˆ¶ï¼šenableä¸ºé«˜æ—¶è®¡æ•°ï¼Œä¸ºä½æ—¶ä¿æŒ
- åŒå‘è®¡æ•°ï¼šup_down=1ä¸Šè®¡æ•°ï¼Œup_down=0ä¸‹è®¡æ•°
- æº¢å‡ºæ£€æµ‹ï¼šä¸Šè®¡æ•°255â†’0æ—¶overflow=1ï¼Œä¸‹è®¡æ•°0â†’255æ—¶overflow=1

**è­¦å‘Š**ï¼š
1. ç«¯å£åå¿…é¡»æ˜¯rst_nï¼Œä¸èƒ½æ˜¯rstï¼
2. å¤ä½é€»è¾‘å¿…é¡»æ˜¯negedge rst_nï¼Œä¸èƒ½æ˜¯negedge rstï¼
3. å¤ä½æ¡ä»¶å¿…é¡»æ˜¯if (!rst_n)ï¼Œä¸èƒ½æ˜¯if (!rst)ï¼
            """,
            "testbench": None,  # éœ€è¦ç”¨æˆ·æä¾›æˆ–ç”Ÿæˆ
            "complexity": "simple"
        },
        
        "adder_16bit": {
            "description": """
è®¾è®¡ä¸€ä¸ª16ä½åŠ æ³•å™¨æ¨¡å—adder_16bitï¼Œä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ¥å£è§„èŒƒå®ç°ï¼š

**å…³é”®è¦æ±‚ - æ¥å£å¿…é¡»å®Œå…¨åŒ¹é…**:
```verilog
module adder_16bit (
    input  [15:0] a,        // ç¬¬ä¸€ä¸ª16ä½æ“ä½œæ•°
    input  [15:0] b,        // ç¬¬äºŒä¸ª16ä½æ“ä½œæ•°
    input         cin,      // è¾“å…¥è¿›ä½
    output [15:0] sum,      // 16ä½å’Œè¾“å‡º
    output        cout,     // è¾“å‡ºè¿›ä½
    output        overflow  // æº¢å‡ºæ ‡å¿—ï¼ˆæœ‰ç¬¦å·è¿ç®—ï¼‰
);
```

**åŠŸèƒ½è¦æ±‚**:
1. **åŠ æ³•è¿ç®—**: å®ç°16ä½äºŒè¿›åˆ¶åŠ æ³• sum = a + b + cin
2. **è¿›ä½å¤„ç†**: æ­£ç¡®è®¡ç®—è¾“å‡ºè¿›ä½ cout
3. **æº¢å‡ºæ£€æµ‹**: æ£€æµ‹æœ‰ç¬¦å·æ•°æº¢å‡ºï¼ˆå½“ä¸¤ä¸ªåŒå·æ•°ç›¸åŠ ç»“æœå˜å·æ—¶ï¼‰
4. **å…¨ç»„åˆè¦†ç›–**: æ”¯æŒæ‰€æœ‰å¯èƒ½çš„16ä½è¾“å…¥ç»„åˆ
5. **è¾¹ç•Œå¤„ç†**: æ­£ç¡®å¤„ç†æœ€å¤§å€¼(0xFFFF)å’Œæœ€å°å€¼(0x0000)

**è®¾è®¡è¦æ±‚**:
- ä½¿ç”¨ç»„åˆé€»è¾‘å®ç°
- å¯ä»¥é‡‡ç”¨è¡Œæ³¢è¿›ä½æˆ–è¶…å‰è¿›ä½ç»“æ„
- ç¡®ä¿æ—¶åºæ€§èƒ½è‰¯å¥½
- ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ˜“äºç»¼åˆ

**ä¸¥æ ¼è­¦å‘Š**ï¼š
1. æ¨¡å—åå¿…é¡»æ˜¯adder_16bitï¼Œä¸èƒ½æ˜¯å…¶ä»–åç§°ï¼
2. ç«¯å£åå¿…é¡»å®Œå…¨åŒ¹é…ä¸Šè¿°æ¥å£è§„èŒƒï¼
3. æ‰€æœ‰ç«¯å£ä½å®½å¿…é¡»æ­£ç¡®ï¼ša[15:0], b[15:0], sum[15:0]
4. overflowä¿¡å·å¿…é¡»æ­£ç¡®å®ç°æœ‰ç¬¦å·æº¢å‡ºæ£€æµ‹
5. å¿…é¡»æ˜¯çº¯ç»„åˆé€»è¾‘ï¼Œä¸èƒ½æœ‰æ—¶é’Ÿæˆ–å¤ä½ä¿¡å·

**æµ‹è¯•éªŒè¯è¦æ±‚**:
è®¾è®¡å¿…é¡»é€šè¿‡ä»¥ä¸‹æµ‹è¯•ï¼š
- åŸºæœ¬åŠ æ³•è¿ç®—æµ‹è¯•
- è¿›ä½ä¼ æ’­æµ‹è¯•  
- æº¢å‡ºæ£€æµ‹æµ‹è¯•
- è¾¹ç•Œå€¼æµ‹è¯•ï¼ˆ0x0000, 0xFFFFç­‰ï¼‰
- éšæœºæ•°æ®æµ‹è¯•
            """,
            "testbench": None,  # å°†åˆ›å»ºä¸“é—¨çš„æµ‹è¯•å°
            "complexity": "medium"
        },
        
        "simple_adder": {
            "description": """
è®¾è®¡ä¸€ä¸ªç®€å•çš„8ä½åŠ æ³•å™¨ï¼Œæ”¯æŒåŸºæœ¬çš„äºŒè¿›åˆ¶åŠ æ³•è¿ç®—ã€‚

æ¨¡å—æ¥å£ï¼š
```verilog
module simple_8bit_adder (
    input  [7:0] a,         // ç¬¬ä¸€ä¸ª8ä½æ“ä½œæ•°
    input  [7:0] b,         // ç¬¬äºŒä¸ª8ä½æ“ä½œæ•°
    input        cin,       // è¾“å…¥è¿›ä½
    output [7:0] sum,       // 8ä½å’Œ
    output       cout       // è¾“å‡ºè¿›ä½
);
```

ğŸ¯ åŠŸèƒ½è¦æ±‚ï¼š
1. å®ç°8ä½äºŒè¿›åˆ¶åŠ æ³•è¿ç®—ï¼šsum = a + b + cin
2. æ­£ç¡®è®¡ç®—è¾“å‡ºè¿›ä½ï¼šcout
3. æ”¯æŒæ‰€æœ‰å¯èƒ½çš„è¾“å…¥ç»„åˆï¼ˆ0åˆ°255ï¼‰
4. å¤„ç†è¿›ä½ä¼ æ’­

ğŸ’¡ è®¾è®¡æç¤ºï¼š
- å¯ä»¥ä½¿ç”¨ç®€å•çš„è¡Œæ³¢è¿›ä½é“¾
- ç¡®ä¿æ‰€æœ‰è¾¹ç•Œæ¡ä»¶æ­£ç¡®å¤„ç†
- ä»£ç è¦ç®€æ´æ¸…æ™°ï¼Œæ˜“äºç†è§£
            """,
            "testbench": "test_cases/simple_8bit_adder_tb.v",
            "complexity": "simple"
        },
        
        "adder": {
            "description": """
è®¾è®¡ä¸€ä¸ª16ä½è¶…å‰è¿›ä½åŠ æ³•å™¨ï¼ˆCarry Lookahead Adderï¼‰ï¼Œå®ç°é«˜æ•ˆçš„å¹¶è¡ŒåŠ æ³•è¿ç®—ã€‚

æ¨¡å—æ¥å£ï¼š
```verilog
module carry_lookahead_adder_16bit (
    input  [15:0] a,        // ç¬¬ä¸€ä¸ª16ä½æ“ä½œæ•°
    input  [15:0] b,        // ç¬¬äºŒä¸ª16ä½æ“ä½œæ•°  
    input         cin,      // è¾“å…¥è¿›ä½
    output [15:0] sum,      // 16ä½å’Œ
    output        cout      // è¾“å‡ºè¿›ä½
);
```

ğŸ¯ åŠŸèƒ½è¦æ±‚ï¼š
1. å®ç°16ä½äºŒè¿›åˆ¶åŠ æ³•è¿ç®—ï¼šsum = a + b + cin
2. æ­£ç¡®è®¡ç®—è¾“å‡ºè¿›ä½ï¼šcout
3. ä½¿ç”¨è¶…å‰è¿›ä½æŠ€æœ¯æé«˜æ€§èƒ½ï¼Œè€Œä¸æ˜¯ç®€å•çš„è¡Œæ³¢è¿›ä½
4. æ”¯æŒæ‰€æœ‰å¯èƒ½çš„è¾“å…¥ç»„åˆ

ğŸ“Š è¶…å‰è¿›ä½åŠ æ³•å™¨è®¾è®¡è¦ç‚¹ï¼š
1. **è¿›ä½ç”Ÿæˆ (Generate)**: Gi = Ai & Bi
2. **è¿›ä½ä¼ æ’­ (Propagate)**: Pi = Ai ^ Bi
3. **è¶…å‰è¿›ä½è®¡ç®—**: 
   - C1 = G0 + P0Ã—C0
   - C2 = G1 + P1Ã—G0 + P1Ã—P0Ã—C0
   - C3 = G2 + P2Ã—G1 + P2Ã—P1Ã—G0 + P2Ã—P1Ã—P0Ã—C0
   - ...
4. **æ±‚å’Œ**: Si = Pi ^ Ci
            """,
            "testbench": "test_cases/carry_lookahead_adder_tb.v",
            "complexity": "advanced"
        }
    }
    
    # é¢„å®šä¹‰çš„å®éªŒé…ç½®
    EXPERIMENT_CONFIGS = {
        "quick": {"max_iterations": 3, "timeout_per_iteration": 120, "deep_analysis": False},
        "standard": {"max_iterations": 2, "timeout_per_iteration": 300, "deep_analysis": True},
        "thorough": {"max_iterations": 8, "timeout_per_iteration": 600, "deep_analysis": True},
        "debug": {"max_iterations": 10, "timeout_per_iteration": 900, "deep_analysis": True}
    }
    
    def __init__(self, design_type: str = "alu", 
                 config_profile: str = "standard",
                 custom_config: Dict[str, Any] = None,
                 testbench_path: str = None,
                 custom_requirements: str = None,
                 output_dir: str = None):
        """åˆå§‹åŒ–ç»Ÿä¸€TDDæµ‹è¯•"""
        self.design_type = design_type
        self.config_profile = config_profile
        self.testbench_path = testbench_path
        self.custom_requirements = custom_requirements
        
        # å®éªŒé…ç½®
        base_config = self.EXPERIMENT_CONFIGS.get(config_profile, self.EXPERIMENT_CONFIGS["standard"])
        if custom_config:
            base_config.update(custom_config)
        self.experiment_config = base_config
        
        # ç”Ÿæˆå®éªŒIDå’Œè¾“å‡ºç›®å½•
        self.experiment_id = f"unified_tdd_{design_type}_{int(time.time())}"
        
        # åˆ›å»ºä¸“ç”¨è¾“å‡ºç›®å½•
        if output_dir:
            self.output_dir = Path(output_dir)
        else:
            self.output_dir = project_root / "tdd_experiments" / self.experiment_id
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ§ª ç»Ÿä¸€TDDæµ‹è¯•åˆå§‹åŒ–")
        print(f"   è®¾è®¡ç±»å‹: {design_type}")
        print(f"   é…ç½®æ¡£æ¡ˆ: {config_profile}")
        print(f"   å®éªŒID: {self.experiment_id}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def get_design_requirements(self) -> str:
        """è·å–è®¾è®¡éœ€æ±‚"""
        if self.custom_requirements:
            return self.custom_requirements
        
        template = self.DESIGN_TEMPLATES.get(self.design_type)
        if not template:
            raise ValueError(f"æœªçŸ¥çš„è®¾è®¡ç±»å‹: {self.design_type}")
        
        return template["description"]
    
    def get_testbench_path(self) -> Optional[str]:
        """è·å–æµ‹è¯•å°è·¯å¾„"""
        if self.testbench_path:
            return self.testbench_path
        
        template = self.DESIGN_TEMPLATES.get(self.design_type)
        if template and template.get("testbench"):
            tb_path = project_root / template["testbench"]
            if tb_path.exists():
                return str(tb_path)
        
        return None
    
    async def setup_framework(self):
        """è®¾ç½®TDDæ¡†æ¶"""
        print("ğŸ—ï¸ åˆå§‹åŒ–TDDæ¡†æ¶...")
        
        # 1. åˆ›å»ºå¢å¼ºç‰ˆæ ‡å‡†ç»„ä»¶
        config = FrameworkConfig.from_env()
        self.coordinator = EnhancedCentralizedCoordinator(config)
        
        # 2. æ³¨å†Œå¢å¼ºç‰ˆæ™ºèƒ½ä½“
        self.verilog_agent = EnhancedRealVerilogAgent(config)
        self.reviewer_agent = EnhancedRealCodeReviewAgent(config)
        
        self.coordinator.register_agent(self.verilog_agent)
        self.coordinator.register_agent(self.reviewer_agent)
        
        print("   âœ… å¢å¼ºç‰ˆåè°ƒå™¨å’Œæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
        
        # 3. åˆ›å»ºæµ‹è¯•é©±åŠ¨é…ç½®
        tdd_config = TestDrivenConfig(
            max_iterations=self.experiment_config.get("max_iterations", 2),
            enable_deep_analysis=self.experiment_config.get("deep_analysis", True),
            timeout_per_iteration=self.experiment_config.get("timeout_per_iteration", 300),
            save_iteration_logs=True
        )
        
        # 4. å‡çº§ä¸ºæµ‹è¯•é©±åŠ¨åè°ƒå™¨
        self.tdd_coordinator = create_test_driven_coordinator(self.coordinator, tdd_config)
        
        print("   âœ… æµ‹è¯•é©±åŠ¨æ‰©å±•å¯ç”¨æˆåŠŸ")
        
        return True
    
    async def run_experiment(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„TDDå®éªŒ"""
        experiment_start_time = time.time()
        
        print("=" * 80)
        print(f"ğŸš€ å¼€å§‹ç»Ÿä¸€TDDå®éªŒ: {self.design_type.upper()}")
        print("=" * 80)
        
        try:
            # 1. è®¾ç½®æ¡†æ¶
            await self.setup_framework()
            
            # 2. è·å–è®¾è®¡éœ€æ±‚å’Œæµ‹è¯•å°
            design_requirements = self.get_design_requirements()
            testbench_path = self.get_testbench_path()
            
            print(f"ğŸ“‹ è®¾è®¡éœ€æ±‚å·²å‡†å¤‡")
            if testbench_path:
                print(f"ğŸ¯ æµ‹è¯•å°: {Path(testbench_path).name}")
            else:
                print("ğŸ¯ æµ‹è¯•å°: å°†ç”±AIç”Ÿæˆ")
            
            print(f"âš™ï¸ é…ç½®: {self.config_profile} ({self.experiment_config})")
            
            # 3. æ‰§è¡Œæµ‹è¯•é©±åŠ¨ä»»åŠ¡
            print(f"ğŸ”„ å¯åŠ¨æµ‹è¯•é©±åŠ¨å¼€å‘å¾ªç¯...")
            print(f"   æœ€å¤§è¿­ä»£æ¬¡æ•°: {self.experiment_config.get('max_iterations', 2)}")
            print(f"   æ¯æ¬¡è¿­ä»£è¶…æ—¶: {self.experiment_config.get('timeout_per_iteration', 300)}ç§’")
            
            result = await self.tdd_coordinator.execute_test_driven_task(
                task_description=design_requirements,
                testbench_path=testbench_path
            )
            
            # 4. åˆ†æç»“æœ
            experiment_duration = time.time() - experiment_start_time
            analysis = await self._analyze_experiment_result(result, experiment_duration)
            
            # 5. ä¿å­˜å®éªŒæŠ¥å‘Š
            await self._save_experiment_report(analysis)
            
            return analysis
            
        except Exception as e:
            print(f"âŒ å®éªŒæ‰§è¡Œå¼‚å¸¸: {str(e)}")
            error_result = {
                "success": False,
                "error": str(e),
                "experiment_id": self.experiment_id,
                "duration": time.time() - experiment_start_time
            }
            await self._save_experiment_report(error_result)
            return error_result
    
    async def _analyze_experiment_result(self, result: Dict[str, Any], duration: float) -> Dict[str, Any]:
        """åˆ†æå®éªŒç»“æœ"""
        print("=" * 80)
        print("ğŸ“Š å®éªŒç»“æœåˆ†æ")
        print("=" * 80)
        
        analysis = {
            "experiment_id": self.experiment_id,
            "design_type": self.design_type,
            "config_profile": self.config_profile,
            "success": result.get("success", False),
            "total_duration": duration,
            "timestamp": time.time(),
            "detailed_result": result
        }
        
        if result.get("success"):
            print("ğŸ‰ å®éªŒæˆåŠŸå®Œæˆï¼")
            
            iterations = result.get("total_iterations", 0)
            final_design = result.get("final_design", [])
            
            print(f"   ğŸ“ˆ æ€»è¿­ä»£æ¬¡æ•°: {iterations}")
            print(f"   â±ï¸ æ€»è€—æ—¶: {duration:.2f} ç§’")
            print(f"   ğŸ“ æœ€ç»ˆè®¾è®¡æ–‡ä»¶: {len(final_design)} ä¸ª")
            
            analysis["summary"] = {
                "iterations_used": iterations,
                "efficiency": f"æˆåŠŸç‡: 100%",
                "files_generated": len(final_design),
                "completion_reason": result.get("completion_reason", "tests_passed"),
                "average_iteration_time": duration / max(iterations, 1)
            }
            
            # æ˜¾ç¤ºè®¾è®¡æ–‡ä»¶ä¿¡æ¯
            if final_design:
                print(f"ğŸ“„ ç”Ÿæˆçš„è®¾è®¡æ–‡ä»¶:")
                for i, file_info in enumerate(final_design, 1):
                    if isinstance(file_info, dict):
                        file_path = file_info.get('path', str(file_info))
                    else:
                        file_path = str(file_info)
                    print(f"   {i}. {Path(file_path).name}")
            
        else:
            print("âŒ å®éªŒæœªèƒ½å®Œæˆ")
            
            iterations = result.get("total_iterations", 0)
            error = result.get("error", "æœªçŸ¥é”™è¯¯")
            
            print(f"   ğŸ“ˆ å·²ç”¨è¿­ä»£æ¬¡æ•°: {iterations}")
            print(f"   â±ï¸ æ€»è€—æ—¶: {duration:.2f} ç§’")
            print(f"   âŒ å¤±è´¥åŸå› : {error}")
            
            analysis["summary"] = {
                "iterations_used": iterations,
                "completion_reason": result.get("completion_reason", "failed"),
                "error": error,
                "partial_progress": iterations > 0
            }
            
            # åˆ†æéƒ¨åˆ†ç»“æœ
            partial_results = result.get("partial_results", [])
            if partial_results:
                print(f"ğŸ” è¿­ä»£å†å²åˆ†æ:")
                for i, iteration in enumerate(partial_results, 1):
                    iter_result = iteration.get("result", {})
                    success = iter_result.get("all_tests_passed", False)
                    print(f"   ç¬¬{i}æ¬¡è¿­ä»£: {'âœ… é€šè¿‡' if success else 'âŒ å¤±è´¥'}")
        
        # æ˜¾ç¤ºä¼šè¯ä¿¡æ¯
        session_id = result.get("session_id")
        if session_id:
            session_info = self.tdd_coordinator.get_session_info(session_id)
            if session_info:
                print(f"ğŸ“‹ ä¼šè¯è¯¦æƒ…:")
                print(f"   ä¼šè¯ID: {session_id}")
                print(f"   çŠ¶æ€: {session_info.get('status', 'unknown')}")
        
        print("=" * 80)
        
        # å¤åˆ¶å…³é”®æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
        await self._copy_experiment_files(result)
        
        return analysis
    
    async def _save_experiment_report(self, analysis: Dict[str, Any]):
        """ä¿å­˜å®éªŒæŠ¥å‘Šåˆ°ä¸“ç”¨ç›®å½•"""
        # ä¿å­˜è¯¦ç»†çš„å®éªŒæŠ¥å‘Š
        report_path = self.output_dir / "experiment_report.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜ç®€åŒ–çš„ç»“æœæ‘˜è¦
        summary_path = self.output_dir / "experiment_summary.txt"
        await self._save_text_summary(analysis, summary_path)
        
        print(f"ğŸ’¾ å®éªŒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.output_dir}")
        print(f"   ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_path.name}")
        print(f"   ğŸ“‹ ç»“æœæ‘˜è¦: {summary_path.name}")
    
    async def _save_text_summary(self, analysis: Dict[str, Any], summary_path: Path):
        """ä¿å­˜äººç±»å¯è¯»çš„æ–‡æœ¬æ‘˜è¦"""
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("ğŸ§ª TDDå®éªŒç»“æœæ‘˜è¦\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"å®éªŒID: {analysis['experiment_id']}\n")
            f.write(f"è®¾è®¡ç±»å‹: {analysis['design_type']}\n")
            f.write(f"é…ç½®æ¡£æ¡ˆ: {analysis['config_profile']}\n")
            f.write(f"å®éªŒçŠ¶æ€: {'âœ… æˆåŠŸ' if analysis['success'] else 'âŒ å¤±è´¥'}\n")
            f.write(f"æ€»è€—æ—¶: {analysis['total_duration']:.2f} ç§’\n")
            f.write(f"æ—¶é—´æˆ³: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(analysis['timestamp']))}\n\n")
            
            if analysis.get('success'):
                summary = analysis.get('summary', {})
                f.write("ğŸ“Š æˆåŠŸç»Ÿè®¡:\n")
                f.write(f"- è¿­ä»£æ¬¡æ•°: {summary.get('iterations_used', 0)}\n")
                f.write(f"- ç”Ÿæˆæ–‡ä»¶: {summary.get('files_generated', 0)} ä¸ª\n")
                f.write(f"- å®ŒæˆåŸå› : {summary.get('completion_reason', 'tests_passed')}\n")
                f.write(f"- å¹³å‡è¿­ä»£æ—¶é—´: {summary.get('average_iteration_time', 0):.2f} ç§’\n\n")
                
                # æµ‹è¯•ç»“æœ
                test_results = analysis.get('detailed_result', {}).get('test_results', {})
                if test_results:
                    f.write("ğŸ§ª æµ‹è¯•ç»“æœ:\n")
                    f.write(f"- æµ‹è¯•çŠ¶æ€: {'âœ… é€šè¿‡' if test_results.get('all_tests_passed') else 'âŒ å¤±è´¥'}\n")
                    f.write(f"- æµ‹è¯•é˜¶æ®µ: {test_results.get('stage', 'unknown')}\n")
                    f.write(f"- è¿”å›ç : {test_results.get('return_code', -1)}\n")
                    if test_results.get('test_summary'):
                        f.write(f"- æµ‹è¯•æ‘˜è¦: {test_results['test_summary']}\n")
            else:
                f.write("âŒ å¤±è´¥ä¿¡æ¯:\n")
                error = analysis.get('error', 'æœªçŸ¥é”™è¯¯')
                f.write(f"- é”™è¯¯: {error}\n")
    
    async def _copy_experiment_files(self, result: Dict[str, Any]):
        """å¤åˆ¶å®éªŒç”Ÿæˆçš„æ–‡ä»¶åˆ°è¾“å‡ºç›®å½•"""
        try:
            # åˆ›å»ºå­ç›®å½•
            artifacts_dir = self.output_dir / "artifacts"
            logs_dir = self.output_dir / "logs"
            artifacts_dir.mkdir(exist_ok=True)
            logs_dir.mkdir(exist_ok=True)
            
            # å¤åˆ¶è®¾è®¡æ–‡ä»¶
            final_design = result.get('final_design', [])
            for file_ref in final_design:
                if isinstance(file_ref, str):
                    # ä»å­—ç¬¦ä¸²ä¸­æå–æ–‡ä»¶è·¯å¾„
                    if "file_path='" in file_ref:
                        start = file_ref.find("file_path='") + len("file_path='")
                        end = file_ref.find("'", start)
                        file_path = file_ref[start:end]
                    else:
                        file_path = file_ref
                else:
                    file_path = str(file_ref)
                
                source_path = Path(file_path)
                if source_path.exists():
                    dest_path = artifacts_dir / source_path.name
                    import shutil
                    shutil.copy2(source_path, dest_path)
                    print(f"   ğŸ“ å¤åˆ¶æ–‡ä»¶: {source_path.name}")
            
            # å¤åˆ¶æµ‹è¯•ç»“æœä¸­çš„è®¾è®¡æ–‡ä»¶
            test_results = result.get('test_results', {})
            design_files = test_results.get('design_files', [])
            for file_path in design_files:
                source_path = Path(file_path)
                if source_path.exists():
                    dest_path = artifacts_dir / source_path.name
                    import shutil
                    shutil.copy2(source_path, dest_path)
                    print(f"   ğŸ“ å¤åˆ¶è®¾è®¡æ–‡ä»¶: {source_path.name}")
            
            # ä¿å­˜ä»¿çœŸè¾“å‡º
            if test_results.get('simulation_stdout'):
                sim_output_path = logs_dir / "simulation_output.log"
                with open(sim_output_path, 'w', encoding='utf-8') as f:
                    f.write(test_results['simulation_stdout'])
                print(f"   ğŸ“ ä¿å­˜ä»¿çœŸè¾“å‡º: {sim_output_path.name}")
            
            # ä¿å­˜ç¼–è¯‘è¾“å‡º
            if test_results.get('compile_stdout'):
                compile_output_path = logs_dir / "compile_output.log"
                with open(compile_output_path, 'w', encoding='utf-8') as f:
                    f.write(test_results['compile_stdout'])
                print(f"   ğŸ“ ä¿å­˜ç¼–è¯‘è¾“å‡º: {compile_output_path.name}")
                
        except Exception as e:
            print(f"âš ï¸ å¤åˆ¶æ–‡ä»¶æ—¶å‡ºç°è­¦å‘Š: {str(e)}")


def create_argument_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='ç»Ÿä¸€æµ‹è¯•é©±åŠ¨å¼€å‘(TDD)æµ‹è¯•å…¥å£',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä½¿ç”¨é¢„å®šä¹‰çš„ALUæ¨¡æ¿ï¼Œæ ‡å‡†é…ç½®
  python unified_tdd_test.py --design alu
  
  # ä½¿ç”¨è¶…å‰è¿›ä½åŠ æ³•å™¨æ¨¡æ¿ï¼Œå¿«é€Ÿæµ‹è¯•
  python unified_tdd_test.py --design adder --config quick
  
  # è‡ªå®šä¹‰è®¾è®¡éœ€æ±‚
  python unified_tdd_test.py --design custom --requirements "è®¾è®¡ä¸€ä¸ªUARTæ¨¡å—" --testbench uart_tb.v
  
  # è°ƒè¯•æ¨¡å¼ï¼Œæ›´å¤šè¿­ä»£æ¬¡æ•°
  python unified_tdd_test.py --design alu --config debug --iterations 12
        """
    )
    
    parser.add_argument('--design', '-d', 
                       choices=['alu', 'counter', 'adder_16bit', 'simple_adder', 'adder', 'custom'],
                       default='simple_adder',
                       help='è®¾è®¡ç±»å‹ (é»˜è®¤: simple_adder)')
    
    parser.add_argument('--config', '-c',
                       choices=['quick', 'standard', 'thorough', 'debug'],
                       default='standard',
                       help='é…ç½®æ¡£æ¡ˆ (é»˜è®¤: standard)')
    
    parser.add_argument('--testbench', '-t',
                       help='æµ‹è¯•å°æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--requirements', '-r',
                       help='è‡ªå®šä¹‰è®¾è®¡éœ€æ±‚æ–‡æœ¬')
    
    parser.add_argument('--iterations', '-i',
                       type=int,
                       help='æœ€å¤§è¿­ä»£æ¬¡æ•° (è¦†ç›–é…ç½®æ¡£æ¡ˆ)')
    
    parser.add_argument('--timeout',
                       type=int,
                       help='æ¯æ¬¡è¿­ä»£è¶…æ—¶ç§’æ•° (è¦†ç›–é…ç½®æ¡£æ¡ˆ)')
    
    parser.add_argument('--no-deep-analysis',
                       action='store_true',
                       help='ç¦ç”¨æ·±åº¦åˆ†æ')
    
    parser.add_argument('--output-dir', '-o',
                       help='å®éªŒè¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: tdd_experiments/å®éªŒID)')
    
    return parser


async def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = create_argument_parser()
    args = parser.parse_args()
    
    print("ğŸ§ª ç»Ÿä¸€æµ‹è¯•é©±åŠ¨å¼€å‘(TDD)æµ‹è¯•å…¥å£")
    print("=" * 50)
    
    # æ„å»ºè‡ªå®šä¹‰é…ç½®
    custom_config = {}
    if args.iterations:
        custom_config['max_iterations'] = args.iterations
    if args.timeout:
        custom_config['timeout_per_iteration'] = args.timeout
    if args.no_deep_analysis:
        custom_config['deep_analysis'] = False
    
    # åˆ›å»ºå¹¶è¿è¡Œå®éªŒ
    experiment = UnifiedTDDTest(
        design_type=args.design,
        config_profile=args.config,
        custom_config=custom_config if custom_config else None,
        testbench_path=args.testbench,
        custom_requirements=args.requirements,
        output_dir=args.output_dir
    )
    
    try:
        result = await experiment.run_experiment()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print(f"ğŸ å®éªŒå®Œæˆ")
        if result["success"]:
            print("âœ… è®¾è®¡æˆåŠŸå®Œæˆå¹¶é€šè¿‡æ‰€æœ‰æµ‹è¯•ï¼")
            print("ğŸ¯ æµ‹è¯•é©±åŠ¨å¼€å‘åŠŸèƒ½éªŒè¯æˆåŠŸ")
        else:
            print("âŒ è®¾è®¡æœªèƒ½é€šè¿‡æ‰€æœ‰æµ‹è¯•")
            print("ğŸ” å¯ä»¥æŸ¥çœ‹æ—¥å¿—åˆ†æè¿­ä»£æ”¹è¿›è¿‡ç¨‹")
            print(f"ğŸ“Š å®éªŒæŠ¥å‘Š: unified_tdd_report_{experiment.experiment_id}.json")
        
        return result["success"]
        
    except Exception as e:
        print(f"ğŸ’¥ å®éªŒæ‰§è¡Œå¼‚å¸¸: {str(e)}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)