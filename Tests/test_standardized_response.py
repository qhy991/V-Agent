#!/usr/bin/env python3
"""
æµ‹è¯•æ ‡å‡†åŒ–å“åº”æ ¼å¼ç³»ç»Ÿ

Test Standardized Response Format System
"""

import asyncio
import json
import logging
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from core.response_format import (
    StandardizedResponse, ResponseBuilder, ResponseFormat, 
    TaskStatus, ResponseType, QualityMetrics, FileReference,
    create_success_response, create_error_response, create_progress_response
)
from core.response_parser import ResponseParser, ResponseParseError
from core.llm_coordinator_agent import LLMCoordinatorAgent
from config.config import FrameworkConfig

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_response_creation():
    """æµ‹è¯•å“åº”åˆ›å»º"""
    print("ğŸ§ª æµ‹è¯•å“åº”åˆ›å»ºåŠŸèƒ½...")
    
    # 1. æµ‹è¯•ResponseBuilder
    builder = ResponseBuilder("TestAgent", "test_agent_01", "task_123")
    
    builder.add_generated_file(
        "/output/test.v", "verilog", "Generated test Verilog module"
    ).add_issue(
        "warning", "medium", "Clock signal might have setup time violations"
    ).add_next_step(
        "Run timing analysis"
    ).add_metadata("test_key", "test_value")
    
    quality_metrics = QualityMetrics(
        overall_score=0.85,
        syntax_score=0.95,
        functionality_score=0.80,
        test_coverage=0.75,
        documentation_quality=0.90
    )
    
    response = builder.build(
        response_type=ResponseType.TASK_COMPLETION,
        status=TaskStatus.SUCCESS,
        message="Successfully generated Verilog module with comprehensive testing",
        completion_percentage=100.0,
        quality_metrics=quality_metrics
    )
    
    # 2. æµ‹è¯•ä¸åŒæ ¼å¼çš„è¾“å‡º
    print("\nğŸ“‹ JSONæ ¼å¼è¾“å‡º:")
    json_output = response.format_response(ResponseFormat.JSON)
    print(json_output[:500] + "..." if len(json_output) > 500 else json_output)
    
    print("\nğŸ“‹ XMLæ ¼å¼è¾“å‡º:")
    xml_output = response.format_response(ResponseFormat.XML)
    print(xml_output[:500] + "..." if len(xml_output) > 500 else xml_output)
    
    print("\nğŸ“‹ Markdownæ ¼å¼è¾“å‡º:")
    md_output = response.format_response(ResponseFormat.MARKDOWN)
    print(md_output[:800] + "..." if len(md_output) > 800 else md_output)
    
    print("âœ… å“åº”åˆ›å»ºæµ‹è¯•é€šè¿‡")
    return json_output, xml_output, md_output

def test_response_parsing():
    """æµ‹è¯•å“åº”è§£æ"""
    print("\nğŸ§ª æµ‹è¯•å“åº”è§£æåŠŸèƒ½...")
    
    parser = ResponseParser()
    
    # 1. æµ‹è¯•JSONè§£æ
    json_response = {
        "agent_name": "VerilogDesignAgent",
        "agent_id": "verilog_designer_01",
        "response_type": "task_completion",
        "task_id": "task_456",
        "timestamp": "2024-01-01T10:00:00",
        "status": "success",
        "completion_percentage": 100.0,
        "message": "Successfully designed ALU module",
        "generated_files": [
            {
                "path": "/output/alu.v",
                "file_type": "verilog",
                "description": "32-bit ALU module"
            }
        ],
        "modified_files": [],
        "reference_files": [],
        "issues": [],
        "next_steps": ["Generate testbench"],
        "metadata": {"complexity": "medium"}
    }
    
    try:
        parsed_response = parser.parse_response(json.dumps(json_response), ResponseFormat.JSON)
        print(f"âœ… JSONè§£ææˆåŠŸ: {parsed_response.agent_name}")
        print(f"   çŠ¶æ€: {parsed_response.status.value}")
        print(f"   å®Œæˆåº¦: {parsed_response.completion_percentage}%")
        print(f"   ç”Ÿæˆæ–‡ä»¶: {len(parsed_response.generated_files)}")
    except ResponseParseError as e:
        print(f"âŒ JSONè§£æå¤±è´¥: {str(e)}")
    
    # 2. æµ‹è¯•Markdownè§£æ
    markdown_response = """# Agent Response: TestbenchAgent

## ğŸ“‹ Basic Information
- **Agent**: TestbenchAgent (`testbench_gen_01`)
- **Task ID**: `task_789`
- **Status**: success
- **Progress**: 100.0%
- **Timestamp**: 2024-01-01T11:00:00

## ğŸ’¬ Message
Successfully generated comprehensive testbench for ALU module

## ğŸ“ Files
### Generated Files
- **output/alu_tb.v** (testbench): Comprehensive ALU testbench with edge cases

## ğŸš€ Next Steps
1. Run simulation with generated testbench
2. Analyze coverage report
"""
    
    try:
        parsed_md = parser.parse_response(markdown_response, ResponseFormat.MARKDOWN)
        print(f"âœ… Markdownè§£ææˆåŠŸ: {parsed_md.agent_name}")
        print(f"   çŠ¶æ€: {parsed_md.status.value}")
        print(f"   ä¸‹ä¸€æ­¥æ•°é‡: {len(parsed_md.next_steps)}")
    except ResponseParseError as e:
        print(f"âŒ Markdownè§£æå¤±è´¥: {str(e)}")
    
    # 3. æµ‹è¯•è‡ªåŠ¨æ ¼å¼æ£€æµ‹
    try:
        auto_parsed = parser.parse_response(json.dumps(json_response))
        print(f"âœ… è‡ªåŠ¨æ£€æµ‹è§£ææˆåŠŸ: {auto_parsed.agent_name}")
    except ResponseParseError as e:
        print(f"âŒ è‡ªåŠ¨æ£€æµ‹è§£æå¤±è´¥: {str(e)}")
    
    print("âœ… å“åº”è§£ææµ‹è¯•é€šè¿‡")
    return parsed_response

def test_validation():
    """æµ‹è¯•å“åº”éªŒè¯"""
    print("\nğŸ§ª æµ‹è¯•å“åº”éªŒè¯åŠŸèƒ½...")
    
    parser = ResponseParser()
    
    # 1. åˆ›å»ºæœ‰æ•ˆå“åº”
    valid_response = StandardizedResponse(
        agent_name="ValidAgent",
        agent_id="valid_01",
        response_type=ResponseType.TASK_COMPLETION,
        task_id="valid_task",
        timestamp="2024-01-01T12:00:00",
        status=TaskStatus.SUCCESS,
        completion_percentage=100.0,
        message="Task completed successfully",
        generated_files=[],
        modified_files=[],
        reference_files=[],
        issues=[]
    )
    
    validation_errors = parser.validate_response(valid_response)
    if not validation_errors:
        print("âœ… æœ‰æ•ˆå“åº”éªŒè¯é€šè¿‡")
    else:
        print(f"âŒ æœ‰æ•ˆå“åº”éªŒè¯å¤±è´¥: {validation_errors}")
    
    # 2. åˆ›å»ºæ— æ•ˆå“åº”
    invalid_response = StandardizedResponse(
        agent_name="",  # ç©ºåç§°
        agent_id="",    # ç©ºID
        response_type=ResponseType.TASK_COMPLETION,
        task_id="",     # ç©ºä»»åŠ¡ID
        timestamp="2024-01-01T12:00:00",
        status=TaskStatus.SUCCESS,
        completion_percentage=150.0,  # è¶…å‡ºèŒƒå›´
        message="",     # ç©ºæ¶ˆæ¯
        generated_files=[],
        modified_files=[],
        reference_files=[],
        issues=[]
    )
    
    validation_errors = parser.validate_response(invalid_response)
    if validation_errors:
        print(f"âœ… æ— æ•ˆå“åº”éªŒè¯æ­£ç¡®è¯†åˆ«é”™è¯¯: {len(validation_errors)} ä¸ªé—®é¢˜")
        for error in validation_errors:
            print(f"   - {error}")
    else:
        print("âŒ æ— æ•ˆå“åº”éªŒè¯å¤±è´¥ï¼šåº”è¯¥æ£€æµ‹å‡ºé”™è¯¯")
    
    print("âœ… å“åº”éªŒè¯æµ‹è¯•é€šè¿‡")

async def test_coordinator_integration():
    """æµ‹è¯•åè°ƒè€…é›†æˆ"""
    print("\nğŸ§ª æµ‹è¯•åè°ƒè€…é›†æˆåŠŸèƒ½...")
    
    try:
        # åˆ›å»ºé…ç½®
        config = FrameworkConfig.from_env()
        
        # åˆ›å»ºåè°ƒè€…
        coordinator = LLMCoordinatorAgent(config)
        
        # æµ‹è¯•è®¾ç½®å“åº”æ ¼å¼
        coordinator.set_preferred_response_format(ResponseFormat.JSON)
        
        # æµ‹è¯•è·å–æ ¼å¼è¯´æ˜
        instructions = coordinator.get_response_format_instructions()
        print("âœ… æ ¼å¼è¯´æ˜è·å–æˆåŠŸ")
        print(f"   è¯´æ˜é•¿åº¦: {len(instructions)} å­—ç¬¦")
        
        # æµ‹è¯•å“åº”å¤„ç†
        mock_raw_response = {
            "success": True,
            "message": "Test task completed",
            "generated_files": ["/output/test.v"],
            "agent_name": "TestAgent"
        }
        
        processed_response = await coordinator._process_agent_response(
            agent_id="test_agent",
            raw_response=mock_raw_response,
            task_id="test_task"
        )
        
        print("âœ… å“åº”å¤„ç†æˆåŠŸ")
        print(f"   å¤„ç†åçŠ¶æ€: {processed_response.get('status')}")
        print(f"   æ–‡ä»¶å¼•ç”¨æ•°é‡: {len(processed_response.get('file_references', []))}")
        
        # æµ‹è¯•æ ‡å‡†åŒ–å“åº”å¤„ç†
        standardized_raw = {
            "standardized_response": json.dumps({
                "agent_name": "StandardizedAgent",
                "agent_id": "std_01",
                "response_type": "task_completion",
                "task_id": "std_task",
                "timestamp": "2024-01-01T13:00:00",
                "status": "success",
                "completion_percentage": 100.0,
                "message": "Standardized response test",
                "generated_files": [],
                "modified_files": [],
                "reference_files": [],
                "issues": [],
                "next_steps": [],
                "metadata": {}
            }, ensure_ascii=False)
        }
        
        std_processed = await coordinator._process_agent_response(
            agent_id="std_agent",
            raw_response=standardized_raw,
            task_id="std_task"
        )
        
        print("âœ… æ ‡å‡†åŒ–å“åº”å¤„ç†æˆåŠŸ")
        print(f"   Agentåç§°: {std_processed.get('agent_name')}")
        print(f"   å“åº”ç±»å‹: {std_processed.get('response_type')}")
        
    except Exception as e:
        print(f"âŒ åè°ƒè€…é›†æˆæµ‹è¯•å¤±è´¥: {str(e)}")
        return False
    
    print("âœ… åè°ƒè€…é›†æˆæµ‹è¯•é€šè¿‡")
    return True

async def test_base_agent_integration():
    """æµ‹è¯•BaseAgenté›†æˆ"""
    print("\nğŸ§ª æµ‹è¯•BaseAgenté›†æˆåŠŸèƒ½...")
    
    # ç”±äºBaseAgentæ˜¯æŠ½è±¡ç±»ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„å®ç°
    from core.base_agent import BaseAgent
    from core.enums import AgentCapability
    
    class TestAgent(BaseAgent):
        def get_capabilities(self):
            return {AgentCapability.CODE_GENERATION}
        
        def get_specialty_description(self):
            return "Test agent for response format testing"
        
        async def execute_enhanced_task(self, enhanced_prompt, original_message, file_contents):
            return {"success": True, "message": "Test task completed"}
    
    try:
        agent = TestAgent("test_agent", "test", {AgentCapability.CODE_GENERATION})
        
        # æµ‹è¯•å“åº”æ„å»ºå™¨åˆ›å»º
        builder = agent.create_response_builder("test_task")
        print("âœ… å“åº”æ„å»ºå™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ ¼å¼åŒ–å“åº”åˆ›å»º
        success_response = agent.create_success_response_formatted(
            task_id="test_task",
            message="Test success message",
            generated_files=["/output/test.v"],
            format_type=ResponseFormat.JSON
        )
        
        print("âœ… æˆåŠŸå“åº”åˆ›å»ºæˆåŠŸ")
        print(f"   å“åº”é•¿åº¦: {len(success_response)} å­—ç¬¦")
        
        # æµ‹è¯•é”™è¯¯å“åº”åˆ›å»º
        error_response = agent.create_error_response_formatted(
            task_id="test_task", 
            error_message="Test error",
            format_type=ResponseFormat.MARKDOWN
        )
        
        print("âœ… é”™è¯¯å“åº”åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•è¿›åº¦å“åº”åˆ›å»º
        progress_response = agent.create_progress_response_formatted(
            task_id="test_task",
            message="Test in progress",
            completion_percentage=50.0,
            next_steps=["Continue testing", "Validate results"]
        )
        
        print("âœ… è¿›åº¦å“åº”åˆ›å»ºæˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ BaseAgenté›†æˆæµ‹è¯•å¤±è´¥: {str(e)}")
        return False
    
    print("âœ… BaseAgenté›†æˆæµ‹è¯•é€šè¿‡")
    return True

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ ‡å‡†åŒ–å“åº”æ ¼å¼ç³»ç»Ÿ")
    print("=" * 60)
    
    success_count = 0
    total_tests = 5
    
    try:
        # 1. å“åº”åˆ›å»ºæµ‹è¯•
        test_response_creation()
        success_count += 1
        
        # 2. å“åº”è§£ææµ‹è¯•
        test_response_parsing()
        success_count += 1
        
        # 3. å“åº”éªŒè¯æµ‹è¯•
        test_validation()
        success_count += 1
        
        # 4. åè°ƒè€…é›†æˆæµ‹è¯•
        if await test_coordinator_integration():
            success_count += 1
        
        # 5. BaseAgenté›†æˆæµ‹è¯•
        if await test_base_agent_integration():
            success_count += 1
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {success_count}/{total_tests} é€šè¿‡")
    
    if success_count == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ ‡å‡†åŒ–å“åº”æ ¼å¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å…·ä½“é”™è¯¯ä¿¡æ¯")
    
    return success_count == total_tests

if __name__ == "__main__":
    asyncio.run(main())