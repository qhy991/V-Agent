#!/usr/bin/env python3
"""
LLMé€šä¿¡æ¨¡å—é›†æˆæµ‹è¯•
æµ‹è¯•åœ¨å®é™…åœºæ™¯ä¸­çš„åŠŸèƒ½è¡¨ç°
"""

import asyncio
import sys
import time
from typing import List, Dict

def test_basic_functionality():
    """æµ‹è¯•åŸºç¡€åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºç¡€åŠŸèƒ½...")
    
    try:
        from core.llm_communication import UnifiedLLMClientManager, LLMCallContext, CallType, PromptTemplateEngine
        from core.schema_system.framework_config import FrameworkConfig
        from core.schema_system.enums import AgentCapability
        
        # åˆ›å»ºé…ç½®
        config = FrameworkConfig.from_env()
        
        # åˆ›å»ºæ¨¡æ¿å¼•æ“
        template_engine = PromptTemplateEngine()
        
        # åˆ›å»ºå®¢æˆ·ç«¯ç®¡ç†å™¨
        llm_manager = UnifiedLLMClientManager(
            agent_id="test_verilog_agent",
            role="verilog_designer",
            config=config
        )
        
        print("âœ… åŸºç¡€ç»„ä»¶åˆ›å»ºæˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_template_generation():
    """æµ‹è¯•æ¨¡æ¿ç”ŸæˆåŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡æ¿ç”ŸæˆåŠŸèƒ½...")
    
    try:
        from core.llm_communication import PromptTemplateEngine, CallType
        from core.schema_system.enums import AgentCapability
        
        template_engine = PromptTemplateEngine()
        
        # æµ‹è¯•Verilogè®¾è®¡å¸ˆæ¨¡æ¿
        verilog_prompt = await template_engine.build_system_prompt(
            role="verilog_designer",
            call_type=CallType.FUNCTION_CALLING,
            agent_id="test_verilog_agent",
            capabilities={AgentCapability.CODE_GENERATION, AgentCapability.MODULE_DESIGN}
        )
        
        print(f"âœ… Verilogè®¾è®¡å¸ˆæ¨¡æ¿ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(verilog_prompt)} å­—ç¬¦")
        print(f"ğŸ“ æ¨¡æ¿é¢„è§ˆ: {verilog_prompt[:200]}...")
        
        # æµ‹è¯•ä»£ç å®¡æŸ¥å¸ˆæ¨¡æ¿
        reviewer_prompt = await template_engine.build_system_prompt(
            role="code_reviewer",
            call_type=CallType.FUNCTION_CALLING,
            agent_id="test_reviewer_agent",
            capabilities={AgentCapability.CODE_REVIEW, AgentCapability.TEST_GENERATION}
        )
        
        print(f"âœ… ä»£ç å®¡æŸ¥å¸ˆæ¨¡æ¿ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(reviewer_prompt)} å­—ç¬¦")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡æ¿ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_conversation_handling():
    """æµ‹è¯•å¯¹è¯å¤„ç†åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•å¯¹è¯å¤„ç†åŠŸèƒ½...")
    
    try:
        from core.llm_communication import UnifiedLLMClientManager, LLMCallContext, CallType
        from core.schema_system.framework_config import FrameworkConfig
        
        config = FrameworkConfig.from_env()
        llm_manager = UnifiedLLMClientManager(
            agent_id="test_conversation_agent",
            role="verilog_designer",
            config=config
        )
        
        # æ¨¡æ‹Ÿå¯¹è¯å†å²
        conversation = [
            {"role": "user", "content": "è¯·è®¾è®¡ä¸€ä¸ª4ä½è®¡æ•°å™¨æ¨¡å—"},
            {"role": "assistant", "content": "æˆ‘æ¥ä¸ºæ‚¨è®¾è®¡ä¸€ä¸ª4ä½è®¡æ•°å™¨æ¨¡å—ã€‚"},
            {"role": "user", "content": "éœ€è¦æ·»åŠ å¤ä½åŠŸèƒ½"}
        ]
        
        # æµ‹è¯•å¯¹è¯æ„å»º
        user_message = llm_manager._build_user_message(conversation)
        print(f"âœ… å¯¹è¯æ„å»ºæˆåŠŸï¼Œç”¨æˆ·æ¶ˆæ¯é•¿åº¦: {len(user_message)} å­—ç¬¦")
        print(f"ğŸ“ ç”¨æˆ·æ¶ˆæ¯é¢„è§ˆ: {user_message[:150]}...")
        
        # æµ‹è¯•æ€§èƒ½ç»Ÿè®¡
        stats = llm_manager.get_performance_stats()
        print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡: {stats}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¯¹è¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•é”™è¯¯å¤„ç†åŠŸèƒ½...")
    
    try:
        from core.llm_communication import UnifiedLLMClientManager, LLMCallContext, CallType
        from core.schema_system.framework_config import FrameworkConfig
        
        config = FrameworkConfig.from_env()
        
        # æµ‹è¯•æ— æ•ˆå‚æ•°å¤„ç†
        try:
            llm_manager = UnifiedLLMClientManager(
                agent_id="",  # ç©ºagent_id
                role="invalid_role",
                config=config
            )
            print("âš ï¸ åº”è¯¥æ£€æµ‹åˆ°æ— æ•ˆå‚æ•°")
        except Exception as e:
            print(f"âœ… æ­£ç¡®æ•è·æ— æ•ˆå‚æ•°é”™è¯¯: {type(e).__name__}")
        
        # æµ‹è¯•é…ç½®éªŒè¯
        try:
            llm_manager = UnifiedLLMClientManager(
                agent_id="test_agent",
                role="verilog_designer",
                config=None  # ç©ºé…ç½®
            )
            print("âš ï¸ åº”è¯¥æ£€æµ‹åˆ°ç©ºé…ç½®")
        except Exception as e:
            print(f"âœ… æ­£ç¡®æ•è·é…ç½®é”™è¯¯: {type(e).__name__}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_performance_monitoring():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•æ€§èƒ½ç›‘æ§åŠŸèƒ½...")
    
    try:
        from core.llm_communication import UnifiedLLMClientManager, LLMCallContext, CallType
        from core.schema_system.framework_config import FrameworkConfig
        
        config = FrameworkConfig.from_env()
        llm_manager = UnifiedLLMClientManager(
            agent_id="performance_test_agent",
            role="verilog_designer",
            config=config
        )
        
        # æ¨¡æ‹Ÿå¤šæ¬¡è°ƒç”¨
        for i in range(3):
            # æ¨¡æ‹Ÿè°ƒç”¨ç»Ÿè®¡
            llm_manager.stats["total_calls"] += 1
            llm_manager.stats["successful_calls"] += 1
            llm_manager.stats["total_duration"] += 0.5 + i * 0.1
            
            if i % 2 == 0:
                llm_manager.stats["cache_hits"] += 1
            else:
                llm_manager.stats["cache_misses"] += 1
        
        # è·å–æ€§èƒ½ç»Ÿè®¡
        stats = llm_manager.get_performance_stats()
        
        print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡è¯¦æƒ…:")
        print(f"   - æ€»è°ƒç”¨æ¬¡æ•°: {stats['total_calls']}")
        print(f"   - æˆåŠŸè°ƒç”¨æ¬¡æ•°: {stats['successful_calls']}")
        print(f"   - æˆåŠŸç‡: {stats['success_rate']:.2%}")
        print(f"   - å¹³å‡è€—æ—¶: {stats['average_duration']:.3f}ç§’")
        print(f"   - ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.2%}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹LLMé€šä¿¡æ¨¡å—é›†æˆæµ‹è¯•...\n")
    
    # åŸºç¡€åŠŸèƒ½æµ‹è¯•
    basic_success = test_basic_functionality()
    
    # æ¨¡æ¿ç”Ÿæˆæµ‹è¯•
    template_success = await test_template_generation()
    
    # å¯¹è¯å¤„ç†æµ‹è¯•
    conversation_success = await test_conversation_handling()
    
    # é”™è¯¯å¤„ç†æµ‹è¯•
    error_success = test_error_handling()
    
    # æ€§èƒ½ç›‘æ§æµ‹è¯•
    performance_success = test_performance_monitoring()
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“‹ é›†æˆæµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"   åŸºç¡€åŠŸèƒ½: {'âœ… é€šè¿‡' if basic_success else 'âŒ å¤±è´¥'}")
    print(f"   æ¨¡æ¿ç”Ÿæˆ: {'âœ… é€šè¿‡' if template_success else 'âŒ å¤±è´¥'}")
    print(f"   å¯¹è¯å¤„ç†: {'âœ… é€šè¿‡' if conversation_success else 'âŒ å¤±è´¥'}")
    print(f"   é”™è¯¯å¤„ç†: {'âœ… é€šè¿‡' if error_success else 'âŒ å¤±è´¥'}")
    print(f"   æ€§èƒ½ç›‘æ§: {'âœ… é€šè¿‡' if performance_success else 'âŒ å¤±è´¥'}")
    
    all_success = all([basic_success, template_success, conversation_success, error_success, performance_success])
    
    if all_success:
        print("\nğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼LLMé€šä¿¡æ¨¡å—å·²å‡†å¤‡å°±ç»ªã€‚")
        print("ğŸ’¡ ç°åœ¨å¯ä»¥åœ¨å®é™…é¡¹ç›®ä¸­ä½¿ç”¨è¿™ä¸ªæ¨¡å—äº†ã€‚")
        return True
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1) 