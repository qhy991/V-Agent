#!/usr/bin/env python3
"""
å¢å¼ºçš„LLMå®¢æˆ·ç«¯ - é€‚é…ç‰ˆæœ¬

Enhanced LLM Client for Centralized Agent Framework
"""

import asyncio
import aiohttp
import json
import time
import logging
import hashlib
from typing import Dict, Any, Optional, List, Tuple
from contextlib import asynccontextmanager
from config.config import LLMConfig


class ConversationContext:
    """å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†ç±»"""
    
    def __init__(self, conversation_id: str, system_prompt: str = None):
        self.conversation_id = conversation_id
        self.system_prompt = system_prompt
        self.system_prompt_hash = self._hash_prompt(system_prompt) if system_prompt else None
        self.messages: List[Dict[str, str]] = []
        self.total_tokens = 0
        self.created_at = time.time()
        self.last_accessed = time.time()
        
    def _hash_prompt(self, prompt: str) -> str:
        """è®¡ç®—promptçš„å“ˆå¸Œå€¼ç”¨äºå¿«é€Ÿæ¯”è¾ƒ"""
        return hashlib.md5(prompt.encode('utf-8')).hexdigest()
    
    def add_message(self, role: str, content: str, tokens: int = 0):
        """æ·»åŠ æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡"""
        self.messages.append({
            "role": role,
            "content": content,
            "timestamp": time.time(),
            "tokens": tokens
        })
        self.total_tokens += tokens
        self.last_accessed = time.time()
    
    def get_optimized_messages(self, max_tokens: int = 8000, 
                             preserve_system: bool = True) -> List[Dict[str, str]]:
        """è·å–ä¼˜åŒ–åçš„æ¶ˆæ¯åˆ—è¡¨"""
        if not self.messages:
            return []
        
        # å¦‚æœæ€»tokenæ•°åœ¨é™åˆ¶å†…ï¼Œç›´æ¥è¿”å›
        if self.total_tokens <= max_tokens:
            return [{"role": msg["role"], "content": msg["content"]} 
                   for msg in self.messages]
        
        # éœ€è¦å‹ç¼©ä¸Šä¸‹æ–‡
        optimized_messages = []
        current_tokens = 0
        
        # ä¿ç•™system promptï¼ˆå¦‚æœéœ€è¦ï¼‰
        if preserve_system and self.messages[0]["role"] == "system":
            system_msg = self.messages[0]
            optimized_messages.append({
                "role": system_msg["role"], 
                "content": system_msg["content"]
            })
            current_tokens += system_msg.get("tokens", len(system_msg["content"]) // 4)
        
        # ä»æœ€æ–°çš„æ¶ˆæ¯å¼€å§‹ï¼Œä¿ç•™å°½å¯èƒ½å¤šçš„ä¸Šä¸‹æ–‡
        for msg in reversed(self.messages):
            if msg["role"] == "system":
                continue  # system promptå·²ç»å¤„ç†è¿‡äº†
            
            msg_tokens = msg.get("tokens", len(msg["content"]) // 4)
            if current_tokens + msg_tokens <= max_tokens:
                optimized_messages.insert(1, {  # æ’å…¥åˆ°system promptä¹‹å
                    "role": msg["role"], 
                    "content": msg["content"]
                })
                current_tokens += msg_tokens
            else:
                break
        
        return optimized_messages
    
    def update_system_prompt(self, new_system_prompt: str):
        """æ›´æ–°system prompt"""
        self.system_prompt = new_system_prompt
        self.system_prompt_hash = self._hash_prompt(new_system_prompt)
        # æ¸…é™¤æ—§æ¶ˆæ¯ï¼Œå› ä¸ºsystem promptæ”¹å˜äº†
        self.messages = []
        self.total_tokens = 0


class OptimizedLLMClient:
    """ä¼˜åŒ–çš„LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒæ™ºèƒ½ç¼“å­˜å’Œä¸Šä¸‹æ–‡ç®¡ç†"""
    
    def __init__(self, config: LLMConfig, parent_client=None):
        self.config = config
        self.parent_client = parent_client  # å¼•ç”¨çˆ¶å®¢æˆ·ç«¯ä»¥è°ƒç”¨å…¶æ–¹æ³•
        provider_name = getattr(config, 'provider', 'unknown')
        self.logger = logging.getLogger(f"OptimizedLLMClient-{provider_name}")
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0,
            "errors": 0,
            "connection_errors": 0,
            "retries": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "context_optimizations": 0,
            "token_savings": 0
        }
        
        # è¿æ¥é‡è¯•é…ç½®
        self.retry_config = {
            "max_retries": config.retry_attempts,
            "base_delay": config.retry_delay,
            "max_delay": 10.0,
            "exponential_base": 2.0
        }
        
        # æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ
        self.conversation_contexts: Dict[str, ConversationContext] = {}
        self.system_prompt_cache: Dict[str, str] = {}  # hash -> prompt
        self.max_contexts = 100  # æœ€å¤§ç¼“å­˜å¯¹è¯æ•°
        self.max_context_age = 3600  # æœ€å¤§ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰
        
        # ä¼˜åŒ–é…ç½®
        self.optimization_config = {
            "enable_system_cache": True,
            "enable_context_compression": True,
            "max_context_tokens": 8000,
            "preserve_system_in_compression": True,
            "min_context_messages": 3  # æœ€å°‘ä¿ç•™çš„æ¶ˆæ¯æ•°
        }
        
        self.logger.debug(f"ğŸš€ ä¼˜åŒ–LLMå®¢æˆ·ç«¯åˆå§‹åŒ–: {provider_name}, {config.model_name}")
    
    @asynccontextmanager
    async def _get_session(self) -> aiohttp.ClientSession:
        """æä¾›ä¸€ä¸ªä¸´æ—¶çš„ã€å®‰å…¨å…³é—­çš„aiohttpä¼šè¯"""
        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            yield session
    
    def _get_conversation_context(self, conversation_id: str) -> ConversationContext:
        """è·å–æˆ–åˆ›å»ºå¯¹è¯ä¸Šä¸‹æ–‡"""
        if conversation_id not in self.conversation_contexts:
            # æ¸…ç†è¿‡æœŸä¸Šä¸‹æ–‡
            self._cleanup_expired_contexts()
            
            # å¦‚æœè¾¾åˆ°æœ€å¤§æ•°é‡ï¼Œåˆ é™¤æœ€æ—§çš„
            if len(self.conversation_contexts) >= self.max_contexts:
                oldest_id = min(self.conversation_contexts.keys(), 
                              key=lambda k: self.conversation_contexts[k].last_accessed)
                del self.conversation_contexts[oldest_id]
                self.logger.debug(f"ğŸ—‘ï¸ åˆ é™¤æœ€æ—§å¯¹è¯ä¸Šä¸‹æ–‡: {oldest_id}")
            
            self.conversation_contexts[conversation_id] = ConversationContext(conversation_id)
            self.logger.debug(f"ğŸ†• åˆ›å»ºæ–°å¯¹è¯ä¸Šä¸‹æ–‡: {conversation_id}")
        
        return self.conversation_contexts[conversation_id]
    
    def _cleanup_expired_contexts(self):
        """æ¸…ç†è¿‡æœŸçš„å¯¹è¯ä¸Šä¸‹æ–‡"""
        current_time = time.time()
        expired_ids = [
            conv_id for conv_id, context in self.conversation_contexts.items()
            if current_time - context.last_accessed > self.max_context_age
        ]
        
        for conv_id in expired_ids:
            del self.conversation_contexts[conv_id]
            self.logger.debug(f"ğŸ—‘ï¸ æ¸…ç†è¿‡æœŸå¯¹è¯ä¸Šä¸‹æ–‡: {conv_id}")
    
    def _should_include_system_prompt(self, context: ConversationContext, 
                                    new_system_prompt: str = None) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åœ¨å½“å‰è°ƒç”¨ä¸­åŒ…å«system prompt"""
        if not self.optimization_config["enable_system_cache"]:
            return True
        
        # å¦‚æœæ²¡æœ‰system promptï¼Œä¸éœ€è¦åŒ…å«
        if not new_system_prompt and not context.system_prompt:
            return False
        
        # å¦‚æœsystem promptå‘ç”Ÿäº†å˜åŒ–ï¼Œéœ€è¦åŒ…å«
        if new_system_prompt:
            new_hash = context._hash_prompt(new_system_prompt)
            if new_hash != context.system_prompt_hash:
                return True
        
        # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œéœ€è¦åŒ…å«
        if not context.messages:
            return True
        
        # å…¶ä»–æƒ…å†µä¸éœ€è¦é‡å¤åŒ…å«
        return False
    
    async def send_prompt_optimized(self, 
                                  conversation_id: str,
                                  user_message: str,
                                  system_prompt: str = None,
                                  temperature: float = None,
                                  max_tokens: int = None,
                                  json_mode: bool = False,
                                  force_refresh_system: bool = False) -> str:
        """ä¼˜åŒ–çš„æç¤ºå‘é€æ–¹æ³•ï¼Œæ”¯æŒæ™ºèƒ½ç¼“å­˜å’Œä¸Šä¸‹æ–‡ç®¡ç†"""
        start_time = time.time()
        
        # ç®€åŒ–æ—¥å¿—è¾“å‡º - åªè®°å½•å…³é”®ä¿¡æ¯
        self.logger.info(f"ğŸš€ ä¼˜åŒ–LLMè°ƒç”¨ - å¯¹è¯ID: {conversation_id}")
        
        # è·å–å¯¹è¯ä¸Šä¸‹æ–‡
        context = self._get_conversation_context(conversation_id)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°system prompt
        if system_prompt and (force_refresh_system or 
                             context.system_prompt_hash != context._hash_prompt(system_prompt)):
            context.update_system_prompt(system_prompt)
            self.logger.info(f"ğŸ”„ æ›´æ–°system prompt: {len(system_prompt)} å­—ç¬¦")
        
        # åˆ¤æ–­æ˜¯å¦åŒ…å«system prompt
        include_system = force_refresh_system or self._should_include_system_prompt(context, system_prompt)
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        
        if include_system and context.system_prompt:
            messages.append({"role": "system", "content": context.system_prompt})
            self.stats["cache_misses"] += 1
            self.logger.info(f"ğŸ“‹ åŒ…å«system prompt: {len(context.system_prompt)} å­—ç¬¦")
        else:
            self.stats["cache_hits"] += 1
            self.logger.info(f"âš¡ ä½¿ç”¨ç¼“å­˜çš„system prompt")
        
        # æ·»åŠ å†å²æ¶ˆæ¯ï¼ˆå¦‚æœå¯ç”¨ä¸Šä¸‹æ–‡å‹ç¼©ï¼‰
        if self.optimization_config["enable_context_compression"]:
            optimized_messages = context.get_optimized_messages(
                max_tokens=self.optimization_config["max_context_tokens"],
                preserve_system=self.optimization_config["preserve_system_in_compression"]
            )
            
            # ğŸ¯ æ–°å¢ï¼šå»é‡é€»è¾‘ï¼Œé¿å…é‡å¤çš„å¯¹è¯å†…å®¹
            deduplicated_messages = []
            seen_contents = set()
            
            for msg in optimized_messages:
                content = msg.get("content", "")
                # å¯¹äºç”¨æˆ·æ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«é‡å¤çš„ä»»åŠ¡æè¿°
                if msg.get("role") == "user" and "ğŸ“‹ åè°ƒæ™ºèƒ½ä½“åˆ†é…çš„ä»»åŠ¡" in content:
                    # æå–ä»»åŠ¡æè¿°çš„æ ¸å¿ƒéƒ¨åˆ†ï¼ˆå»é™¤é‡å¤çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰
                    lines = content.split('\n')
                    core_content = []
                    in_task_section = False
                    
                    for line in lines:
                        if "ğŸ“‹ åè°ƒæ™ºèƒ½ä½“åˆ†é…çš„ä»»åŠ¡" in line:
                            if not in_task_section:
                                core_content.append(line)
                                in_task_section = True
                        elif in_task_section and line.strip().startswith("**"):
                            core_content.append(line)
                        elif in_task_section and line.strip() and not line.strip().startswith("**"):
                            core_content.append(line)
                        elif in_task_section and not line.strip():
                            break
                    
                    content = '\n'.join(core_content)
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»è§è¿‡ç›¸åŒçš„å†…å®¹
                content_hash = hash(content)
                if content_hash not in seen_contents:
                    seen_contents.add(content_hash)
                    deduplicated_messages.append(msg)
                else:
                    self.logger.info(f"ğŸ”„ [DEDUP_DEBUG] è·³è¿‡é‡å¤å†…å®¹: {content[:50]}...")
            
            messages.extend(deduplicated_messages)
            
            if len(deduplicated_messages) < len(optimized_messages):
                self.stats["context_optimizations"] += 1
                self.logger.debug(f"ğŸ—œï¸ ä¸Šä¸‹æ–‡å‹ç¼©å’Œå»é‡: {len(optimized_messages)} -> {len(deduplicated_messages)} æ¶ˆæ¯")
        else:
            # å³ä½¿ä¸å¯ç”¨å‹ç¼©ï¼Œä¹Ÿè¦è¿›è¡Œå»é‡
            deduplicated_messages = []
            seen_contents = set()
            
            for msg in context.messages:
                content = msg.get("content", "")
                content_hash = hash(content)
                if content_hash not in seen_contents:
                    seen_contents.add(content_hash)
                    deduplicated_messages.append(msg)
            
            messages.extend(deduplicated_messages)
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({"role": "user", "content": user_message})
        
        # è®°å½•æœ€ç»ˆå‘é€çš„æ¶ˆæ¯ç»“æ„
        self.logger.info(f"ğŸ“¤ [SYSTEM_PROMPT_DEBUG] æœ€ç»ˆæ¶ˆæ¯ç»“æ„: {len(messages)} æ¡æ¶ˆæ¯")
        for i, msg in enumerate(messages):
            self.logger.info(f"ğŸ“¤ [SYSTEM_PROMPT_DEBUG] æ¶ˆæ¯ {i}: role={msg['role']}, é•¿åº¦={len(msg['content'])}")
        
        # è®°å½•tokenä½¿ç”¨æƒ…å†µ
        total_tokens = sum(len(msg["content"]) // 4 for msg in messages)
        self.stats["total_tokens"] += total_tokens
        
        # è°ƒç”¨åŸå§‹å‘é€æ–¹æ³•
        try:
            response = await self._send_prompt_internal(messages, temperature, max_tokens, json_mode)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            context.add_message("user", user_message, len(user_message) // 4)
            context.add_message("assistant", response, len(response) // 4)
            
            # è®°å½•æ€§èƒ½ç»Ÿè®¡
            duration = time.time() - start_time
            self.stats["total_requests"] += 1
            self.stats["total_time"] += duration
            
            self.logger.info(f"âœ… ä¼˜åŒ–è¯·æ±‚å®Œæˆ - Token: {total_tokens}, æ—¶é—´: {duration:.2f}s")
            
            return response
            
        except Exception as e:
            self.stats["errors"] += 1
            self.logger.error(f"âŒ ä¼˜åŒ–è¯·æ±‚å¤±è´¥: {str(e)}")
            raise
    
    async def _send_prompt_internal(self, messages: List[Dict[str, str]],
                                  temperature: float = None,
                                  max_tokens: int = None,
                                  json_mode: bool = False) -> str:
        """å†…éƒ¨å‘é€æ–¹æ³•ï¼Œå¤ç”¨åŸæœ‰çš„å‘é€é€»è¾‘"""
        # æ„å»ºpromptå’Œsystem_prompt
        system_prompt = None
        user_prompt = ""
        
        for msg in messages:
            # ğŸ”§ ä¿®å¤ï¼šå®‰å…¨å¤„ç†Noneå€¼
            if msg is None or "role" not in msg:
                continue
                
            content = msg.get("content", "")
            if content is None:
                content = ""
                
            if msg["role"] == "system":
                system_prompt = content
            elif msg["role"] == "user":
                user_prompt += f"User: {content}\n\n"
            elif msg["role"] == "assistant":
                user_prompt += f"Assistant: {content}\n\n"
        
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿user_promptä¸ä¸ºNone
        user_prompt_str = user_prompt if user_prompt is not None else ""
        return await self._send_prompt_direct(user_prompt_str.strip(), system_prompt, 
                                            temperature, max_tokens, json_mode)
    
    async def _send_prompt_direct(self, prompt: str, system_prompt: str = None,
                                temperature: float = None, max_tokens: int = None,
                                json_mode: bool = False) -> str:
        """ç›´æ¥å‘é€æç¤ºï¼Œå§”æ‰˜ç»™çˆ¶å®¢æˆ·ç«¯"""
        if not self.parent_client:
            raise Exception("çˆ¶å®¢æˆ·ç«¯æœªè®¾ç½®ï¼Œæ— æ³•å‘é€è¯·æ±‚")
        
        # å§”æ‰˜ç»™çˆ¶å®¢æˆ·ç«¯çš„send_promptæ–¹æ³•
        return await self.parent_client.send_prompt(
            prompt, system_prompt, temperature, max_tokens, json_mode
        )
    
    def _get_cache_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.stats["cache_hits"] + self.stats["cache_misses"]
        return self.stats["cache_hits"] / total if total > 0 else 0.0
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            "cache_hit_rate": self._get_cache_hit_rate(),
            "active_contexts": len(self.conversation_contexts),
            "average_time": self.stats["total_time"] / max(1, self.stats["total_requests"]),
            "success_rate": 1 - (self.stats["errors"] / max(1, self.stats["total_requests"]))
        }
    
    def clear_conversation_context(self, conversation_id: str):
        """æ¸…é™¤ç‰¹å®šå¯¹è¯çš„ä¸Šä¸‹æ–‡"""
        if conversation_id in self.conversation_contexts:
            del self.conversation_contexts[conversation_id]
            self.logger.info(f"ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯ä¸Šä¸‹æ–‡: {conversation_id}")
    
    def clear_all_contexts(self):
        """æ¸…é™¤æ‰€æœ‰å¯¹è¯ä¸Šä¸‹æ–‡"""
        self.conversation_contexts.clear()
        self.logger.info("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰å¯¹è¯ä¸Šä¸‹æ–‡")


class EnhancedLLMClient:
    """
    å¢å¼ºçš„LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¼‚æ­¥è¯·æ±‚å’Œå¤šç§æä¾›å•†
    """
    
    def __init__(self, config: LLMConfig):
        self.config = config
        provider_name = getattr(config, 'provider', 'unknown')
        self.logger = logging.getLogger(f"LLMClient-{provider_name}")
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0,
            "errors": 0,
            "connection_errors": 0,
            "retries": 0
        }
        
        # è¿æ¥é‡è¯•é…ç½®
        self.retry_config = {
            "max_retries": config.retry_attempts,
            "base_delay": config.retry_delay,
            "max_delay": 10.0,
            "exponential_base": 2.0
        }
        
        # åˆ›å»ºä¼˜åŒ–å®¢æˆ·ç«¯å®ä¾‹ï¼Œä¼ å…¥è‡ªèº«ä½œä¸ºçˆ¶å®¢æˆ·ç«¯
        self.optimized_client = OptimizedLLMClient(config, parent_client=self)
        
        self.logger.debug(f"ğŸš€ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–: {provider_name}, {config.model_name}")
    
    @asynccontextmanager
    async def _get_session(self) -> aiohttp.ClientSession:
        """æä¾›ä¸€ä¸ªä¸´æ—¶çš„ã€å®‰å…¨å…³é—­çš„aiohttpä¼šè¯"""
        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            yield session
    
    async def generate_async(self, prompt: str, system_prompt: str = None,
                           temperature: float = None, max_tokens: int = None) -> str:
        """å¼‚æ­¥ç”Ÿæˆå“åº”"""
        return await self.send_prompt(prompt, system_prompt, temperature, max_tokens)
    
    async def send_prompt(self, prompt: str, system_prompt: str = None,
                         temperature: float = None, max_tokens: int = None,
                         json_mode: bool = False) -> str:
        """å‘é€æç¤ºåˆ°LLMå¹¶è¿”å›å“åº”"""
        start_time = time.time()
        max_retries = self.retry_config["max_retries"]
        base_delay = self.retry_config["base_delay"]
        last_exception = None
        
        # è®°å½•è¯·æ±‚å¼€å§‹ - ç®€åŒ–è¾“å‡º
        self.logger.info(f"ğŸ¤– LLMè°ƒç”¨ - æ¨¡å‹: {self.config.model_name}")
        
        # åªè®°å½•å…³é”®ä¿¡æ¯
        if system_prompt:
            self.logger.info(f"ğŸ“‹ System Prompt: {len(system_prompt)} å­—ç¬¦")
        
        self.logger.info(f"ğŸ‘¤ User Prompt: {len(prompt)} å­—ç¬¦")
        self.logger.info("="*50)
        
        for attempt in range(max_retries):
            try:
                async with self._get_session() as session:
                    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
                    temperature = temperature or self.config.temperature
                    max_tokens = max_tokens or self.config.max_tokens
                    
                    self.logger.debug(f"ğŸ”§ LLMå‚æ•° - æ¸©åº¦: {temperature}, æœ€å¤§Token: {max_tokens}")
                    
                    # åˆ¤æ–­æä¾›å•†
                    provider_name = self.config.provider
                    base_url = self.config.api_base_url
                    
                    if (provider_name.lower() in ["local", "ollama"] or
                        "11434" in str(base_url) or
                        "ollama" in str(base_url).lower()):
                        response_content = await self._send_ollama_request(
                            session, prompt, system_prompt, temperature, max_tokens, json_mode)
                    else:
                        response_content = await self._send_openai_compatible_request(
                            session, prompt, system_prompt, temperature, max_tokens, json_mode)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    duration = time.time() - start_time
                    self.stats["total_requests"] += 1
                    self.stats["total_time"] += duration
                    self.stats["total_tokens"] += len(prompt) + len(response_content)
                    if attempt > 0:
                        self.stats["retries"] += 1
                    
                    # è®°å½•å“åº”å†…å®¹ - ç®€åŒ–è¾“å‡º
                    self.logger.info(f"ğŸ¤– LLMå“åº”: {len(response_content)} å­—ç¬¦, {duration:.2f}s")
                    self.logger.info(f"ğŸ¤– å“åº”å†…å®¹: {response_content[:200]}{'...' if len(response_content) > 200 else ''}")
                    self.logger.info("="*50)
                    
                    return response_content
                    
            except aiohttp.ClientError as e:
                last_exception = e
                self.stats["connection_errors"] += 1
                self.logger.warning(f"âš ï¸ è¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                
                if attempt < max_retries - 1:
                    delay = min(base_delay * (self.retry_config["exponential_base"] ** attempt), 
                              self.retry_config["max_delay"])
                    await asyncio.sleep(delay)
                else:
                    break
                    
            except Exception as e:
                last_exception = e
                self.stats["errors"] += 1
                self.logger.error(f"âŒ è¯·æ±‚é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                
                if attempt < max_retries - 1:
                    delay = min(base_delay * (self.retry_config["exponential_base"] ** attempt), 
                              self.retry_config["max_delay"])
                    await asyncio.sleep(delay)
                else:
                    break
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        error_msg = f"LLMè¯·æ±‚å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡ã€‚æœ€åé”™è¯¯: {str(last_exception)}"
        self.logger.error(f"âŒ {error_msg}")
        raise Exception(error_msg)
    
    async def _send_openai_compatible_request(self, session: aiohttp.ClientSession, 
                                            prompt: str, system_prompt: str,
                                            temperature: float, max_tokens: int, 
                                            json_mode: bool) -> str:
        """å‘é€OpenAIå…¼å®¹è¯·æ±‚"""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        payload = {
            "model": self.config.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        if json_mode:
            payload["response_format"] = {"type": "json_object"}
        
        headers = {"Content-Type": "application/json"}
        if self.config.api_key:
            headers["Authorization"] = f"Bearer {self.config.api_key}"
        
        url = f"{self.config.api_base_url.rstrip('/')}/chat/completions"
        
        async with session.post(url, json=payload, headers=headers) as response:
            if response.status != 200:
                error_text = await response.text()
                raise aiohttp.ClientResponseError(
                    request_info=response.request_info,
                    history=response.history,
                    status=response.status,
                    message=error_text
                )
            
            response_data = await response.json()
            return response_data["choices"][0]["message"]["content"]
    
    async def _send_ollama_request(self, session: aiohttp.ClientSession,
                                 prompt: str, system_prompt: str,
                                 temperature: float, max_tokens: int,
                                 json_mode: bool) -> str:
        """å‘é€Ollamaè¯·æ±‚"""
        
        # æ„å»ºOllamaæ ¼å¼çš„prompt
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"System: {system_prompt}\n\nUser: {prompt}"
        
        payload = {
            "model": self.config.model_name,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens
            }
        }
        
        if json_mode:
            payload["format"] = "json"
        
        url = f"{self.config.api_base_url.rstrip('/')}/api/generate"
        
        async with session.post(url, json=payload) as response:
            if response.status != 200:
                error_text = await response.text()
                raise aiohttp.ClientResponseError(
                    request_info=response.request_info,
                    history=response.history,
                    status=response.status,
                    message=error_text
                )
            
            response_data = await response.json()
            return response_data.get("response", "")
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯å¹¶è®°å½•ç»Ÿè®¡ä¿¡æ¯"""
        self.logger.info(f"LLMå®¢æˆ·ç«¯å·²å…³é—­ - ç»Ÿè®¡: {self.get_stats()}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        total_requests = max(1, self.stats["total_requests"])
        return {
            **self.stats,
            "average_time": self.stats["total_time"] / total_requests,
            "success_rate": 1 - (self.stats["errors"] / total_requests)
        }
    
    # æ–°å¢ï¼šä¼˜åŒ–æ–¹æ³•æ¥å£
    async def send_prompt_optimized(self, 
                                  conversation_id: str,
                                  user_message: str,
                                  system_prompt: str = None,
                                  temperature: float = None,
                                  max_tokens: int = None,
                                  json_mode: bool = False,
                                  force_refresh_system: bool = False) -> str:
        """ä¼˜åŒ–çš„æç¤ºå‘é€æ–¹æ³•"""
        return await self.optimized_client.send_prompt_optimized(
            conversation_id, user_message, system_prompt, 
            temperature, max_tokens, json_mode, force_refresh_system
        )
    
    def get_optimization_stats(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        return self.optimized_client.get_optimization_stats()
    
    def clear_conversation_context(self, conversation_id: str):
        """æ¸…é™¤ç‰¹å®šå¯¹è¯çš„ä¸Šä¸‹æ–‡"""
        self.optimized_client.clear_conversation_context(conversation_id)
    
    def clear_all_contexts(self):
        """æ¸…é™¤æ‰€æœ‰å¯¹è¯ä¸Šä¸‹æ–‡"""
        self.optimized_client.clear_all_contexts()