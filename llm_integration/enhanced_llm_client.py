#!/usr/bin/env python3
"""
å¢å¼ºçš„LLMå®¢æˆ·ç«¯ - é€‚é…ç‰ˆæœ¬

Enhanced LLM Client for Centralized Agent Framework
"""

import asyncio
import aiohttp
import json
import time
import logging
from typing import Dict, Any, Optional, List
from contextlib import asynccontextmanager
from config.config import LLMConfig


class EnhancedLLMClient:
    """
    å¢å¼ºçš„LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¼‚æ­¥è¯·æ±‚å’Œå¤šç§æä¾›å•†
    """
    
    def __init__(self, config: LLMConfig):
        self.config = config
        provider_name = getattr(config, 'provider', 'unknown')
        self.logger = logging.getLogger(f"LLMClient-{provider_name}")
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0,
            "errors": 0,
            "connection_errors": 0,
            "retries": 0
        }
        
        # è¿æ¥é‡è¯•é…ç½®
        self.retry_config = {
            "max_retries": config.retry_attempts,
            "base_delay": config.retry_delay,
            "max_delay": 10.0,
            "exponential_base": 2.0
        }
        
        self.logger.info(f"ğŸš€ åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ - æä¾›å•†: {provider_name}, æ¨¡å‹: {config.model_name}")
    
    @asynccontextmanager
    async def _get_session(self) -> aiohttp.ClientSession:
        """æä¾›ä¸€ä¸ªä¸´æ—¶çš„ã€å®‰å…¨å…³é—­çš„aiohttpä¼šè¯"""
        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            yield session
    
    async def generate_async(self, prompt: str, system_prompt: str = None,
                           temperature: float = None, max_tokens: int = None) -> str:
        """å¼‚æ­¥ç”Ÿæˆå“åº”"""
        return await self.send_prompt(prompt, system_prompt, temperature, max_tokens)
    
    async def send_prompt(self, prompt: str, system_prompt: str = None,
                         temperature: float = None, max_tokens: int = None,
                         json_mode: bool = False) -> str:
        """å‘é€æç¤ºåˆ°LLMå¹¶è¿”å›å“åº”"""
        start_time = time.time()
        max_retries = self.retry_config["max_retries"]
        base_delay = self.retry_config["base_delay"]
        last_exception = None
        
        # è®°å½•è¯·æ±‚å¼€å§‹
        self.logger.info(f"ğŸ¤– å¼€å§‹LLMè¯·æ±‚ - æ¨¡å‹: {self.config.model_name}, JSONæ¨¡å¼: {json_mode}")
        
        # è¯¦ç»†è®°å½•å¯¹è¯å†…å®¹
        if system_prompt:
            self.logger.info(f"ğŸ“‹ System Prompt ({len(system_prompt)} å­—ç¬¦):")
            self.logger.info(f"ğŸ“‹ {system_prompt}")
        
        self.logger.info(f"ğŸ‘¤ User Prompt ({len(prompt)} å­—ç¬¦):")
        self.logger.info(f"ğŸ‘¤ {prompt}")
        
        for attempt in range(max_retries):
            try:
                async with self._get_session() as session:
                    # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
                    temperature = temperature or self.config.temperature
                    max_tokens = max_tokens or self.config.max_tokens
                    
                    self.logger.debug(f"ğŸ”§ LLMå‚æ•° - æ¸©åº¦: {temperature}, æœ€å¤§Token: {max_tokens}")
                    
                    # åˆ¤æ–­æä¾›å•†
                    provider_name = self.config.provider
                    base_url = self.config.api_base_url
                    
                    if (provider_name.lower() in ["local", "ollama"] or
                        "11434" in str(base_url) or
                        "ollama" in str(base_url).lower()):
                        response_content = await self._send_ollama_request(
                            session, prompt, system_prompt, temperature, max_tokens, json_mode)
                    else:
                        response_content = await self._send_openai_compatible_request(
                            session, prompt, system_prompt, temperature, max_tokens, json_mode)
                    
                    # æ›´æ–°ç»Ÿè®¡
                    duration = time.time() - start_time
                    self.stats["total_requests"] += 1
                    self.stats["total_time"] += duration
                    self.stats["total_tokens"] += len(prompt) + len(response_content)
                    if attempt > 0:
                        self.stats["retries"] += 1
                    
                    # è¯¦ç»†è®°å½•å“åº”å†…å®¹
                    self.logger.info(f"ğŸ¤– LLMå“åº” ({len(response_content)} å­—ç¬¦, {duration:.2f}s):")
                    self.logger.info(f"ğŸ¤– {response_content}")
                    
                    self.logger.debug(f"ğŸ“Š LLMç»Ÿè®¡ - è€—æ—¶: {duration:.2f}s, å°è¯•æ¬¡æ•°: {attempt + 1}, æ€»Token: {len(prompt) + len(response_content)}")
                    return response_content
                    
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                last_exception = e
                self.stats["connection_errors"] += 1
                delay = min(base_delay * (self.retry_config["exponential_base"] ** attempt), 
                          self.retry_config["max_delay"])
                self.logger.warning(f"LLMè¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {type(e).__name__}, å°†åœ¨ {delay:.1f}såé‡è¯•")
                if attempt < max_retries - 1:
                    await asyncio.sleep(delay)
            except Exception as e:
                last_exception = e
                self.stats["errors"] += 1
                self.logger.error(f"LLMè¯·æ±‚å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(base_delay)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        self.stats["errors"] += 1
        error_msg = f"LLMè¯·æ±‚æœ€ç»ˆå¤±è´¥ï¼Œå·²å°è¯• {max_retries} æ¬¡: {str(last_exception)}"
        self.logger.error(error_msg)
        raise Exception(error_msg)
    
    async def _send_openai_compatible_request(self, session: aiohttp.ClientSession, 
                                            prompt: str, system_prompt: str,
                                            temperature: float, max_tokens: int, 
                                            json_mode: bool) -> str:
        """å‘é€OpenAIå…¼å®¹è¯·æ±‚"""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        payload = {
            "model": self.config.model_name,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        if json_mode:
            payload["response_format"] = {"type": "json_object"}
        
        headers = {"Content-Type": "application/json"}
        if self.config.api_key:
            headers["Authorization"] = f"Bearer {self.config.api_key}"
        
        url = f"{self.config.api_base_url.rstrip('/')}/chat/completions"
        
        async with session.post(url, json=payload, headers=headers) as response:
            if response.status != 200:
                error_text = await response.text()
                raise aiohttp.ClientResponseError(
                    request_info=response.request_info,
                    history=response.history,
                    status=response.status,
                    message=error_text
                )
            
            response_data = await response.json()
            return response_data["choices"][0]["message"]["content"]
    
    async def _send_ollama_request(self, session: aiohttp.ClientSession,
                                 prompt: str, system_prompt: str,
                                 temperature: float, max_tokens: int,
                                 json_mode: bool) -> str:
        """å‘é€Ollamaè¯·æ±‚"""
        # æ„å»ºOllamaæ ¼å¼çš„prompt
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"System: {system_prompt}\n\nUser: {prompt}"
        
        payload = {
            "model": self.config.model_name,
            "prompt": full_prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens
            }
        }
        
        if json_mode:
            payload["format"] = "json"
        
        url = f"{self.config.api_base_url.rstrip('/')}/api/generate"
        
        async with session.post(url, json=payload) as response:
            if response.status != 200:
                error_text = await response.text()
                raise aiohttp.ClientResponseError(
                    request_info=response.request_info,
                    history=response.history,
                    status=response.status,
                    message=error_text
                )
            
            response_data = await response.json()
            return response_data.get("response", "")
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯å¹¶è®°å½•ç»Ÿè®¡ä¿¡æ¯"""
        self.logger.info(f"LLMå®¢æˆ·ç«¯å·²å…³é—­ - ç»Ÿè®¡: {self.get_stats()}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        total_requests = max(1, self.stats["total_requests"])
        return {
            **self.stats,
            "average_time": self.stats["total_time"] / total_requests,
            "success_rate": 1 - (self.stats["errors"] / total_requests)
        }