"""
ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ç®¡ç†å™¨
æ•´åˆæ‰€æœ‰æ™ºèƒ½ä½“çš„LLMè°ƒç”¨é€»è¾‘ï¼Œæ¶ˆé™¤é‡å¤ä»£ç 
"""

import time
import logging
import asyncio
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from enum import Enum

from core.schema_system.framework_config import FrameworkConfig
from llm_integration.enhanced_llm_client import EnhancedLLMClient


class CallType(Enum):
    """LLMè°ƒç”¨ç±»åž‹"""
    FUNCTION_CALLING = "function_calling"
    TRADITIONAL = "traditional"
    OPTIMIZED = "optimized"


@dataclass
class LLMCallContext:
    """LLMè°ƒç”¨ä¸Šä¸‹æ–‡"""
    conversation_id: str
    agent_id: str
    role: str
    is_first_call: bool
    conversation_length: int
    total_length: int
    system_prompt_hash: Optional[str] = None


class UnifiedLLMClientManager:
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ç®¡ç†å™¨"""
    
    def __init__(self, agent_id: str, role: str, config: FrameworkConfig):
        self.agent_id = agent_id
        self.role = role
        self.config = config
        self.llm_client = EnhancedLLMClient(config.llm)
        self.logger = logging.getLogger(f"LLMClientManager.{agent_id}")
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "total_duration": 0.0,
            "cache_hits": 0,
            "cache_misses": 0
        }
    
    async def call_llm_for_function_calling(self, conversation: List[Dict[str, str]], 
                                          system_prompt_builder=None) -> str:
        """ç»Ÿä¸€çš„LLMè°ƒç”¨æ–¹æ³• - æå–è‡ªä¸‰ä¸ªæ™ºèƒ½ä½“çš„å…±åŒé€»è¾‘"""
        start_time = time.time()
        self.stats["total_calls"] += 1
        
        try:
            # ç”Ÿæˆå¯¹è¯ID
            conversation_id = f"{self.agent_id}_{int(time.time())}"
            
            # æž„å»ºç”¨æˆ·æ¶ˆæ¯
            user_message = self._build_user_message(conversation)
            
            # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨
            assistant_messages = [msg for msg in conversation if msg["role"] == "assistant"]
            is_first_call = len(assistant_messages) == 0
            
            # æž„å»ºè°ƒç”¨ä¸Šä¸‹æ–‡
            context = LLMCallContext(
                conversation_id=conversation_id,
                agent_id=self.agent_id,
                role=self.role,
                is_first_call=is_first_call,
                conversation_length=len(conversation),
                total_length=sum(len(msg.get('content', '')) for msg in conversation)
            )
            
            self.logger.info(f"ðŸ”„ [{self.role.upper()}] å‡†å¤‡LLMè°ƒç”¨ - å¯¹è¯åŽ†å²é•¿åº¦: {len(conversation)}, assistantæ¶ˆæ¯æ•°: {len(assistant_messages)}, æ˜¯å¦é¦–æ¬¡è°ƒç”¨: {is_first_call}")
            
            # èŽ·å–System Prompt
            system_prompt = None
            if is_first_call and system_prompt_builder:
                try:
                    if asyncio.iscoroutinefunction(system_prompt_builder):
                        system_prompt = await system_prompt_builder()
                    else:
                        system_prompt = system_prompt_builder()
                    # å®‰å…¨å¤„ç†system_prompt
                    if system_prompt is None:
                        system_prompt = ""
                    self.logger.debug(f"ðŸ“ [{self.role.upper()}] é¦–æ¬¡è°ƒç”¨ - æž„å»ºSystem Prompt - é•¿åº¦: {len(system_prompt)}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ System Promptæž„å»ºå¤±è´¥: {e}")
                    system_prompt = ""
            
            # ðŸ”§ ä¿®å¤ï¼šå®‰å…¨å¤„ç†user_message
            safe_user_message = user_message.strip() if user_message is not None else ""
            
            # è°ƒç”¨ä¼˜åŒ–çš„LLMå®¢æˆ·ç«¯
            response = await self.llm_client.send_prompt_optimized(
                conversation_id=conversation_id,
                user_message=safe_user_message,
                system_prompt=system_prompt,
                temperature=0.3,
                max_tokens=4000,
                force_refresh_system=is_first_call
            )
            
            # ðŸ”§ ä¿®å¤ï¼šç¡®ä¿å“åº”ä¸ä¸ºNone
            if response is None:
                self.logger.warning("âš ï¸ LLMå“åº”ä¸ºç©ºï¼Œè¿”å›žé»˜è®¤å“åº”")
                response = "æˆ‘ç†è§£äº†æ‚¨çš„è¯·æ±‚ï¼Œä½†å½“å‰æ— æ³•ç”Ÿæˆæœ‰æ•ˆå“åº”ã€‚è¯·ç¨åŽé‡è¯•ã€‚"
            
            # è®°å½•æˆåŠŸ
            duration = time.time() - start_time
            self.stats["successful_calls"] += 1
            self.stats["total_duration"] += duration
            
            self.logger.info(f"âœ… [{self.role.upper()}] LLMè°ƒç”¨æˆåŠŸ - è€—æ—¶: {duration:.2f}ç§’, å“åº”é•¿åº¦: {len(response) if response else 0}")
            
            return response
            
        except Exception as e:
            # è®°å½•å¤±è´¥
            duration = time.time() - start_time
            self.stats["failed_calls"] += 1
            self.stats["total_duration"] += duration
            
            self.logger.error(f"âŒ [{self.role.upper()}] ä¼˜åŒ–LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            
            # å›žé€€åˆ°ä¼ ç»Ÿæ–¹å¼
            self.logger.warning("âš ï¸ å›žé€€åˆ°ä¼ ç»ŸLLMè°ƒç”¨æ–¹å¼")
            return await self._call_llm_traditional(conversation, system_prompt_builder)
    
    async def _call_llm_traditional(self, conversation: List[Dict[str, str]], 
                                   system_prompt_builder=None) -> str:
        """ä¼ ç»ŸLLMè°ƒç”¨æ–¹æ³• - ç»Ÿä¸€å®žçŽ°"""
        llm_start_time = time.time()
        total_length = 0
        conversation_id = f"{self.agent_id}_{int(time.time())}"
        
        try:
            # ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿè®°å½•LLMè°ƒç”¨å¼€å§‹
            from core.unified_logging_system import get_global_logging_system
            logging_system = get_global_logging_system()
            
            # è®¡ç®—å¯¹è¯æ€»é•¿åº¦
            total_length = sum(len(msg.get('content', '')) for msg in conversation)
            
            # è®°å½•LLMè°ƒç”¨å¼€å§‹
            logging_system.log_llm_call(
                agent_id=self.agent_id,
                model_name=getattr(self.config.llm, 'model_name', 'claude-3.5-sonnet'),
                prompt_length=total_length,
                conversation_length=len(conversation),
                conversation_id=conversation_id
            )
            
            # æž„å»ºå®Œæ•´çš„prompt
            full_prompt = ""
            system_prompt = ""
            if system_prompt_builder:
                try:
                    if asyncio.iscoroutinefunction(system_prompt_builder):
                        system_prompt = await system_prompt_builder()
                    else:
                        system_prompt = system_prompt_builder()
                    # å®‰å…¨å¤„ç†system_prompt
                    if system_prompt is None:
                        system_prompt = ""
                except Exception as e:
                    self.logger.warning(f"âš ï¸ System Promptæž„å»ºå¤±è´¥: {e}")
                    system_prompt = ""
            
            for msg in conversation:
                if msg["role"] == "system":
                    system_prompt = msg["content"]  # è¦†ç›–é»˜è®¤system prompt
                elif msg["role"] == "user":
                    full_prompt += f"User: {msg['content']}\n\n"
                elif msg["role"] == "assistant":
                    full_prompt += f"Assistant: {msg['content']}\n\n"
            
            # ðŸ”§ ä¿®å¤ï¼šå®‰å…¨å¤„ç†full_prompt
            safe_full_prompt = full_prompt.strip() if full_prompt is not None else ""
            
            # è°ƒç”¨ä¼ ç»ŸLLMå®¢æˆ·ç«¯
            if hasattr(self.llm_client, 'send_prompt'):
                response = await self.llm_client.send_prompt(
                    prompt=safe_full_prompt,
                    system_prompt=system_prompt,
                    temperature=0.3,
                    max_tokens=4000
                )
            else:
                # ä½¿ç”¨OptimizedLLMClientçš„æ–¹æ³•
                response = await self.llm_client.send_prompt_optimized(
                    conversation_id=conversation_id,
                    user_message=safe_full_prompt,
                    system_prompt=system_prompt,
                    temperature=0.3,
                    max_tokens=4000
                )
            
            # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
            if response is None:
                raise Exception("LLMå“åº”ä¸ºç©º")
            
            # è®°å½•LLMè°ƒç”¨æˆåŠŸ
            duration = time.time() - llm_start_time
            logging_system.log_llm_call(
                agent_id=self.agent_id,
                model_name=getattr(self.config.llm, 'model_name', 'claude-3.5-sonnet'),
                prompt_length=total_length,
                response_length=len(response) if response else 0,
                duration=duration,
                success=True,
                conversation_id=conversation_id
            )
            
            return response
            
        except Exception as e:
            # è®°å½•LLMè°ƒç”¨å¤±è´¥
            duration = time.time() - llm_start_time
            logging_system.log_llm_call(
                agent_id=self.agent_id,
                model_name=getattr(self.config.llm, 'model_name', 'claude-3.5-sonnet'),
                prompt_length=total_length,
                duration=duration,
                success=False,
                error_info={"error": str(e)},
                conversation_id=conversation_id
            )
            
            self.logger.error(f"âŒ [{self.role.upper()}] ä¼ ç»ŸLLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def _build_user_message(self, conversation: List[Dict[str, str]]) -> str:
        """æž„å»ºç”¨æˆ·æ¶ˆæ¯"""
        user_message = ""
        for msg in conversation:
            # ðŸ”§ ä¿®å¤ï¼šå®‰å…¨å¤„ç†Noneå€¼å’Œç¼ºå¤±å­—æ®µ
            if msg is None:
                continue
                
            content = msg.get('content', '')
            if content is None:
                content = ''
                
            role = msg.get('role', '')
            if not role:
                continue
                
            if role == "user":
                user_message += f"{content}\n\n"
            elif role == "assistant":
                user_message += f"Assistant: {content}\n\n"
        return user_message
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """èŽ·å–æ€§èƒ½ç»Ÿè®¡"""
        total_calls = self.stats["total_calls"]
        if total_calls == 0:
            return self.stats
        
        return {
            **self.stats,
            "success_rate": self.stats["successful_calls"] / total_calls,
            "average_duration": self.stats["total_duration"] / total_calls,
            "cache_hit_rate": self.stats["cache_hits"] / max(total_calls, 1)
        }
    
    async def call_llm_traditional(self, conversation: List[Dict[str, str]], 
                                 system_prompt_builder=None) -> str:
        """ç»Ÿä¸€çš„ä¼ ç»ŸLLMè°ƒç”¨æ–¹æ³•"""
        return await self._call_llm_traditional(conversation, system_prompt_builder)