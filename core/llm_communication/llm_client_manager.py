"""
ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ç®¡ç†å™¨
æ•´åˆæ‰€æœ‰æ™ºèƒ½ä½“çš„LLMè°ƒç”¨é€»è¾‘ï¼Œæ¶ˆé™¤é‡å¤ä»£ç 
"""

import time
import logging
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from enum import Enum

from core.schema_system.framework_config import FrameworkConfig
from llm_integration.enhanced_llm_client import OptimizedLLMClient


class LLMCallType(Enum):
    """LLMè°ƒç”¨ç±»å‹"""
    FUNCTION_CALLING = "function_calling"
    TRADITIONAL = "traditional"
    OPTIMIZED = "optimized"


@dataclass
class LLMCallContext:
    """LLMè°ƒç”¨ä¸Šä¸‹æ–‡"""
    conversation_id: str
    agent_id: str
    role: str
    is_first_call: bool
    conversation_length: int
    total_length: int
    system_prompt_hash: Optional[str] = None


class LLMClientManager:
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ç®¡ç†å™¨"""
    
    def __init__(self, agent_id: str, role: str, config: FrameworkConfig):
        self.agent_id = agent_id
        self.role = role
        self.config = config
        self.llm_client = OptimizedLLMClient(config.llm)
        self.logger = logging.getLogger(f"LLMClientManager.{agent_id}")
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_calls": 0,
            "successful_calls": 0,
            "failed_calls": 0,
            "total_duration": 0.0,
            "cache_hits": 0,
            "cache_misses": 0
        }
    
    async def call_llm_for_function_calling(self, conversation: List[Dict[str, str]], 
                                          system_prompt_builder=None) -> str:
        """ç»Ÿä¸€çš„LLMè°ƒç”¨æ–¹æ³• - æå–è‡ªä¸‰ä¸ªæ™ºèƒ½ä½“çš„å…±åŒé€»è¾‘"""
        start_time = time.time()
        self.stats["total_calls"] += 1
        
        try:
            # ç”Ÿæˆå¯¹è¯ID
            conversation_id = f"{self.agent_id}_{int(time.time())}"
            
            # æ„å»ºç”¨æˆ·æ¶ˆæ¯
            user_message = self._build_user_message(conversation)
            
            # åˆ¤æ–­æ˜¯å¦é¦–æ¬¡è°ƒç”¨
            assistant_messages = [msg for msg in conversation if msg["role"] == "assistant"]
            is_first_call = len(assistant_messages) == 0
            
            # æ„å»ºè°ƒç”¨ä¸Šä¸‹æ–‡
            context = LLMCallContext(
                conversation_id=conversation_id,
                agent_id=self.agent_id,
                role=self.role,
                is_first_call=is_first_call,
                conversation_length=len(conversation),
                total_length=sum(len(msg.get('content', '')) for msg in conversation)
            )
            
            self.logger.info(f"ğŸ”„ [{self.role.upper()}] å‡†å¤‡LLMè°ƒç”¨ - å¯¹è¯å†å²é•¿åº¦: {len(conversation)}, assistantæ¶ˆæ¯æ•°: {len(assistant_messages)}, æ˜¯å¦é¦–æ¬¡è°ƒç”¨: {is_first_call}")
            
            # è·å–System Prompt
            system_prompt = None
            if is_first_call and system_prompt_builder:
                system_prompt = system_prompt_builder()
                self.logger.debug(f"ğŸ“ [{self.role.upper()}] é¦–æ¬¡è°ƒç”¨ - æ„å»ºSystem Prompt - é•¿åº¦: {len(system_prompt)}")
            
            # è°ƒç”¨ä¼˜åŒ–çš„LLMå®¢æˆ·ç«¯
            response = await self.llm_client.send_prompt_optimized(
                conversation_id=conversation_id,
                user_message=user_message.strip(),
                system_prompt=system_prompt,
                temperature=0.3,
                max_tokens=4000,
                force_refresh_system=is_first_call
            )
            
            # è®°å½•æˆåŠŸ
            duration = time.time() - start_time
            self.stats["successful_calls"] += 1
            self.stats["total_duration"] += duration
            
            self.logger.info(f"ğŸ” [{self.role.upper()}] LLMå“åº”é•¿åº¦: {len(response)}")
            
            return response
            
        except Exception as e:
            # è®°å½•å¤±è´¥
            duration = time.time() - start_time
            self.stats["failed_calls"] += 1
            self.stats["total_duration"] += duration
            
            self.logger.error(f"âŒ [{self.role.upper()}] ä¼˜åŒ–LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
            self.logger.warning("âš ï¸ å›é€€åˆ°ä¼ ç»ŸLLMè°ƒç”¨æ–¹å¼")
            return await self._call_llm_traditional(conversation, system_prompt_builder)
    
    async def _call_llm_traditional(self, conversation: List[Dict[str, str]], 
                                   system_prompt_builder=None) -> str:
        """ä¼ ç»ŸLLMè°ƒç”¨æ–¹æ³• - ç»Ÿä¸€å®ç°"""
        llm_start_time = time.time()
        
        try:
            # ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿè®°å½•LLMè°ƒç”¨å¼€å§‹
            from core.unified_logging_system import get_global_logging_system
            logging_system = get_global_logging_system()
            
            # è®¡ç®—å¯¹è¯æ€»é•¿åº¦
            total_length = sum(len(msg.get('content', '')) for msg in conversation)
            conversation_id = f"{self.agent_id}_{int(time.time())}"
            
            # è®°å½•LLMè°ƒç”¨å¼€å§‹
            logging_system.log_llm_call(
                agent_id=self.agent_id,
                model_name="claude-3.5-sonnet",
                prompt_length=total_length,
                conversation_length=len(conversation),
                conversation_id=conversation_id
            )
            
            # æ„å»ºå®Œæ•´çš„prompt
            full_prompt = ""
            system_prompt = system_prompt_builder() if system_prompt_builder else ""
            
            for msg in conversation:
                if msg["role"] == "system":
                    system_prompt = msg["content"]  # è¦†ç›–é»˜è®¤system prompt
                elif msg["role"] == "user":
                    full_prompt += f"User: {msg['content']}\n\n"
                elif msg["role"] == "assistant":
                    full_prompt += f"Assistant: {msg['content']}\n\n"
            
            # è°ƒç”¨ä¼ ç»ŸLLMå®¢æˆ·ç«¯
            response = await self.llm_client.send_prompt(
                prompt=full_prompt.strip(),
                system_prompt=system_prompt,
                temperature=0.3,
                max_tokens=4000
            )
            
            # è®°å½•LLMè°ƒç”¨æˆåŠŸ
            duration = time.time() - llm_start_time
            logging_system.log_llm_call(
                agent_id=self.agent_id,
                model_name="claude-3.5-sonnet",
                prompt_length=total_length,
                response_length=len(response),
                duration=duration,
                success=True,
                conversation_id=conversation_id
            )
            
            return response
            
        except Exception as e:
            # è®°å½•LLMè°ƒç”¨å¤±è´¥
            duration = time.time() - llm_start_time
            logging_system.log_llm_call(
                agent_id=self.agent_id,
                model_name="claude-3.5-sonnet",
                prompt_length=total_length,
                duration=duration,
                success=False,
                error_info={"error": str(e)},
                conversation_id=conversation_id
            )
            
            self.logger.error(f"âŒ [{self.role.upper()}] ä¼ ç»ŸLLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def _build_user_message(self, conversation: List[Dict[str, str]]) -> str:
        """æ„å»ºç”¨æˆ·æ¶ˆæ¯"""
        user_message = ""
        for msg in conversation:
            if msg["role"] == "user":
                user_message += f"{msg['content']}\n\n"
            elif msg["role"] == "assistant":
                user_message += f"Assistant: {msg['content']}\n\n"
        return user_message
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        total_calls = self.stats["total_calls"]
        if total_calls == 0:
            return self.stats
        
        return {
            **self.stats,
            "success_rate": self.stats["successful_calls"] / total_calls,
            "average_duration": self.stats["total_duration"] / total_calls,
            "cache_hit_rate": self.stats["cache_hits"] / max(total_calls, 1)
        } 