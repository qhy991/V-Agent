#!/usr/bin/env python3
"""
File Operation Manager - æ–‡ä»¶æ“ä½œç®¡ç†å™¨
=====================================

ä»BaseAgentä¸­æå–çš„æ–‡ä»¶æ“ä½œåŠŸèƒ½ï¼Œè´Ÿè´£æ–‡ä»¶è¯»å†™ã€å†…å®¹æ¸…ç†å’ŒéªŒè¯ã€‚
"""

import os
import re
import time
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass

from ..types import FileReference


@dataclass
class FileOperationConfig:
    """æ–‡ä»¶æ“ä½œé…ç½®"""
    default_artifacts_dir: str = "./output"
    enable_cache: bool = True
    max_file_size: int = 10 * 1024 * 1024  # 10MB
    allowed_extensions: List[str] = None
    backup_enabled: bool = True
    encoding: str = "utf-8"


class FileOperationManager:
    """æ–‡ä»¶æ“ä½œç®¡ç†å™¨"""
    
    def __init__(self, config: FileOperationConfig = None, logger: Optional[logging.Logger] = None):
        self.config = config or FileOperationConfig()
        self.logger = logger or logging.getLogger(__name__)
        
        # æ–‡ä»¶ç¼“å­˜
        self.file_cache: Dict[str, str] = {}
        self.file_metadata_cache: Dict[str, Dict] = {}
        
        # æ“ä½œç»Ÿè®¡
        self.operation_stats = {
            'total_reads': 0,
            'total_writes': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'errors': 0
        }
        
        # åˆå§‹åŒ–é»˜è®¤ç›®å½•
        self._ensure_default_directories()
    
    def _ensure_default_directories(self):
        """ç¡®ä¿é»˜è®¤ç›®å½•å­˜åœ¨"""
        try:
            Path(self.config.default_artifacts_dir).mkdir(parents=True, exist_ok=True)
            self.logger.debug(f"ç¡®ä¿ç›®å½•å­˜åœ¨: {self.config.default_artifacts_dir}")
        except Exception as e:
            self.logger.warning(f"åˆ›å»ºé»˜è®¤ç›®å½•å¤±è´¥: {e}")
    
    async def read_file(self, file_path: str, use_cache: bool = True) -> Optional[str]:
        """è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            if use_cache and self.config.enable_cache and file_path in self.file_cache:
                self.operation_stats['cache_hits'] += 1
                self.logger.debug(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜æ–‡ä»¶: {file_path}")
                return self.file_cache[file_path]
            
            self.operation_stats['cache_misses'] += 1
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                self.logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return None
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(file_path)
            if file_size > self.config.max_file_size:
                self.logger.warning(f"âš ï¸ æ–‡ä»¶è¿‡å¤§: {file_path} ({file_size} bytes)")
                return None
            
            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding=self.config.encoding) as f:
                content = f.read()
            
            # ç¼“å­˜å†…å®¹
            if self.config.enable_cache:
                self.file_cache[file_path] = content
                self.file_metadata_cache[file_path] = {
                    "size": len(content),
                    "read_time": time.time(),
                    "file_size": file_size
                }
            
            self.operation_stats['total_reads'] += 1
            self.logger.info(f"âœ… æˆåŠŸè¯»å–æ–‡ä»¶: {file_path} ({len(content)} bytes)")
            return content
            
        except Exception as e:
            self.operation_stats['errors'] += 1
            self.logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            return None
    
    async def write_file(self, content: str, file_path: str = None, 
                        filename: str = None, directory: str = None, 
                        file_type: str = "text", backup: bool = None) -> Optional[FileReference]:
        """å†™å…¥æ–‡ä»¶å†…å®¹"""
        try:
            # ç¡®å®šæ–‡ä»¶è·¯å¾„
            if file_path:
                target_path = Path(file_path)
            else:
                if not filename:
                    self.logger.error("âŒ å¿…é¡»æä¾›filenameæˆ–file_path")
                    return None
                
                dir_path = Path(directory) if directory else Path(self.config.default_artifacts_dir)
                target_path = dir_path / filename
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            target_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å¤‡ä»½ç°æœ‰æ–‡ä»¶
            if (backup or self.config.backup_enabled) and target_path.exists():
                backup_path = target_path.with_suffix(f"{target_path.suffix}.backup")
                target_path.rename(backup_path)
                self.logger.info(f"ğŸ“¦ å¤‡ä»½æ–‡ä»¶: {backup_path}")
            
            # æ¸…ç†å†…å®¹
            cleaned_content = self.clean_file_content(content, file_type)
            
            # å†™å…¥æ–‡ä»¶
            with open(target_path, 'w', encoding=self.config.encoding) as f:
                f.write(cleaned_content)
            
            # æ›´æ–°ç¼“å­˜
            if self.config.enable_cache:
                self.file_cache[str(target_path)] = cleaned_content
                self.file_metadata_cache[str(target_path)] = {
                    "size": len(cleaned_content),
                    "write_time": time.time(),
                    "file_type": file_type
                }
            
            # åˆ›å»ºæ–‡ä»¶å¼•ç”¨
            file_ref = FileReference(
                file_path=str(target_path),
                file_type=file_type,
                description=f"å†™å…¥çš„{file_type}æ–‡ä»¶",
                metadata={
                    "size": len(cleaned_content),
                    "creation_time": time.time(),
                    "file_type": file_type
                }
            )
            
            self.operation_stats['total_writes'] += 1
            self.logger.info(f"ğŸ’¾ æˆåŠŸå†™å…¥æ–‡ä»¶: {target_path}")
            return file_ref
            
        except Exception as e:
            self.operation_stats['errors'] += 1
            self.logger.error(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥ {file_path or filename}: {str(e)}")
            return None
    
    def clean_file_content(self, content: str, file_type: str) -> str:
        """æ¸…ç†æ–‡ä»¶å†…å®¹"""
        if not content:
            return ""
        
        cleaned_content = content.strip()
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œç‰¹å®šæ¸…ç†
        if file_type in ["verilog", "systemverilog"]:
            return self._clean_verilog_content(cleaned_content)
        elif file_type in ["testbench", "tb"]:
            return self._clean_testbench_content(cleaned_content)
        elif file_type == "json":
            return self._clean_json_content(cleaned_content)
        else:
            return self._clean_general_content(cleaned_content)
    
    def _clean_verilog_content(self, content: str) -> str:
        """æ¸…ç†Verilogå†…å®¹"""
        # æå–Verilogä»£ç 
        extracted_code = self.extract_verilog_code(content)
        
        if extracted_code and len(extracted_code.strip()) > 0:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«Verilogå…³é”®å­—
            verilog_keywords = ['module', 'endmodule', 'input', 'output', 'wire', 'reg', 'always', 'assign']
            if any(keyword in extracted_code.lower() for keyword in verilog_keywords):
                self.logger.info(f"ğŸ§¹ ä½¿ç”¨æå–çš„Verilogä»£ç ")
                return extracted_code
        
        # å¦‚æœæå–å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¸…ç†
        return self._traditional_clean_content(content)
    
    def _clean_testbench_content(self, content: str) -> str:
        """æ¸…ç†æµ‹è¯•å°å†…å®¹"""
        # ç§»é™¤markdownæ ¼å¼
        content = re.sub(r'```(?:verilog|systemverilog)?\s*\n', '', content)
        content = re.sub(r'\n```\s*$', '', content)
        
        # ç§»é™¤ä»£ç å—æ ‡è®°
        content = re.sub(r'^```.*$', '', content, flags=re.MULTILINE)
        content = re.sub(r'```$', '', content)
        
        return content.strip()
    
    def _clean_json_content(self, content: str) -> str:
        """æ¸…ç†JSONå†…å®¹"""
        # ç§»é™¤markdownä»£ç å—
        content = re.sub(r'```json\s*\n', '', content)
        content = re.sub(r'\n```\s*$', '', content)
        
        # å°è¯•è§£æJSONä»¥éªŒè¯æ ¼å¼
        try:
            import json
            json.loads(content)
            return content.strip()
        except json.JSONDecodeError:
            self.logger.warning("JSONæ ¼å¼æ— æ•ˆï¼Œå°è¯•ä¿®å¤...")
            return content.strip()
    
    def _clean_general_content(self, content: str) -> str:
        """æ¸…ç†é€šç”¨å†…å®¹"""
        return self._traditional_clean_content(content)
    
    def _traditional_clean_content(self, content: str) -> str:
        """ä¼ ç»Ÿå†…å®¹æ¸…ç†æ–¹æ³•"""
        # ç§»é™¤markdownæ ¼å¼æ ‡è®°
        content = re.sub(r'```(?:[a-zA-Z]+)?\s*\n', '', content)
        content = re.sub(r'\n```\s*$', '', content)
        
        # ç§»é™¤ä»£ç å—æ ‡è®°
        content = re.sub(r'^```.*$', '', content, flags=re.MULTILINE)
        content = re.sub(r'```$', '', content)
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½è¡Œ
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        
        return content.strip()
    
    def extract_verilog_code(self, content: str) -> str:
        """æå–Verilogä»£ç """
        if not content:
            return ""
        
        # æ–¹æ³•1: æŸ¥æ‰¾module...endmoduleç»“æ„
        module_pattern = r'(module\s+\w+.*?endmodule)'
        matches = re.findall(module_pattern, content, re.DOTALL | re.IGNORECASE)
        
        if matches:
            self.logger.debug(f"ğŸ” æ‰¾åˆ° {len(matches)} ä¸ªVerilogæ¨¡å—")
            return '\n\n'.join(matches)
        
        # æ–¹æ³•2: æŸ¥æ‰¾ä»£ç å—
        code_block_pattern = r'```(?:verilog|systemverilog)?\s*\n(.*?)\n```'
        matches = re.findall(code_block_pattern, content, re.DOTALL)
        
        if matches:
            self.logger.debug(f"ğŸ” æ‰¾åˆ° {len(matches)} ä¸ªä»£ç å—")
            return '\n\n'.join(matches)
        
        # æ–¹æ³•3: æŸ¥æ‰¾åŒ…å«Verilogå…³é”®å­—çš„å†…å®¹
        verilog_keywords = ['module', 'endmodule', 'input', 'output', 'wire', 'reg', 'always', 'assign']
        lines = content.split('\n')
        verilog_lines = []
        in_verilog_block = False
        
        for line in lines:
            line_lower = line.lower()
            if any(keyword in line_lower for keyword in verilog_keywords):
                in_verilog_block = True
            
            if in_verilog_block:
                verilog_lines.append(line)
            
            if 'endmodule' in line_lower:
                in_verilog_block = False
        
        if verilog_lines:
            self.logger.debug(f"ğŸ” æå–åˆ° {len(verilog_lines)} è¡ŒVerilogä»£ç ")
            return '\n'.join(verilog_lines)
        
        return content
    
    def is_valid_verilog_code(self, code: str) -> bool:
        """éªŒè¯Verilogä»£ç """
        if not code or not code.strip():
            return False
        
        # åŸºæœ¬æ£€æŸ¥
        code_lower = code.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åŸºæœ¬Verilogç»“æ„
        has_module = 'module' in code_lower
        has_endmodule = 'endmodule' in code_lower
        
        if not has_module or not has_endmodule:
            return False
        
        # æ£€æŸ¥æ¨¡å—åæ ¼å¼
        module_match = re.search(r'module\s+(\w+)', code, re.IGNORECASE)
        if not module_match:
            return False
        
        module_name = module_match.group(1)
        if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', module_name):
            return False
        
        # æ£€æŸ¥åŸºæœ¬è¯­æ³•
        verilog_keywords = ['input', 'output', 'wire', 'reg', 'always', 'assign', 'begin', 'end']
        has_keywords = any(keyword in code_lower for keyword in verilog_keywords)
        
        if not has_keywords:
            return False
        
        # æ£€æŸ¥æ‹¬å·åŒ¹é…
        open_parens = code.count('(')
        close_parens = code.count(')')
        if open_parens != close_parens:
            return False
        
        return True
    
    def detect_file_type(self, file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç±»å‹"""
        if not file_path:
            return "unknown"
        
        # æ ¹æ®æ‰©å±•ååˆ¤æ–­
        ext = Path(file_path).suffix.lower()
        
        type_mapping = {
            '.v': 'verilog',
            '.sv': 'systemverilog',
            '.tb': 'testbench',
            '.json': 'json',
            '.txt': 'text',
            '.md': 'markdown',
            '.py': 'python',
            '.sh': 'shell',
            '.tcl': 'tcl'
        }
        
        return type_mapping.get(ext, "unknown")
    
    def determine_file_type(self, filename: str, content: str) -> str:
        """æ ¹æ®æ–‡ä»¶åå’Œå†…å®¹ç¡®å®šæ–‡ä»¶ç±»å‹"""
        # é¦–å…ˆæ ¹æ®æ–‡ä»¶ååˆ¤æ–­
        file_type = self.detect_file_type(filename)
        
        if file_type != "unknown":
            return file_type
        
        # æ ¹æ®å†…å®¹åˆ¤æ–­
        content_lower = content.lower()
        
        if 'module' in content_lower and 'endmodule' in content_lower:
            return 'verilog'
        elif 'testbench' in content_lower or 'tb' in content_lower:
            return 'testbench'
        elif content.strip().startswith('{') and content.strip().endswith('}'):
            return 'json'
        elif '```' in content:
            return 'markdown'
        
        return 'text'
    
    def clear_cache(self, file_path: str = None):
        """æ¸…é™¤ç¼“å­˜"""
        if file_path:
            self.file_cache.pop(file_path, None)
            self.file_metadata_cache.pop(file_path, None)
            self.logger.debug(f"æ¸…é™¤æ–‡ä»¶ç¼“å­˜: {file_path}")
        else:
            self.file_cache.clear()
            self.file_metadata_cache.clear()
            self.logger.debug("æ¸…é™¤æ‰€æœ‰æ–‡ä»¶ç¼“å­˜")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        return {
            'cache_size': len(self.file_cache),
            'metadata_size': len(self.file_metadata_cache),
            'total_size': sum(len(content) for content in self.file_cache.values()),
            'cache_hits': self.operation_stats['cache_hits'],
            'cache_misses': self.operation_stats['cache_misses']
        }
    
    def get_operation_stats(self) -> Dict[str, Any]:
        """è·å–æ“ä½œç»Ÿè®¡"""
        stats = self.operation_stats.copy()
        
        # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
        total_cache_access = stats['cache_hits'] + stats['cache_misses']
        if total_cache_access > 0:
            stats['cache_hit_rate'] = stats['cache_hits'] / total_cache_access
        else:
            stats['cache_hit_rate'] = 0.0
        
        return stats
    
    def clear_stats(self):
        """æ¸…é™¤ç»Ÿè®¡"""
        self.operation_stats = {
            'total_reads': 0,
            'total_writes': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'errors': 0
        }
    
    def list_files(self, directory: str = None, pattern: str = "*") -> List[str]:
        """åˆ—å‡ºæ–‡ä»¶"""
        try:
            dir_path = Path(directory) if directory else Path(self.config.default_artifacts_dir)
            files = list(dir_path.glob(pattern))
            return [str(f) for f in files if f.is_file()]
        except Exception as e:
            self.logger.error(f"åˆ—å‡ºæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def get_file_info(self, file_path: str) -> Optional[Dict[str, Any]]:
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        try:
            path = Path(file_path)
            if not path.exists():
                return None
            
            stat = path.stat()
            return {
                'path': str(path),
                'size': stat.st_size,
                'modified_time': stat.st_mtime,
                'created_time': stat.st_ctime,
                'file_type': self.detect_file_type(str(path)),
                'in_cache': str(path) in self.file_cache
            }
        except Exception as e:
            self.logger.error(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")
            return None 