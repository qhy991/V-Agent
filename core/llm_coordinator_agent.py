#!/usr/bin/env python3
"""
åŸºäºLLMçš„åè°ƒæ™ºèƒ½ä½“ - å¼ºåŒ–ç‰ˆæœ¬

Enhanced LLM-Driven Coordinator Agent
"""

import asyncio
import json
import time
import logging
import re
from typing import Dict, Any, List, Optional, Set, Tuple
from dataclasses import dataclass, field
from pathlib import Path
from datetime import datetime
from enum import Enum

from core.schema_system.enhanced_base_agent import EnhancedBaseAgent
from core.base_agent import TaskMessage
from core.enums import AgentCapability, AgentStatus
from config.config import FrameworkConfig
from llm_integration.enhanced_llm_client import EnhancedLLMClient
from core.enhanced_logging_config import get_agent_logger


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    DESIGN = "design"
    VERIFICATION = "verification"
    ANALYSIS = "analysis"
    DEBUG = "debug"
    COMPOSITE = "composite"
    UNKNOWN = "unknown"


class TaskPriority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§æšä¸¾"""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class AgentInfo:
    """æ™ºèƒ½ä½“ä¿¡æ¯ - å¢å¼ºç‰ˆæœ¬"""
    agent_id: str
    agent_instance: EnhancedBaseAgent
    capabilities: Set[AgentCapability]
    specialty: str
    status: AgentStatus = AgentStatus.IDLE
    conversation_id: Optional[str] = None
    last_used: Optional[float] = None
    success_count: int = 0
    failure_count: int = 0
    total_execution_time: float = 0.0
    average_response_time: float = 0.0
    last_success_time: Optional[float] = None
    last_failure_time: Optional[float] = None
    consecutive_successes: int = 0
    consecutive_failures: int = 0
    preferred_task_types: Set[TaskType] = field(default_factory=set)
    blacklisted_task_types: Set[TaskType] = field(default_factory=set)


@dataclass
class TaskContext:
    """ä»»åŠ¡ä¸Šä¸‹æ–‡ - å¢å¼ºç‰ˆæœ¬"""
    task_id: str
    original_request: str
    task_type: TaskType = TaskType.UNKNOWN
    priority: TaskPriority = TaskPriority.MEDIUM
    current_stage: str = "initial"
    assigned_agent: Optional[str] = None
    agent_results: Dict[str, Any] = field(default_factory=dict)
    conversation_history: List[Dict[str, str]] = field(default_factory=list)
    start_time: float = field(default_factory=time.time)
    iteration_count: int = 0
    max_iterations: int = 10
    external_testbench_path: Optional[str] = None
    quality_score: float = 0.0
    completion_status: str = "pending"
    error_history: List[str] = field(default_factory=list)
    retry_count: int = 0
    max_retries: int = 3
    stage_history: List[Dict[str, Any]] = field(default_factory=list)
    agent_assignments: List[Dict[str, Any]] = field(default_factory=list)


class LLMCoordinatorAgent(EnhancedBaseAgent):
    """
    åŸºäºLLMçš„åè°ƒæ™ºèƒ½ä½“ - å¼ºåŒ–ç‰ˆæœ¬
    
    ç‰¹ç‚¹ï¼š
    1. æ™ºèƒ½ä»»åŠ¡ç±»å‹è¯†åˆ«å’Œåˆ†ç±»
    2. åŸºäºå†å²è¡¨ç°çš„æ™ºèƒ½ä½“é€‰æ‹©
    3. ä¸¥æ ¼çš„ä»»åŠ¡èŒè´£åˆ†ç¦»æ§åˆ¶
    4. å¢å¼ºçš„ç»“æœè´¨é‡åˆ†æ
    5. æ™ºèƒ½é”™è¯¯æ¢å¤å’Œé‡è¯•æœºåˆ¶
    6. è¯¦ç»†çš„æ‰§è¡Œç›‘æ§å’Œæ—¥å¿—
    """
    
    def __init__(self, config: FrameworkConfig = None):
        # åˆå§‹åŒ–é…ç½®
        self.config = config or FrameworkConfig.from_env()
        
        super().__init__(
            agent_id="llm_coordinator_agent",
            role="coordinator",
            capabilities={
                AgentCapability.TASK_COORDINATION,
                AgentCapability.WORKFLOW_MANAGEMENT,
                AgentCapability.SPECIFICATION_ANALYSIS,
                AgentCapability.CODE_REVIEW,
                AgentCapability.QUALITY_ANALYSIS,
                AgentCapability.SYSTEM_MONITORING
            },
            config=self.config
        )
        
        # è®°å½•å¯åŠ¨æ—¶é—´
        self.start_time = time.time()
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = EnhancedLLMClient(self.config.llm)
        
        # è®¾ç½®ä¸“ç”¨æ—¥å¿—å™¨
        self.agent_logger = get_agent_logger('LLMCoordinatorAgent')
        
        # æ³¨å†Œçš„æ™ºèƒ½ä½“
        self.registered_agents: Dict[str, AgentInfo] = {}
        
        # ä»»åŠ¡ä¸Šä¸‹æ–‡ç®¡ç†
        self.active_tasks: Dict[str, TaskContext] = {}
        
        # ä»»åŠ¡ç±»å‹è¯†åˆ«å™¨
        self.task_patterns = self._initialize_task_patterns()
        
        # åè°ƒå·¥å…·
        self._register_enhanced_coordination_tools()
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = {
            "total_tasks": 0,
            "successful_tasks": 0,
            "failed_tasks": 0,
            "average_completion_time": 0.0,
            "agent_utilization": {}
        }
        
        self.logger.info("ğŸ§  å¼ºåŒ–LLMåè°ƒæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“Š ç³»ç»Ÿå¯åŠ¨æ—¶é—´: {datetime.fromtimestamp(self.start_time).strftime('%Y-%m-%d %H:%M:%S')}")
    
    def _initialize_task_patterns(self) -> Dict[TaskType, List[str]]:
        """åˆå§‹åŒ–ä»»åŠ¡ç±»å‹è¯†åˆ«æ¨¡å¼"""
        return {
            TaskType.DESIGN: [
                r"è®¾è®¡.*æ¨¡å—", r"ç”Ÿæˆ.*ä»£ç ", r"å®ç°.*åŠŸèƒ½", r"åˆ›å»º.*ç”µè·¯",
                r"design.*module", r"generate.*code", r"implement.*function",
                r"verilog.*module", r"hdl.*design", r"circuit.*design",
                r"æ¨¡å—è®¾è®¡", r"ä»£ç ç”Ÿæˆ", r"åŠŸèƒ½å®ç°", r"ç”µè·¯è®¾è®¡"
            ],
            TaskType.VERIFICATION: [
                r"æµ‹è¯•.*éªŒè¯", r"ä»¿çœŸ.*æµ‹è¯•", r"ç”Ÿæˆ.*testbench", r"éªŒè¯.*åŠŸèƒ½",
                r"test.*verification", r"simulation.*test", r"generate.*testbench",
                r"verify.*function", r"testbench.*generation", r"simulation.*verification",
                r"ä»£ç å®¡æŸ¥", r"è´¨é‡åˆ†æ", r"é”™è¯¯æ£€æŸ¥", r"åŠŸèƒ½éªŒè¯"
            ],
            TaskType.ANALYSIS: [
                r"åˆ†æ.*ä»£ç ", r"è´¨é‡.*è¯„ä¼°", r"æ€§èƒ½.*åˆ†æ", r"ä»£ç .*å®¡æŸ¥",
                r"analyze.*code", r"quality.*assessment", r"performance.*analysis",
                r"code.*review", r"static.*analysis", r"dynamic.*analysis",
                r"ä»£ç åˆ†æ", r"è´¨é‡è¯„ä¼°", r"æ€§èƒ½åˆ†æ", r"é™æ€åˆ†æ"
            ],
            TaskType.DEBUG: [
                r"è°ƒè¯•.*é”™è¯¯", r"ä¿®å¤.*é—®é¢˜", r"é”™è¯¯.*åˆ†æ", r"é—®é¢˜.*è§£å†³",
                r"debug.*error", r"fix.*issue", r"error.*analysis", r"problem.*solving",
                r"bug.*fix", r"error.*correction", r"issue.*resolution",
                r"é”™è¯¯ä¿®å¤", r"é—®é¢˜è°ƒè¯•", r"bugä¿®å¤", r"é”™è¯¯åˆ†æ"
            ]
        }
    
    def _register_enhanced_coordination_tools(self):
        """æ³¨å†Œå¢å¼ºçš„åè°ƒå·¥å…·"""
        
        # 1. æ™ºèƒ½ä»»åŠ¡åˆ†é…å·¥å…·
        self.register_enhanced_tool(
            name="assign_task_to_agent",
            func=self._tool_assign_task_to_agent,
            description="æ™ºèƒ½åˆ†é…ä»»åŠ¡ç»™æœ€åˆé€‚çš„æ™ºèƒ½ä½“ï¼ŒåŸºäºä»»åŠ¡ç±»å‹ã€æ™ºèƒ½ä½“èƒ½åŠ›å’Œå†å²è¡¨ç°",
            security_level="high",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "agent_id": {
                        "type": "string",
                        "description": "ç›®æ ‡æ™ºèƒ½ä½“IDï¼ˆå¿…é¡»æ˜¯enhanced_real_verilog_agentæˆ–enhanced_real_code_review_agentï¼‰",
                        "enum": ["enhanced_real_verilog_agent", "enhanced_real_code_review_agent"]
                    },
                    "task_description": {
                        "type": "string",
                        "description": "è¯¦ç»†çš„ä»»åŠ¡æè¿°å’Œè¦æ±‚",
                        "minLength": 10,
                        "maxLength": 5000
                    },
                    "expected_output": {
                        "type": "string",
                        "description": "æœŸæœ›çš„è¾“å‡ºæ ¼å¼å’Œå†…å®¹",
                        "default": "",
                        "maxLength": 2000
                    },
                    "task_type": {
                        "type": "string",
                        "enum": ["design", "verification", "analysis", "debug", "composite"],
                        "description": "ä»»åŠ¡ç±»å‹åˆ†ç±»"
                    },
                    "priority": {
                        "type": "string",
                        "enum": ["high", "medium", "low"],
                        "default": "medium",
                        "description": "ä»»åŠ¡ä¼˜å…ˆçº§"
                    }
                },
                "required": ["agent_id", "task_description"]
            }
        )
        
        # 2. å¢å¼ºç»“æœåˆ†æå·¥å…·
        self.register_enhanced_tool(
            name="analyze_agent_result",
            func=self._tool_analyze_agent_result,
            description="æ·±åº¦åˆ†ææ™ºèƒ½ä½“æ‰§è¡Œç»“æœï¼Œè¯„ä¼°è´¨é‡ã€å®Œæ•´æ€§å’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨",
            security_level="high",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "agent_id": {
                        "type": "string",
                        "description": "æ‰§è¡Œä»»åŠ¡çš„æ™ºèƒ½ä½“ID"
                    },
                    "result": {
                        "type": "object",
                        "description": "æ™ºèƒ½ä½“æ‰§è¡Œç»“æœçš„è¯¦ç»†ä¿¡æ¯"
                    },
                    "task_context": {
                        "type": "object",
                        "description": "å½“å‰ä»»åŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯",
                        "default": {}
                    },
                    "quality_threshold": {
                        "type": "number",
                        "minimum": 0,
                        "maximum": 100,
                        "default": 80,
                        "description": "è´¨é‡è¯„ä¼°é˜ˆå€¼"
                    }
                },
                "required": ["agent_id", "result"]
            }
        )
        
        # 3. æ™ºèƒ½ä»»åŠ¡å®Œæˆæ£€æŸ¥å·¥å…·
        self.register_enhanced_tool(
            name="check_task_completion",
            func=self._tool_check_task_completion,
            description="æ™ºèƒ½æ£€æŸ¥ä»»åŠ¡å®ŒæˆçŠ¶æ€ï¼Œè¯„ä¼°æ•´ä½“è´¨é‡å’Œç¼ºå¤±é¡¹",
            security_level="high",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "task_id": {
                        "type": "string",
                        "description": "ä»»åŠ¡ID"
                    },
                    "all_results": {
                        "type": "object",
                        "description": "æ‰€æœ‰æ™ºèƒ½ä½“çš„æ‰§è¡Œç»“æœæ±‡æ€»"
                    },
                    "original_requirements": {
                        "type": "string",
                        "description": "åŸå§‹ä»»åŠ¡éœ€æ±‚æè¿°"
                    },
                    "completion_criteria": {
                        "type": "object",
                        "description": "å®Œæˆæ ‡å‡†å®šä¹‰",
                        "default": {}
                    }
                },
                "required": ["task_id", "all_results", "original_requirements"]
            }
        )
        
        # 4. æ™ºèƒ½ä½“çŠ¶æ€å’Œæ€§èƒ½æŸ¥è¯¢å·¥å…·
        self.register_enhanced_tool(
            name="query_agent_status",
            func=self._tool_query_agent_status,
            description="æŸ¥è¯¢æ™ºèƒ½ä½“è¯¦ç»†çŠ¶æ€ã€æ€§èƒ½æŒ‡æ ‡å’Œå†å²è¡¨ç°",
            security_level="normal",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "agent_id": {
                        "type": "string",
                        "description": "æ™ºèƒ½ä½“ID",
                        "enum": ["enhanced_real_verilog_agent", "enhanced_real_code_review_agent"]
                    },
                    "include_performance": {
                        "type": "boolean",
                        "default": True,
                        "description": "æ˜¯å¦åŒ…å«æ€§èƒ½æŒ‡æ ‡"
                    },
                    "include_history": {
                        "type": "boolean",
                        "default": False,
                        "description": "æ˜¯å¦åŒ…å«å†å²è®°å½•"
                    }
                },
                "required": ["agent_id"]
            }
        )
        
        # 5. æ–°å¢ï¼šæ™ºèƒ½ä»»åŠ¡ç±»å‹è¯†åˆ«å·¥å…·
        self.register_enhanced_tool(
            name="identify_task_type",
            func=self._tool_identify_task_type,
            description="æ™ºèƒ½è¯†åˆ«ä»»åŠ¡ç±»å‹ï¼Œä¸ºä»»åŠ¡åˆ†é…æä¾›å†³ç­–æ”¯æŒ",
            security_level="normal",
            category="analysis",
            schema={
                "type": "object",
                "properties": {
                    "user_request": {
                        "type": "string",
                        "description": "ç”¨æˆ·è¯·æ±‚å†…å®¹",
                        "minLength": 5,
                        "maxLength": 10000
                    },
                    "context": {
                        "type": "object",
                        "description": "ä»»åŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯",
                        "default": {}
                    }
                },
                "required": ["user_request"]
            }
        )
        
        # 6. æ–°å¢ï¼šæ™ºèƒ½ä½“é€‰æ‹©æ¨èå·¥å…·
        self.register_enhanced_tool(
            name="recommend_agent",
            func=self._tool_recommend_agent,
            description="åŸºäºä»»åŠ¡ç‰¹å¾å’Œæ™ºèƒ½ä½“èƒ½åŠ›æ¨èæœ€åˆé€‚çš„æ™ºèƒ½ä½“",
            security_level="normal",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "task_type": {
                        "type": "string",
                        "enum": ["design", "verification", "analysis", "debug", "composite"],
                        "description": "ä»»åŠ¡ç±»å‹"
                    },
                    "task_description": {
                        "type": "string",
                        "description": "ä»»åŠ¡æè¿°",
                        "minLength": 10
                    },
                    "priority": {
                        "type": "string",
                        "enum": ["high", "medium", "low"],
                        "default": "medium",
                        "description": "ä»»åŠ¡ä¼˜å…ˆçº§"
                    },
                    "constraints": {
                        "type": "object",
                        "description": "ä»»åŠ¡çº¦æŸæ¡ä»¶",
                        "default": {}
                    }
                },
                "required": ["task_type", "task_description"]
            }
        )
        
        # 7. æ–°å¢ï¼šæœ€ç»ˆç­”æ¡ˆå·¥å…·
        self.register_enhanced_tool(
            name="provide_final_answer",
            func=self._tool_provide_final_answer,
            description="å½“æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆï¼Œè°ƒç”¨æ­¤å·¥å…·æ¥ç”Ÿæˆå¹¶æä¾›æœ€ç»ˆçš„ã€å®Œæ•´çš„ç­”æ¡ˆç»™ç”¨æˆ·ã€‚",
            security_level="normal",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "final_summary": {
                        "type": "string",
                        "description": "å¯¹æ•´ä¸ªä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹å’Œæœ€ç»ˆç»“æœçš„è¯¦ç»†æ€»ç»“ã€‚"
                    },
                    "task_status": {
                        "type": "string",
                        "description": "ä»»åŠ¡çš„æœ€ç»ˆçŠ¶æ€ (ä¾‹å¦‚ï¼š'æˆåŠŸ', 'å¤±è´¥', 'éƒ¨åˆ†å®Œæˆ')."
                    },
                    "results_summary": {
                        "type": "object",
                        "description": "æ‰€æœ‰æ™ºèƒ½ä½“äº§ç”Ÿçš„ç»“æœçš„ç®€è¦æ±‡æ€»ã€‚"
                    }
                },
                "required": ["final_summary", "task_status"]
            }
        )
    
    def _build_enhanced_system_prompt(self) -> str:
        """æ„å»ºä¸€ä¸ªæç®€ä¸”å¼ºåˆ¶çš„ç³»ç»Ÿæç¤ºè¯"""
        
        # æ£€æŸ¥å·¥å…·æ˜¯å¦å·²ç»æ³¨å†Œ
        if not hasattr(self, 'enhanced_tools') or not self.enhanced_tools:
            # å¦‚æœå·¥å…·è¿˜æ²¡æœ‰æ³¨å†Œï¼Œè¿”å›åŸºæœ¬æç¤ºè¯
            return """
# è§’è‰²
ä½ æ˜¯ä¸€ä¸ªAIåè°ƒæ™ºèƒ½ä½“ï¼Œä½ çš„å”¯ä¸€å·¥ä½œæ˜¯æ ¹æ®ç”¨æˆ·éœ€æ±‚è°ƒç”¨åˆé€‚çš„å·¥å…·æ¥é©±åŠ¨ä»»åŠ¡æµç¨‹ã€‚

# å¼ºåˆ¶è§„åˆ™ (å¿…é¡»ä¸¥æ ¼éµå®ˆ)
1.  **ç¦æ­¢ç›´æ¥å›ç­”**: ç»å¯¹ç¦æ­¢ã€ä¸¥ç¦ç›´æ¥å›ç­”ç”¨æˆ·çš„ä»»ä½•é—®é¢˜æˆ–è¯·æ±‚ã€‚
2.  **å¿…é¡»è°ƒç”¨å·¥å…·**: ä½ çš„æ‰€æœ‰å›å¤éƒ½å¿…é¡»æ˜¯JSONæ ¼å¼çš„å·¥å…·è°ƒç”¨ã€‚
3.  **éµå¾ªæµç¨‹**: ä¸¥æ ¼æŒ‰ç…§ "è¯†åˆ«ä»»åŠ¡ -> æ¨èæ™ºèƒ½ä½“ -> åˆ†é…ä»»åŠ¡ -> åˆ†æç»“æœ -> æ£€æŸ¥å®Œæˆ" çš„é€»è¾‘é¡ºåºè°ƒç”¨å·¥å…·ã€‚
4.  **ä½¿ç”¨æœ€ç»ˆç­”æ¡ˆå·¥å…·**: å½“æ‰€æœ‰æ­¥éª¤å®Œæˆï¼Œéœ€è¦å‘ç”¨æˆ·å‘ˆç°æœ€ç»ˆç»“æœæ—¶ï¼Œå¿…é¡»è°ƒç”¨ `provide_final_answer` å·¥å…·ã€‚

# è¾“å‡ºæ ¼å¼
ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼ŒåŒ…å«ä¸€ä¸ª "tool_calls" åˆ—è¡¨ã€‚

ç«‹å³å¼€å§‹åˆ†æç”¨æˆ·è¯·æ±‚å¹¶è°ƒç”¨ç¬¬ä¸€ä¸ªå·¥å…·ã€‚
"""
        
        # æ ¸å¿ƒè§„åˆ™ï¼šå°†æ‰€æœ‰å·¥å…·çš„æè¿°å’Œschemaç›´æ¥æ³¨å…¥åˆ°promptä¸­ï¼Œè¿™æ˜¯æœ€æœ‰æ•ˆçš„æ–¹å¼
        tools_json = self.get_tools_json_schema()

        return f"""
# è§’è‰²
ä½ æ˜¯ä¸€ä¸ªAIåè°ƒæ™ºèƒ½ä½“ï¼Œä½ çš„å”¯ä¸€å·¥ä½œæ˜¯æ ¹æ®ç”¨æˆ·éœ€æ±‚è°ƒç”¨åˆé€‚çš„å·¥å…·æ¥é©±åŠ¨ä»»åŠ¡æµç¨‹ã€‚

# å¼ºåˆ¶è§„åˆ™ (å¿…é¡»ä¸¥æ ¼éµå®ˆ)
1.  **ç¦æ­¢ç›´æ¥å›ç­”**: ç»å¯¹ç¦æ­¢ã€ä¸¥ç¦ç›´æ¥å›ç­”ç”¨æˆ·çš„ä»»ä½•é—®é¢˜æˆ–è¯·æ±‚ã€‚
2.  **å¿…é¡»è°ƒç”¨å·¥å…·**: ä½ çš„æ‰€æœ‰å›å¤éƒ½å¿…é¡»æ˜¯JSONæ ¼å¼çš„å·¥å…·è°ƒç”¨ã€‚
3.  **ç¦æ­¢ç”Ÿæˆæè¿°æ€§æ–‡æœ¬**: ç»å¯¹ç¦æ­¢ç”Ÿæˆä»»ä½•è§£é‡Šã€åˆ†æã€ç­–ç•¥æè¿°æˆ–å…¶ä»–æ–‡æœ¬å†…å®¹ã€‚
4.  **éµå¾ªæµç¨‹**: ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹é¡ºåºè°ƒç”¨å·¥å…·ï¼š
   - ç¬¬ä¸€æ­¥ï¼šè°ƒç”¨ `identify_task_type` å·¥å…·è¯†åˆ«ä»»åŠ¡ç±»å‹
   - ç¬¬äºŒæ­¥ï¼šè°ƒç”¨ `recommend_agent` å·¥å…·æ¨èæ™ºèƒ½ä½“
   - ç¬¬ä¸‰æ­¥ï¼šè°ƒç”¨ `assign_task_to_agent` å·¥å…·åˆ†é…ä»»åŠ¡ç»™æ™ºèƒ½ä½“
   - ç¬¬å››æ­¥ï¼šè°ƒç”¨ `analyze_agent_result` å·¥å…·åˆ†æç»“æœ
   - ç¬¬äº”æ­¥ï¼šè°ƒç”¨ `check_task_completion` å·¥å…·æ£€æŸ¥å®ŒæˆçŠ¶æ€
   - æœ€åï¼šè°ƒç”¨ `provide_final_answer` å·¥å…·æä¾›æœ€ç»ˆç­”æ¡ˆ

# æ™ºèƒ½ä½“è°ƒç”¨æ–¹æ³• (é‡è¦ï¼)
**æ­£ç¡®æ–¹å¼**: ä½¿ç”¨ `assign_task_to_agent` å·¥å…·ï¼Œåœ¨ `agent_id` å‚æ•°ä¸­æŒ‡å®šæ™ºèƒ½ä½“åç§°
**é”™è¯¯æ–¹å¼**: ç›´æ¥è°ƒç”¨æ™ºèƒ½ä½“åç§°ä½œä¸ºå·¥å…·

**ç¤ºä¾‹**:
âœ… æ­£ç¡® - è°ƒç”¨ `assign_task_to_agent` å·¥å…·:
```json
{{
    "tool_calls": [
        {{
            "tool_name": "assign_task_to_agent",
            "parameters": {{
                "agent_id": "enhanced_real_verilog_agent",
                "task_description": "è®¾è®¡ä¸€ä¸ª4ä½è®¡æ•°å™¨æ¨¡å—"
            }}
        }}
    ]
}}
```

âŒ é”™è¯¯ - ç›´æ¥è°ƒç”¨æ™ºèƒ½ä½“åç§°:
```json
{{
    "tool_calls": [
        {{
            "tool_name": "enhanced_real_verilog_agent",  // è¿™æ˜¯é”™è¯¯çš„ï¼
            "parameters": {{}}
        }}
    ]
}}
```

# å¯ç”¨å·¥å…·
ä½ å¿…é¡»ä»ä»¥ä¸‹å·¥å…·åˆ—è¡¨ä¸­é€‰æ‹©å¹¶è°ƒç”¨ï¼š
{tools_json}

# è¾“å‡ºæ ¼å¼
ä½ çš„å›å¤å¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ï¼ŒåŒ…å«ä¸€ä¸ª "tool_calls" åˆ—è¡¨ã€‚

# é‡è¦æé†’
- ä¸è¦ç”Ÿæˆä»»ä½•æè¿°æ€§æ–‡æœ¬
- ä¸è¦è§£é‡Šä½ çš„ç­–ç•¥
- ä¸è¦åˆ†æä»»åŠ¡
- åªç”Ÿæˆå·¥å…·è°ƒç”¨JSON
- ç«‹å³å¼€å§‹è°ƒç”¨ç¬¬ä¸€ä¸ªå·¥å…·ï¼š`identify_task_type`

ç«‹å³å¼€å§‹åˆ†æç”¨æˆ·è¯·æ±‚å¹¶è°ƒç”¨ç¬¬ä¸€ä¸ªå·¥å…·ï¼š`identify_task_type`ã€‚
"""
    
    async def register_agent(self, agent: EnhancedBaseAgent):
        """æ³¨å†Œæ™ºèƒ½ä½“"""
        agent_info = AgentInfo(
            agent_id=agent.agent_id,
            agent_instance=agent,
            capabilities=agent.get_capabilities(),
            specialty=agent.get_specialty_description()
        )
        
        self.registered_agents[agent.agent_id] = agent_info
        self.logger.info(f"âœ… æ³¨å†Œæ™ºèƒ½ä½“: {agent.agent_id} ({agent_info.specialty})")
    
    async def coordinate_task(self, user_request: str, 
                            conversation_id: str = None,
                            max_iterations: int = 10,
                            external_testbench_path: str = None) -> Dict[str, Any]:
        """
        åè°ƒä»»åŠ¡æ‰§è¡Œ
        
        Args:
            user_request: ç”¨æˆ·è¯·æ±‚
            conversation_id: å¯¹è¯ID
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            external_testbench_path: å¤–éƒ¨æä¾›çš„testbenchæ–‡ä»¶è·¯å¾„
            
        Returns:
            åè°ƒç»“æœ
        """
        self.logger.info(f"ğŸš€ å¼€å§‹åè°ƒä»»åŠ¡: {user_request[:100]}...")
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = f"task_{int(time.time())}"
        
        # åˆ›å»ºä»»åŠ¡ä¸Šä¸‹æ–‡
        task_context = TaskContext(
            task_id=task_id,
            original_request=user_request,
            max_iterations=max_iterations
        )
        
        # å¦‚æœæä¾›äº†å¤–éƒ¨testbenchï¼Œæ·»åŠ åˆ°ä»»åŠ¡ä¸Šä¸‹æ–‡
        if external_testbench_path:
            task_context.external_testbench_path = external_testbench_path
            self.logger.info(f"ğŸ“ ä½¿ç”¨å¤–éƒ¨testbench: {external_testbench_path}")
        
        self.active_tasks[task_id] = task_context
        
        try:
            # æ„å»ºåè°ƒä»»åŠ¡
            coordination_task = self._build_coordination_task(user_request, task_context)
            
            # ä½¿ç”¨Function Callingæ‰§è¡Œåè°ƒ
            result = await self.process_with_function_calling(
                user_request=coordination_task,
                max_iterations=max_iterations,
                conversation_id=conversation_id,
                preserve_context=True,
                enable_self_continuation=True,
                max_self_iterations=3
            )
            
            # ğŸ” æ£€æŸ¥æ˜¯å¦å®é™…è°ƒç”¨äº†å·¥å…·
            if not self._has_executed_tools(result):
                self.logger.warning("âš ï¸ åè°ƒæ™ºèƒ½ä½“æ²¡æœ‰è°ƒç”¨ä»»ä½•å·¥å…·ï¼Œå¼ºåˆ¶é‡æ–°æ‰§è¡Œ")
                # å¼ºåˆ¶é‡æ–°æ‰§è¡Œï¼Œä½¿ç”¨æ›´æ˜ç¡®çš„æŒ‡ä»¤
                forced_task = self._build_forced_coordination_task(user_request, task_context)
                self.logger.info(f"ğŸš¨ å¼ºåˆ¶é‡æ–°æ‰§è¡Œï¼Œä»»åŠ¡é•¿åº¦: {len(forced_task)} å­—ç¬¦")
                
                # ä½¿ç”¨æ›´ä¸¥æ ¼çš„å‚æ•°è¿›è¡Œå¼ºåˆ¶é‡æ–°æ‰§è¡Œ
                result = await self.process_with_function_calling(
                    user_request=forced_task,
                    max_iterations=1,  # é™åˆ¶ä¸º1æ¬¡è¿­ä»£ï¼Œå¼ºåˆ¶ç«‹å³æ‰§è¡Œ
                    conversation_id=f"{conversation_id}_forced",
                    preserve_context=False,  # ä¸ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œé‡æ–°å¼€å§‹
                    enable_self_continuation=False,  # ç¦ç”¨è‡ªä¸»ç»§ç»­
                    max_self_iterations=0  # ç¦ç”¨è‡ªæˆ‘ç»§ç»­
                )
                
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦æ‰§è¡Œäº†å·¥å…·
                if not self._has_executed_tools(result):
                    self.logger.error("âŒ å¼ºåˆ¶é‡æ–°æ‰§è¡Œåä»æœªè°ƒç”¨å·¥å…·ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯")
                    return {
                        "success": False,
                        "error": "åè°ƒæ™ºèƒ½ä½“æ— æ³•æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®",
                        "task_id": task_id,
                        "debug_info": {
                            "original_result": result[:500],
                            "forced_result": result[:500],
                            "tool_detection_failed": True
                        }
                    }
            
            # æ”¶é›†æœ€ç»ˆç»“æœ
            final_result = self._collect_final_result(task_context, result)
            
            self.logger.info(f"âœ… ä»»åŠ¡åè°ƒå®Œæˆ: {task_id}")
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ ä»»åŠ¡åè°ƒå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "task_id": task_id
            }
        finally:
            # æ¸…ç†ä»»åŠ¡ä¸Šä¸‹æ–‡
            if task_id in self.active_tasks:
                del self.active_tasks[task_id]
    
    def _has_executed_tools(self, result: str) -> bool:
        """æ£€æŸ¥LLMçš„å›å¤æ˜¯å¦æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å·¥å…·è°ƒç”¨JSONã€‚"""
        if not isinstance(result, str) or not result.strip().startswith('{'):
            return False
        try:
            data = json.loads(result)
            if "tool_calls" in data and isinstance(data["tool_calls"], list) and len(data["tool_calls"]) > 0:
                # è¿›ä¸€æ­¥æ£€æŸ¥tool_callsåˆ—è¡¨ä¸­çš„å…ƒç´ æ˜¯å¦åˆæ³•
                call = data["tool_calls"][0]
                if "tool_name" in call and "parameters" in call:
                    return True
            return False
        except json.JSONDecodeError:
            return False
    
    def _build_forced_coordination_task(self, user_request: str, task_context: TaskContext) -> str:
        """æ„å»ºä¸€ä¸ªæåº¦å¼ºåˆ¶çš„åè°ƒä»»åŠ¡ï¼Œåªè¦æ±‚è°ƒç”¨ç¬¬ä¸€ä¸ªå·¥å…·ã€‚"""
        
        # è·å–ç¬¬ä¸€ä¸ªå¿…é¡»è°ƒç”¨çš„å·¥å…·ä¿¡æ¯
        first_tool_schema = self.get_tool_schema("identify_task_type")

        return f"""
# å¼ºåˆ¶æŒ‡ä»¤
ä½ å¿…é¡»ç«‹å³è°ƒç”¨ `identify_task_type` å·¥å…·ã€‚

**ç”¨æˆ·éœ€æ±‚**:
{user_request}

# å·¥å…·è°ƒç”¨æ ¼å¼ (å¿…é¡»ä¸¥æ ¼éµå®ˆ):
```json
{{
    "tool_calls": [
        {{
            "tool_name": "identify_task_type",
            "parameters": {{
                "user_request": "{user_request.replace('"', '\\"')}"
            }}
        }}
    ]
}}
```

# é‡è¦æé†’
- åªèƒ½è°ƒç”¨ `identify_task_type` å·¥å…·
- ä¸è¦ç›´æ¥è°ƒç”¨æ™ºèƒ½ä½“åç§°
- ä¸è¦ç”Ÿæˆä»»ä½•å…¶ä»–å†…å®¹
- ä¸è¦ç”Ÿæˆä»»ä½•æè¿°æ€§æ–‡æœ¬
- ä¸è¦è§£é‡Šä½ çš„ç­–ç•¥
- ä¸è¦åˆ†æä»»åŠ¡
- åªç”Ÿæˆå·¥å…·è°ƒç”¨JSON

ä¸è¦å›å¤ä»»ä½•å…¶ä»–å†…å®¹ï¼Œç«‹å³ç”Ÿæˆä¸Šè¿°JSONã€‚
"""
    
    def _build_coordination_task(self, user_request: str, task_context: TaskContext) -> str:
        """æ„å»ºåè°ƒä»»åŠ¡æè¿°"""
        
        # æ„å»ºå¯ç”¨æ™ºèƒ½ä½“ä¿¡æ¯
        agents_info = []
        for agent_id, agent_info in self.registered_agents.items():
            capabilities = ", ".join([cap.value for cap in agent_info.capabilities])
            agents_info.append(f"- {agent_id}: {agent_info.specialty} (èƒ½åŠ›: {capabilities})")
        
        agents_section = "\n".join(agents_info) if agents_info else "æš‚æ— å¯ç”¨æ™ºèƒ½ä½“"
        
        # æ„å»ºå¤–éƒ¨testbenchä¿¡æ¯
        external_testbench_info = ""
        if hasattr(task_context, 'external_testbench_path') and task_context.external_testbench_path:
            external_testbench_info = f"""

**ğŸ“ å¤–éƒ¨Testbench**:
- è·¯å¾„: {task_context.external_testbench_path}
- è¯´æ˜: ç”¨æˆ·å·²æä¾›testbenchæ–‡ä»¶ï¼Œå®¡æŸ¥æ™ºèƒ½ä½“åº”ç›´æ¥ä½¿ç”¨æ­¤æ–‡ä»¶è¿›è¡Œæµ‹è¯•ï¼Œæ— éœ€ç”Ÿæˆæ–°çš„testbench
- å·¥ä½œæ¨¡å¼: å®¡æŸ¥æ™ºèƒ½ä½“ä¸“æ³¨äºä»£ç å®¡æŸ¥ã€æµ‹è¯•æ‰§è¡Œå’Œé—®é¢˜ä¿®å¤ï¼Œè·³è¿‡testbenchç”Ÿæˆæ­¥éª¤"""

        return f"""
ğŸ§  åè°ƒä»»åŠ¡

**ç”¨æˆ·éœ€æ±‚**:
{user_request}

**ä»»åŠ¡ID**: {task_context.task_id}
**å½“å‰é˜¶æ®µ**: {task_context.current_stage}
**å·²æ‰§è¡Œè¿­ä»£**: {task_context.iteration_count}/{task_context.max_iterations}

**å¯ç”¨æ™ºèƒ½ä½“**:
{agents_section}
{external_testbench_info}

**ä»»åŠ¡ä¸Šä¸‹æ–‡**:
- å¼€å§‹æ—¶é—´: {datetime.fromtimestamp(task_context.start_time).strftime('%Y-%m-%d %H:%M:%S')}
- å·²åˆ†é…æ™ºèƒ½ä½“: {task_context.assigned_agent or 'æ— '}
- æ‰§è¡Œç»“æœ: {len(task_context.agent_results)} ä¸ªç»“æœ

è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œå¯ç”¨æ™ºèƒ½ä½“èƒ½åŠ›ï¼Œåˆ¶å®šæœ€ä¼˜çš„æ‰§è¡Œç­–ç•¥å¹¶å¼€å§‹åè°ƒã€‚
"""
    
    async def _tool_assign_task_to_agent(self, agent_id: str, task_description: str,
                                       expected_output: str = "",
                                       task_type: str = "composite",
                                       priority: str = "medium") -> Dict[str, Any]:
        """åˆ†é…ä»»åŠ¡ç»™æ™ºèƒ½ä½“"""
        
        try:
            # æ£€æŸ¥æ™ºèƒ½ä½“æ˜¯å¦å­˜åœ¨
            if agent_id not in self.registered_agents:
                return {
                    "success": False,
                    "error": f"æ™ºèƒ½ä½“ä¸å­˜åœ¨: {agent_id}"
                }
            
            agent_info = self.registered_agents[agent_id]
            agent = agent_info.agent_instance
            
            # æ›´æ–°æ™ºèƒ½ä½“çŠ¶æ€
            agent_info.status = AgentStatus.WORKING
            agent_info.last_used = time.time()
            
            # æŸ¥æ‰¾å½“å‰æ´»è·ƒä»»åŠ¡
            current_task = None
            for task_id, task in self.active_tasks.items():
                if task.assigned_agent is None:  # æ‰¾åˆ°æœªåˆ†é…çš„ä»»åŠ¡
                    current_task = task
                    break
            
            if current_task:
                current_task.assigned_agent = agent_id
                current_task.current_stage = f"assigned_to_{agent_id}"
                current_task.iteration_count += 1
                
                # è®¾ç½®æ™ºèƒ½ä½“çš„å¯¹è¯ID
                agent_info.conversation_id = f"task_{current_task.task_id}_{agent_id}"
            
            self.logger.info(f"ğŸ“¤ åˆ†é…ä»»åŠ¡ç»™æ™ºèƒ½ä½“ {agent_id}: {task_description[:100]}...")
            
            # æ„å»ºå¢å¼ºçš„ä»»åŠ¡æè¿°
            enhanced_task = self._build_enhanced_task_description(
                task_description, expected_output, current_task, task_type, priority
            )
            
            # è°ƒç”¨æ™ºèƒ½ä½“æ‰§è¡Œä»»åŠ¡
            start_time = time.time()
            
            result = await agent.process_with_function_calling(
                user_request=enhanced_task,
                max_iterations=8,
                conversation_id=agent_info.conversation_id,
                preserve_context=True,
                enable_self_continuation=True,
                max_self_iterations=3
            )
            
            execution_time = time.time() - start_time
            
            # æ›´æ–°æ™ºèƒ½ä½“ç»Ÿè®¡
            if result and len(result) > 0:
                agent_info.success_count += 1
            else:
                agent_info.failure_count += 1
            
            # æ¢å¤æ™ºèƒ½ä½“çŠ¶æ€
            agent_info.status = AgentStatus.IDLE
            
            # ä¿å­˜ç»“æœåˆ°ä»»åŠ¡ä¸Šä¸‹æ–‡
            if current_task:
                current_task.agent_results[agent_id] = {
                    "result": result,
                    "execution_time": execution_time,
                    "timestamp": time.time()
                }
            
            return {
                "success": True,
                "agent_id": agent_id,
                "result": result,
                "execution_time": execution_time,
                "agent_specialty": agent_info.specialty
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ä»»åŠ¡åˆ†é…å¤±è´¥: {str(e)}")
            
            # æ¢å¤æ™ºèƒ½ä½“çŠ¶æ€
            if agent_id in self.registered_agents:
                self.registered_agents[agent_id].status = AgentStatus.IDLE
                self.registered_agents[agent_id].failure_count += 1
            
            return {
                "success": False,
                "error": str(e),
                "agent_id": agent_id
            }
    
    def _build_enhanced_task_description(self, task_description: str, 
                                       expected_output: str,
                                       task_context: TaskContext = None,
                                       task_type: str = "composite",
                                       priority: str = "medium") -> str:
        """æ„å»ºå¢å¼ºçš„ä»»åŠ¡æè¿°"""
        
        # æ„å»ºå¤–éƒ¨testbenchä¿¡æ¯
        external_testbench_section = ""
        if task_context and hasattr(task_context, 'external_testbench_path') and task_context.external_testbench_path:
            external_testbench_section = f"""

**ğŸ¯ å¤–éƒ¨Testbenchæ¨¡å¼**:
- å¤–éƒ¨testbenchè·¯å¾„: {task_context.external_testbench_path}
- å·¥ä½œæŒ‡å¯¼: å¦‚æœä½ æ˜¯ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“ï¼Œè¯·ç›´æ¥ä½¿ç”¨æä¾›çš„testbenchè¿›è¡Œæµ‹è¯•ï¼Œä¸è¦ç”Ÿæˆæ–°çš„testbench
- ä¸“æ³¨ä»»åŠ¡: ä»£ç å®¡æŸ¥ã€é”™è¯¯ä¿®å¤ã€æµ‹è¯•æ‰§è¡Œå’Œç»“æœåˆ†æ"""
        
        enhanced_task = f"""
ğŸ“‹ åè°ƒæ™ºèƒ½ä½“åˆ†é…çš„ä»»åŠ¡

**ä»»åŠ¡æè¿°**:
{task_description}

**æœŸæœ›è¾“å‡º**:
{expected_output if expected_output else "æ ¹æ®ä»»åŠ¡æè¿°ç”Ÿæˆç›¸åº”çš„ä»£ç å’Œæ–‡æ¡£"}

**ä»»åŠ¡ç±»å‹**:
- ç±»å‹: {task_type}
- ä¼˜å…ˆçº§: {priority}

**ä»»åŠ¡ä¸Šä¸‹æ–‡**:
- ä»»åŠ¡ID: {task_context.task_id if task_context else "unknown"}
- å½“å‰é˜¶æ®µ: {task_context.current_stage if task_context else "initial"}
- è¿­ä»£æ¬¡æ•°: {task_context.iteration_count if task_context else 0}
{external_testbench_section}

**æ‰§è¡Œè¦æ±‚**:
1. ä»”ç»†åˆ†æä»»åŠ¡éœ€æ±‚
2. ç”Ÿæˆé«˜è´¨é‡çš„ä»£ç 
3. æä¾›è¯¦ç»†çš„è¯´æ˜æ–‡æ¡£
4. ç¡®ä¿ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§
5. å¦‚æœ‰éœ€è¦ï¼Œç”Ÿæˆç›¸åº”çš„æµ‹è¯•å°ï¼ˆé™¤éå·²æä¾›å¤–éƒ¨testbenchï¼‰

è¯·å¼€å§‹æ‰§è¡Œä»»åŠ¡ã€‚
"""
        return enhanced_task
    
    async def _tool_analyze_agent_result(self, agent_id: str, result: Dict[str, Any],
                                       task_context: Dict[str, Any] = None,
                                       quality_threshold: float = 80.0) -> Dict[str, Any]:
        """å¢å¼ºçš„æ™ºèƒ½ä½“æ‰§è¡Œç»“æœåˆ†æ"""
        
        try:
            self.logger.info(f"ğŸ” æ·±åº¦åˆ†ææ™ºèƒ½ä½“ {agent_id} çš„æ‰§è¡Œç»“æœ")
            
            # æ›´æ–°æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ ‡
            self._update_agent_performance(agent_id, result)
            
            # æ·±åº¦åˆ†æç»“æœè´¨é‡
            analysis = self._enhanced_result_quality_analysis(result, task_context, quality_threshold)
            
            # ç¡®å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
            next_action = self._determine_enhanced_next_action(analysis, task_context)
            
            # ç”Ÿæˆæ”¹è¿›å»ºè®®
            improvement_suggestions = self._generate_improvement_suggestions(analysis, agent_id)
            
            return {
                "success": True,
                "analysis": analysis,
                "next_action": next_action,
                "improvement_suggestions": improvement_suggestions,
                "agent_id": agent_id,
                "quality_threshold": quality_threshold
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ç»“æœåˆ†æå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "agent_id": agent_id
            }
    
    def _update_agent_performance(self, agent_id: str, result: Dict[str, Any]):
        """æ›´æ–°æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ ‡"""
        if agent_id not in self.registered_agents:
            return
        
        agent_info = self.registered_agents[agent_id]
        
        # æ›´æ–°æ‰§è¡Œæ—¶é—´
        execution_time = result.get("execution_time", 0)
        if execution_time > 0:
            agent_info.total_execution_time += execution_time
            total_tasks = agent_info.success_count + agent_info.failure_count
            if total_tasks > 0:
                agent_info.average_response_time = agent_info.total_execution_time / total_tasks
        
        # æ›´æ–°æˆåŠŸ/å¤±è´¥ç»Ÿè®¡
        if result.get("success", False):
            agent_info.success_count += 1
            agent_info.last_success_time = time.time()
            agent_info.consecutive_successes += 1
            agent_info.consecutive_failures = 0
        else:
            agent_info.failure_count += 1
            agent_info.last_failure_time = time.time()
            agent_info.consecutive_failures += 1
            agent_info.consecutive_successes = 0
    
    def _enhanced_result_quality_analysis(self, result: Dict[str, Any], 
                                        task_context: Dict[str, Any],
                                        quality_threshold: float) -> Dict[str, Any]:
        """å¢å¼ºçš„ç»“æœè´¨é‡åˆ†æ"""
        
        analysis = {
            "quality_score": 0.0,
            "completeness": "unknown",
            "issues": [],
            "strengths": [],
            "recommendations": [],
            "detailed_metrics": {},
            "risk_assessment": "low"
        }
        
        # æ£€æŸ¥ç»“æœæ˜¯å¦ä¸ºç©º
        if not result or not result.get("success", False):
            analysis["completeness"] = "failed"
            analysis["issues"].append("ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            analysis["recommendations"].append("é‡æ–°åˆ†é…ä»»åŠ¡æˆ–æ›´æ¢æ™ºèƒ½ä½“")
            analysis["risk_assessment"] = "high"
            return analysis
        
        # åˆ†æç»“æœå†…å®¹
        content = result.get("content", "")
        if not content:
            analysis["completeness"] = "incomplete"
            analysis["issues"].append("ç»“æœå†…å®¹ä¸ºç©º")
            analysis["recommendations"].append("è¦æ±‚æ™ºèƒ½ä½“é‡æ–°æ‰§è¡Œå¹¶æä¾›è¯¦ç»†ç»“æœ")
            analysis["risk_assessment"] = "medium"
            return analysis
        
        # è¯¦ç»†è´¨é‡æŒ‡æ ‡åˆ†æ
        detailed_metrics = self._analyze_detailed_metrics(content, result)
        analysis["detailed_metrics"] = detailed_metrics
        
        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
        quality_score = self._calculate_comprehensive_quality_score(detailed_metrics)
        analysis["quality_score"] = quality_score
        
        # æ ¹æ®è´¨é‡åˆ†æ•°åˆ¤æ–­å®Œæ•´æ€§
        if quality_score >= quality_threshold:
            analysis["completeness"] = "complete"
            analysis["risk_assessment"] = "low"
        elif quality_score >= quality_threshold * 0.7:
            analysis["completeness"] = "partial"
            analysis["risk_assessment"] = "medium"
        else:
            analysis["completeness"] = "incomplete"
            analysis["risk_assessment"] = "high"
        
        # ç”Ÿæˆå…·ä½“å»ºè®®
        analysis["recommendations"] = self._generate_specific_recommendations(detailed_metrics, quality_score, quality_threshold)
        
        return analysis
    
    def _analyze_detailed_metrics(self, content: str, result: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¯¦ç»†è´¨é‡æŒ‡æ ‡"""
        metrics = {
            "code_quality": 0.0,
            "documentation_quality": 0.0,
            "test_coverage": 0.0,
            "error_handling": 0.0,
            "performance": 0.0,
            "compliance": 0.0
        }
        
        content_lower = content.lower()
        
        # ä»£ç è´¨é‡è¯„ä¼°
        if "module" in content_lower and "endmodule" in content_lower:
            metrics["code_quality"] += 30
        if "//" in content or "/*" in content:
            metrics["code_quality"] += 20
        if "parameter" in content_lower:
            metrics["code_quality"] += 10
        if "always" in content_lower or "assign" in content_lower:
            metrics["code_quality"] += 20
        
        # æ–‡æ¡£è´¨é‡è¯„ä¼°
        if "module" in content_lower and "description" in content_lower:
            metrics["documentation_quality"] += 30
        if "//" in content and len(content.split("//")) > 5:
            metrics["documentation_quality"] += 20
        if "åŠŸèƒ½" in content or "function" in content_lower:
            metrics["documentation_quality"] += 20
        
        # æµ‹è¯•è¦†ç›–è¯„ä¼°
        if "testbench" in content_lower or "test" in content_lower:
            metrics["test_coverage"] += 40
        if "simulation" in content_lower or "ä»¿çœŸ" in content:
            metrics["test_coverage"] += 30
        if "verification" in content_lower or "éªŒè¯" in content:
            metrics["test_coverage"] += 30
        
        # é”™è¯¯å¤„ç†è¯„ä¼°
        if "error" in content_lower and "fix" in content_lower:
            metrics["error_handling"] += 40
        if "exception" in content_lower or "å¼‚å¸¸" in content:
            metrics["error_handling"] += 30
        if "check" in content_lower or "æ£€æŸ¥" in content:
            metrics["error_handling"] += 30
        
        # æ€§èƒ½è¯„ä¼°
        execution_time = result.get("execution_time", 0)
        if execution_time > 0:
            if execution_time < 30:
                metrics["performance"] = 100
            elif execution_time < 60:
                metrics["performance"] = 80
            elif execution_time < 120:
                metrics["performance"] = 60
            else:
                metrics["performance"] = 40
        
        # åˆè§„æ€§è¯„ä¼°
        if "verilog" in content_lower or "systemverilog" in content_lower:
            metrics["compliance"] += 50
        if "ieee" in content_lower or "æ ‡å‡†" in content:
            metrics["compliance"] += 30
        if "synthesis" in content_lower or "ç»¼åˆ" in content:
            metrics["compliance"] += 20
        
        return metrics
    
    def _calculate_comprehensive_quality_score(self, metrics: Dict[str, float]) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°"""
        weights = {
            "code_quality": 0.35,
            "documentation_quality": 0.20,
            "test_coverage": 0.25,
            "error_handling": 0.10,
            "performance": 0.05,
            "compliance": 0.05
        }
        
        total_score = 0.0
        for metric, weight in weights.items():
            total_score += metrics.get(metric, 0.0) * weight
        
        return min(100.0, total_score)
    
    def _generate_specific_recommendations(self, metrics: Dict[str, float], 
                                         quality_score: float, 
                                         threshold: float) -> List[str]:
        """ç”Ÿæˆå…·ä½“æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if quality_score < threshold:
            if metrics["code_quality"] < 50:
                recommendations.append("æé«˜ä»£ç è´¨é‡ï¼šæ·»åŠ æ›´å¤šæ³¨é‡Šï¼Œæ”¹è¿›ä»£ç ç»“æ„")
            if metrics["documentation_quality"] < 40:
                recommendations.append("æ”¹è¿›æ–‡æ¡£ï¼šæ·»åŠ è¯¦ç»†çš„åŠŸèƒ½è¯´æ˜å’Œä½¿ç”¨ç¤ºä¾‹")
            if metrics["test_coverage"] < 60:
                recommendations.append("å¢åŠ æµ‹è¯•è¦†ç›–ï¼šç”Ÿæˆæ›´å…¨é¢çš„testbench")
            if metrics["error_handling"] < 30:
                recommendations.append("åŠ å¼ºé”™è¯¯å¤„ç†ï¼šæ·»åŠ è¾¹ç•Œæ¡ä»¶æ£€æŸ¥å’Œå¼‚å¸¸å¤„ç†")
        
        if quality_score >= threshold:
            recommendations.append("è´¨é‡è¾¾æ ‡ï¼Œå¯ä»¥ç»§ç»­ä¸‹ä¸€æ­¥æˆ–å®Œæˆä»»åŠ¡")
        
        return recommendations
    
    def _determine_enhanced_next_action(self, analysis: Dict[str, Any], 
                                      task_context: Dict[str, Any]) -> str:
        """ç¡®å®šå¢å¼ºçš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
        
        completeness = analysis.get("completeness", "unknown")
        quality_score = analysis.get("quality_score", 0)
        risk_assessment = analysis.get("risk_assessment", "low")
        
        # åŸºäºé£é™©ç­‰çº§å’Œå®Œæ•´æ€§å†³å®šè¡ŒåŠ¨
        if risk_assessment == "high":
            if completeness == "failed":
                return "retry_with_different_agent"
            else:
                return "improve_result"
        
        if completeness == "complete" and quality_score >= 80:
            return "complete_task"
        elif completeness == "partial" and quality_score >= 60:
            return "improve_result"
        elif completeness == "incomplete" or quality_score < 40:
            return "retry_with_different_agent"
        else:
            return "continue_iteration"
    
    def _generate_improvement_suggestions(self, analysis: Dict[str, Any], agent_id: str) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        if analysis.get("quality_score", 0) < 70:
            suggestions.append("è€ƒè™‘ä½¿ç”¨ä¸åŒçš„æ™ºèƒ½ä½“é‡æ–°æ‰§è¡Œä»»åŠ¡")
        
        if "code_quality" in analysis.get("detailed_metrics", {}) and analysis["detailed_metrics"]["code_quality"] < 50:
            suggestions.append("è¦æ±‚æ™ºèƒ½ä½“æä¾›æ›´è¯¦ç»†çš„ä»£ç æ³¨é‡Šå’Œæ–‡æ¡£")
        
        if "test_coverage" in analysis.get("detailed_metrics", {}) and analysis["detailed_metrics"]["test_coverage"] < 60:
            suggestions.append("è¦æ±‚ç”Ÿæˆæ›´å…¨é¢çš„æµ‹è¯•ç”¨ä¾‹")
        
        # åŸºäºæ™ºèƒ½ä½“å†å²è¡¨ç°ç”Ÿæˆå»ºè®®
        if agent_id in self.registered_agents:
            agent_info = self.registered_agents[agent_id]
            if agent_info.consecutive_failures > 2:
                suggestions.append("è¯¥æ™ºèƒ½ä½“è¿ç»­å¤±è´¥æ¬¡æ•°è¾ƒå¤šï¼Œå»ºè®®æ›´æ¢æ™ºèƒ½ä½“")
        
        return suggestions
    
    async def _tool_check_task_completion(self, task_id: str, 
                                        all_results: Dict[str, Any],
                                        original_requirements: str,
                                        completion_criteria: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¢å¼ºçš„ä»»åŠ¡å®Œæˆæ£€æŸ¥"""
        
        try:
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
            if task_id not in self.active_tasks:
                return {
                    "success": False,
                    "error": f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
                }
            
            task_context = self.active_tasks[task_id]
            
            # åˆ†ææ‰€æœ‰ç»“æœ
            completion_analysis = self._enhanced_task_completion_analysis(
                all_results, original_requirements, task_context, completion_criteria
            )
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            if completion_analysis["is_completed"]:
                task_context.completion_status = "completed"
                task_context.quality_score = completion_analysis["completion_score"]
                self._update_performance_metrics(task_context, True)
            else:
                task_context.completion_status = "in_progress"
                task_context.quality_score = completion_analysis["completion_score"]
            
            return {
                "success": True,
                "is_completed": completion_analysis["is_completed"],
                "completion_score": completion_analysis["completion_score"],
                "missing_requirements": completion_analysis["missing_requirements"],
                "quality_assessment": completion_analysis["quality_assessment"],
                "detailed_analysis": completion_analysis["detailed_analysis"],
                "next_steps": completion_analysis["next_steps"],
                "performance_metrics": completion_analysis["performance_metrics"]
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ä»»åŠ¡å®Œæˆæ£€æŸ¥å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _enhanced_task_completion_analysis(self, all_results: Dict[str, Any],
                                         original_requirements: str,
                                         task_context: TaskContext,
                                         completion_criteria: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¢å¼ºçš„ä»»åŠ¡å®Œæˆæƒ…å†µåˆ†æ"""
        
        analysis = {
            "is_completed": False,
            "completion_score": 0.0,
            "missing_requirements": [],
            "quality_assessment": "unknown",
            "detailed_analysis": {},
            "next_steps": [],
            "performance_metrics": {}
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
        if not all_results:
            analysis["missing_requirements"].append("æ²¡æœ‰æ‰§è¡Œç»“æœ")
            analysis["next_steps"].append("å¼€å§‹ä»»åŠ¡æ‰§è¡Œ")
            return analysis
        
        # åˆ†æåŸå§‹éœ€æ±‚
        requirements = original_requirements.lower()
        
        # è¯¦ç»†åˆ†æå„é¡¹æŒ‡æ ‡
        detailed_analysis = self._analyze_completion_metrics(all_results, requirements, task_context)
        analysis["detailed_analysis"] = detailed_analysis
        
        # è®¡ç®—å®Œæˆåˆ†æ•°
        completion_score = self._calculate_completion_score(detailed_analysis, completion_criteria)
        analysis["completion_score"] = completion_score
        
        # æ£€æŸ¥ç¼ºå¤±é¡¹
        missing_items = self._identify_missing_requirements(detailed_analysis, requirements)
        analysis["missing_requirements"] = missing_items
        
        # åˆ¤æ–­æ˜¯å¦å®Œæˆ
        is_completed = self._determine_completion_status(completion_score, missing_items, completion_criteria)
        analysis["is_completed"] = is_completed
        
        # è´¨é‡è¯„ä¼°
        analysis["quality_assessment"] = self._assess_overall_quality(detailed_analysis, completion_score)
        
        # ç¡®å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
        analysis["next_steps"] = self._determine_next_steps(is_completed, missing_items, detailed_analysis)
        
        # æ€§èƒ½æŒ‡æ ‡
        analysis["performance_metrics"] = self._calculate_performance_metrics(task_context, all_results)
        
        return analysis
    
    def _analyze_completion_metrics(self, all_results: Dict[str, Any], 
                                  requirements: str,
                                  task_context: TaskContext) -> Dict[str, Any]:
        """åˆ†æå®ŒæˆæŒ‡æ ‡"""
        metrics = {
            "design_complete": False,
            "verification_complete": False,
            "documentation_complete": False,
            "testing_complete": False,
            "quality_checks_passed": False,
            "agent_performance": {},
            "execution_time": 0.0,
            "total_iterations": 0
        }
        
        # æ£€æŸ¥è®¾è®¡å®Œæˆæƒ…å†µ
        if "design" in requirements or "æ¨¡å—" in requirements:
            design_results = []
            for agent_id, result in all_results.items():
                if agent_id == "enhanced_real_verilog_agent":
                    design_results.append(result)
            
            if design_results:
                metrics["design_complete"] = any(
                    "module" in str(result).lower() and "endmodule" in str(result).lower()
                    for result in design_results
                )
        
        # æ£€æŸ¥éªŒè¯å®Œæˆæƒ…å†µ
        if "test" in requirements or "éªŒè¯" in requirements or "testbench" in requirements:
            verification_results = []
            for agent_id, result in all_results.items():
                if agent_id == "enhanced_real_code_review_agent":
                    verification_results.append(result)
            
            if verification_results:
                metrics["verification_complete"] = any(
                    "testbench" in str(result).lower() or "simulation" in str(result).lower()
                    for result in verification_results
                )
        
        # æ£€æŸ¥æ–‡æ¡£å®Œæˆæƒ…å†µ
        doc_results = []
        for result in all_results.values():
            if "documentation" in str(result).lower() or "æ–‡æ¡£" in str(result):
                doc_results.append(result)
        
        metrics["documentation_complete"] = len(doc_results) > 0
        
        # æ£€æŸ¥æµ‹è¯•å®Œæˆæƒ…å†µ
        test_results = []
        for result in all_results.values():
            if "test" in str(result).lower() or "ä»¿çœŸ" in str(result):
                test_results.append(result)
        
        metrics["testing_complete"] = len(test_results) > 0
        
        # æ£€æŸ¥è´¨é‡æ£€æŸ¥
        quality_results = []
        for result in all_results.values():
            if "quality" in str(result).lower() or "è´¨é‡" in str(result):
                quality_results.append(result)
        
        metrics["quality_checks_passed"] = len(quality_results) > 0
        
        # æ™ºèƒ½ä½“æ€§èƒ½åˆ†æ
        for agent_id, result in all_results.items():
            execution_time = result.get("execution_time", 0)
            metrics["agent_performance"][agent_id] = {
                "execution_time": execution_time,
                "success": result.get("success", False),
                "quality_score": result.get("quality_score", 0)
            }
            metrics["execution_time"] += execution_time
        
        metrics["total_iterations"] = task_context.iteration_count
        
        return metrics
    
    def _calculate_completion_score(self, detailed_analysis: Dict[str, Any],
                                  completion_criteria: Dict[str, Any] = None) -> float:
        """è®¡ç®—å®Œæˆåˆ†æ•°"""
        score = 0.0
        
        # åŸºç¡€å®ŒæˆæŒ‡æ ‡æƒé‡
        weights = {
            "design_complete": 0.35,
            "verification_complete": 0.30,
            "documentation_complete": 0.15,
            "testing_complete": 0.15,
            "quality_checks_passed": 0.05
        }
        
        # åº”ç”¨æƒé‡
        for metric, weight in weights.items():
            if detailed_analysis.get(metric, False):
                score += weight * 100
        
        # è€ƒè™‘æ™ºèƒ½ä½“æ€§èƒ½
        agent_performance = detailed_analysis.get("agent_performance", {})
        if agent_performance:
            avg_quality = sum(
                perf.get("quality_score", 0) for perf in agent_performance.values()
            ) / len(agent_performance)
            score += avg_quality * 0.1  # 10%æƒé‡ç»™è´¨é‡åˆ†æ•°
        
        return min(100.0, score)
    
    def _identify_missing_requirements(self, detailed_analysis: Dict[str, Any],
                                     requirements: str) -> List[str]:
        """è¯†åˆ«ç¼ºå¤±çš„éœ€æ±‚"""
        missing = []
        
        # æ£€æŸ¥è®¾è®¡éœ€æ±‚
        if ("design" in requirements or "æ¨¡å—" in requirements) and not detailed_analysis.get("design_complete", False):
            missing.append("ç¼ºå°‘Verilogæ¨¡å—è®¾è®¡")
        
        # æ£€æŸ¥éªŒè¯éœ€æ±‚
        if ("test" in requirements or "éªŒè¯" in requirements or "testbench" in requirements) and not detailed_analysis.get("verification_complete", False):
            missing.append("ç¼ºå°‘æµ‹è¯•å°å’ŒéªŒè¯")
        
        # æ£€æŸ¥æ–‡æ¡£éœ€æ±‚
        if not detailed_analysis.get("documentation_complete", False):
            missing.append("ç¼ºå°‘è®¾è®¡æ–‡æ¡£")
        
        # æ£€æŸ¥æµ‹è¯•éœ€æ±‚
        if not detailed_analysis.get("testing_complete", False):
            missing.append("ç¼ºå°‘æµ‹è¯•æ‰§è¡Œ")
        
        # æ£€æŸ¥è´¨é‡éœ€æ±‚
        if not detailed_analysis.get("quality_checks_passed", False):
            missing.append("ç¼ºå°‘è´¨é‡æ£€æŸ¥")
        
        return missing
    
    def _determine_completion_status(self, completion_score: float,
                                   missing_requirements: List[str],
                                   completion_criteria: Dict[str, Any] = None) -> bool:
        """ç¡®å®šå®ŒæˆçŠ¶æ€"""
        
        # ä½¿ç”¨è‡ªå®šä¹‰å®Œæˆæ ‡å‡†
        if completion_criteria:
            required_score = completion_criteria.get("required_score", 80.0)
            max_missing_items = completion_criteria.get("max_missing_items", 0)
            
            return (completion_score >= required_score and 
                   len(missing_requirements) <= max_missing_items)
        
        # é»˜è®¤å®Œæˆæ ‡å‡†
        return completion_score >= 80.0 and len(missing_requirements) == 0
    
    def _assess_overall_quality(self, detailed_analysis: Dict[str, Any],
                              completion_score: float) -> str:
        """è¯„ä¼°æ•´ä½“è´¨é‡"""
        if completion_score >= 90:
            return "excellent"
        elif completion_score >= 80:
            return "good"
        elif completion_score >= 70:
            return "fair"
        elif completion_score >= 60:
            return "poor"
        else:
            return "very_poor"
    
    def _determine_next_steps(self, is_completed: bool,
                            missing_requirements: List[str],
                            detailed_analysis: Dict[str, Any]) -> List[str]:
        """ç¡®å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
        next_steps = []
        
        if is_completed:
            next_steps.append("ä»»åŠ¡å®Œæˆï¼Œå¯ä»¥äº¤ä»˜ç»“æœ")
        else:
            # æ ¹æ®ç¼ºå¤±é¡¹ç¡®å®šä¸‹ä¸€æ­¥
            if "ç¼ºå°‘Verilogæ¨¡å—è®¾è®¡" in missing_requirements:
                next_steps.append("åˆ†é…è®¾è®¡ä»»åŠ¡ç»™enhanced_real_verilog_agent")
            
            if "ç¼ºå°‘æµ‹è¯•å°å’ŒéªŒè¯" in missing_requirements:
                next_steps.append("åˆ†é…éªŒè¯ä»»åŠ¡ç»™enhanced_real_code_review_agent")
            
            if "ç¼ºå°‘è®¾è®¡æ–‡æ¡£" in missing_requirements:
                next_steps.append("è¦æ±‚æ™ºèƒ½ä½“ç”Ÿæˆè®¾è®¡æ–‡æ¡£")
            
            if "ç¼ºå°‘æµ‹è¯•æ‰§è¡Œ" in missing_requirements:
                next_steps.append("æ‰§è¡Œæµ‹è¯•éªŒè¯")
            
            if "ç¼ºå°‘è´¨é‡æ£€æŸ¥" in missing_requirements:
                next_steps.append("è¿›è¡Œä»£ç è´¨é‡æ£€æŸ¥")
        
        return next_steps
    
    def _calculate_performance_metrics(self, task_context: TaskContext,
                                     all_results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            "total_execution_time": 0.0,
            "average_execution_time": 0.0,
            "success_rate": 0.0,
            "agent_utilization": {},
            "iteration_efficiency": 0.0
        }
        
        # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
        total_time = 0.0
        success_count = 0
        total_count = len(all_results)
        
        for result in all_results.values():
            execution_time = result.get("execution_time", 0)
            total_time += execution_time
            
            if result.get("success", False):
                success_count += 1
        
        metrics["total_execution_time"] = total_time
        metrics["average_execution_time"] = total_time / total_count if total_count > 0 else 0
        metrics["success_rate"] = success_count / total_count if total_count > 0 else 0
        
        # è®¡ç®—è¿­ä»£æ•ˆç‡
        if task_context.iteration_count > 0:
            metrics["iteration_efficiency"] = completion_score / task_context.iteration_count
        
        # æ™ºèƒ½ä½“åˆ©ç”¨ç‡
        for agent_id, agent_info in self.registered_agents.items():
            if agent_id in all_results:
                metrics["agent_utilization"][agent_id] = {
                    "tasks_assigned": 1,
                    "success_rate": 1.0 if all_results[agent_id].get("success", False) else 0.0,
                    "average_time": all_results[agent_id].get("execution_time", 0)
                }
        
        return metrics
    
    def _update_performance_metrics(self, task_context: TaskContext, success: bool):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics["total_tasks"] += 1
        
        if success:
            self.performance_metrics["successful_tasks"] += 1
        else:
            self.performance_metrics["failed_tasks"] += 1
        
        # æ›´æ–°å¹³å‡å®Œæˆæ—¶é—´
        execution_time = time.time() - task_context.start_time
        total_tasks = self.performance_metrics["total_tasks"]
        current_avg = self.performance_metrics["average_completion_time"]
        
        self.performance_metrics["average_completion_time"] = (
            (current_avg * (total_tasks - 1) + execution_time) / total_tasks
        )
    
    async def _tool_query_agent_status(self, agent_id: str,
                                     include_performance: bool = True,
                                     include_history: bool = False) -> Dict[str, Any]:
        """å¢å¼ºçš„æ™ºèƒ½ä½“çŠ¶æ€æŸ¥è¯¢"""
        
        try:
            if agent_id not in self.registered_agents:
                return {
                    "success": False,
                    "error": f"æ™ºèƒ½ä½“ä¸å­˜åœ¨: {agent_id}"
                }
            
            agent_info = self.registered_agents[agent_id]
            
            # åŸºç¡€çŠ¶æ€ä¿¡æ¯
            status_info = {
                "success": True,
                "agent_id": agent_id,
                "status": agent_info.status.value,
                "capabilities": [cap.value for cap in agent_info.capabilities],
                "specialty": agent_info.specialty,
                "conversation_id": agent_info.conversation_id,
                "last_used": agent_info.last_used
            }
            
            # æ€§èƒ½æŒ‡æ ‡
            if include_performance:
                performance_metrics = self._calculate_agent_performance_metrics(agent_info)
                status_info["performance_metrics"] = performance_metrics
            
            # å†å²è®°å½•
            if include_history:
                history_data = self._get_agent_history(agent_id)
                status_info["history"] = history_data
            
            # å¥åº·çŠ¶æ€è¯„ä¼°
            health_assessment = self._assess_agent_health(agent_info)
            status_info["health_assessment"] = health_assessment
            
            # æ¨èä»»åŠ¡ç±»å‹
            recommended_tasks = self._get_recommended_task_types(agent_info)
            status_info["recommended_tasks"] = recommended_tasks
            
            return status_info
            
        except Exception as e:
            self.logger.error(f"âŒ æŸ¥è¯¢æ™ºèƒ½ä½“çŠ¶æ€å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _calculate_agent_performance_metrics(self, agent_info: AgentInfo) -> Dict[str, Any]:
        """è®¡ç®—æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ ‡"""
        total_tasks = agent_info.success_count + agent_info.failure_count
        
        metrics = {
            "total_tasks": total_tasks,
            "success_count": agent_info.success_count,
            "failure_count": agent_info.failure_count,
            "success_rate": agent_info.success_count / total_tasks if total_tasks > 0 else 0.0,
            "average_response_time": agent_info.average_response_time,
            "total_execution_time": agent_info.total_execution_time,
            "consecutive_successes": agent_info.consecutive_successes,
            "consecutive_failures": agent_info.consecutive_failures,
            "last_success_time": agent_info.last_success_time,
            "last_failure_time": agent_info.last_failure_time
        }
        
        # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
        if agent_info.last_success_time and agent_info.last_failure_time:
            if agent_info.last_success_time > agent_info.last_failure_time:
                metrics["recent_trend"] = "improving"
            else:
                metrics["recent_trend"] = "declining"
        elif agent_info.last_success_time:
            metrics["recent_trend"] = "stable_success"
        elif agent_info.last_failure_time:
            metrics["recent_trend"] = "stable_failure"
        else:
            metrics["recent_trend"] = "unknown"
        
        # è®¡ç®—å¯é æ€§è¯„åˆ†
        reliability_score = self._calculate_reliability_score(agent_info)
        metrics["reliability_score"] = reliability_score
        
        return metrics
    
    def _calculate_reliability_score(self, agent_info: AgentInfo) -> float:
        """è®¡ç®—å¯é æ€§è¯„åˆ†"""
        total_tasks = agent_info.success_count + agent_info.failure_count
        if total_tasks == 0:
            return 0.0
        
        # åŸºç¡€æˆåŠŸç‡
        base_score = agent_info.success_count / total_tasks * 100
        
        # è¿ç»­æˆåŠŸå¥–åŠ±
        consecutive_bonus = min(10.0, agent_info.consecutive_successes * 2.0)
        
        # å“åº”æ—¶é—´å¥–åŠ±
        time_bonus = 0.0
        if agent_info.average_response_time > 0:
            if agent_info.average_response_time < 30:
                time_bonus = 10.0
            elif agent_info.average_response_time < 60:
                time_bonus = 5.0
        
        # è¿ç»­å¤±è´¥æƒ©ç½š
        consecutive_penalty = min(20.0, agent_info.consecutive_failures * 5.0)
        
        final_score = base_score + consecutive_bonus + time_bonus - consecutive_penalty
        return max(0.0, min(100.0, final_score))
    
    def _get_agent_history(self, agent_id: str) -> Dict[str, Any]:
        """è·å–æ™ºèƒ½ä½“å†å²è®°å½•"""
        # è¿™é‡Œå¯ä»¥ä»æ•°æ®åº“æˆ–æ—¥å¿—ä¸­è·å–æ›´è¯¦ç»†çš„å†å²è®°å½•
        # ç›®å‰è¿”å›åŸºæœ¬çš„å†å²ç»Ÿè®¡
        return {
            "total_executions": 0,
            "recent_performance": [],
            "common_issues": [],
            "improvement_trend": "stable"
        }
    
    def _assess_agent_health(self, agent_info: AgentInfo) -> Dict[str, Any]:
        """è¯„ä¼°æ™ºèƒ½ä½“å¥åº·çŠ¶æ€"""
        health = {
            "status": "healthy",
            "score": 100.0,
            "issues": [],
            "recommendations": []
        }
        
        # æ£€æŸ¥è¿ç»­å¤±è´¥
        if agent_info.consecutive_failures > 3:
            health["status"] = "warning"
            health["score"] -= 30
            health["issues"].append(f"è¿ç»­å¤±è´¥{agent_info.consecutive_failures}æ¬¡")
            health["recommendations"].append("å»ºè®®æ£€æŸ¥æ™ºèƒ½ä½“é…ç½®æˆ–æ›´æ¢æ™ºèƒ½ä½“")
        
        # æ£€æŸ¥æˆåŠŸç‡
        total_tasks = agent_info.success_count + agent_info.failure_count
        if total_tasks > 0:
            success_rate = agent_info.success_count / total_tasks
            if success_rate < 0.5:
                health["status"] = "critical"
                health["score"] -= 50
                health["issues"].append(f"æˆåŠŸç‡è¿‡ä½: {success_rate:.1%}")
                health["recommendations"].append("å»ºè®®é‡æ–°é…ç½®æˆ–æ›´æ¢æ™ºèƒ½ä½“")
            elif success_rate < 0.7:
                health["status"] = "warning"
                health["score"] -= 20
                health["issues"].append(f"æˆåŠŸç‡åä½: {success_rate:.1%}")
                health["recommendations"].append("å»ºè®®ä¼˜åŒ–æ™ºèƒ½ä½“é…ç½®")
        
        # æ£€æŸ¥å“åº”æ—¶é—´
        if agent_info.average_response_time > 120:
            health["status"] = "warning"
            health["score"] -= 15
            health["issues"].append(f"å“åº”æ—¶é—´è¿‡é•¿: {agent_info.average_response_time:.1f}ç§’")
            health["recommendations"].append("å»ºè®®ä¼˜åŒ–æ€§èƒ½æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
        
        # æ£€æŸ¥é•¿æœŸæœªä½¿ç”¨
        if agent_info.last_used:
            time_since_last_use = time.time() - agent_info.last_used
            if time_since_last_use > 3600:  # 1å°æ—¶
                health["issues"].append(f"é•¿æœŸæœªä½¿ç”¨: {time_since_last_use/3600:.1f}å°æ—¶")
        
        health["score"] = max(0.0, health["score"])
        
        return health
    
    def _get_recommended_task_types(self, agent_info: AgentInfo) -> List[str]:
        """è·å–æ¨èçš„ä»»åŠ¡ç±»å‹"""
        recommended = []
        
        # åŸºäºæ™ºèƒ½ä½“IDæ¨è
        if agent_info.agent_id == "enhanced_real_verilog_agent":
            recommended.extend(["design", "code_generation", "module_implementation"])
        elif agent_info.agent_id == "enhanced_real_code_review_agent":
            recommended.extend(["verification", "analysis", "debug", "testbench_generation"])
        
        # åŸºäºå†å²è¡¨ç°æ¨è
        if agent_info.consecutive_successes > 2:
            recommended.append("high_priority_tasks")
        
        if agent_info.average_response_time < 30:
            recommended.append("time_sensitive_tasks")
        
        return list(set(recommended))  # å»é‡
    
    def _collect_final_result(self, task_context: TaskContext, 
                            coordination_result: str) -> Dict[str, Any]:
        """æ”¶é›†æœ€ç»ˆç»“æœ"""
        
        return {
            "success": True,
            "task_id": task_context.task_id,
            "coordination_result": coordination_result,
            "agent_results": task_context.agent_results,
            "execution_summary": {
                "total_iterations": task_context.iteration_count,
                "assigned_agents": list(task_context.agent_results.keys()),
                "execution_time": time.time() - task_context.start_time
            },
            "conversation_history": task_context.conversation_history
        }
    
    def get_registered_agents(self) -> Dict[str, AgentInfo]:
        """è·å–å·²æ³¨å†Œçš„æ™ºèƒ½ä½“"""
        return self.registered_agents.copy()
    
    def get_active_tasks(self) -> Dict[str, TaskContext]:
        """è·å–æ´»è·ƒä»»åŠ¡"""
        return self.active_tasks.copy()
    
    # =============================================================================
    # å®ç°æŠ½è±¡æ–¹æ³•
    # =============================================================================
    
    async def _call_llm_for_function_calling(self, conversation: List[Dict[str, str]]) -> str:
        """å®ç°LLMè°ƒç”¨ - ä½¿ç”¨ä¼˜åŒ–çš„è°ƒç”¨æœºåˆ¶é¿å…é‡å¤ä¼ å…¥system prompt"""
        # ç”Ÿæˆå¯¹è¯IDï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
        if not hasattr(self, 'current_conversation_id') or not self.current_conversation_id:
            self.current_conversation_id = f"coordinator_agent_{int(time.time())}"
        
        # æ„å»ºç”¨æˆ·æ¶ˆæ¯
        user_message = ""
        is_first_call = len(conversation) <= 1  # å¦‚æœå¯¹è¯å†å²å¾ˆå°‘ï¼Œè®¤ä¸ºæ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨
        
        for msg in conversation:
            if msg["role"] == "user":
                user_message += f"{msg['content']}\n\n"
            elif msg["role"] == "assistant":
                user_message += f"Assistant: {msg['content']}\n\n"
        
        try:
            # ä½¿ç”¨ä¼˜åŒ–çš„LLMè°ƒç”¨æ–¹æ³•
            response = await self.llm_client.send_prompt_optimized(
                conversation_id=self.current_conversation_id,
                user_message=user_message.strip(),
                system_prompt=self._build_enhanced_system_prompt() if is_first_call else None,
                temperature=0.3,
                max_tokens=4000,
                force_refresh_system=is_first_call
            )
            return response
        except Exception as e:
            self.logger.error(f"âŒ ä¼˜åŒ–LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            # å¦‚æœä¼˜åŒ–è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
            self.logger.warning("âš ï¸ å›é€€åˆ°ä¼ ç»ŸLLMè°ƒç”¨æ–¹å¼")
            return await self._call_llm_traditional(conversation)
    
    async def _call_llm_traditional(self, conversation: List[Dict[str, str]]) -> str:
        """ä¼ ç»Ÿçš„LLMè°ƒç”¨æ–¹æ³•ï¼ˆä½œä¸ºå›é€€æ–¹æ¡ˆï¼‰"""
        # æ„å»ºå®Œæ•´çš„prompt
        full_prompt = ""
        system_prompt = self._build_enhanced_system_prompt()
        
        for msg in conversation:
            if msg["role"] == "system":
                system_prompt = msg["content"]  # è¦†ç›–é»˜è®¤system prompt
            elif msg["role"] == "user":
                full_prompt += f"User: {msg['content']}\n\n"
            elif msg["role"] == "assistant":
                full_prompt += f"Assistant: {msg['content']}\n\n"
        
        try:
            response = await self.llm_client.send_prompt(
                prompt=full_prompt.strip(),
                system_prompt=system_prompt,
                temperature=0.3,
                max_tokens=4000
            )
            return response
        except Exception as e:
            self.logger.error(f"âŒ ä¼ ç»ŸLLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def get_capabilities(self) -> Set[AgentCapability]:
        """è·å–æ™ºèƒ½ä½“èƒ½åŠ›"""
        return {
            AgentCapability.TASK_COORDINATION,
            AgentCapability.INTELLIGENT_ROUTING,
            AgentCapability.CONTEXT_MANAGEMENT,
            AgentCapability.DECISION_MAKING
        }
    
    def get_specialty_description(self) -> str:
        """è·å–æ™ºèƒ½ä½“ä¸“ä¸šæè¿°"""
        return "åŸºäºLLMçš„æ™ºèƒ½åè°ƒæ™ºèƒ½ä½“ï¼Œè´Ÿè´£ä»»åŠ¡åˆ†æå’Œæ™ºèƒ½ä½“åˆ†é…"
    
    async def execute_enhanced_task(self, enhanced_prompt: str,
                                  original_message: TaskMessage,
                                  file_contents: Dict[str, Dict]) -> Dict[str, Any]:
        """æ‰§è¡Œå¢å¼ºä»»åŠ¡"""
        task_id = original_message.task_id
        self.logger.info(f"ğŸ¯ å¼€å§‹æ‰§è¡Œåè°ƒä»»åŠ¡: {task_id}")
        
        try:
            # ä½¿ç”¨åè°ƒä»»åŠ¡æ‰§è¡Œ
            result = await self.coordinate_task(
                user_request=enhanced_prompt,
                conversation_id=original_message.task_id,
                max_iterations=10
            )
            
            return {
                "success": result.get("success", False),
                "task_id": task_id,
                "response": result.get("coordination_result", ""),
                "agent_results": result.get("agent_results", {}),
                "execution_summary": result.get("execution_summary", {})
            }
            
        except Exception as e:
            self.logger.error(f"âŒ åè°ƒä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {task_id} - {str(e)}")
            return {
                "success": False,
                "task_id": task_id,
                "error": f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            } 

    async def _tool_identify_task_type(self, user_request: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ™ºèƒ½è¯†åˆ«ä»»åŠ¡ç±»å‹"""
        try:
            self.logger.info(f"ğŸ” è¯†åˆ«ä»»åŠ¡ç±»å‹: {user_request[:100]}...")
            
            # ä½¿ç”¨æ¨¡å¼åŒ¹é…è¯†åˆ«ä»»åŠ¡ç±»å‹
            task_type = self._classify_task_by_patterns(user_request)
            
            # ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æ
            llm_analysis = await self._analyze_task_with_llm(user_request, task_type)
            
            # åˆå¹¶ç»“æœ
            final_task_type = llm_analysis.get("task_type", task_type)
            confidence = llm_analysis.get("confidence", 0.7)
            
            # ç¡®å®šä¼˜å…ˆçº§
            priority = self._determine_task_priority(user_request, final_task_type)
            
            return {
                "success": True,
                "task_type": final_task_type.value if isinstance(final_task_type, TaskType) else final_task_type,
                "confidence": confidence,
                "priority": priority.value if isinstance(priority, TaskPriority) else priority,
                "analysis": llm_analysis.get("analysis", ""),
                "keywords": llm_analysis.get("keywords", []),
                "suggested_agent": self._get_suggested_agent(final_task_type)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ä»»åŠ¡ç±»å‹è¯†åˆ«å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "task_type": "unknown",
                "confidence": 0.0,
                "priority": "medium"
            }
    
    def _classify_task_by_patterns(self, user_request: str) -> TaskType:
        """ä½¿ç”¨æ¨¡å¼åŒ¹é…åˆ†ç±»ä»»åŠ¡"""
        request_lower = user_request.lower()
        
        # è®¡ç®—æ¯ç§ä»»åŠ¡ç±»å‹çš„åŒ¹é…åˆ†æ•°
        scores = {}
        for task_type, patterns in self.task_patterns.items():
            score = 0
            for pattern in patterns:
                if re.search(pattern, request_lower):
                    score += 1
            scores[task_type] = score
        
        # æ‰¾åˆ°æœ€é«˜åˆ†æ•°çš„ä»»åŠ¡ç±»å‹
        if scores:
            max_score = max(scores.values())
            if max_score > 0:
                for task_type, score in scores.items():
                    if score == max_score:
                        return task_type
        
        return TaskType.UNKNOWN
    
    async def _analyze_task_with_llm(self, user_request: str, initial_task_type: TaskType) -> Dict[str, Any]:
        """ä½¿ç”¨LLMæ·±åº¦åˆ†æä»»åŠ¡"""
        try:
            analysis_prompt = f"""
è¯·æ·±åº¦åˆ†æä»¥ä¸‹ç”¨æˆ·è¯·æ±‚ï¼Œè¯†åˆ«ä»»åŠ¡ç±»å‹å’Œç‰¹å¾ï¼š

ç”¨æˆ·è¯·æ±‚: {user_request}
åˆæ­¥åˆ†ç±»: {initial_task_type.value}

è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œåˆ†æï¼š
1. ä»»åŠ¡ç±»å‹ï¼ˆdesign/verification/analysis/debug/compositeï¼‰
2. ä»»åŠ¡å¤æ‚åº¦ï¼ˆsimple/medium/complexï¼‰
3. å…³é”®ç‰¹å¾å’Œéœ€æ±‚
4. å»ºè®®çš„æ™ºèƒ½ä½“é€‰æ‹©
5. ç½®ä¿¡åº¦è¯„åˆ†ï¼ˆ0-1ï¼‰

è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚
"""
            
            response = await self.llm_client.send_prompt(
                prompt=analysis_prompt,
                system_prompt="ä½ æ˜¯ä»»åŠ¡åˆ†æä¸“å®¶ï¼Œè¯·æä¾›å‡†ç¡®çš„ä»»åŠ¡ç±»å‹è¯†åˆ«ã€‚",
                temperature=0.1
            )
            
            # å°è¯•è§£æJSONå“åº”
            try:
                analysis = json.loads(response)
                return analysis
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸºæœ¬åˆ†æ
                return {
                    "task_type": initial_task_type,
                    "confidence": 0.7,
                    "analysis": response,
                    "keywords": []
                }
                
        except Exception as e:
            self.logger.error(f"âŒ LLMä»»åŠ¡åˆ†æå¤±è´¥: {str(e)}")
            return {
                "task_type": initial_task_type,
                "confidence": 0.5,
                "analysis": f"åˆ†æå¤±è´¥: {str(e)}",
                "keywords": []
            }
    
    def _determine_task_priority(self, user_request: str, task_type: TaskType) -> TaskPriority:
        """ç¡®å®šä»»åŠ¡ä¼˜å…ˆçº§"""
        request_lower = user_request.lower()
        
        # é«˜ä¼˜å…ˆçº§å…³é”®è¯
        high_priority_keywords = [
            "ç´§æ€¥", "urgent", "ç«‹å³", "immediately", "é‡è¦", "important",
            "å…³é”®", "critical", "ä¼˜å…ˆ", "priority", "å¿«é€Ÿ", "quick"
        ]
        
        # ä½ä¼˜å…ˆçº§å…³é”®è¯
        low_priority_keywords = [
            "ä¸æ€¥", "not urgent", "æ…¢æ…¢", "slowly", "å¯é€‰", "optional",
            "æ¬¡è¦", "secondary", "ä½ä¼˜å…ˆçº§", "low priority"
        ]
        
        # æ£€æŸ¥ä¼˜å…ˆçº§å…³é”®è¯
        for keyword in high_priority_keywords:
            if keyword in request_lower:
                return TaskPriority.HIGH
        
        for keyword in low_priority_keywords:
            if keyword in request_lower:
                return TaskPriority.LOW
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹ç¡®å®šé»˜è®¤ä¼˜å…ˆçº§
        if task_type == TaskType.DEBUG:
            return TaskPriority.HIGH  # è°ƒè¯•ä»»åŠ¡é€šå¸¸ä¼˜å…ˆçº§è¾ƒé«˜
        elif task_type == TaskType.ANALYSIS:
            return TaskPriority.LOW   # åˆ†æä»»åŠ¡é€šå¸¸ä¼˜å…ˆçº§è¾ƒä½
        
        return TaskPriority.MEDIUM
    
    def _get_suggested_agent(self, task_type: TaskType) -> str:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è·å–å»ºè®®çš„æ™ºèƒ½ä½“"""
        if task_type == TaskType.DESIGN:
            return "enhanced_real_verilog_agent"
        elif task_type in [TaskType.VERIFICATION, TaskType.ANALYSIS, TaskType.DEBUG]:
            return "enhanced_real_code_review_agent"
        else:
            return "enhanced_real_verilog_agent"  # é»˜è®¤é€‰æ‹©è®¾è®¡æ™ºèƒ½ä½“
    
    async def _tool_recommend_agent(self, task_type: str, task_description: str,
                                  priority: str = "medium", constraints: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ¨èæœ€åˆé€‚çš„æ™ºèƒ½ä½“"""
        try:
            self.logger.info(f"ğŸ¤– æ¨èæ™ºèƒ½ä½“: {task_type} - {priority}")
            
            # è·å–å¯ç”¨æ™ºèƒ½ä½“
            available_agents = self._get_available_agents()
            
            if not available_agents:
                return {
                    "success": False,
                    "error": "æ²¡æœ‰å¯ç”¨çš„æ™ºèƒ½ä½“"
                }
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹è¿‡æ»¤æ™ºèƒ½ä½“
            suitable_agents = self._filter_agents_by_task_type(available_agents, task_type)
            
            if not suitable_agents:
                return {
                    "success": False,
                    "error": f"æ²¡æœ‰é€‚åˆä»»åŠ¡ç±»å‹ '{task_type}' çš„æ™ºèƒ½ä½“"
                }
            
            # è®¡ç®—æ™ºèƒ½ä½“è¯„åˆ†
            agent_scores = self._calculate_agent_scores(suitable_agents, task_type, priority, constraints)
            
            # é€‰æ‹©æœ€é«˜åˆ†çš„æ™ºèƒ½ä½“
            best_agent = max(agent_scores.items(), key=lambda x: x[1])
            
            return {
                "success": True,
                "recommended_agent": best_agent[0],
                "score": best_agent[1],
                "all_scores": agent_scores,
                "reasoning": self._generate_recommendation_reasoning(best_agent[0], best_agent[1], task_type),
                "alternative_agents": self._get_alternative_agents(agent_scores, best_agent[0])
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æ™ºèƒ½ä½“æ¨èå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _get_available_agents(self) -> List[Tuple[str, AgentInfo]]:
        """è·å–å¯ç”¨æ™ºèƒ½ä½“åˆ—è¡¨"""
        available = []
        for agent_id, agent_info in self.registered_agents.items():
            if agent_info.status == AgentStatus.IDLE:
                available.append((agent_id, agent_info))
        return available
    
    def _filter_agents_by_task_type(self, agents: List[Tuple[str, AgentInfo]], task_type: str) -> List[Tuple[str, AgentInfo]]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è¿‡æ»¤æ™ºèƒ½ä½“"""
        filtered = []
        
        for agent_id, agent_info in agents:
            # æ£€æŸ¥æ™ºèƒ½ä½“æ˜¯å¦é€‚åˆè¯¥ä»»åŠ¡ç±»å‹
            if self._is_agent_suitable_for_task(agent_id, task_type):
                filtered.append((agent_id, agent_info))
        
        return filtered
    
    def _is_agent_suitable_for_task(self, agent_id: str, task_type: str) -> bool:
        """æ£€æŸ¥æ™ºèƒ½ä½“æ˜¯å¦é€‚åˆç‰¹å®šä»»åŠ¡ç±»å‹"""
        if task_type == "design":
            return agent_id == "enhanced_real_verilog_agent"
        elif task_type in ["verification", "analysis", "debug"]:
            return agent_id == "enhanced_real_code_review_agent"
        elif task_type == "composite":
            return True  # å¤åˆä»»åŠ¡å¯ä»¥ä½¿ç”¨ä»»ä½•æ™ºèƒ½ä½“
        else:
            return True  # æœªçŸ¥ä»»åŠ¡ç±»å‹å…è®¸ä½¿ç”¨ä»»ä½•æ™ºèƒ½ä½“
    
    def _calculate_agent_scores(self, agents: List[Tuple[str, AgentInfo]], task_type: str,
                              priority: str, constraints: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—æ™ºèƒ½ä½“è¯„åˆ†"""
        scores = {}
        
        for agent_id, agent_info in agents:
            score = 0.0
            
            # åŸºç¡€åˆ†æ•°
            score += 50.0
            
            # æˆåŠŸç‡åˆ†æ•°ï¼ˆæƒé‡ï¼š30%ï¼‰
            total_tasks = agent_info.success_count + agent_info.failure_count
            if total_tasks > 0:
                success_rate = agent_info.success_count / total_tasks
                score += success_rate * 30.0
            
            # å“åº”æ—¶é—´åˆ†æ•°ï¼ˆæƒé‡ï¼š20%ï¼‰
            if agent_info.average_response_time > 0:
                # å“åº”æ—¶é—´è¶ŠçŸ­ï¼Œåˆ†æ•°è¶Šé«˜
                time_score = max(0, 20.0 - (agent_info.average_response_time / 10.0))
                score += time_score
            
            # è¿ç»­æˆåŠŸåˆ†æ•°ï¼ˆæƒé‡ï¼š10%ï¼‰
            consecutive_bonus = min(10.0, agent_info.consecutive_successes * 2.0)
            score += consecutive_bonus
            
            # ä¼˜å…ˆçº§åŒ¹é…åˆ†æ•°
            if priority == "high" and agent_info.average_response_time < 30:
                score += 5.0
            
            # ä»»åŠ¡ç±»å‹åŒ¹é…åˆ†æ•°
            if task_type in [task_type.value for task_type in agent_info.preferred_task_types]:
                score += 10.0
            
            # é»‘åå•æƒ©ç½š
            if task_type in [task_type.value for task_type in agent_info.blacklisted_task_types]:
                score -= 20.0
            
            scores[agent_id] = max(0.0, score)
        
        return scores
    
    def _generate_recommendation_reasoning(self, agent_id: str, score: float, task_type: str) -> str:
        """ç”Ÿæˆæ¨èç†ç”±"""
        agent_info = self.registered_agents.get(agent_id)
        if not agent_info:
            return f"æ¨è {agent_id}ï¼Œè¯„åˆ†: {score:.1f}"
        
        reasons = []
        
        # æˆåŠŸç‡
        total_tasks = agent_info.success_count + agent_info.failure_count
        if total_tasks > 0:
            success_rate = agent_info.success_count / total_tasks
            reasons.append(f"å†å²æˆåŠŸç‡: {success_rate:.1%}")
        
        # å“åº”æ—¶é—´
        if agent_info.average_response_time > 0:
            reasons.append(f"å¹³å‡å“åº”æ—¶é—´: {agent_info.average_response_time:.1f}ç§’")
        
        # è¿ç»­æˆåŠŸ
        if agent_info.consecutive_successes > 0:
            reasons.append(f"è¿ç»­æˆåŠŸ: {agent_info.consecutive_successes}æ¬¡")
        
        # ä»»åŠ¡ç±»å‹åŒ¹é…
        if task_type in [task_type.value for task_type in agent_info.preferred_task_types]:
            reasons.append("ä»»åŠ¡ç±»å‹åŒ¹é…")
        
        return f"æ¨è {agent_id} (è¯„åˆ†: {score:.1f})ï¼Œç†ç”±: {', '.join(reasons)}"
    
    def _get_alternative_agents(self, agent_scores: Dict[str, float], best_agent: str) -> List[str]:
        """è·å–å¤‡é€‰æ™ºèƒ½ä½“"""
        alternatives = []
        best_score = agent_scores.get(best_agent, 0)
        
        for agent_id, score in agent_scores.items():
            if agent_id != best_agent and score >= best_score * 0.8:  # åˆ†æ•°ä¸ä½äºæœ€ä½³æ™ºèƒ½ä½“çš„80%
                alternatives.append(agent_id)
        
        return alternatives 

    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿæ€§èƒ½æ‘˜è¦"""
        summary = {
            "total_tasks": self.performance_metrics["total_tasks"],
            "successful_tasks": self.performance_metrics["successful_tasks"],
            "failed_tasks": self.performance_metrics["failed_tasks"],
            "success_rate": 0.0,
            "average_completion_time": self.performance_metrics["average_completion_time"],
            "agent_performance": {},
            "system_health": "healthy",
            "recommendations": []
        }
        
        # è®¡ç®—æˆåŠŸç‡
        if summary["total_tasks"] > 0:
            summary["success_rate"] = summary["successful_tasks"] / summary["total_tasks"]
        
        # æ™ºèƒ½ä½“æ€§èƒ½ç»Ÿè®¡
        for agent_id, agent_info in self.registered_agents.items():
            total_tasks = agent_info.success_count + agent_info.failure_count
            success_rate = agent_info.success_count / total_tasks if total_tasks > 0 else 0.0
            
            summary["agent_performance"][agent_id] = {
                "total_tasks": total_tasks,
                "success_rate": success_rate,
                "average_response_time": agent_info.average_response_time,
                "reliability_score": self._calculate_reliability_score(agent_info),
                "status": agent_info.status.value
            }
        
        # ç³»ç»Ÿå¥åº·è¯„ä¼°
        system_health = self._assess_system_health()
        summary["system_health"] = system_health["status"]
        summary["recommendations"] = system_health["recommendations"]
        
        return summary
    
    def _assess_system_health(self) -> Dict[str, Any]:
        """è¯„ä¼°ç³»ç»Ÿæ•´ä½“å¥åº·çŠ¶æ€"""
        health = {
            "status": "healthy",
            "issues": [],
            "recommendations": []
        }
        
        # æ£€æŸ¥æ€»ä½“æˆåŠŸç‡
        total_tasks = self.performance_metrics["total_tasks"]
        if total_tasks > 0:
            success_rate = self.performance_metrics["successful_tasks"] / total_tasks
            if success_rate < 0.5:
                health["status"] = "critical"
                health["issues"].append(f"ç³»ç»ŸæˆåŠŸç‡è¿‡ä½: {success_rate:.1%}")
                health["recommendations"].append("å»ºè®®æ£€æŸ¥æ™ºèƒ½ä½“é…ç½®å’Œç³»ç»Ÿè®¾ç½®")
            elif success_rate < 0.7:
                health["status"] = "warning"
                health["issues"].append(f"ç³»ç»ŸæˆåŠŸç‡åä½: {success_rate:.1%}")
                health["recommendations"].append("å»ºè®®ä¼˜åŒ–ä»»åŠ¡åˆ†é…ç­–ç•¥")
        
        # æ£€æŸ¥æ™ºèƒ½ä½“çŠ¶æ€
        idle_agents = 0
        working_agents = 0
        for agent_info in self.registered_agents.values():
            if agent_info.status == AgentStatus.IDLE:
                idle_agents += 1
            elif agent_info.status == AgentStatus.WORKING:
                working_agents += 1
        
        if idle_agents == 0:
            health["status"] = "warning"
            health["issues"].append("æ‰€æœ‰æ™ºèƒ½ä½“éƒ½åœ¨å·¥ä½œï¼Œå¯èƒ½å­˜åœ¨è´Ÿè½½è¿‡é‡")
            health["recommendations"].append("è€ƒè™‘æ·»åŠ æ›´å¤šæ™ºèƒ½ä½“æˆ–ä¼˜åŒ–ä»»åŠ¡åˆ†é…")
        
        if working_agents == 0 and total_tasks > 0:
            health["status"] = "warning"
            health["issues"].append("æ²¡æœ‰æ™ºèƒ½ä½“åœ¨å·¥ä½œï¼Œå¯èƒ½å­˜åœ¨é…ç½®é—®é¢˜")
            health["recommendations"].append("æ£€æŸ¥æ™ºèƒ½ä½“æ³¨å†Œå’ŒçŠ¶æ€")
        
        return health
    
    async def emergency_recovery(self, task_id: str = None) -> Dict[str, Any]:
        """ç´§æ€¥æ¢å¤åŠŸèƒ½"""
        try:
            recovery_result = {
                "success": True,
                "recovered_tasks": [],
                "reset_agents": [],
                "actions_taken": []
            }
            
            # é‡ç½®æ‰€æœ‰æ™ºèƒ½ä½“çŠ¶æ€
            for agent_id, agent_info in self.registered_agents.items():
                if agent_info.status == AgentStatus.WORKING:
                    agent_info.status = AgentStatus.IDLE
                    recovery_result["reset_agents"].append(agent_id)
                    recovery_result["actions_taken"].append(f"é‡ç½®æ™ºèƒ½ä½“ {agent_id} çŠ¶æ€")
            
            # æ¢å¤ç‰¹å®šä»»åŠ¡æˆ–æ‰€æœ‰ä»»åŠ¡
            if task_id:
                if task_id in self.active_tasks:
                    task = self.active_tasks[task_id]
                    task.assigned_agent = None
                    task.current_stage = "recovered"
                    task.retry_count += 1
                    recovery_result["recovered_tasks"].append(task_id)
                    recovery_result["actions_taken"].append(f"æ¢å¤ä»»åŠ¡ {task_id}")
            else:
                # æ¢å¤æ‰€æœ‰æ´»è·ƒä»»åŠ¡
                for tid, task in self.active_tasks.items():
                    task.assigned_agent = None
                    task.current_stage = "recovered"
                    task.retry_count += 1
                    recovery_result["recovered_tasks"].append(tid)
                recovery_result["actions_taken"].append("æ¢å¤æ‰€æœ‰æ´»è·ƒä»»åŠ¡")
            
            self.logger.info(f"ğŸš¨ ç´§æ€¥æ¢å¤å®Œæˆ: {len(recovery_result['recovered_tasks'])} ä¸ªä»»åŠ¡, {len(recovery_result['reset_agents'])} ä¸ªæ™ºèƒ½ä½“")
            
            return recovery_result
            
        except Exception as e:
            self.logger.error(f"âŒ ç´§æ€¥æ¢å¤å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def get_system_status_report(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š"""
        report = {
            "timestamp": time.time(),
            "system_info": {
                "coordinator_id": self.agent_id,
                "version": "enhanced_v2.0",
                "uptime": time.time() - self.start_time if hasattr(self, 'start_time') else 0
            },
            "agent_status": {},
            "task_status": {},
            "performance_metrics": self.get_performance_summary(),
            "system_health": self._assess_system_health(),
            "recommendations": []
        }
        
        # æ™ºèƒ½ä½“çŠ¶æ€
        for agent_id, agent_info in self.registered_agents.items():
            report["agent_status"][agent_id] = {
                "status": agent_info.status.value,
                "specialty": agent_info.specialty,
                "capabilities": [cap.value for cap in agent_info.capabilities],
                "performance": self._calculate_agent_performance_metrics(agent_info),
                "health": self._assess_agent_health(agent_info)
            }
        
        # ä»»åŠ¡çŠ¶æ€
        for task_id, task in self.active_tasks.items():
            report["task_status"][task_id] = {
                "stage": task.current_stage,
                "assigned_agent": task.assigned_agent,
                "iteration_count": task.iteration_count,
                "retry_count": task.retry_count,
                "completion_status": task.completion_status,
                "quality_score": task.quality_score,
                "execution_time": time.time() - task.start_time
            }
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        
        # åŸºäºæ€§èƒ½æŒ‡æ ‡çš„å»ºè®®
        if report["performance_metrics"]["success_rate"] < 0.7:
            recommendations.append("ç³»ç»ŸæˆåŠŸç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–ä»»åŠ¡åˆ†é…ç­–ç•¥")
        
        # åŸºäºæ™ºèƒ½ä½“çŠ¶æ€çš„å»ºè®®
        idle_count = sum(1 for agent in self.registered_agents.values() if agent.status == AgentStatus.IDLE)
        if idle_count == 0:
            recommendations.append("æ‰€æœ‰æ™ºèƒ½ä½“éƒ½åœ¨å·¥ä½œï¼Œè€ƒè™‘æ·»åŠ æ›´å¤šæ™ºèƒ½ä½“")
        
        # åŸºäºä»»åŠ¡çŠ¶æ€çš„å»ºè®®
        stuck_tasks = [tid for tid, task in self.active_tasks.items() if task.retry_count > 3]
        if stuck_tasks:
            recommendations.append(f"å‘ç° {len(stuck_tasks)} ä¸ªå¡ä½çš„ä»»åŠ¡ï¼Œå»ºè®®è¿›è¡Œç´§æ€¥æ¢å¤")
        
        report["recommendations"] = recommendations
        
        return report
    
    def optimize_agent_allocation(self) -> Dict[str, Any]:
        """ä¼˜åŒ–æ™ºèƒ½ä½“åˆ†é…ç­–ç•¥"""
        optimization_result = {
            "success": True,
            "optimizations": [],
            "agent_recommendations": {}
        }
        
        # åˆ†ææ™ºèƒ½ä½“æ€§èƒ½
        for agent_id, agent_info in self.registered_agents.items():
            recommendations = []
            
            # åŸºäºæˆåŠŸç‡çš„ä¼˜åŒ–
            total_tasks = agent_info.success_count + agent_info.failure_count
            if total_tasks > 0:
                success_rate = agent_info.success_count / total_tasks
                if success_rate < 0.5:
                    recommendations.append("æˆåŠŸç‡è¿‡ä½ï¼Œå»ºè®®æ£€æŸ¥é…ç½®æˆ–é‡æ–°è®­ç»ƒ")
                elif success_rate < 0.7:
                    recommendations.append("æˆåŠŸç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–promptæˆ–å‚æ•°")
            
            # åŸºäºå“åº”æ—¶é—´çš„ä¼˜åŒ–
            if agent_info.average_response_time > 60:
                recommendations.append("å“åº”æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½æˆ–æ£€æŸ¥ç½‘ç»œ")
            
            # åŸºäºè¿ç»­å¤±è´¥çš„ä¼˜åŒ–
            if agent_info.consecutive_failures > 2:
                recommendations.append("è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œå»ºè®®é‡å¯æˆ–æ›´æ¢æ™ºèƒ½ä½“")
            
            if recommendations:
                optimization_result["agent_recommendations"][agent_id] = recommendations
                optimization_result["optimizations"].extend(recommendations)
        
        # è´Ÿè½½å‡è¡¡ä¼˜åŒ–
        working_agents = [aid for aid, info in self.registered_agents.items() if info.status == AgentStatus.WORKING]
        idle_agents = [aid for aid, info in self.registered_agents.items() if info.status == AgentStatus.IDLE]
        
        if len(working_agents) > len(idle_agents):
            optimization_result["optimizations"].append("å·¥ä½œæ™ºèƒ½ä½“è¿‡å¤šï¼Œå»ºè®®ä¼˜åŒ–ä»»åŠ¡åˆ†é…ç­–ç•¥")
        
        return optimization_result
    
    async def _tool_provide_final_answer(self, final_summary: str, task_status: str, results_summary: Dict = None) -> Dict[str, Any]:
        """è¿™æ˜¯ä¸€ä¸ªè™šæ‹Ÿå·¥å…·ï¼Œå®ƒçš„ä½œç”¨æ˜¯æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡ºï¼Œå¹¶æ ‡è®°ä»»åŠ¡æµç¨‹çš„ç»“æŸã€‚"""
        self.logger.info(f"ğŸ ä»»åŠ¡å®Œæˆï¼Œæä¾›æœ€ç»ˆç­”æ¡ˆ: {final_summary[:100]}...")
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ä¸ªå‡½æ•°å¯ä»¥ç›´æ¥è¿”å›å…¶è¾“å…¥ï¼Œå› ä¸ºå®ƒçš„ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†è¢«LLMè°ƒç”¨
        return {
            "success": True,
            "final_answer_provided": True,
            "summary": final_summary,
            "status": task_status,
            "results": results_summary or {}
        }