#!/usr/bin/env python3
"""
åŸºäºLLMçš„åè°ƒæ™ºèƒ½ä½“ - å¼ºåŒ–ç‰ˆæœ¬

Enhanced LLM-Driven Coordinator Agent
"""

import asyncio
import json
import time
import logging
import re
import hashlib
from typing import Dict, Any, List, Optional, Set, Tuple
from dataclasses import dataclass, field
from pathlib import Path
from datetime import datetime
from enum import Enum

from core.schema_system.enhanced_base_agent import EnhancedBaseAgent
from core.base_agent import TaskMessage
from core.enums import AgentCapability, AgentStatus
from config.config import FrameworkConfig
from llm_integration.enhanced_llm_client import EnhancedLLMClient
from core.enhanced_logging_config import get_agent_logger
from core.llm_communication import UnifiedLLMClientManager, SystemPromptBuilder, CallType
from core.task_file_context import TaskFileContext, FileType, get_task_context, set_task_context


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    DESIGN = "design"
    VERIFICATION = "verification"
    ANALYSIS = "analysis"
    DEBUG = "debug"
    COMPOSITE = "composite"
    UNKNOWN = "unknown"


class TaskPriority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§æšä¸¾"""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class AgentInfo:
    """æ™ºèƒ½ä½“ä¿¡æ¯ - å¢å¼ºç‰ˆæœ¬"""
    agent_id: str
    agent_instance: EnhancedBaseAgent
    capabilities: Set[AgentCapability]
    specialty: str
    status: AgentStatus = AgentStatus.IDLE
    conversation_id: Optional[str] = None
    last_used: Optional[float] = None
    success_count: int = 0
    failure_count: int = 0
    total_execution_time: float = 0.0
    average_response_time: float = 0.0
    last_success_time: Optional[float] = None
    last_failure_time: Optional[float] = None
    consecutive_successes: int = 0
    consecutive_failures: int = 0
    preferred_task_types: Set[TaskType] = field(default_factory=set)
    blacklisted_task_types: Set[TaskType] = field(default_factory=set)


@dataclass
class TaskContext:
    """ä»»åŠ¡ä¸Šä¸‹æ–‡ - å¢å¼ºç‰ˆæœ¬"""
    task_id: str
    original_request: str
    task_type: TaskType = TaskType.UNKNOWN
    priority: TaskPriority = TaskPriority.MEDIUM
    current_stage: str = "initial"
    assigned_agent: Optional[str] = None
    agent_results: Dict[str, Any] = field(default_factory=dict)
    conversation_history: List[Dict[str, str]] = field(default_factory=list)
    start_time: float = field(default_factory=time.time)
    iteration_count: int = 0
    max_iterations: int = 20
    external_testbench_path: Optional[str] = None
    quality_score: float = 0.0
    completion_status: str = "pending"
    error_history: List[str] = field(default_factory=list)
    retry_count: int = 0
    max_retries: int = 3
    stage_history: List[Dict[str, Any]] = field(default_factory=list)
    agent_assignments: List[Dict[str, Any]] = field(default_factory=list)
    design_file_path: Optional[str] = None
    # ğŸ†• æ·»åŠ å®éªŒè·¯å¾„å­—æ®µï¼Œä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„å®éªŒç›®å½•
    experiment_path: Optional[str] = None
    
    # ğŸ†• å¢å¼ºæ•°æ®æ”¶é›†å­—æ®µ - ç”¨äºGradioå¯è§†åŒ–
    tool_executions: List[Dict[str, Any]] = field(default_factory=list)
    agent_interactions: List[Dict[str, Any]] = field(default_factory=list) 
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    workflow_stages: List[Dict[str, Any]] = field(default_factory=list)
    file_operations: List[Dict[str, Any]] = field(default_factory=list)
    execution_timeline: List[Dict[str, Any]] = field(default_factory=list)
    # ğŸ†• æ–°å¢ï¼šLLMå¯¹è¯è®°å½•
    llm_conversations: List[Dict[str, Any]] = field(default_factory=list)
    
    def add_conversation_message(self, role: str, content: str, agent_id: str = None, 
                               tool_info: Dict[str, Any] = None, metadata: Dict[str, Any] = None):
        """æ·»åŠ å¯¹è¯æ¶ˆæ¯åˆ°å†å²è®°å½•"""
        # ğŸ”§ å®‰å…¨å¤„ç†contentå‚æ•°ï¼Œç¡®ä¿å®ƒæ˜¯å­—ç¬¦ä¸²
        if content is None:
            safe_content = ""
        elif hasattr(content, '__await__'):  # æ£€æŸ¥æ˜¯å¦ä¸ºåç¨‹å¯¹è±¡
            safe_content = f"[åç¨‹å¯¹è±¡: {type(content).__name__}]"
        else:
            safe_content = str(content)
        
        message = {
            "timestamp": time.time(),
            "role": role,
            "content": safe_content,
            "agent_id": agent_id or "unknown",
        }
        
        if tool_info:
            message["tool_info"] = tool_info
        if metadata:
            message["metadata"] = metadata
            
        self.conversation_history.append(message)
        
        # è®°å½•æ—¥å¿—
        import logging
        logger = logging.getLogger("TaskContext")
        logger.info(f"ğŸ“ è®°å½•å¯¹è¯æ¶ˆæ¯: {role} - {agent_id or 'unknown'} - é•¿åº¦: {len(safe_content)}")
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """è·å–å¯¹è¯ç»Ÿè®¡æ‘˜è¦"""
        agents_involved = list(set(msg.get('agent_id', 'unknown') for msg in self.conversation_history))
        message_types = {}
        
        for msg in self.conversation_history:
            role = msg.get('role', 'unknown')
            message_types[role] = message_types.get(role, 0) + 1
        
        return {
            "total_messages": len(self.conversation_history),
            "agents_involved": agents_involved,
            "message_types": message_types,
            "conversation_duration": time.time() - self.start_time if self.conversation_history else 0
        }
    
    def get_agent_conversation_count(self, agent_id: str) -> int:
        """è·å–ç‰¹å®šæ™ºèƒ½ä½“çš„å¯¹è¯æ¶ˆæ¯æ•°é‡"""
        return len([msg for msg in self.conversation_history if msg.get('agent_id') == agent_id])
    
    def get_tool_calls_summary(self) -> Dict[str, Any]:
        """è·å–å·¥å…·è°ƒç”¨ç»Ÿè®¡æ‘˜è¦"""
        tool_calls = [msg for msg in self.conversation_history if msg.get('tool_info')]
        
        tool_names = []
        successful_calls = 0
        
        for msg in tool_calls:
            tool_info = msg.get('tool_info', {})
            if 'tool_name' in tool_info:
                tool_names.append(tool_info['tool_name'])
            if tool_info.get('success', False):
                successful_calls += 1
        
        return {
            "total_tool_calls": len(tool_calls),
            "successful_calls": successful_calls,
            "failure_rate": (len(tool_calls) - successful_calls) / max(len(tool_calls), 1),
            "unique_tools_used": list(set(tool_names)),
            "tool_usage_count": {name: tool_names.count(name) for name in set(tool_names)}
        }
    
    def add_tool_execution(self, tool_name: str, parameters: Dict[str, Any], 
                          agent_id: str, success: bool = True, 
                          result: Any = None, error: str = None, 
                          execution_time: float = 0.0):
        """è®°å½•å·¥å…·è°ƒç”¨æ‰§è¡Œ"""
        tool_execution = {
            "timestamp": time.time(),
            "tool_name": tool_name,
            "parameters": parameters,
            "agent_id": agent_id,
            "success": success,
            "result": str(result)[:5000] if result else None,  # é™åˆ¶ç»“æœé•¿åº¦
            "error": error,
            "execution_time": execution_time
        }
        self.tool_executions.append(tool_execution)
        
        # åŒæ—¶è®°å½•åˆ°æ‰§è¡Œæ—¶é—´çº¿
        self.execution_timeline.append({
            "timestamp": time.time(),
            "event_type": "tool_execution",
            "agent_id": agent_id,
            "tool_name": tool_name,
            "success": success,
            "duration": execution_time
        })
    
    def add_file_operation(self, operation_type: str, file_path: str, 
                          agent_id: str, success: bool = True, 
                          file_size: int = 0, error: str = None):
        """è®°å½•æ–‡ä»¶æ“ä½œ"""
        file_operation = {
            "timestamp": time.time(),
            "operation_type": operation_type,  # read, write, create, delete
            "file_path": file_path,
            "agent_id": agent_id,
            "success": success,
            "file_size": file_size,
            "error": error
        }
        self.file_operations.append(file_operation)
        
        # åŒæ—¶è®°å½•åˆ°æ‰§è¡Œæ—¶é—´çº¿
        self.execution_timeline.append({
            "timestamp": time.time(),
            "event_type": "file_operation",
            "agent_id": agent_id,
            "operation_type": operation_type,
            "file_path": file_path,
            "success": success
        })
    
    def add_workflow_stage(self, stage_name: str, description: str, 
                          agent_id: str = None, duration: float = 0.0, 
                          success: bool = True, metadata: Dict[str, Any] = None):
        """è®°å½•å·¥ä½œæµé˜¶æ®µ"""
        stage = {
            "timestamp": time.time(),
            "stage_name": stage_name,
            "description": description,
            "agent_id": agent_id,
            "duration": duration,
            "success": success,
            "metadata": metadata or {}
        }
        self.workflow_stages.append(stage)
        
        # åŒæ—¶è®°å½•åˆ°æ‰§è¡Œæ—¶é—´çº¿
        self.execution_timeline.append({
            "timestamp": time.time(),
            "event_type": "workflow_stage",
            "stage_name": stage_name,
            "agent_id": agent_id,
            "success": success,
            "duration": duration
        })
    
    def update_performance_metrics(self, metrics: Dict[str, Any]):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics.update(metrics)
    
    def get_data_collection_summary(self) -> Dict[str, Any]:
        """è·å–æ•°æ®æ”¶é›†æ‘˜è¦"""
        return {
            "tool_executions": {
                "total": len(self.tool_executions),
                "successful": len([t for t in self.tool_executions if t.get("success", True)]),
                "failed": len([t for t in self.tool_executions if not t.get("success", True)]),
                "unique_tools": list(set(t.get("tool_name") for t in self.tool_executions)),
                "total_execution_time": sum(t.get("execution_time", 0) or 0 for t in self.tool_executions)
            },
            "file_operations": {
                "total": len(self.file_operations),
                "successful": len([f for f in self.file_operations if f.get("success", True)]),
                "failed": len([f for f in self.file_operations if not f.get("success", True)]),
                "operation_types": list(set(f.get("operation_type") for f in self.file_operations)),
                "total_file_size": sum(f.get("file_size", 0) or 0 for f in self.file_operations)
            },
            "workflow_stages": {
                "total": len(self.workflow_stages),
                "successful": len([w for w in self.workflow_stages if w.get("success", True)]),
                "failed": len([w for w in self.workflow_stages if not w.get("success", True)]),
                "total_duration": sum(w.get("duration", 0) or 0 for w in self.workflow_stages)
            },
            "agent_interactions": {
                "total": len(self.agent_interactions),
                "unique_agents": list(set(i.get("target_agent_id") for i in self.agent_interactions)),
                "successful": len([i for i in self.agent_interactions if i.get("success", True)]),
                "failed": len([i for i in self.agent_interactions if not i.get("success", True)])
            },
            "execution_timeline": {
                "total_events": len(self.execution_timeline),
                "event_types": list(set(e.get("event_type") for e in self.execution_timeline))
            },
            "llm_conversations": {
                "total": len(self.llm_conversations),
                "successful": len([l for l in self.llm_conversations if l.get("success", True)]),
                "failed": len([l for l in self.llm_conversations if not l.get("success", True)]),
                "unique_agents": list(set(l.get("agent_id") for l in self.llm_conversations)),
                "unique_models": list(set(l.get("model_name") for l in self.llm_conversations)),
                "total_duration": sum(l.get("duration", 0) or 0 for l in self.llm_conversations),
                "first_calls": len([l for l in self.llm_conversations if l.get("is_first_call", False)]),
                "total_tokens": sum(l.get("total_tokens", 0) or 0 for l in self.llm_conversations)
            }
        }
    
    def add_llm_conversation(self, agent_id: str, conversation_id: str,
                           system_prompt: str, user_message: str, 
                           assistant_response: str, model_name: str = None,
                           duration: float = 0.0, success: bool = True,
                           error_info: str = None, is_first_call: bool = False,
                           temperature: float = None, max_tokens: int = None,
                           prompt_tokens: int = None, completion_tokens: int = None,
                           total_tokens: int = None):
        """è®°å½•LLMå¯¹è¯"""
        # å®‰å…¨å¤„ç†å¯èƒ½ä¸ºNoneçš„å­—ç¬¦ä¸²å‚æ•°
        safe_system_prompt = system_prompt or ""
        safe_user_message = user_message or ""
        safe_assistant_response = assistant_response or ""
        
        llm_conversation = {
            "timestamp": time.time(),
            "agent_id": agent_id,
            "conversation_id": conversation_id,
            "system_prompt": safe_system_prompt[:5000] + ("..." if len(safe_system_prompt) > 5000 else ""),  # é™åˆ¶é•¿åº¦
            "user_message": safe_user_message[:10000] + ("..." if len(safe_user_message) > 10000 else ""),  # é™åˆ¶é•¿åº¦
            "assistant_response": safe_assistant_response[:10000] + ("..." if len(safe_assistant_response) > 10000 else ""),  # é™åˆ¶é•¿åº¦
            "model_name": model_name,
            "duration": duration,
            "success": success,
            "error_info": error_info,
            "is_first_call": is_first_call,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens
        }
        self.llm_conversations.append(llm_conversation)
        
        # åŒæ—¶è®°å½•åˆ°æ‰§è¡Œæ—¶é—´çº¿
        self.execution_timeline.append({
            "timestamp": time.time(),
            "event_type": "llm_conversation",
            "agent_id": agent_id,
            "conversation_id": conversation_id,
            "model_name": model_name,
            "success": success,
            "duration": duration,
            "is_first_call": is_first_call
        })


class LLMCoordinatorAgent(EnhancedBaseAgent):
    """
    åŸºäºLLMçš„åè°ƒæ™ºèƒ½ä½“ - å¼ºåŒ–ç‰ˆæœ¬
    
    ç‰¹ç‚¹ï¼š
    1. æ™ºèƒ½ä»»åŠ¡ç±»å‹è¯†åˆ«å’Œåˆ†ç±»
    2. åŸºäºå†å²è¡¨ç°çš„æ™ºèƒ½ä½“é€‰æ‹©
    3. ä¸¥æ ¼çš„ä»»åŠ¡èŒè´£åˆ†ç¦»æ§åˆ¶
    4. å¢å¼ºçš„ç»“æœè´¨é‡åˆ†æ
    5. æ™ºèƒ½é”™è¯¯æ¢å¤å’Œé‡è¯•æœºåˆ¶
    6. è¯¦ç»†çš„æ‰§è¡Œç›‘æ§å’Œæ—¥å¿—
    """
    
    def __init__(self, config: FrameworkConfig = None):
        # åˆå§‹åŒ–é…ç½®
        self.config = config or FrameworkConfig.from_env()
        
        super().__init__(
            agent_id="llm_coordinator_agent",
            role="coordinator",
            capabilities={
                AgentCapability.TASK_COORDINATION,
                AgentCapability.WORKFLOW_MANAGEMENT,
                AgentCapability.SPECIFICATION_ANALYSIS,
                AgentCapability.CODE_REVIEW,
                AgentCapability.QUALITY_ANALYSIS,
                AgentCapability.SYSTEM_MONITORING
            },
            config=self.config
        )
        
        # è®°å½•å¯åŠ¨æ—¶é—´
        self.start_time = time.time()
        
        # ğŸ”§ æ–°å¢ï¼šè·¨ä»»åŠ¡æ–‡ä»¶ä¸Šä¸‹æ–‡ç®¡ç†
        self.global_file_context = {}  # å­˜å‚¨å…¨å±€æ–‡ä»¶ä¸Šä¸‹æ–‡ï¼Œkeyä¸ºæ–‡ä»¶è·¯å¾„ï¼Œvalueä¸ºæ–‡ä»¶ä¿¡æ¯
        self.conversation_file_history = {}  # å­˜å‚¨å¯¹è¯çº§åˆ«çš„æ–‡ä»¶å†å²
        
        # åˆå§‹åŒ–LLMé€šä¿¡æ¨¡å—
        self.llm_manager = UnifiedLLMClientManager(
            agent_id="llm_coordinator_agent",
            role="coordinator",
            config=self.config
        )
        self.prompt_builder = SystemPromptBuilder()
        
        # è®¾ç½®ä¸“ç”¨æ—¥å¿—å™¨
        self.agent_logger = get_agent_logger('LLMCoordinatorAgent')
        
        # æ³¨å†Œçš„æ™ºèƒ½ä½“
        self.registered_agents: Dict[str, AgentInfo] = {}
        
        # ä»»åŠ¡ä¸Šä¸‹æ–‡ç®¡ç†
        self.active_tasks: Dict[str, TaskContext] = {}
        
        # ä»»åŠ¡ç±»å‹è¯†åˆ«å™¨
        self.task_patterns = self._initialize_task_patterns()
        
        # åè°ƒå·¥å…·
        self._register_enhanced_coordination_tools()
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = {
            "total_tasks": 0,
            "successful_tasks": 0,
            "failed_tasks": 0,
            "average_completion_time": 0.0,
            "agent_utilization": {}
        }
        
        self.logger.debug("ğŸ§  LLMåè°ƒæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_task_patterns(self) -> Dict[TaskType, List[str]]:
        """åˆå§‹åŒ–ä»»åŠ¡ç±»å‹è¯†åˆ«æ¨¡å¼"""
        return {
            TaskType.DESIGN: [
                r"è®¾è®¡.*æ¨¡å—", r"ç”Ÿæˆ.*ä»£ç ", r"å®ç°.*åŠŸèƒ½", r"åˆ›å»º.*ç”µè·¯",
                r"design.*module", r"generate.*code", r"implement.*function",
                r"verilog.*module", r"hdl.*design", r"circuit.*design",
                r"æ¨¡å—è®¾è®¡", r"ä»£ç ç”Ÿæˆ", r"åŠŸèƒ½å®ç°", r"ç”µè·¯è®¾è®¡"
            ],
            TaskType.VERIFICATION: [
                r"æµ‹è¯•.*éªŒè¯", r"ä»¿çœŸ.*æµ‹è¯•", r"ç”Ÿæˆ.*testbench", r"éªŒè¯.*åŠŸèƒ½",
                r"test.*verification", r"simulation.*test", r"generate.*testbench",
                r"verify.*function", r"testbench.*generation", r"simulation.*verification",
                r"ä»£ç å®¡æŸ¥", r"è´¨é‡åˆ†æ", r"é”™è¯¯æ£€æŸ¥", r"åŠŸèƒ½éªŒè¯"
            ],
            TaskType.ANALYSIS: [
                r"åˆ†æ.*ä»£ç ", r"è´¨é‡.*è¯„ä¼°", r"æ€§èƒ½.*åˆ†æ", r"ä»£ç .*å®¡æŸ¥",
                r"analyze.*code", r"quality.*assessment", r"performance.*analysis",
                r"code.*review", r"static.*analysis", r"dynamic.*analysis",
                r"ä»£ç åˆ†æ", r"è´¨é‡è¯„ä¼°", r"æ€§èƒ½åˆ†æ", r"é™æ€åˆ†æ"
            ],
            TaskType.DEBUG: [
                r"è°ƒè¯•.*é”™è¯¯", r"ä¿®å¤.*é—®é¢˜", r"é”™è¯¯.*åˆ†æ", r"é—®é¢˜.*è§£å†³",
                r"debug.*error", r"fix.*issue", r"error.*analysis", r"problem.*solving",
                r"bug.*fix", r"error.*correction", r"issue.*resolution",
                r"é”™è¯¯ä¿®å¤", r"é—®é¢˜è°ƒè¯•", r"bugä¿®å¤", r"é”™è¯¯åˆ†æ"
            ]
        }
    
    def _register_enhanced_coordination_tools(self):
        """æ³¨å†Œå¢å¼ºçš„åè°ƒå·¥å…·"""
        
        # 1. æ™ºèƒ½ä»»åŠ¡åˆ†é…å·¥å…·
        self.register_enhanced_tool(
            name="assign_task_to_agent",
            func=self._tool_assign_task_to_agent,
            description="æ™ºèƒ½åˆ†é…ä»»åŠ¡ç»™æœ€åˆé€‚çš„æ™ºèƒ½ä½“ï¼ŒåŸºäºä»»åŠ¡ç±»å‹ã€æ™ºèƒ½ä½“èƒ½åŠ›å’Œå†å²è¡¨ç°",
            security_level="high",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "agent_id": {
                        "type": "string",
                        "description": "ç›®æ ‡æ™ºèƒ½ä½“IDï¼ˆå¿…é¡»æ˜¯enhanced_real_verilog_agentæˆ–enhanced_real_code_review_agentï¼‰",
                        "enum": ["enhanced_real_verilog_agent", "enhanced_real_code_review_agent"]
                    },
                    "task_description": {
                        "type": "string",
                        "description": "è¯¦ç»†çš„ä»»åŠ¡æè¿°å’Œè¦æ±‚",
                        "minLength": 10,
                        "maxLength": 5000
                    },
                    "expected_output": {
                        "type": "string",
                        "description": "æœŸæœ›çš„è¾“å‡ºæ ¼å¼å’Œå†…å®¹",
                        "default": "",
                        "maxLength": 2000
                    },
                    "task_type": {
                        "type": "string",
                        "enum": ["design", "verification", "analysis", "debug", "composite"],
                        "description": "ä»»åŠ¡ç±»å‹åˆ†ç±»"
                    },
                    "priority": {
                        "type": "string",
                        "enum": ["high", "medium", "low"],
                        "default": "medium",
                        "description": "ä»»åŠ¡ä¼˜å…ˆçº§"
                    }
                },
                "required": ["agent_id", "task_description"]
            }
        )
        
        # 2. å¢å¼ºç»“æœåˆ†æå·¥å…·
        self.register_enhanced_tool(
            name="analyze_agent_result",
            func=self._tool_analyze_agent_result,
            description="æ·±åº¦åˆ†ææ™ºèƒ½ä½“æ‰§è¡Œç»“æœï¼Œè¯„ä¼°è´¨é‡ã€å®Œæ•´æ€§å’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨",
            security_level="high",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "agent_id": {
                        "type": "string",
                        "description": "æ‰§è¡Œä»»åŠ¡çš„æ™ºèƒ½ä½“ID"
                    },
                    "result": {
                        "type": "object",
                        "description": "æ™ºèƒ½ä½“æ‰§è¡Œç»“æœçš„è¯¦ç»†ä¿¡æ¯"
                    },
                    "task_context": {
                        "type": "object",
                        "description": "å½“å‰ä»»åŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯",
                        "default": {}
                    },
                    "quality_threshold": {
                        "type": "number",
                        "minimum": 0,
                        "maximum": 100,
                        "default": 80,
                        "description": "è´¨é‡è¯„ä¼°é˜ˆå€¼"
                    }
                },
                "required": ["agent_id", "result"]
            }
        )
        
        # 3. æ™ºèƒ½ä»»åŠ¡å®Œæˆæ£€æŸ¥å·¥å…·
        self.register_enhanced_tool(
            name="check_task_completion",
            func=self._tool_check_task_completion,
            description="æ™ºèƒ½æ£€æŸ¥ä»»åŠ¡å®ŒæˆçŠ¶æ€ï¼Œè¯„ä¼°æ•´ä½“è´¨é‡å’Œç¼ºå¤±é¡¹",
            security_level="high",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "task_id": {
                        "type": "string",
                        "description": "ä»»åŠ¡ID"
                    },
                    "all_results": {
                        "type": "object",
                        "description": "æ‰€æœ‰æ™ºèƒ½ä½“çš„æ‰§è¡Œç»“æœæ±‡æ€»"
                    },
                    "original_requirements": {
                        "type": "string",
                        "description": "åŸå§‹ä»»åŠ¡éœ€æ±‚æè¿°"
                    },
                    "completion_criteria": {
                        "type": "object",
                        "description": "å®Œæˆæ ‡å‡†å®šä¹‰",
                        "default": {}
                    }
                },
                "required": ["task_id", "all_results", "original_requirements"]
            }
        )
        
        # 4. æ™ºèƒ½ä½“çŠ¶æ€å’Œæ€§èƒ½æŸ¥è¯¢å·¥å…·
        self.register_enhanced_tool(
            name="query_agent_status",
            func=self._tool_query_agent_status,
            description="æŸ¥è¯¢æ™ºèƒ½ä½“è¯¦ç»†çŠ¶æ€ã€æ€§èƒ½æŒ‡æ ‡å’Œå†å²è¡¨ç°",
            security_level="normal",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "agent_id": {
                        "type": "string",
                        "description": "æ™ºèƒ½ä½“ID",
                        "enum": ["enhanced_real_verilog_agent", "enhanced_real_code_review_agent"]
                    },
                    "include_performance": {
                        "type": "boolean",
                        "default": True,
                        "description": "æ˜¯å¦åŒ…å«æ€§èƒ½æŒ‡æ ‡"
                    },
                    "include_history": {
                        "type": "boolean",
                        "default": False,
                        "description": "æ˜¯å¦åŒ…å«å†å²è®°å½•"
                    }
                },
                "required": ["agent_id"]
            }
        )
        
        # 5. æ–°å¢ï¼šæ™ºèƒ½ä»»åŠ¡ç±»å‹è¯†åˆ«å·¥å…·
        self.register_enhanced_tool(
            name="identify_task_type",
            func=self._tool_identify_task_type,
            description="æ™ºèƒ½è¯†åˆ«ä»»åŠ¡ç±»å‹ï¼Œä¸ºä»»åŠ¡åˆ†é…æä¾›å†³ç­–æ”¯æŒ",
            security_level="normal",
            category="analysis",
            schema={
                "type": "object",
                "properties": {
                    "user_request": {
                        "type": "string",
                        "description": "ç”¨æˆ·è¯·æ±‚å†…å®¹",
                        "minLength": 5,
                        "maxLength": 10000
                    },
                    "context": {
                        "type": "object",
                        "description": "ä»»åŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯",
                        "default": {}
                    }
                },
                "required": ["user_request"]
            }
        )
        
        # 6. æ–°å¢ï¼šæ™ºèƒ½ä½“é€‰æ‹©æ¨èå·¥å…·
        self.register_enhanced_tool(
            name="recommend_agent",
            func=self._tool_recommend_agent,
            description="åŸºäºä»»åŠ¡ç‰¹å¾å’Œæ™ºèƒ½ä½“èƒ½åŠ›æ¨èæœ€åˆé€‚çš„æ™ºèƒ½ä½“",
            security_level="normal",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "task_type": {
                        "type": "string",
                        "enum": ["design", "verification", "analysis", "debug", "composite"],
                        "description": "ä»»åŠ¡ç±»å‹"
                    },
                    "task_description": {
                        "type": "string",
                        "description": "ä»»åŠ¡æè¿°",
                        "minLength": 10
                    },
                    "priority": {
                        "type": "string",
                        "enum": ["high", "medium", "low"],
                        "default": "medium",
                        "description": "ä»»åŠ¡ä¼˜å…ˆçº§"
                    },
                    "constraints": {
                        "type": "object",
                        "description": "ä»»åŠ¡çº¦æŸæ¡ä»¶",
                        "default": {}
                    }
                },
                "required": ["task_type", "task_description"]
            }
        )
        
        # 7. æ–°å¢ï¼šæœ€ç»ˆç­”æ¡ˆå·¥å…·
        self.register_enhanced_tool(
            name="provide_final_answer",
            func=self._tool_provide_final_answer,
            description="å½“æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆï¼Œè°ƒç”¨æ­¤å·¥å…·æ¥ç”Ÿæˆå¹¶æä¾›æœ€ç»ˆçš„ã€å®Œæ•´çš„ç­”æ¡ˆç»™ç”¨æˆ·ã€‚",
            security_level="normal",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "final_summary": {
                        "type": "string",
                        "description": "å¯¹æ•´ä¸ªä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹å’Œæœ€ç»ˆç»“æœçš„è¯¦ç»†æ€»ç»“ã€‚"
                    },
                    "task_status": {
                        "type": "string",
                        "description": "ä»»åŠ¡çš„æœ€ç»ˆçŠ¶æ€ (ä¾‹å¦‚ï¼š'æˆåŠŸ', 'å¤±è´¥', 'éƒ¨åˆ†å®Œæˆ')."
                    },
                    "results_summary": {
                        "type": "object",
                        "description": "æ‰€æœ‰æ™ºèƒ½ä½“äº§ç”Ÿçš„ç»“æœçš„ç®€è¦æ±‡æ€»ã€‚"
                    }
                },
                "required": ["final_summary", "task_status"]
            }
        )
        
        # 8. æ–°å¢ï¼šå·¥å…·ä½¿ç”¨æŒ‡å¯¼å·¥å…·
        self.register_enhanced_tool(
            name="get_tool_usage_guide",
            func=self._tool_get_tool_usage_guide,
            description="è·å–LLMCoordinatorAgentçš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼ï¼ŒåŒ…æ‹¬å¯ç”¨å·¥å…·ã€å‚æ•°è¯´æ˜ã€è°ƒç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µã€‚",
            security_level="normal",
            category="help",
            schema={
                "type": "object",
                "properties": {
                    "include_examples": {
                        "type": "boolean",
                        "default": True,
                        "description": "æ˜¯å¦åŒ…å«è°ƒç”¨ç¤ºä¾‹"
                    },
                    "include_best_practices": {
                        "type": "boolean",
                        "default": True,
                        "description": "æ˜¯å¦åŒ…å«æœ€ä½³å®è·µ"
                    }
                },
                "additionalProperties": False
            }
        )
    
    async def _build_enhanced_system_prompt(self) -> str:
        """æ„å»ºæ”¯æŒåŠ¨æ€å†³ç­–å’Œå¤šæ™ºèƒ½ä½“åä½œçš„ç³»ç»Ÿæç¤ºè¯"""
        
        # ä½¿ç”¨SystemPromptBuilderæ„å»ºç³»ç»Ÿæç¤ºè¯
        return await self.prompt_builder.build_system_prompt(
            role="coordinator",
            call_type=CallType.FUNCTION_CALLING,
            agent_id=self.agent_id,
            capabilities=self._capabilities,
            metadata={
                "has_tools": hasattr(self, 'enhanced_tools') and bool(self.enhanced_tools),
                "tools_count": len(self.enhanced_tools) if hasattr(self, 'enhanced_tools') else 0
            }
        )
    async def register_agent(self, agent: EnhancedBaseAgent):
        """æ³¨å†Œæ™ºèƒ½ä½“"""
        agent_info = AgentInfo(
            agent_id=agent.agent_id,
            agent_instance=agent,
            capabilities=agent.get_capabilities(),
            specialty=agent.get_specialty_description()
        )
        
        self.registered_agents[agent.agent_id] = agent_info
        self.logger.info(f"âœ… æ³¨å†Œæ™ºèƒ½ä½“: {agent.agent_id} ({agent_info.specialty})")
    
    async def coordinate_task(self, user_request: str, 
                            conversation_id: str = None,
                            max_iterations: int = 20,
                            external_testbench_path: str = None) -> Dict[str, Any]:
        """
        åè°ƒä»»åŠ¡æ‰§è¡Œ
        
        Args:
            user_request: ç”¨æˆ·è¯·æ±‚
            conversation_id: å¯¹è¯ID
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            external_testbench_path: å¤–éƒ¨æä¾›çš„testbenchæ–‡ä»¶è·¯å¾„
            
        Returns:
            åè°ƒç»“æœ
        """
        self.logger.info(f"ğŸš€ å¼€å§‹åè°ƒä»»åŠ¡: {user_request[:100]}...")
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = f"task_{int(time.time())}"
        
        # ğŸ†• åˆ›å»ºç‹¬ç«‹çš„å®éªŒç›®å½•
        from core.experiment_manager import ExperimentManager
        experiment_manager = ExperimentManager()
        
        # ğŸ¯ ä¼˜åŒ–ï¼šä½¿ç”¨æ™ºèƒ½å®éªŒåç§°ç”Ÿæˆ
        experiment_name = experiment_manager.generate_experiment_name(user_request, task_type="design")
        
        # åˆ›å»ºå®éªŒ
        experiment_info = experiment_manager.create_experiment(
            experiment_name=experiment_name,
            task_description=user_request,
            metadata={
                "task_id": task_id,
                "created_by": "llm_coordinator_agent",
                "conversation_id": conversation_id
            }
        )
        
        # åˆ›å»ºä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«å®éªŒè·¯å¾„
        task_context = TaskContext(
            task_id=task_id,
            original_request=user_request,
            max_iterations=max_iterations,
            experiment_path=experiment_info.workspace_path
        )
        
        # ğŸ†• è®°å½•åˆå§‹ç”¨æˆ·è¯·æ±‚åˆ°å¯¹è¯å†å²
        task_context.add_conversation_message(
            role="user",
            content=user_request,
            agent_id="user",
            metadata={"task_id": task_id, "conversation_id": conversation_id}
        )
        
        # å¦‚æœæä¾›äº†å¤–éƒ¨testbenchï¼Œæ·»åŠ åˆ°ä»»åŠ¡ä¸Šä¸‹æ–‡
        if external_testbench_path:
            task_context.external_testbench_path = external_testbench_path
            self.logger.info(f"ğŸ“ ä½¿ç”¨å¤–éƒ¨testbench: {external_testbench_path}")
            # è®°å½•å¤–éƒ¨testbenchä¿¡æ¯
            task_context.add_conversation_message(
                role="system",
                content=f"ä½¿ç”¨å¤–éƒ¨testbenchæ–‡ä»¶: {external_testbench_path}",
                agent_id=self.agent_id,
                metadata={"type": "external_testbench"}
            )
        
        self.active_tasks[task_id] = task_context
        
        # ğŸ†• è®¾ç½®ä»»åŠ¡ä¸Šä¸‹æ–‡åˆ°å½“å‰å®ä¾‹ï¼Œç”¨äºåç»­å¯¹è¯è®°å½•
        self.current_task_context = task_context
        
        try:
            # æ„å»ºåè°ƒä»»åŠ¡
            coordination_task = self._build_coordination_task(user_request, task_context)
            
            # ğŸ†• è®°å½•ç³»ç»Ÿæç¤ºï¼ˆåè°ƒä»»åŠ¡ï¼‰åˆ°å¯¹è¯å†å²
            task_context.add_conversation_message(
                role="system",
                content=coordination_task,
                agent_id=self.agent_id,
                metadata={"type": "coordination_task", "task_stage": "initial"}
            )
            
            # ä½¿ç”¨Function Callingæ‰§è¡Œåè°ƒ
            result = await self.process_with_function_calling(
                user_request=coordination_task,
                max_iterations=max_iterations,
                conversation_id=conversation_id,
                preserve_context=True,
                enable_self_continuation=True,
                max_self_iterations=3
            )
            
            # ğŸ”§ æ–°å¢ï¼šæ ¼å¼ä¿®å¤ - å¦‚æœLLMè¾“å‡ºäº†é”™è¯¯æ ¼å¼ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤
            result = self._fix_tool_call_format(result)
            
            # ğŸ†• è®°å½•åè°ƒå™¨çš„å“åº”åˆ°å¯¹è¯å†å²
            task_context.add_conversation_message(
                role="assistant",
                content=result,
                agent_id=self.agent_id,
                metadata={"type": "coordination_response", "task_stage": "initial"}
            )
            
            # ğŸ” æ£€æŸ¥æ˜¯å¦å®é™…è°ƒç”¨äº†å·¥å…·æˆ–ä»»åŠ¡å·²å®Œæˆ
            self.logger.info(f"ğŸ” æ£€æŸ¥å·¥å…·è°ƒç”¨: ç»“æœé•¿åº¦={len(result)}, å†…å®¹é¢„è§ˆ={result[:100]}...")
            tools_executed = self._has_executed_tools(result)
            self.logger.info(f"ğŸ” å·¥å…·è°ƒç”¨æ£€æŸ¥ç»“æœ: {tools_executed}")
            
            # ğŸ”§ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯ä»»åŠ¡å®ŒæˆçŠ¶æ€
            if tools_executed:
                # æ£€æŸ¥æ˜¯å¦å·²ç»æä¾›äº†æœ€ç»ˆç­”æ¡ˆ
                if self._has_final_answer_in_history(task_context):
                    self.logger.info("âœ… æ£€æµ‹åˆ°æœ€ç»ˆç­”æ¡ˆå·²æä¾›ï¼Œä»»åŠ¡å®Œæˆ")
                    return self._collect_final_result(task_context, result)
            
            if not tools_executed:
                self.logger.warning("âš ï¸ åè°ƒæ™ºèƒ½ä½“æ²¡æœ‰è°ƒç”¨ä»»ä½•å·¥å…·ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶é‡æ–°æ‰§è¡Œ")
                self.logger.info(f"ğŸ” åŸå§‹ç»“æœå†…å®¹: {result[:2000]}...")
                
                # ğŸ”§ ä¿®æ”¹ï¼šåªæœ‰åœ¨ç‰¹å®šæ¡ä»¶ä¸‹æ‰å¼ºåˆ¶é‡æ–°æ‰§è¡Œ
                if self._should_force_reexecution(task_context, result):
                    # å¼ºåˆ¶é‡æ–°æ‰§è¡Œï¼Œä½¿ç”¨æ›´æ˜ç¡®çš„æŒ‡ä»¤
                    forced_task = self._build_forced_coordination_task(user_request, task_context)
                    self.logger.info(f"ğŸš¨ å¼ºåˆ¶é‡æ–°æ‰§è¡Œï¼Œä»»åŠ¡é•¿åº¦: {len(forced_task)} å­—ç¬¦")
                    
                    # ä½¿ç”¨æ›´ä¸¥æ ¼çš„å‚æ•°è¿›è¡Œå¼ºåˆ¶é‡æ–°æ‰§è¡Œ
                    result = await self.process_with_function_calling(
                        user_request=forced_task,
                        max_iterations=1,  # é™åˆ¶ä¸º1æ¬¡è¿­ä»£ï¼Œå¼ºåˆ¶ç«‹å³æ‰§è¡Œ
                        conversation_id=f"{conversation_id}_forced",
                        preserve_context=False,  # ä¸ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œé‡æ–°å¼€å§‹
                        enable_self_continuation=False,  # ç¦ç”¨è‡ªä¸»ç»§ç»­
                        max_self_iterations=0  # ç¦ç”¨è‡ªæˆ‘ç»§ç»­
                    )
                    
                    # å†æ¬¡æ£€æŸ¥æ˜¯å¦æ‰§è¡Œäº†å·¥å…·
                    if not self._has_executed_tools(result):
                        self.logger.error("âŒ å¼ºåˆ¶é‡æ–°æ‰§è¡Œåä»æœªè°ƒç”¨å·¥å…·ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯")
                        self.logger.error(f"ğŸ” å¼ºåˆ¶æ‰§è¡Œç»“æœ: {result[:2000]}...")
                        return {
                            "success": False,
                            "error": "åè°ƒæ™ºèƒ½ä½“æ— æ³•æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®",
                            "task_id": task_id,
                            "debug_info": {
                                "original_result": result[:500],
                                "forced_result": result[:500],
                                "tool_detection_failed": True
                            }
                        }
                else:
                    self.logger.info("âœ… æ— éœ€å¼ºåˆ¶é‡æ–°æ‰§è¡Œï¼Œä»»åŠ¡å¯èƒ½å·²å®Œæˆ")
                    return self._collect_final_result(task_context, result)
            
            # ğŸ” æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†assign_task_to_agentå·¥å…·
            self.logger.info(f"ğŸ” æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†assign_task_to_agentå·¥å…·...")
            assign_task_called = self._check_assign_task_called(result)
            self.logger.info(f"ğŸ” assign_task_to_agentè°ƒç”¨æ£€æŸ¥: {assign_task_called}")
            
            if not assign_task_called:
                self.logger.warning("âš ï¸ æ²¡æœ‰è°ƒç”¨assign_task_to_agentå·¥å…·ï¼Œå¼ºåˆ¶è°ƒç”¨")
                # å¼ºåˆ¶è°ƒç”¨assign_task_to_agent
                forced_assign_result = await self._force_assign_task(user_request, task_context)
                if not forced_assign_result.get("success", False):
                    self.logger.error("âŒ å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¤±è´¥")
                    return {
                        "success": False,
                        "error": "å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¤±è´¥",
                        "task_id": task_id,
                        "debug_info": forced_assign_result
                    }
            
            # ğŸ”„ å¼€å§‹æŒç»­åè°ƒå¾ªç¯ - ç›‘å¬æ™ºèƒ½ä½“ç»“æœå¹¶ç»§ç»­åè°ƒ
            final_result = await self._run_coordination_loop(task_context, result, conversation_id, max_iterations)
            
            # ğŸ†• è®°å½•ä»»åŠ¡å®Œæˆ
            task_context.add_conversation_message(
                role="system",
                content=f"ä»»åŠ¡åè°ƒå®Œæˆï¼Œä»»åŠ¡ID: {task_id}",
                agent_id=self.agent_id,
                metadata={"type": "task_completion", "success": True}
            )
            
            self.logger.info(f"âœ… ä»»åŠ¡åè°ƒå®Œæˆ: {task_id}, å¯¹è¯å†å²é•¿åº¦: {len(task_context.conversation_history)}")
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ ä»»åŠ¡åè°ƒå¤±è´¥: {str(e)}")
            
            # ğŸ†• è®°å½•é”™è¯¯åˆ°å¯¹è¯å†å²
            if 'task_context' in locals():
                task_context.add_conversation_message(
                    role="error",
                    content=f"ä»»åŠ¡åè°ƒå¤±è´¥: {str(e)}",
                    agent_id=self.agent_id,
                    metadata={"type": "task_error", "error_type": type(e).__name__}
                )
            
            return {
                "success": False,
                "error": str(e),
                "task_id": task_id
            }
        finally:
            # æ¸…ç†ä»»åŠ¡ä¸Šä¸‹æ–‡
            if task_id in self.active_tasks:
                del self.active_tasks[task_id]
    
    def _fix_tool_call_format(self, result: str) -> str:
        """ä¿®å¤å·¥å…·è°ƒç”¨æ ¼å¼ - å°†é”™è¯¯çš„å•å·¥å…·æ ¼å¼è½¬æ¢ä¸ºæ­£ç¡®çš„tool_callsæ•°ç»„æ ¼å¼"""
        if not isinstance(result, str) or not result.strip():
            return result
        
        # æå–JSONå†…å®¹
        json_content = self._extract_json_from_response(result.strip())
        if not json_content:
            return result
        
        try:
            data = json.loads(json_content)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯çš„å•ä¸ªå·¥å…·æ ¼å¼
            if "tool_name" in data and "parameters" in data and "tool_calls" not in data:
                self.logger.warning(f"ğŸ”§ æ£€æµ‹åˆ°é”™è¯¯çš„å•å·¥å…·æ ¼å¼ï¼Œè‡ªåŠ¨ä¿®å¤ä¸ºtool_callsæ•°ç»„æ ¼å¼")
                
                # è½¬æ¢ä¸ºæ­£ç¡®çš„æ ¼å¼
                fixed_data = {
                    "tool_calls": [
                        {
                            "tool_name": data["tool_name"],
                            "parameters": data["parameters"]
                        }
                    ]
                }
                
                # ç”Ÿæˆä¿®å¤åçš„å“åº”
                fixed_json = json.dumps(fixed_data, ensure_ascii=False, indent=2)
                fixed_result = f"```json\n{fixed_json}\n```"
                
                self.logger.info(f"âœ… å·²ä¿®å¤å·¥å…·è°ƒç”¨æ ¼å¼ï¼š{data['tool_name']}")
                return fixed_result
            
        except json.JSONDecodeError:
            self.logger.debug("JSONè§£æå¤±è´¥ï¼Œä¿æŒåŸå§‹æ ¼å¼")
        
        return result

    def _has_executed_tools(self, result: str) -> bool:
        """æ£€æŸ¥å¹¶ä¿®å¤LLMçš„å›å¤ï¼Œç¡®ä¿æ˜¯æœ‰æ•ˆçš„å·¥å…·è°ƒç”¨JSONã€‚"""
        if not isinstance(result, str) or not result.strip():
            self.logger.debug(f"ğŸ” å·¥å…·æ£€æµ‹å¤±è´¥: ç»“æœä¸ºç©ºæˆ–éå­—ç¬¦ä¸²ç±»å‹")
            return False
        
        # æå–JSONå†…å®¹ï¼Œæ”¯æŒmarkdownä»£ç å—æ ¼å¼
        json_content = self._extract_json_from_response(result.strip())
        if not json_content:
            self.logger.debug(f"ğŸ” å·¥å…·æ£€æµ‹å¤±è´¥: æ— æ³•æå–JSONå†…å®¹")
            return False
            
        try:
            data = json.loads(json_content)
            self.logger.debug(f"ğŸ” è§£æJSONæˆåŠŸ: {list(data.keys())}")
            
            # ğŸ”§ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯ä»»åŠ¡å®Œæˆè¯„ä¼°ç»“æœ
            if self._is_task_completion_assessment(data):
                self.logger.info(f"âœ… æ£€æµ‹åˆ°ä»»åŠ¡å®Œæˆè¯„ä¼°ç»“æœï¼Œè®¤ä¸ºä»»åŠ¡å·²å®Œæˆ")
                return True
            
            # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯çš„å•ä¸ªå·¥å…·æ ¼å¼ï¼Œå¹¶è‡ªåŠ¨è½¬æ¢ä¸ºæ­£ç¡®çš„tool_callsæ ¼å¼
            if "tool_name" in data and "parameters" in data and "tool_calls" not in data:
                self.logger.warning(f"ğŸ”§ æ£€æµ‹åˆ°é”™è¯¯çš„å•å·¥å…·æ ¼å¼ï¼Œè‡ªåŠ¨è½¬æ¢ä¸ºtool_callsæ•°ç»„æ ¼å¼")
                # è½¬æ¢ä¸ºæ­£ç¡®çš„æ ¼å¼
                fixed_data = {
                    "tool_calls": [
                        {
                            "tool_name": data["tool_name"],
                            "parameters": data["parameters"]
                        }
                    ]
                }
                # æ›´æ–°resultï¼ˆç”¨äºåç»­å¤„ç†ï¼‰
                fixed_json = json.dumps(fixed_data, ensure_ascii=False, indent=2)
                # è¿™é‡Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥ä¿®æ”¹ä¼ å…¥çš„resultï¼Œä½†å¯ä»¥è®°å½•é—®é¢˜
                self.logger.info(f"ğŸ”§ ä¿®å¤åçš„æ ¼å¼: {fixed_json}")
                return True  # æ ¼å¼å¯ä»¥ä¿®å¤ï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆçš„å·¥å…·è°ƒç”¨
            
            # æ£€æŸ¥æ­£ç¡®çš„tool_callsæ ¼å¼
            if "tool_calls" in data and isinstance(data["tool_calls"], list) and len(data["tool_calls"]) > 0:
                # è¿›ä¸€æ­¥æ£€æŸ¥tool_callsåˆ—è¡¨ä¸­çš„å…ƒç´ æ˜¯å¦åˆæ³•
                call = data["tool_calls"][0]
                if "tool_name" in call and "parameters" in call:
                    tool_name = call["tool_name"]
                    self.logger.info(f"âœ… æ£€æµ‹åˆ°æœ‰æ•ˆå·¥å…·è°ƒç”¨: {tool_name}")
                    return True
                else:
                    self.logger.debug(f"ğŸ” å·¥å…·è°ƒç”¨æ ¼å¼ä¸å®Œæ•´: {call}")
            else:
                self.logger.debug(f"ğŸ” æœªæ‰¾åˆ°æœ‰æ•ˆçš„tool_calls: {data}")
            return False
        except json.JSONDecodeError as e:
            self.logger.debug(f"ğŸ” JSONè§£æå¤±è´¥: {e}")
            return False
    
    def _is_task_completion_assessment(self, data: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ˜¯ä»»åŠ¡å®Œæˆè¯„ä¼°ç»“æœ"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»åŠ¡å®Œæˆç›¸å…³çš„å­—æ®µ
        completion_fields = [
            "completion_rate", "quality_score", "needs_continuation", 
            "task_status", "final_summary", "results_summary"
        ]
        
        # å¦‚æœåŒ…å«completion_rateä¸”å€¼ä¸º100ï¼Œæˆ–è€…åŒ…å«task_statusä¸”å€¼ä¸ºsuccessï¼Œè®¤ä¸ºæ˜¯å®Œæˆè¯„ä¼°
        if "completion_rate" in data and data["completion_rate"] == 100:
            return True
        
        if "task_status" in data and data["task_status"] == "success":
            return True
        
        # å¦‚æœåŒ…å«needs_continuationä¸”å€¼ä¸ºfalseï¼Œä¹Ÿè®¤ä¸ºæ˜¯å®Œæˆè¯„ä¼°
        if "needs_continuation" in data and data["needs_continuation"] is False:
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªå®Œæˆç›¸å…³å­—æ®µ
        completion_field_count = sum(1 for field in completion_fields if field in data)
        if completion_field_count >= 2:
            return True
        
        return False
    
    def _has_task_completion_in_history(self, task_context: TaskContext) -> bool:
        """æ£€æŸ¥å¯¹è¯å†å²ä¸­æ˜¯å¦æœ‰ä»»åŠ¡å®ŒæˆæŒ‡ç¤ºå™¨"""
        for msg in task_context.conversation_history:
            if msg.get("role") == "assistant":
                content = msg.get("content", "")
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»åŠ¡å®Œæˆç›¸å…³çš„å…³é”®è¯
                if any(keyword in content.lower() for keyword in [
                    "completion_rate", "quality_score", "needs_continuation", 
                    "task_status", "final_summary", "results_summary",
                    "ä»»åŠ¡å·²å®Œå…¨å®Œæˆ", "ä»»åŠ¡æˆåŠŸå®Œæˆ", "æ‰€æœ‰è¦æ±‚éƒ½å·²æ»¡è¶³",
                    "provide_final_answer"
                ]):
                    return True
        return False
    
    def _has_final_answer_in_history(self, task_context: TaskContext) -> bool:
        """æ£€æŸ¥å¯¹è¯å†å²ä¸­æ˜¯å¦å·²ç»æä¾›äº†æœ€ç»ˆç­”æ¡ˆ"""
        for msg in task_context.conversation_history:
            if msg.get("role") == "assistant":
                content = msg.get("content", "")
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ€ç»ˆç­”æ¡ˆç›¸å…³çš„å…³é”®è¯
                if any(keyword in content.lower() for keyword in [
                    "provide_final_answer", "final_summary", "task_status", "success"
                ]):
                    return True
        return False
    
    def _should_force_reexecution(self, task_context: TaskContext, result: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å¼ºåˆ¶é‡æ–°æ‰§è¡Œ"""
        # å¦‚æœå·²ç»æœ‰æ™ºèƒ½ä½“ç»“æœï¼Œè¯´æ˜ä»»åŠ¡å·²ç»å¼€å§‹æ‰§è¡Œï¼Œä¸éœ€è¦å¼ºåˆ¶é‡æ–°æ‰§è¡Œ
        if task_context.agent_results:
            self.logger.info("âœ… å·²æœ‰æ™ºèƒ½ä½“ç»“æœï¼Œæ— éœ€å¼ºåˆ¶é‡æ–°æ‰§è¡Œ")
            return False
        
        # å¦‚æœå¯¹è¯å†å²ä¸­å·²ç»æœ‰assign_task_to_agentè°ƒç”¨ï¼Œä¹Ÿä¸éœ€è¦å¼ºåˆ¶é‡æ–°æ‰§è¡Œ
        for msg in task_context.conversation_history:
            if msg.get("role") == "assistant":
                content = msg.get("content", "")
                if "assign_task_to_agent" in content:
                    self.logger.info("âœ… å·²æœ‰ä»»åŠ¡åˆ†é…è®°å½•ï¼Œæ— éœ€å¼ºåˆ¶é‡æ–°æ‰§è¡Œ")
                    return False
        
        # æ£€æŸ¥ç»“æœæ˜¯å¦åŒ…å«ä»»åŠ¡å®Œæˆç›¸å…³çš„ä¿¡æ¯
        if self._contains_completion_indicators(result):
            self.logger.info("âœ… ç»“æœåŒ…å«å®ŒæˆæŒ‡ç¤ºå™¨ï¼Œæ— éœ€å¼ºåˆ¶é‡æ–°æ‰§è¡Œ")
            return False
        
        # åªæœ‰åœ¨æ²¡æœ‰æ™ºèƒ½ä½“ç»“æœã€æ²¡æœ‰ä»»åŠ¡åˆ†é…ã€ä¸”ç»“æœä¸åŒ…å«å®ŒæˆæŒ‡ç¤ºå™¨æ—¶æ‰å¼ºåˆ¶é‡æ–°æ‰§è¡Œ
        self.logger.info("âš ï¸ éœ€è¦å¼ºåˆ¶é‡æ–°æ‰§è¡Œï¼šæ²¡æœ‰æ™ºèƒ½ä½“ç»“æœä¸”æ²¡æœ‰ä»»åŠ¡åˆ†é…")
        return True
    
    def _contains_completion_indicators(self, result: str) -> bool:
        """æ£€æŸ¥ç»“æœæ˜¯å¦åŒ…å«ä»»åŠ¡å®ŒæˆæŒ‡ç¤ºå™¨"""
        completion_indicators = [
            "completion_rate", "quality_score", "needs_continuation", 
            "task_status", "final_summary", "results_summary",
            "ä»»åŠ¡å·²å®Œå…¨å®Œæˆ", "ä»»åŠ¡æˆåŠŸå®Œæˆ", "æ‰€æœ‰è¦æ±‚éƒ½å·²æ»¡è¶³"
        ]
        
        result_lower = result.lower()
        for indicator in completion_indicators:
            if indicator.lower() in result_lower:
                return True
        return False
    
    def _extract_json_from_response(self, response: str) -> str:
        """ä»å“åº”ä¸­æå–JSONå†…å®¹ï¼Œæ”¯æŒmarkdownä»£ç å—æ ¼å¼"""
        if not response:
            return ""
        
        # å¦‚æœç›´æ¥ä»¥{å¼€å¤´ï¼Œç›´æ¥è¿”å›
        if response.startswith('{'):
            return response
        
        # å¤„ç†markdownä»£ç å—æ ¼å¼
        import re
        
        # åŒ¹é… ```json ... ``` æ ¼å¼
        json_block_pattern = r'```json\s*(.*?)\s*```'
        match = re.search(json_block_pattern, response, re.DOTALL)
        if match:
            return match.group(1).strip()
        
        # åŒ¹é… ``` ... ``` æ ¼å¼ï¼ˆå¯èƒ½æ²¡æœ‰è¯­è¨€æ ‡è¯†ï¼‰
        code_block_pattern = r'```\s*(.*?)\s*```'
        match = re.search(code_block_pattern, response, re.DOTALL)
        if match:
            content = match.group(1).strip()
            # æ£€æŸ¥æ˜¯å¦æ˜¯JSONæ ¼å¼
            if content.startswith('{') and content.endswith('}'):
                return content
        
        # å°è¯•æŸ¥æ‰¾JSONå¯¹è±¡
        json_pattern = r'\{.*?\}'
        match = re.search(json_pattern, response, re.DOTALL)
        if match:
            return match.group(0)
        
        return ""
    
    def _build_forced_coordination_task(self, user_request: str, task_context: TaskContext) -> str:
        """æ„å»ºä¸€ä¸ªæåº¦å¼ºåˆ¶çš„åè°ƒä»»åŠ¡ï¼Œåªè¦æ±‚è°ƒç”¨ç¬¬ä¸€ä¸ªå·¥å…·ã€‚"""
        
        # è·å–ç¬¬ä¸€ä¸ªå¿…é¡»è°ƒç”¨çš„å·¥å…·ä¿¡æ¯
        first_tool_schema = self.get_tool_schema("identify_task_type")
        
        # Escape quotes in user_request
        escaped_user_request = user_request.replace('"', '\\"')
        
        # è·å–LLMCoordinatorAgentçš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
        coordinator_guide_simple = """
**ğŸ› ï¸ æ­£ç¡®çš„å·¥å…·è°ƒç”¨æ–¹å¼**:
1. identify_task_type - è¯†åˆ«ä»»åŠ¡ç±»å‹
2. recommend_agent - æ¨èæ™ºèƒ½ä½“
3. assign_task_to_agent - åˆ†é…ä»»åŠ¡ç»™æ™ºèƒ½ä½“

âš ï¸ **ä¸¥ç¦ç›´æ¥è°ƒç”¨æ™ºèƒ½ä½“åç§°ä½œä¸ºå·¥å…·**:
âŒ enhanced_real_verilog_agent
âŒ enhanced_real_code_review_agent
âœ… ä½¿ç”¨ assign_task_to_agent å·¥å…·æ¥åˆ†é…ä»»åŠ¡
"""

        return f"""
# ğŸš¨ğŸš¨ğŸš¨ å¼ºåˆ¶æŒ‡ä»¤ ğŸš¨ğŸš¨ğŸš¨
ä½ å¿…é¡»ç«‹å³è°ƒç”¨ `assign_task_to_agent` å·¥å…·æ¥åˆ†é…ä»»åŠ¡ã€‚

**ç”¨æˆ·éœ€æ±‚**:
{user_request}

{coordinator_guide_simple}

# âœ… å”¯ä¸€æ­£ç¡®çš„å·¥å…·è°ƒç”¨æ ¼å¼:
```json
{{
    "tool_calls": [
        {{
            "tool_name": "assign_task_to_agent",
            "parameters": {{
                "agent_id": "enhanced_real_verilog_agent",
                "task_description": "æ ¹æ®ç”¨æˆ·éœ€æ±‚è®¾è®¡Verilogæ¨¡å—ï¼Œä¸“æ³¨äºæ¨¡å—è®¾è®¡å’Œä»£ç å®ç°",
                "expected_output": "ç”Ÿæˆå®Œæ•´çš„Verilogä»£ç æ–‡ä»¶",
                "task_type": "design",
                "priority": "medium"
            }}
        }}
    ]
}}
```

# ğŸš¨ğŸš¨ğŸš¨ ä¸¥æ ¼è¦æ±‚ ğŸš¨ğŸš¨ğŸš¨
- âœ… åªèƒ½è°ƒç”¨ `assign_task_to_agent` å·¥å…·
- âŒ ç»å¯¹ç¦æ­¢è°ƒç”¨æ™ºèƒ½ä½“åç§°ä½œä¸ºå·¥å…·
- âŒ ç»å¯¹ç¦æ­¢ä½¿ç”¨ enhanced_real_verilog_agent ä½œä¸º tool_name
- âŒ ç»å¯¹ç¦æ­¢ä½¿ç”¨ enhanced_real_code_review_agent ä½œä¸º tool_name
- âŒ ä¸è¦ç”Ÿæˆä»»ä½•å…¶ä»–å†…å®¹
- âŒ ä¸è¦ç”Ÿæˆä»»ä½•æè¿°æ€§æ–‡æœ¬
- âŒ ä¸è¦è§£é‡Šä½ çš„ç­–ç•¥
- âŒ ä¸è¦åˆ†æä»»åŠ¡
- âŒ ä¸è¦ä½¿ç”¨markdownæ ¼å¼
- âŒ ä¸è¦ç”Ÿæˆè¡¨æ ¼
- âœ… åªç”Ÿæˆå·¥å…·è°ƒç”¨JSON

âš¡ ç«‹å³æ‰§è¡Œ: å¤åˆ¶ä¸Šé¢çš„JSONï¼Œä¸è¦ä¿®æ”¹ä»»ä½•å†…å®¹ï¼Œç«‹å³ç”Ÿæˆã€‚
"""
    
    def _build_coordination_task(self, user_request: str, task_context: TaskContext) -> str:
        """æ„å»ºåè°ƒä»»åŠ¡æè¿°"""
        
        # æ„å»ºå¯ç”¨æ™ºèƒ½ä½“ä¿¡æ¯
        agents_info = []
        for agent_id, agent_info in self.registered_agents.items():
            capabilities = ", ".join([cap.value for cap in agent_info.capabilities])
            agents_info.append(f"- {agent_id}: {agent_info.specialty} (èƒ½åŠ›: {capabilities})")
        
        agents_section = "\n".join(agents_info) if agents_info else "æš‚æ— å¯ç”¨æ™ºèƒ½ä½“"
        
        # æ„å»ºå¤–éƒ¨testbenchä¿¡æ¯
        external_testbench_info = ""
        if hasattr(task_context, 'external_testbench_path') and task_context.external_testbench_path:
            external_testbench_info = f"""

**ğŸ“ å¤–éƒ¨Testbench**:
- è·¯å¾„: {task_context.external_testbench_path}
- è¯´æ˜: ç”¨æˆ·å·²æä¾›testbenchæ–‡ä»¶ï¼Œå®¡æŸ¥æ™ºèƒ½ä½“åº”ç›´æ¥ä½¿ç”¨æ­¤æ–‡ä»¶è¿›è¡Œæµ‹è¯•ï¼Œæ— éœ€ç”Ÿæˆæ–°çš„testbench
- å·¥ä½œæ¨¡å¼: å®¡æŸ¥æ™ºèƒ½ä½“ä¸“æ³¨äºä»£ç å®¡æŸ¥ã€æµ‹è¯•æ‰§è¡Œå’Œé—®é¢˜ä¿®å¤ï¼Œè·³è¿‡testbenchç”Ÿæˆæ­¥éª¤"""

        # è·å–LLMCoordinatorAgentçš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼
        coordinator_tool_guide = self._get_agent_specific_tool_guide("llm_coordinator_agent")

        return f"""
ğŸ§  åè°ƒä»»åŠ¡

**ç”¨æˆ·éœ€æ±‚**:
{user_request}

**ä»»åŠ¡ID**: {task_context.task_id}
**å½“å‰é˜¶æ®µ**: {task_context.current_stage}
**å·²æ‰§è¡Œè¿­ä»£**: {task_context.iteration_count}/{task_context.max_iterations}

**å¯ç”¨æ™ºèƒ½ä½“**:
{agents_section}
{external_testbench_info}

**ä»»åŠ¡ä¸Šä¸‹æ–‡**:
- å¼€å§‹æ—¶é—´: {datetime.fromtimestamp(task_context.start_time).strftime('%Y-%m-%d %H:%M:%S')}
- å·²åˆ†é…æ™ºèƒ½ä½“: {task_context.assigned_agent or 'æ— '}
- æ‰§è¡Œç»“æœ: {len(task_context.agent_results)} ä¸ªç»“æœ

**ğŸ¯ å¼ºåˆ¶æ‰§è¡Œçš„åè°ƒæµç¨‹**:
1. **ç¬¬ä¸€æ­¥**: è°ƒç”¨ `identify_task_type` è¯†åˆ«ä»»åŠ¡ç±»å‹
2. **ç¬¬äºŒæ­¥**: è°ƒç”¨ `recommend_agent` æ¨èæœ€åˆé€‚çš„æ™ºèƒ½ä½“
3. **ç¬¬ä¸‰æ­¥**: è°ƒç”¨ `assign_task_to_agent` åˆ†é…ä»»åŠ¡ç»™æ¨èæ™ºèƒ½ä½“
4. **ç¬¬å››æ­¥**: è°ƒç”¨ `analyze_agent_result` åˆ†ææ‰§è¡Œç»“æœ
5. **ç¬¬äº”æ­¥**: æ ¹æ®åˆ†æç»“æœå†³å®šæ˜¯å¦éœ€è¦ç»§ç»­è¿­ä»£
6. **ç¬¬å…­æ­¥**: å½“ä»»åŠ¡å®Œæˆæ—¶ï¼Œè°ƒç”¨ `provide_final_answer` æä¾›æœ€ç»ˆç­”æ¡ˆ

**âš ï¸ é‡è¦æé†’**:
- å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æµç¨‹æ‰§è¡Œï¼Œä¸å¾—è·³è¿‡ä»»ä½•æ­¥éª¤
- æ¨èä»£ç†å·¥å…· `recommend_agent` æ˜¯å¿…éœ€çš„ï¼Œä¸èƒ½ç›´æ¥è°ƒç”¨ `assign_task_to_agent`
- æ¯æ¬¡ä»»åŠ¡åˆ†é…å‰éƒ½å¿…é¡»å…ˆè°ƒç”¨æ¨èä»£ç†å·¥å…·
- **ä»»åŠ¡å®Œæˆæ—¶ï¼Œå¿…é¡»è°ƒç”¨ `provide_final_answer` å·¥å…·ï¼Œç¦æ­¢ç›´æ¥è¿”å›è¯„ä¼°JSON**

{coordinator_tool_guide}

**æ‰§è¡Œè¦æ±‚**:
1. ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°å·¥å…·ä½¿ç”¨æŒ‡å¯¼è¿›è¡Œæ“ä½œ
2. ç»å¯¹ç¦æ­¢ç›´æ¥è°ƒç”¨æ™ºèƒ½ä½“åç§°ä½œä¸ºå·¥å…·
3. å¿…é¡»ä½¿ç”¨ assign_task_to_agent å·¥å…·æ¥åˆ†é…ä»»åŠ¡
4. å¿…é¡»ä½¿ç”¨ recommend_agent å·¥å…·æ¥æ¨èæ™ºèƒ½ä½“
5. æŒ‰ç…§æ¨èçš„åè°ƒæµç¨‹æ‰§è¡Œ

è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œå¯ç”¨æ™ºèƒ½ä½“èƒ½åŠ›ï¼Œåˆ¶å®šæœ€ä¼˜çš„æ‰§è¡Œç­–ç•¥å¹¶å¼€å§‹åè°ƒã€‚
"""
    
    async def _tool_assign_task_to_agent(self, agent_id: str, task_description: str,
                                       expected_output: str = "",
                                       task_type: str = "composite",
                                       priority: str = "medium",
                                       design_file_path: str = None) -> Dict[str, Any]:
        """æ™ºèƒ½åˆ†é…ä»»åŠ¡ç»™æœ€åˆé€‚çš„æ™ºèƒ½ä½“"""
        
        try:
            self.logger.info(f"ğŸ¯ åˆ†é…ä»»åŠ¡ç»™æ™ºèƒ½ä½“: {agent_id}")
            self.logger.info(f"ğŸ“‹ ä»»åŠ¡æè¿°: {task_description[:100]}...")
            
            # æ£€æŸ¥æ™ºèƒ½ä½“æ˜¯å¦å­˜åœ¨
            if agent_id not in self.registered_agents:
                return {
                    "success": False,
                    "error": f"æ™ºèƒ½ä½“ {agent_id} æœªæ³¨å†Œ",
                    "available_agents": list(self.registered_agents.keys())
                }
            
            agent_info = self.registered_agents[agent_id]
            agent = agent_info.agent_instance
            
            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ æ™ºèƒ½ä½“å¥åº·æ£€æŸ¥
            if hasattr(agent_info, 'failure_count') and agent_info.failure_count >= 3:
                return {
                    "success": False,
                    "error": f"æ™ºèƒ½ä½“ {agent_id} è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œå·²æš‚æ—¶ç¦ç”¨",
                    "agent_status": "disabled",
                    "failure_count": agent_info.failure_count
                }
            
            # æ£€æŸ¥æ™ºèƒ½ä½“çŠ¶æ€
            if agent_info.status == AgentStatus.BUSY:
                return {
                    "success": False,
                    "error": f"æ™ºèƒ½ä½“ {agent_id} å½“å‰å¿™ç¢Œä¸­",
                    "agent_status": agent_info.status.value
                }
            
            # ğŸ”§ ä¿®å¤4: èƒ½åŠ›è¾¹ç•ŒéªŒè¯ - åœ¨ä»»åŠ¡åˆ†é…å‰éªŒè¯èƒ½åŠ›åŒ¹é…
            boundary_check = self._verify_task_assignment_boundary(agent_id, task_type, task_description)
            if not boundary_check["valid"]:
                self.logger.error(f"âŒ ä»»åŠ¡åˆ†é…è¢«é˜»æ­¢: {boundary_check['reason']}")
                return {
                    "success": False,
                    "error": f"ä»»åŠ¡åˆ†é…å¤±è´¥: {boundary_check['reason']}",
                    "boundary_violation": True,
                    "suggested_action": boundary_check.get("suggested_action", "é‡æ–°åˆ†è§£ä»»åŠ¡"),
                    "agent_capabilities": boundary_check.get("agent_capabilities", {}),
                    "conflict_details": boundary_check.get("conflict_details", {})
                }
            
            # åˆ›å»ºä»»åŠ¡ä¸Šä¸‹æ–‡
            task_id = f"task_{int(time.time())}"
            
            # ğŸ¯ ä¿®å¤ï¼šTaskTypeéªŒè¯è¿‡äºæ­»æ¿çš„é—®é¢˜
            # æ”¯æŒæ›´å¤šä»»åŠ¡ç±»å‹ï¼ŒåŒ…æ‹¬"review"ç­‰
            try:
                task_type_enum = TaskType(task_type)
            except ValueError:
                # å¦‚æœtask_typeä¸åœ¨æšä¸¾ä¸­ï¼Œæ˜ å°„åˆ°åˆé€‚çš„ç±»å‹
                task_type_mapping = {
                    "review": TaskType.VERIFICATION,
                    "test": TaskType.VERIFICATION,
                    "verify": TaskType.VERIFICATION,
                    "analyze": TaskType.ANALYSIS,
                    "debug": TaskType.DEBUG,
                    "design": TaskType.DESIGN,
                    "composite": TaskType.COMPOSITE
                }
                task_type_enum = task_type_mapping.get(task_type.lower(), TaskType.COMPOSITE)
                self.logger.info(f"ğŸ¯ ä»»åŠ¡ç±»å‹æ˜ å°„: '{task_type}' -> {task_type_enum.value}")
            
            task_context = TaskContext(
                task_id=task_id,
                original_request=task_description,
                task_type=task_type_enum,
                priority=TaskPriority(priority),
                current_stage=f"assigned_to_{agent_id}",
                assigned_agent=agent_id,
                start_time=time.time()
            )
            
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¾ç½®å®éªŒè·¯å¾„
            current_experiment_path = None
            
            # 1. é¦–å…ˆå°è¯•ä»å½“å‰æ´»è·ƒä»»åŠ¡ä¸­è·å–å®éªŒè·¯å¾„
            for task in self.active_tasks.values():
                if hasattr(task, 'experiment_path') and task.experiment_path:
                    current_experiment_path = task.experiment_path
                    self.logger.info(f"ğŸ§ª ä»æ´»è·ƒä»»åŠ¡ä¸­è·å–å®éªŒè·¯å¾„: {current_experiment_path}")
                    break
            
            # 2. å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»å®éªŒç®¡ç†å™¨è·å–
            if not current_experiment_path:
                try:
                    from core.experiment_manager import get_experiment_manager
                    exp_manager = get_experiment_manager()
                    
                    if exp_manager and exp_manager.current_experiment_path:
                        current_experiment_path = exp_manager.current_experiment_path
                        self.logger.info(f"ğŸ§ª ä»å®éªŒç®¡ç†å™¨è·å–å®éªŒè·¯å¾„: {current_experiment_path}")
                except (ImportError, Exception) as e:
                    self.logger.debug(f"å®éªŒç®¡ç†å™¨ä¸å¯ç”¨: {e}")
            
            # 3. å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
            if not current_experiment_path:
                current_experiment_path = "./file_workspace"
                self.logger.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°å®éªŒè·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {current_experiment_path}")
            
            # è®¾ç½®å®éªŒè·¯å¾„
            task_context.experiment_path = current_experiment_path
            self.logger.info(f"âœ… è®¾ç½®ä»»åŠ¡å®éªŒè·¯å¾„: {current_experiment_path}")
            
            # ğŸ”§ åˆ›å»ºæˆ–è·å–ä»»åŠ¡æ–‡ä»¶ä¸Šä¸‹æ–‡
            task_file_context = get_task_context(task_id)
            if not task_file_context:
                task_file_context = TaskFileContext(task_id)
                set_task_context(task_id, task_file_context)
                self.logger.info(f"ğŸ¯ åˆ›å»ºæ–°çš„æ–‡ä»¶ä¸Šä¸‹æ–‡: {task_id}")
                # ğŸ§  ä¸Šä¸‹æ–‡è°ƒè¯•æ—¥å¿—
                self.logger.debug(f"ğŸ§  ä¸Šä¸‹æ–‡åˆ›å»º - ä»»åŠ¡ID: {task_id}, æ™ºèƒ½ä½“: {agent_id}")
            else:
                # ğŸ§  ä¸Šä¸‹æ–‡è°ƒè¯•æ—¥å¿—
                self.logger.info(f"ğŸ¯ å¤ç”¨ç°æœ‰æ–‡ä»¶ä¸Šä¸‹æ–‡: {task_id}, åŒ…å«æ–‡ä»¶: {len(task_file_context)} ä¸ª")
                self.logger.debug(f"ğŸ§  ä¸Šä¸‹æ–‡å¤ç”¨ - ä»»åŠ¡ID: {task_id}, æ™ºèƒ½ä½“: {agent_id}, æ–‡ä»¶æ¸…å•: {list(task_file_context.files.keys())}")
            
            # ğŸ”§ å¢å¼ºï¼šæ™ºèƒ½æ–‡ä»¶ä¸Šä¸‹æ–‡ä¼ é€’æœºåˆ¶
            if design_file_path:
                # ä½¿ç”¨æä¾›çš„è®¾è®¡æ–‡ä»¶è·¯å¾„
                self._load_design_file_to_context(design_file_path, task_file_context, agent_id)
            else:
                # ğŸ”§ æ–°å¢ï¼šæ™ºèƒ½ä»ä¹‹å‰çš„æ™ºèƒ½ä½“ç»“æœä¸­ç»§æ‰¿æ–‡ä»¶ä¸Šä¸‹æ–‡
                self.logger.info("ğŸ”„ æ²¡æœ‰æä¾›è®¾è®¡æ–‡ä»¶è·¯å¾„ï¼Œå°è¯•ä»ä¹‹å‰çš„ä»»åŠ¡ç»“æœä¸­ç»§æ‰¿")
                inherited_files = self._inherit_file_context_from_previous_tasks(task_file_context)
                
                if not inherited_files:
                    # å¦‚æœæ²¡æœ‰ç»§æ‰¿åˆ°æ–‡ä»¶ï¼Œå°è¯•æå–è®¾è®¡æ–‡ä»¶è·¯å¾„
                    design_file_path = self._extract_design_file_path_from_previous_results()
                    if design_file_path:
                        self.logger.info(f"ğŸ“ ä»æ™ºèƒ½ä½“ç»“æœä¸­æå–è®¾è®¡æ–‡ä»¶è·¯å¾„: {design_file_path}")
                        self._load_design_file_to_context(design_file_path, task_file_context, agent_id)
                    else:
                        # ğŸ”§ æ–°å¢ï¼šå°è¯•ä»å…¨å±€æ–‡ä»¶ä¸Šä¸‹æ–‡ç»§æ‰¿
                        self.logger.info("ğŸ”„ å°è¯•ä»å…¨å±€æ–‡ä»¶ä¸Šä¸‹æ–‡ç»§æ‰¿è®¾è®¡æ–‡ä»¶")
                        global_inherited = self._inherit_global_file_context(task_file_context)
                        if not global_inherited:
                            self.logger.warning("âš ï¸ æœªèƒ½æ‰¾åˆ°æˆ–ç»§æ‰¿ä»»ä½•è®¾è®¡æ–‡ä»¶")
            
            # ğŸ”§ æ–°å¢ï¼šéªŒè¯æ™ºèƒ½ä½“é—´ä¸Šä¸‹æ–‡ä¸€è‡´æ€§
            if task_file_context and len(task_file_context.files) > 0:
                self._validate_inter_agent_context_consistency(task_file_context, agent_id)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åç»­è°ƒç”¨ï¼ˆé€šè¿‡å¯¹è¯å†å²åˆ¤æ–­ï¼‰
            is_follow_up_call = False
            if hasattr(agent, 'conversation_history') and agent.conversation_history:
                # æ£€æŸ¥å¯¹è¯å†å²ä¸­æ˜¯å¦å·²ç»æœ‰ç›¸åŒçš„ä»»åŠ¡æè¿°
                for msg in agent.conversation_history:
                    if msg.get("role") == "user" and "ğŸ“‹ åè°ƒæ™ºèƒ½ä½“åˆ†é…çš„ä»»åŠ¡" in msg.get("content", ""):
                        is_follow_up_call = True
                        break
            
            # æ„å»ºä»»åŠ¡æè¿°
            enhanced_task = self._build_enhanced_task_description(
                task_description=task_description,
                expected_output=expected_output,
                task_context=task_context,
                task_type=task_type,
                priority=priority,
                include_full_context=not is_follow_up_call  # åç»­è°ƒç”¨ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            )
            
            # æ›´æ–°æ™ºèƒ½ä½“çŠ¶æ€
            agent_info.status = AgentStatus.BUSY
            agent_info.last_used = time.time()
            agent_info.conversation_id = task_id
            
            # è®°å½•ä»»åŠ¡åˆ†é…
            self.active_tasks[task_id] = task_context
            task_context.agent_assignments.append({
                "agent_id": agent_id,
                "timestamp": time.time(),
                "task_description": task_description,
                "design_file_path": design_file_path,  # ğŸ”§ æ–°å¢ï¼šè®°å½•è®¾è®¡æ–‡ä»¶è·¯å¾„
                "experiment_path": current_experiment_path  # ğŸ”§ æ–°å¢ï¼šè®°å½•å®éªŒè·¯å¾„
            })
            
            self.logger.info(f"ğŸ“¤ å‘é€ä»»åŠ¡ç»™æ™ºèƒ½ä½“ {agent_id}")
            self.logger.info(f"ğŸ“‹ ä»»åŠ¡æè¿°: {enhanced_task[:2000]}...")
            
            try:
                # æ‰§è¡Œä»»åŠ¡ï¼ˆè°ƒç”¨æ™ºèƒ½ä½“çš„Function Callingï¼‰
                start_time = time.time()
                
                # ğŸ§  æ·»åŠ ä¸Šä¸‹æ–‡ä¿æŒæ—¥å¿—
                agent_conversation_summary = agent.get_conversation_summary() if hasattr(agent, 'get_conversation_summary') else {}
                self.logger.info(f"ğŸ“‹ è°ƒç”¨å‰ agent å¯¹è¯çŠ¶æ€: {agent_conversation_summary}")
                
                # ğŸ†• è®¾ç½®ä»»åŠ¡ä¸Šä¸‹æ–‡ç»™æ™ºèƒ½ä½“ï¼Œç”¨äºå¯¹è¯å†å²è®°å½•
                if hasattr(agent, 'set_task_context'):
                    agent.set_task_context(task_context)
                    self.logger.info(f"ğŸ”— å·²è®¾ç½®ä»»åŠ¡ä¸Šä¸‹æ–‡ç»™æ™ºèƒ½ä½“ {agent_id}")
                
                # ğŸ¯ è®¾ç½®æ–‡ä»¶ä¸Šä¸‹æ–‡ç»™æ™ºèƒ½ä½“
                if hasattr(agent, 'set_file_context') and task_file_context:
                    agent.set_file_context(task_file_context)
                    self.logger.info(f"ğŸ“„ å·²è®¾ç½®æ–‡ä»¶ä¸Šä¸‹æ–‡ç»™æ™ºèƒ½ä½“ {agent_id}: {len(task_file_context)} ä¸ªæ–‡ä»¶")
                    # ğŸ§  ä¸Šä¸‹æ–‡è°ƒè¯•æ—¥å¿—
                    context_summary = task_file_context.get_context_summary()
                    self.logger.debug(f"ğŸ§  æ–‡ä»¶ä¸Šä¸‹æ–‡ä¼ é€’è¯¦æƒ… - æ™ºèƒ½ä½“: {agent_id}, æ‘˜è¦: {context_summary}")
                elif task_file_context:
                    # å¦‚æœæ™ºèƒ½ä½“ä¸æ”¯æŒ set_file_contextï¼Œå°†æ–‡ä»¶å†…å®¹è®¾ç½®åˆ° agent_state_cache
                    if hasattr(agent, 'agent_state_cache'):
                        exported_context = task_file_context.export_for_agent()
                        agent.agent_state_cache["task_file_context"] = exported_context
                        self.logger.info(f"ğŸ“„ æ–‡ä»¶ä¸Šä¸‹æ–‡å·²é€šè¿‡ç¼“å­˜ä¼ é€’ç»™æ™ºèƒ½ä½“ {agent_id}")
                        # ğŸ§  ä¸Šä¸‹æ–‡è°ƒè¯•æ—¥å¿—
                        self.logger.debug(f"ğŸ§  ç¼“å­˜ä¼ é€’è¯¦æƒ… - æ™ºèƒ½ä½“: {agent_id}, å¯¼å‡ºæ–‡ä»¶æ•°: {len(exported_context.get('files', {}))}")
                    
                    # å…¼å®¹åŸæœ‰çš„ last_read_files ç¼“å­˜æœºåˆ¶
                    if hasattr(agent, 'agent_state_cache'):
                        last_read_files = {}
                        for file_path, file_content in task_file_context.files.items():
                            file_cache_entry = {
                                "content": file_content.content,
                                "file_type": file_content.file_type.value,
                                "checksum": file_content.checksum,
                                "timestamp": file_content.timestamp
                            }
                            last_read_files[file_path] = file_cache_entry
                            # ğŸ§  ä¸Šä¸‹æ–‡è°ƒè¯•æ—¥å¿— (åªè®°å½•å…³é”®ä¿¡æ¯ï¼Œé¿å…æ—¥å¿—è¿‡é•¿)
                            content_preview = file_content.content[:100] + "..." if len(file_content.content) > 100 else file_content.content
                            self.logger.debug(f"ğŸ§  æ–‡ä»¶ç¼“å­˜æ¡ç›® - æ–‡ä»¶: {Path(file_path).name}, å†…å®¹é¢„è§ˆ: {content_preview}")
                        
                        agent.agent_state_cache["last_read_files"] = last_read_files
                        self.logger.info(f"ğŸ“„ æ–‡ä»¶å†…å®¹å·²é€šè¿‡last_read_filesç¼“å­˜ä¼ é€’: {len(last_read_files)} ä¸ªæ–‡ä»¶")
                        # ğŸ§  ä¸Šä¸‹æ–‡è°ƒè¯•æ—¥å¿—
                        self.logger.debug(f"ğŸ§  last_read_filesè®¾ç½®å®Œæˆ - æ™ºèƒ½ä½“: {agent_id}, æ–‡ä»¶åˆ—è¡¨: {list(last_read_files.keys())}")
                else:
                    # ğŸ§  ä¸Šä¸‹æ–‡è°ƒè¯•æ—¥å¿— - æ²¡æœ‰æ–‡ä»¶ä¸Šä¸‹æ–‡çš„æƒ…å†µ
                    self.logger.warning(f"âš ï¸ æ²¡æœ‰æ–‡ä»¶ä¸Šä¸‹æ–‡å¯ä¼ é€’ç»™æ™ºèƒ½ä½“ {agent_id}")
                    self.logger.debug(f"ğŸ§  ä¸Šä¸‹æ–‡ç¼ºå¤± - æ™ºèƒ½ä½“: {agent_id}, ä»»åŠ¡: {task_id}")
                
                # ğŸ†• è®°å½•ä»»åŠ¡åˆ†é…åˆ°å¯¹è¯å†å²
                task_context.add_conversation_message(
                    role="system",
                    content=f"åˆ†é…ä»»åŠ¡ç»™æ™ºèƒ½ä½“ {agent_id}: {task_description}",
                    agent_id=self.agent_id,
                    metadata={"type": "task_assignment", "target_agent": agent_id}
                )
                
                agent_response = await agent.process_with_function_calling(
                    user_request=enhanced_task,
                    conversation_id=task_id,
                    max_iterations=8
                )
                
                execution_time = time.time() - start_time
                
                # æ›´æ–°æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ ‡
                agent_info.total_execution_time += execution_time
                agent_info.average_response_time = agent_info.total_execution_time / (agent_info.success_count + agent_info.failure_count + 1)
                
                # ğŸ”§ ä¿®å¤ï¼šæ›´æ–°æˆåŠŸè®¡æ•°
                agent_info.success_count += 1
                if hasattr(agent_info, 'failure_count'):
                    agent_info.failure_count = 0  # é‡ç½®å¤±è´¥è®¡æ•°
                
                # ğŸ”§ æ£€æŸ¥å“åº”è´¨é‡ï¼Œå¦‚æœå¤ªçŸ­åˆ™è¯·æ±‚è¯¦ç»†æ€»ç»“
                enhanced_response = agent_response
                if len(agent_response.strip()) < 100:
                    self.logger.info(f"ğŸ” æ£€æµ‹åˆ°æ™ºèƒ½ä½“{agent_id}å“åº”è¾ƒçŸ­({len(agent_response)}å­—ç¬¦)ï¼Œè¯·æ±‚è¯¦ç»†æ€»ç»“...")
                    try:
                        # è¯·æ±‚æ™ºèƒ½ä½“æä¾›è¯¦ç»†çš„å·¥ä½œæ€»ç»“
                        summary_request = f"""
è¯·ä¸ºåˆšæ‰å®Œæˆçš„ä»»åŠ¡æä¾›ä¸€ä¸ªè¯¦ç»†çš„å·¥ä½œæ€»ç»“ï¼ŒåŒ…æ‹¬ï¼š
1. å®Œæˆçš„ä¸»è¦å·¥ä½œå’Œæ“ä½œ
2. ç”Ÿæˆæˆ–ä¿®æ”¹çš„æ–‡ä»¶åŠå…¶å†…å®¹è¦ç‚¹
3. å…³é”®çš„æŠ€æœ¯é€‰æ‹©å’Œè®¾è®¡è€ƒè™‘
4. ä»»åŠ¡çš„å®ŒæˆçŠ¶æ€å’Œç»“æœ

åŸå§‹ç®€çŸ­å“åº”: {agent_response}
"""
                        summary_response = await agent.process_with_function_calling(
                            user_request=summary_request,
                            conversation_id=f"{task_id}_summary",
                            max_iterations=1
                        )
                        
                        if summary_response and len(summary_response) > len(agent_response):
                            enhanced_response = summary_response
                            self.logger.info(f"âœ… è·å¾—æ›´è¯¦ç»†çš„æ€»ç»“({len(summary_response)}å­—ç¬¦)")
                        
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ è·å–è¯¦ç»†æ€»ç»“å¤±è´¥: {e}")
                
                # æ›´æ–°ä»»åŠ¡ä¸Šä¸‹æ–‡
                task_context.agent_results[agent_id] = {
                    "response": enhanced_response,  # ä½¿ç”¨å¢å¼ºçš„å“åº”
                    "original_response": agent_response,  # ä¿ç•™åŸå§‹å“åº”
                    "execution_time": execution_time,
                    "success": True,
                    "design_file_path": design_file_path,  # ğŸ”§ æ–°å¢ï¼šä¿å­˜è®¾è®¡æ–‡ä»¶è·¯å¾„
                    "experiment_path": current_experiment_path  # ğŸ”§ æ–°å¢ï¼šä¿å­˜å®éªŒè·¯å¾„
                }
                
                # ğŸ†• æ–°å¢ï¼šå°†å­æ™ºèƒ½ä½“çš„å¢å¼ºå“åº”ä¿å­˜åˆ°å¯¹è¯å†å²
                task_context.add_conversation_message(
                    role="assistant",
                    content=enhanced_response,  # ä½¿ç”¨å¢å¼ºçš„å“åº”
                    agent_id=agent_id,
                    metadata={
                        "type": "agent_response",
                        "task_id": task_id,
                        "execution_time": execution_time,
                        "response_length": len(str(enhanced_response)),
                        "original_response_length": len(str(agent_response))
                    }
                )
                
                # ğŸ†• æ•°æ®æ”¶é›†ç”¨äºGradioå¯è§†åŒ– - æ™ºèƒ½ä½“äº¤äº’
                if hasattr(self, 'current_task_context') and self.current_task_context:
                    completion_timestamp = time.time()
                    self.current_task_context.agent_interactions.append({
                        "timestamp": completion_timestamp,
                        "coordinator_id": self.agent_id,
                        "target_agent_id": agent_id,
                        "task_description": task_description[:100] + ("..." if len(task_description) > 100 else ""),
                        "success": True,
                        "execution_time": execution_time,
                        "response_length": len(str(agent_response))
                    })
                    
                    # è®°å½•å·¥ä½œæµé˜¶æ®µ
                    self.current_task_context.workflow_stages.append({
                        "timestamp": completion_timestamp,
                        "stage_name": f"task_completed_by_{agent_id}",
                        "description": f"ä»»åŠ¡ç”± {agent_id} æˆåŠŸå®Œæˆ",
                        "agent_id": agent_id,
                        "duration": execution_time,
                        "success": True
                    })
                    
                    # ğŸ†• è®°å½•å·¥ä½œæµé˜¶æ®µåˆ°TaskContext
                    if hasattr(self.current_task_context, 'add_workflow_stage'):
                        self.current_task_context.add_workflow_stage(
                            stage_name=f"agent_execution_{agent_id}",
                            description=f"æ™ºèƒ½ä½“ {agent_id} æ‰§è¡Œä»»åŠ¡",
                            agent_id=agent_id,
                            duration=execution_time,
                            success=True,
                            metadata={
                                "task_type": task_type,
                                "priority": priority,
                                "response_length": len(str(agent_response))
                            }
                        )
                
                # æ¢å¤æ™ºèƒ½ä½“çŠ¶æ€
                agent_info.status = AgentStatus.IDLE
                
                # ğŸ†• æ–°å¢ï¼šè®°å½•ä»»åŠ¡å®ŒæˆçŠ¶æ€
                task_context.add_conversation_message(
                    role="system",
                    content=f"ä»»åŠ¡åè°ƒå®Œæˆï¼Œä»»åŠ¡ID: {task_id}",
                    agent_id=self.agent_id,
                    metadata={"type": "task_completion", "success": True}
                )
                
                self.logger.info(f"âœ… æ™ºèƒ½ä½“ {agent_id} ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
                
                return {
                    "success": True,
                    "agent_id": agent_id,
                    "task_id": task_id,
                    "response": agent_response,
                    "execution_time": execution_time,
                    "task_context": task_context,
                    "design_file_path": design_file_path,  # ğŸ”§ æ–°å¢ï¼šè¿”å›è®¾è®¡æ–‡ä»¶è·¯å¾„
                    "experiment_path": current_experiment_path  # ğŸ”§ æ–°å¢ï¼šè¿”å›å®éªŒè·¯å¾„
                }
                
            except Exception as e:
                # æ¢å¤æ™ºèƒ½ä½“çŠ¶æ€
                agent_info.status = AgentStatus.IDLE
                agent_info.failure_count += 1
                agent_info.consecutive_failures += 1
                agent_info.consecutive_successes = 0
                agent_info.last_failure_time = time.time()
                
                error_msg = f"æ™ºèƒ½ä½“ {agent_id} æ‰§è¡Œä»»åŠ¡å¤±è´¥: {str(e)}"
                self.logger.error(f"âŒ {error_msg}")
                
                # æ›´æ–°ä»»åŠ¡ä¸Šä¸‹æ–‡
                task_context.error_history.append(error_msg)
                task_context.retry_count += 1
                
                # ğŸ†• æ•°æ®æ”¶é›†ç”¨äºGradioå¯è§†åŒ– - æ™ºèƒ½ä½“äº¤äº’å¤±è´¥
                if hasattr(self, 'current_task_context') and self.current_task_context:
                    failure_timestamp = time.time()
                    self.current_task_context.agent_interactions.append({
                        "timestamp": failure_timestamp,
                        "coordinator_id": self.agent_id,
                        "target_agent_id": agent_id,
                        "task_description": task_description[:100] + ("..." if len(task_description) > 100 else ""),
                        "success": False,
                        "execution_time": time.time() - start_time,
                        "error": str(e)
                    })
                    
                    # è®°å½•å·¥ä½œæµé˜¶æ®µå¤±è´¥
                    self.current_task_context.workflow_stages.append({
                        "timestamp": failure_timestamp,
                        "stage_name": f"task_failed_by_{agent_id}",
                        "description": f"ä»»åŠ¡ç”± {agent_id} æ‰§è¡Œå¤±è´¥",
                        "agent_id": agent_id,
                        "duration": time.time() - start_time,
                        "success": False,
                        "error": str(e)
                    })
                    
                    # è®°å½•æ‰§è¡Œæ—¶é—´çº¿
                    self.current_task_context.execution_timeline.append({
                        "timestamp": failure_timestamp,
                        "event_type": "agent_failure",
                        "agent_id": agent_id,
                        "description": f"{agent_id} ä»»åŠ¡æ‰§è¡Œå¤±è´¥",
                        "details": {
                            "task_type": task_type,
                            "error": str(e),
                            "priority": priority
                        }
                    })
                
                return {
                    "success": False,
                    "agent_id": agent_id,
                    "task_id": task_id,
                    "error": error_msg,
                    "execution_time": time.time() - start_time
                }
                
        except Exception as e:
            error_msg = f"ä»»åŠ¡åˆ†é…å¤±è´¥: {str(e)}"
            self.logger.error(f"âŒ {error_msg}")
            
            # ğŸ”§ ä¿®å¤ï¼šæ›´æ–°å¤±è´¥è®¡æ•°
            if agent_id in self.registered_agents:
                agent_info = self.registered_agents[agent_id]
                if not hasattr(agent_info, 'failure_count'):
                    agent_info.failure_count = 0
                agent_info.failure_count += 1
                self.logger.warning(f"âš ï¸ æ™ºèƒ½ä½“ {agent_id} å¤±è´¥è®¡æ•°: {agent_info.failure_count}")
            
            return {
                "success": False,
                "error": error_msg
            }
    
    def _build_enhanced_task_description(self, task_description: str, 
                                       expected_output: str,
                                       task_context: TaskContext = None,
                                       task_type: str = "composite",
                                       priority: str = "medium",
                                       include_full_context: bool = True) -> str:
        """æ„å»ºå¢å¼ºçš„ä»»åŠ¡æè¿°"""
        
        # ğŸ”§ ä¿®æ­£1: æ ¹æ®ç›®æ ‡æ™ºèƒ½ä½“è¿‡æ»¤ä»»åŠ¡æè¿°ï¼Œç§»é™¤Verilogæ™ºèƒ½ä½“ä¸åº”æ‰¿æ‹…çš„æµ‹è¯•å°ç”Ÿæˆè¦æ±‚
        filtered_task_description = self._filter_task_description_by_agent(
            task_description, task_context.assigned_agent if task_context else "unknown"
        )
        
        # æ„å»ºå¤–éƒ¨testbenchä¿¡æ¯
        external_testbench_section = ""
        if task_context and hasattr(task_context, 'external_testbench_path') and task_context.external_testbench_path:
            external_testbench_section = f"""

**ğŸ¯ å¤–éƒ¨Testbenchæ¨¡å¼**:
- å¤–éƒ¨testbenchè·¯å¾„: {task_context.external_testbench_path}
- å·¥ä½œæŒ‡å¯¼: å¦‚æœä½ æ˜¯ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“ï¼Œè¯·ç›´æ¥ä½¿ç”¨æä¾›çš„testbenchè¿›è¡Œæµ‹è¯•ï¼Œä¸è¦ç”Ÿæˆæ–°çš„testbench
- ä¸“æ³¨ä»»åŠ¡: ä»£ç å®¡æŸ¥ã€é”™è¯¯ä¿®å¤ã€æµ‹è¯•æ‰§è¡Œå’Œç»“æœåˆ†æ"""
        
        # ğŸ”§ æ–°å¢ï¼šæ„å»ºè®¾è®¡æ–‡ä»¶ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆåŒ…å«å®é™…å†…å®¹ï¼‰
        design_file_section = ""
        task_file_context = get_task_context(task_context.task_id if task_context else "unknown")
        
        if task_file_context and len(task_file_context) > 0:
            # æ„å»ºæ–‡ä»¶ä¸Šä¸‹æ–‡ä¿¡æ¯
            file_summary = []
            primary_design_content = task_file_context.get_primary_design_content()
            
            for file_path, file_content in task_file_context.files.items():
                file_type_display = file_content.file_type.value
                file_size = len(file_content.content)
                is_primary = file_path == task_file_context.primary_design_file
                status = "ğŸ¯ä¸»è¦è®¾è®¡æ–‡ä»¶" if is_primary else f"ğŸ“„{file_type_display}æ–‡ä»¶"
                file_summary.append(f"- {status}: {file_path} ({file_size} å­—ç¬¦)")
            
            design_file_section = f"""

**ğŸ“„ ä»»åŠ¡æ–‡ä»¶ä¸Šä¸‹æ–‡**:
æ–‡ä»¶æ¸…å•:
{chr(10).join(file_summary)}

**ğŸ¯ ä¸»è¦è®¾è®¡æ–‡ä»¶å†…å®¹**:
```verilog
{primary_design_content[:2000] + '...' if primary_design_content and len(primary_design_content) > 2000 else primary_design_content or 'æœªæ‰¾åˆ°ä¸»è¦è®¾è®¡æ–‡ä»¶'}
```

**å·¥ä½œæŒ‡å¯¼**:
- æ‰€æœ‰æ–‡ä»¶å†…å®¹å·²é¢„åŠ è½½åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼Œå¯ç›´æ¥ä½¿ç”¨
- ä¼˜å…ˆä½¿ç”¨ä¸»è¦è®¾è®¡æ–‡ä»¶: {task_file_context.primary_design_file or 'æœªè®¾ç½®'}
- å¦‚éœ€è®¿é—®æ–‡ä»¶å†…å®¹ï¼Œè°ƒç”¨ç›¸å…³å·¥å…·æ—¶æ–‡ä»¶å†…å®¹ä¼šè‡ªåŠ¨æä¾›
- ä¸è¦é‡æ–°è¯»å–æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨é¢„åŠ è½½çš„å†…å®¹"""
        elif task_context and hasattr(task_context, 'design_file_path') and task_context.design_file_path:
            # å›é€€åˆ°æ—§çš„è·¯å¾„æ¨¡å¼
            design_file_section = f"""

**ğŸ“ è®¾è®¡æ–‡ä»¶è·¯å¾„**:
- è®¾è®¡æ–‡ä»¶: {task_context.design_file_path}
- å·¥ä½œæŒ‡å¯¼: è¯·ç›´æ¥ä½¿ç”¨æ­¤è·¯å¾„çš„è®¾è®¡æ–‡ä»¶è¿›è¡Œä»£ç å®¡æŸ¥å’Œæµ‹è¯•å°ç”Ÿæˆ
- é‡è¦æç¤º: è¿™æ˜¯éœ€è¦å®¡æŸ¥çš„å…·ä½“æ–‡ä»¶è·¯å¾„ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨æ­¤æ–‡ä»¶"""
          
        # æ„å»ºå®éªŒè·¯å¾„ä¿¡æ¯ - ä»ä»»åŠ¡ä¸Šä¸‹æ–‡æˆ–è·å–å½“å‰å®éªŒè·¯å¾„
        experiment_path_section = ""
        current_experiment_path = None
        
        if task_context and hasattr(task_context, 'experiment_path') and task_context.experiment_path:
            current_experiment_path = task_context.experiment_path
            self.logger.info(f"ğŸ§ª ä½¿ç”¨ä»»åŠ¡ä¸Šä¸‹æ–‡ä¸­çš„å®éªŒè·¯å¾„: {current_experiment_path}")
        else:
            # å°è¯•ä»æ´»è·ƒä»»åŠ¡ä¸­è·å–å®éªŒè·¯å¾„
            for task in self.active_tasks.values():
                if hasattr(task, 'experiment_path') and task.experiment_path:
                    current_experiment_path = task.experiment_path
                    self.logger.info(f"ğŸ§ª ä»æ´»è·ƒä»»åŠ¡ä¸­è·å–å®éªŒè·¯å¾„: {current_experiment_path}")
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„æ–‡ä»¶å·¥ä½œç©ºé—´è·¯å¾„
            if not current_experiment_path:
                current_experiment_path = "./file_workspace"
                self.logger.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°å®éªŒè·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {current_experiment_path}")
        
        if current_experiment_path:
            experiment_path_section = f"""

**ğŸ“ å®éªŒæ–‡ä»¶è·¯å¾„**:
- å½“å‰å®éªŒè·¯å¾„: {current_experiment_path}
- è®¾è®¡æ–‡ä»¶ä¿å­˜: {current_experiment_path}/designs/
- æµ‹è¯•å°ä¿å­˜: {current_experiment_path}/testbenches/
- æŠ¥å‘Šä¿å­˜: {current_experiment_path}/reports/
- ä¸´æ—¶æ–‡ä»¶: {current_experiment_path}/temp/

**âš ï¸ é‡è¦æ–‡ä»¶ç®¡ç†è¦æ±‚**:
1. æ‰€æœ‰ç”Ÿæˆçš„Verilogä»£ç å¿…é¡»ä¿å­˜ä¸º.væ–‡ä»¶
2. è®¾è®¡æ¨¡å—ä¿å­˜åˆ°designsç›®å½•ï¼Œæµ‹è¯•å°ä¿å­˜åˆ°testbenchesç›®å½•
3. æ–‡æ¡£å’ŒæŠ¥å‘Šä¿å­˜åˆ°reportsç›®å½•
4. å¿…é¡»åœ¨ä»»åŠ¡æ€»ç»“ä¸­è¿”å›æ‰€æœ‰ç”Ÿæˆæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
5. æ–‡ä»¶å‘½ååº”è¯¥æ¸…æ™°ï¼Œé¿å…é‡å¤å’Œå†²çª"""
        
        # æ ¹æ®æ™ºèƒ½ä½“ç±»å‹æ·»åŠ ä¸“ç”¨çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼
        agent_tool_guide = self._get_agent_specific_tool_guide(task_context.assigned_agent if task_context else "unknown")
        
        # æ ¹æ®include_full_contextå‚æ•°å†³å®šæ˜¯å¦åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡
        if include_full_context:
            enhanced_task = f"""
ğŸ“‹ åè°ƒæ™ºèƒ½ä½“åˆ†é…çš„ä»»åŠ¡

**ä»»åŠ¡æè¿°**:
{filtered_task_description}

**æœŸæœ›è¾“å‡º**:
{expected_output if expected_output else "æ ¹æ®ä»»åŠ¡æè¿°ç”Ÿæˆç›¸åº”çš„ä»£ç å’Œæ–‡æ¡£"}

**ä»»åŠ¡ç±»å‹**:
- ç±»å‹: {task_type}
- ä¼˜å…ˆçº§: {priority}

**ä»»åŠ¡ä¸Šä¸‹æ–‡**:
- ä»»åŠ¡ID: {task_context.task_id if task_context else "unknown"}
- å½“å‰é˜¶æ®µ: {task_context.current_stage if task_context else "initial"}
- è¿­ä»£æ¬¡æ•°: {task_context.iteration_count if task_context else 0}
{external_testbench_section}
{design_file_section}
{experiment_path_section}

{agent_tool_guide}

**æ‰§è¡Œè¦æ±‚**:
1. ä»”ç»†åˆ†æä»»åŠ¡éœ€æ±‚
2. æ ¹æ®ä¸Šè¿°å·¥å…·æŒ‡å¯¼é€‰æ‹©åˆé€‚çš„å·¥å…·
3. ç”Ÿæˆé«˜è´¨é‡çš„ä»£ç å¹¶ä¿å­˜ä¸ºæ–‡ä»¶
4. æä¾›è¯¦ç»†çš„è¯´æ˜æ–‡æ¡£
5. ç¡®ä¿ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§
6. **å¼ºåˆ¶è¦æ±‚**: åœ¨ä»»åŠ¡å®Œæˆåï¼Œåœ¨å“åº”ä¸­æ˜ç¡®åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆæ–‡ä»¶çš„è·¯å¾„

è¯·å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼Œä¸¥æ ¼æŒ‰ç…§å·¥å…·ä½¿ç”¨æŒ‡å¯¼è¿›è¡Œæ“ä½œã€‚
"""
        else:
            # ç®€åŒ–ç‰ˆæœ¬ï¼ŒåªåŒ…å«æ ¸å¿ƒä»»åŠ¡æè¿°
            enhanced_task = f"""
ğŸ“‹ ç»§ç»­æ‰§è¡Œä»»åŠ¡

**å½“å‰ä»»åŠ¡**: {filtered_task_description}
{design_file_section}

{agent_tool_guide}

**æ‰§è¡Œè¦æ±‚**:
1. ç»§ç»­ä¹‹å‰çš„ä»»åŠ¡æ‰§è¡Œ
2. æ ¹æ®ä¸Šè¿°å·¥å…·æŒ‡å¯¼é€‰æ‹©åˆé€‚çš„å·¥å…·
3. ç”Ÿæˆé«˜è´¨é‡çš„ä»£ç å¹¶ä¿å­˜ä¸ºæ–‡ä»¶
4. æä¾›è¯¦ç»†çš„è¯´æ˜æ–‡æ¡£
5. ç¡®ä¿ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§
6. **å¼ºåˆ¶è¦æ±‚**: åœ¨ä»»åŠ¡å®Œæˆåï¼Œåœ¨å“åº”ä¸­æ˜ç¡®åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆæ–‡ä»¶çš„è·¯å¾„

è¯·ç»§ç»­æ‰§è¡Œä»»åŠ¡ï¼Œä¸¥æ ¼æŒ‰ç…§å·¥å…·ä½¿ç”¨æŒ‡å¯¼è¿›è¡Œæ“ä½œã€‚
"""
        
        return enhanced_task
    
    def _filter_task_description_by_agent(self, task_description: str, agent_id: str) -> str:
        """æ ¹æ®ç›®æ ‡æ™ºèƒ½ä½“è¿‡æ»¤ä»»åŠ¡æè¿°ï¼Œç§»é™¤ä¸åˆé€‚çš„è¦æ±‚"""
        
        if agent_id == "enhanced_real_verilog_agent":
            # ğŸ”§ ä¿®æ­£: ä¸ºVerilogè®¾è®¡æ™ºèƒ½ä½“ç§»é™¤æµ‹è¯•å°ç”Ÿæˆç›¸å…³è¦æ±‚ - ä½¿ç”¨æ›´ç›´æ¥çš„æ–¹æ³•
            original_desc = task_description
            
            # ğŸ”§ æ–¹æ³•1: ç›´æ¥å­—ç¬¦ä¸²æ›¿æ¢ï¼Œç§»é™¤å¸¸è§çš„æµ‹è¯•å°ç›¸å…³è¡¨è¿°
            filtered_desc = task_description
            
            # ç§»é™¤å¸¸è§çš„æµ‹è¯•å°ç›¸å…³çŸ­è¯­
            testbench_phrases = [
                "å’Œæµ‹è¯•å°",
                "ã€æµ‹è¯•å°", 
                "ä»¥åŠæµ‹è¯•å°",
                "è¿˜æœ‰æµ‹è¯•å°",
                "åŒ…å«æµ‹è¯•å°",
                "ç”Ÿæˆæµ‹è¯•å°",
                "åˆ›å»ºæµ‹è¯•å°",
                "ç¼–å†™æµ‹è¯•å°"
            ]
            
            for phrase in testbench_phrases:
                filtered_desc = filtered_desc.replace(phrase, "")
            
            # ğŸ”§ æ–¹æ³•2: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¸…ç†
            import re
            testbench_patterns = [
                r"ï¼ŒåŒ…å«å®Œæ•´çš„ç«¯å£å®šä¹‰ã€åŠŸèƒ½å®ç°å’Œæµ‹è¯•å°",
                r"åŒ…å«å®Œæ•´çš„ç«¯å£å®šä¹‰ã€åŠŸèƒ½å®ç°å’Œæµ‹è¯•å°",
                r"ç”Ÿæˆ.*?æµ‹è¯•å°.*?è¿›è¡ŒéªŒè¯",
                r"å¹¶.*?ç”Ÿæˆ.*?æµ‹è¯•å°",
                r"ä»¥åŠ.*?æµ‹è¯•å°",
                r"å’Œ.*?æµ‹è¯•å°", 
                r"ã€.*?æµ‹è¯•å°"
            ]
            
            for pattern in testbench_patterns:
                filtered_desc = re.sub(pattern, "", filtered_desc, flags=re.IGNORECASE)
            
            # ğŸ”§ æ–¹æ³•3: æ¸…ç†å¤šä½™çš„æ ‡ç‚¹ç¬¦å·
            filtered_desc = re.sub(r"ï¼Œ\s*$", "", filtered_desc)  # ç§»é™¤æœ«å°¾é€—å·
            filtered_desc = re.sub(r"ã€\s*$", "", filtered_desc)  # ç§»é™¤æœ«å°¾é¡¿å·
            filtered_desc = re.sub(r"å’Œ\s*$", "", filtered_desc)  # ç§»é™¤æœ«å°¾"å’Œ"
            filtered_desc = filtered_desc.strip()
            
            # æ£€æŸ¥æ˜¯å¦åŸæ¥åŒ…å«æµ‹è¯•å°è¦æ±‚
            has_testbench_requirement = ("æµ‹è¯•å°" in original_desc or 
                                       "testbench" in original_desc.lower() or 
                                       "éªŒè¯" in original_desc)
            
            # å¦‚æœåŸæ¥åŒ…å«æµ‹è¯•å°è¦æ±‚ï¼Œæ·»åŠ æ˜ç¡®çš„èŒè´£è¯´æ˜
            if has_testbench_requirement:
                if filtered_desc:
                    filtered_desc += """

ğŸš¨ **é‡è¦è¯´æ˜**: 
- æœ¬ä»»åŠ¡ä»…è¦æ±‚å®ŒæˆVerilogæ¨¡å—è®¾è®¡å’Œä»£ç ç”Ÿæˆ
- æµ‹è¯•å°(testbench)ç”Ÿæˆå’ŒéªŒè¯å·¥ä½œå°†ç”±åç»­çš„ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“è´Ÿè´£
- è¯·ä¸“æ³¨äºè®¾è®¡æ¨¡å—çš„ç«¯å£å®šä¹‰ã€åŠŸèƒ½å®ç°å’Œä»£ç è´¨é‡"""
                else:
                    # å¦‚æœè¿‡æ»¤åä¸ºç©ºï¼Œæä¾›é»˜è®¤çš„è®¾è®¡ä»»åŠ¡æè¿°
                    filtered_desc = """è®¾è®¡Verilogæ¨¡å—ï¼Œä¸“æ³¨äºæ¨¡å—æ¶æ„å’ŒåŠŸèƒ½å®ç°

ğŸš¨ **é‡è¦è¯´æ˜**: 
- æœ¬ä»»åŠ¡ä»…è¦æ±‚å®ŒæˆVerilogæ¨¡å—è®¾è®¡å’Œä»£ç ç”Ÿæˆ
- æµ‹è¯•å°(testbench)ç”Ÿæˆå’ŒéªŒè¯å·¥ä½œå°†ç”±åç»­çš„ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“è´Ÿè´£
- è¯·ä¸“æ³¨äºè®¾è®¡æ¨¡å—çš„ç«¯å£å®šä¹‰ã€åŠŸèƒ½å®ç°å’Œä»£ç è´¨é‡"""
            
            return filtered_desc.strip()
        
        elif agent_id == "enhanced_real_code_review_agent":
            # ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“ä¿æŒåŸå§‹ä»»åŠ¡æè¿°
            return task_description
        
        else:
            # æœªçŸ¥æ™ºèƒ½ä½“ç±»å‹ï¼Œä¿æŒåŸå§‹æè¿°
            return task_description
    
    def _get_agent_specific_tool_guide(self, agent_id: str) -> str:
        """æ ¹æ®æ™ºèƒ½ä½“ç±»å‹è·å–ä¸“ç”¨çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼"""
        
        if agent_id == "enhanced_real_verilog_agent":
            return """
**ğŸ› ï¸ EnhancedRealVerilogAgent ä¸“ç”¨å·¥å…·ä½¿ç”¨æŒ‡å¯¼**

ğŸ“‹ **å¯ç”¨å·¥å…·åˆ—è¡¨**:

ğŸš¨ **é‡è¦æé†’**: æœ¬æ™ºèƒ½ä½“ä¸“æ³¨äºVerilogæ¨¡å—è®¾è®¡ï¼Œ**ç»ä¸è´Ÿè´£æµ‹è¯•å°(testbench)ç”Ÿæˆ**

### 1. **analyze_design_requirements** - è®¾è®¡éœ€æ±‚åˆ†æ
   **åŠŸèƒ½**: åˆ†æå’Œè§£æVerilogè®¾è®¡éœ€æ±‚ï¼Œæå–å…³é”®è®¾è®¡å‚æ•°
   **å‚æ•°**:
   - `requirements` (å¿…å¡«, string): è®¾è®¡éœ€æ±‚æè¿°ï¼ŒåŒ…å«åŠŸèƒ½è§„æ ¼å’Œçº¦æŸæ¡ä»¶
   - `design_type` (å¯é€‰, string): è®¾è®¡ç±»å‹ï¼Œå¯é€‰å€¼: "combinational", "sequential", "mixed", "custom"ï¼Œé»˜è®¤"mixed"
   - `complexity_level` (å¯é€‰, string): è®¾è®¡å¤æ‚åº¦çº§åˆ«ï¼Œå¯é€‰å€¼: "simple", "medium", "complex", "advanced"ï¼Œé»˜è®¤"medium"
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "analyze_design_requirements",
       "parameters": {
           "requirements": "è®¾è®¡ä¸€ä¸ªåä¸ºcounterçš„Verilogæ¨¡å—",
           "design_type": "sequential",
           "complexity_level": "medium"
       }
   }
   ```

### 2. **generate_verilog_code** - Verilogä»£ç ç”Ÿæˆ
   **åŠŸèƒ½**: ç”Ÿæˆé«˜è´¨é‡çš„Verilog HDLä»£ç 
   **å‚æ•°**:
   - `module_name` (å¿…å¡«, string): æ¨¡å—åç§°
   - `requirements` (å¿…å¡«, string): è®¾è®¡éœ€æ±‚å’ŒåŠŸèƒ½æè¿°
   - `input_ports` (å¿…å¡«, array): è¾“å…¥ç«¯å£åˆ—è¡¨ï¼Œæ ¼å¼: [{"name": "ç«¯å£å", "width": ä½å®½, "type": "ç±»å‹"}]
   - `output_ports` (å¿…å¡«, array): è¾“å‡ºç«¯å£åˆ—è¡¨ï¼Œæ ¼å¼åŒä¸Š
   - `coding_style` (å¯é€‰, string): ç¼–ç é£æ ¼ï¼Œå¯é€‰å€¼: "rtl", "behavioral", "structural"ï¼Œé»˜è®¤"rtl"
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "generate_verilog_code",
       "parameters": {
           "module_name": "counter",
           "requirements": "4ä½è®¡æ•°å™¨ï¼Œæ”¯æŒå¤ä½å’Œä½¿èƒ½",
           "input_ports": [
               {"name": "clk", "width": 1, "type": "input"},
               {"name": "rst_n", "width": 1, "type": "input"},
               {"name": "en", "width": 1, "type": "input"}
           ],
           "output_ports": [
               {"name": "count", "width": 4, "type": "output"}
           ],
           "coding_style": "rtl"
       }
   }
   ```

### 3. **analyze_code_quality** - ä»£ç è´¨é‡åˆ†æ
   **åŠŸèƒ½**: åˆ†æVerilogä»£ç è´¨é‡ï¼Œæä¾›è¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
   **å‚æ•°**:
   - `verilog_code` (å¿…å¡«, string): è¦åˆ†æçš„Verilogä»£ç 
   - `module_name` (å¿…å¡«, string): æ¨¡å—åç§°
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "analyze_code_quality",
       "parameters": {
           "verilog_code": "module counter(...); ... endmodule",
           "module_name": "counter"
       }
   }
   ```

### 5. **optimize_verilog_code** - ä»£ç ä¼˜åŒ–
   **åŠŸèƒ½**: ä¼˜åŒ–Verilogä»£ç ï¼Œæ”¯æŒé¢ç§¯ã€é€Ÿåº¦ã€åŠŸè€—ç­‰ä¼˜åŒ–ç›®æ ‡
   **å‚æ•°**:
   - `verilog_code` (å¿…å¡«, string): è¦ä¼˜åŒ–çš„Verilogä»£ç 
   - `optimization_target` (å¿…å¡«, string): ä¼˜åŒ–ç›®æ ‡ï¼Œå¯é€‰å€¼: "area", "speed", "power", "timing"
   - `module_name` (å¿…å¡«, string): æ¨¡å—åç§°
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "optimize_verilog_code",
       "parameters": {
           "verilog_code": "module counter(...); ... endmodule",
           "optimization_target": "area",
           "module_name": "counter"
       }
   }
   ```

â­ **æ¨èæ‰§è¡Œæµç¨‹**:
1. analyze_design_requirements â†’ 2. generate_verilog_code â†’ 3. analyze_code_quality 
â†’ 4. optimize_verilog_code (å¯é€‰)

ğŸ’¡ **èŒè´£è¾¹ç•Œ**: 
- âœ… è´Ÿè´£: Verilogæ¨¡å—è®¾è®¡ã€ç«¯å£å®šä¹‰ã€åŠŸèƒ½å®ç°ã€ä»£ç ç”Ÿæˆ
- âŒ ç¦æ­¢: æµ‹è¯•å°(testbench)ç”Ÿæˆã€ä»¿çœŸéªŒè¯ã€æµ‹è¯•æ‰§è¡Œ
- ğŸ“ è¯´æ˜: æµ‹è¯•å°å’ŒéªŒè¯å·¥ä½œç”±ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“(enhanced_real_code_review_agent)ä¸“é—¨è´Ÿè´£
"""

        elif agent_id == "enhanced_real_code_review_agent":
            return """
**ğŸ› ï¸ EnhancedRealCodeReviewAgent ä¸“ç”¨å·¥å…·ä½¿ç”¨æŒ‡å¯¼**

ğŸ“‹ **å¯ç”¨å·¥å…·åˆ—è¡¨**:

### 1. **generate_testbench** - æµ‹è¯•å°ç”Ÿæˆ
   **åŠŸèƒ½**: ä¸ºVerilogæ¨¡å—ç”Ÿæˆå…¨é¢çš„æµ‹è¯•å°(testbench)
   **å‚æ•°**:
   - `module_name` (å¿…å¡«, string): ç›®æ ‡æ¨¡å—åç§°
   - `module_code` (å¿…å¡«, string): ç›®æ ‡æ¨¡å—ä»£ç  (ä¹Ÿå¯ä½¿ç”¨ `code`, `design_code`)
   - `test_scenarios` (å¯é€‰, array): æµ‹è¯•åœºæ™¯åˆ—è¡¨ (ä¹Ÿå¯ä½¿ç”¨ `test_cases`)
   - `clock_period` (å¯é€‰, number): æ—¶é’Ÿå‘¨æœŸ(ns)ï¼ŒèŒƒå›´0.1-1000.0ï¼Œé»˜è®¤10.0
   - `simulation_time` (å¯é€‰, integer): ä»¿çœŸæ—¶é—´ï¼ŒèŒƒå›´100-1000000ï¼Œé»˜è®¤10000
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "generate_testbench",
       "parameters": {
           "module_name": "counter",
           "module_code": "module counter(...); ... endmodule",
           "test_scenarios": [
               {"name": "basic_test", "description": "åŸºæœ¬åŠŸèƒ½éªŒè¯"},
               {"name": "reset_test", "description": "å¤ä½åŠŸèƒ½æµ‹è¯•"}
           ],
           "clock_period": 10.0,
           "simulation_time": 10000
       }
   }
   ```

### 2. **run_simulation** - ä»¿çœŸæ‰§è¡Œ
   **åŠŸèƒ½**: ä½¿ç”¨ä¸“ä¸šå·¥å…·è¿è¡ŒVerilogä»¿çœŸå’ŒéªŒè¯
   **å‚æ•°**:
   - `module_file` æˆ– `module_code` (å¿…å¡«): æ¨¡å—æ–‡ä»¶è·¯å¾„æˆ–ä»£ç å†…å®¹
   - `testbench_file` æˆ– `testbench_code` (å¿…å¡«): æµ‹è¯•å°æ–‡ä»¶è·¯å¾„æˆ–ä»£ç å†…å®¹
   - `simulator` (å¯é€‰, string): ä»¿çœŸå™¨ç±»å‹ï¼Œå¯é€‰å€¼: "iverilog", "modelsim", "vivado", "auto"ï¼Œé»˜è®¤"iverilog"
   - `simulation_options` (å¯é€‰, object): ä»¿çœŸé€‰é¡¹é…ç½®
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "run_simulation",
       "parameters": {
           "module_file": "counter.v",
           "testbench_file": "testbench_counter.v",
           "simulator": "iverilog",
           "simulation_options": {"timescale": "1ns/1ps"}
       }
   }
   ```

### 3. **use_external_testbench** - å¤–éƒ¨æµ‹è¯•å°ä½¿ç”¨
   **åŠŸèƒ½**: ä½¿ç”¨å¤–éƒ¨æä¾›çš„testbenchæ–‡ä»¶è¿›è¡Œæµ‹è¯•éªŒè¯
   **å‚æ•°**:
   - `design_code` (å¿…å¡«, string): è®¾è®¡ä»£ç 
   - `external_testbench_path` (å¿…å¡«, string): å¤–éƒ¨testbenchæ–‡ä»¶è·¯å¾„
   - `design_module_name` (å¿…å¡«, string): è®¾è®¡æ¨¡å—åç§°
   - `simulator` (å¯é€‰, string): ä»¿çœŸå™¨ç±»å‹ï¼Œé»˜è®¤"iverilog"
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "use_external_testbench",
       "parameters": {
           "design_code": "module counter(...); ... endmodule",
           "external_testbench_path": "./testbenches/counter_tb.v",
           "design_module_name": "counter",
           "simulator": "iverilog"
       }
   }
   ```

### 4. **generate_build_script** - æ„å»ºè„šæœ¬ç”Ÿæˆ
   **åŠŸèƒ½**: ç”Ÿæˆä¸“ä¸šçš„æ„å»ºè„šæœ¬(Makefileæˆ–shellè„šæœ¬)
   **å‚æ•°**:
   - `verilog_files` (å¿…å¡«, array): Verilogæ–‡ä»¶åˆ—è¡¨ (ä¹Ÿå¯ä½¿ç”¨ `design_files`)
   - `testbench_files` (å¿…å¡«, array): æµ‹è¯•å°æ–‡ä»¶åˆ—è¡¨
   - `script_type` (å¯é€‰, string): è„šæœ¬ç±»å‹ï¼Œå¯é€‰å€¼: "makefile", "bash", "tcl", "python"ï¼Œé»˜è®¤"makefile"
   - `target_name` (å¯é€‰, string): ç›®æ ‡åç§°ï¼Œé»˜è®¤"simulation"
   - `build_options` (å¯é€‰, object): æ„å»ºé€‰é¡¹é…ç½®
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "generate_build_script",
       "parameters": {
           "verilog_files": ["counter.v"],
           "testbench_files": ["testbench_counter.v"],
           "script_type": "makefile",
           "target_name": "simulation",
           "build_options": {"simulator": "iverilog"}
       }
   }
   ```

### 5. **execute_build_script** - è„šæœ¬æ‰§è¡Œ
   **åŠŸèƒ½**: å®‰å…¨æ‰§è¡Œæ„å»ºè„šæœ¬è¿›è¡Œç¼–è¯‘å’Œä»¿çœŸ
   **å‚æ•°**:
   - `script_name` (å¿…å¡«, string): è„šæœ¬æ–‡ä»¶å
   - `action` (å¯é€‰, string): æ‰§è¡ŒåŠ¨ä½œï¼Œå¯é€‰å€¼: "all", "compile", "simulate", "clean"ï¼Œé»˜è®¤"all"
   - `arguments` (å¯é€‰, array): é™„åŠ å‚æ•°åˆ—è¡¨
   - `timeout` (å¯é€‰, integer): è¶…æ—¶æ—¶é—´(ç§’)ï¼Œé»˜è®¤300
   - `working_directory` (å¯é€‰, string): å·¥ä½œç›®å½•
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "execute_build_script",
       "parameters": {
           "script_name": "Makefile",
           "action": "all",
           "timeout": 300,
           "working_directory": "./file_workspace"
       }
   }
   ```

### 6. **analyze_test_failures** - æµ‹è¯•å¤±è´¥åˆ†æ
   **åŠŸèƒ½**: åˆ†ææµ‹è¯•å¤±è´¥åŸå› å¹¶æä¾›å…·ä½“ä¿®å¤å»ºè®®
   **å‚æ•°**:
   - `design_code` (å¿…å¡«, string): éœ€è¦åˆ†æçš„è®¾è®¡ä»£ç 
   - `compilation_errors` (å¯é€‰, string): ç¼–è¯‘é”™è¯¯è¾“å‡º
   - `simulation_errors` (å¯é€‰, string): ä»¿çœŸé”™è¯¯è¾“å‡º
   - `test_assertions` (å¯é€‰, string): æµ‹è¯•æ–­è¨€å¤±è´¥ä¿¡æ¯
   - `testbench_code` (å¯é€‰, string): æµ‹è¯•å°ä»£ç 
   - `iteration_number` (å¯é€‰, integer): å½“å‰TDDè¿­ä»£æ¬¡æ•°
   - `previous_fixes` (å¯é€‰, array): ä¹‹å‰å°è¯•çš„ä¿®å¤æ–¹æ³•
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "analyze_test_failures",
       "parameters": {
           "design_code": "module counter(...); ... endmodule",
           "compilation_errors": "Error: undefined signal 'clk'",
           "simulation_errors": "simulation failed at time 100ns",
           "testbench_code": "module testbench; ... endmodule",
           "iteration_number": 1
       }
   }
   ```

â­ **æ¨èæ‰§è¡Œæµç¨‹**:
1. generate_testbench â†’ 2. run_simulation â†’ 3. analyze_test_failures (å¦‚æœ‰é—®é¢˜) 
â†’ 4. generate_build_script â†’ 5. execute_build_script â†’ 6. use_external_testbench (å¦‚æœ‰å¤–éƒ¨æµ‹è¯•å°)

ğŸ’¡ **é‡è¦æç¤º**: ä¸“æ³¨äºä»£ç å®¡æŸ¥ã€æµ‹è¯•å’ŒéªŒè¯ï¼Œä¸è´Ÿè´£Verilogè®¾è®¡
"""

        elif agent_id in ["llm_coordinator_agent", "coordinator", "unknown"]:
            # å¯¹äºLLMCoordinatorAgentæˆ–å…¶ä»–agentï¼Œè¿”å›åè°ƒå·¥å…·æŒ‡å¯¼
            return """
**ğŸ› ï¸ LLMCoordinatorAgent åè°ƒå·¥å…·ä½¿ç”¨æŒ‡å¯¼**

ğŸ“‹ **å¯ç”¨å·¥å…·åˆ—è¡¨**:

### 1. **assign_task_to_agent** - æ™ºèƒ½ä»»åŠ¡åˆ†é…
   **åŠŸèƒ½**: å°†ä»»åŠ¡åˆ†é…ç»™æœ€åˆé€‚çš„æ™ºèƒ½ä½“
   **å‚æ•°**:
   - `agent_id` (å¿…å¡«, string): æ™ºèƒ½ä½“IDï¼Œå¯é€‰å€¼: "enhanced_real_verilog_agent", "enhanced_real_code_review_agent"
   - `task_description` (å¿…å¡«, string): è¯¦ç»†çš„ä»»åŠ¡æè¿°
   - `expected_output` (å¯é€‰, string): æœŸæœ›çš„è¾“å‡ºæ ¼å¼ï¼Œé»˜è®¤ç©ºå­—ç¬¦ä¸²
   - `task_type` (å¯é€‰, string): ä»»åŠ¡ç±»å‹ï¼Œå¯é€‰å€¼: "design", "review", "composite"ï¼Œé»˜è®¤"design"
   - `priority` (å¯é€‰, string): ä»»åŠ¡ä¼˜å…ˆçº§ï¼Œå¯é€‰å€¼: "low", "medium", "high"ï¼Œé»˜è®¤"medium"
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "assign_task_to_agent",
       "parameters": {
           "agent_id": "enhanced_real_verilog_agent",
           "task_description": "è®¾è®¡ä¸€ä¸ªåä¸ºcounterçš„Verilogæ¨¡å—",
           "expected_output": "ç”Ÿæˆå®Œæ•´çš„Verilogä»£ç æ–‡ä»¶",
           "task_type": "design",
           "priority": "medium"
       }
   }
   ```

### 2. **analyze_agent_result** - ç»“æœè´¨é‡åˆ†æ
   **åŠŸèƒ½**: åˆ†ææ™ºèƒ½ä½“æ‰§è¡Œç»“æœçš„è´¨é‡å’Œå®Œæ•´æ€§
   **å‚æ•°**:
   - `agent_id` (å¿…å¡«, string): æ™ºèƒ½ä½“ID
   - `result` (å¿…å¡«, object): æ™ºèƒ½ä½“è¿”å›çš„ç»“æœæ•°æ®
   - `task_context` (å¯é€‰, object): ä»»åŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé»˜è®¤{}
   - `quality_threshold` (å¯é€‰, number): è´¨é‡é˜ˆå€¼ï¼ŒèŒƒå›´0-100ï¼Œé»˜è®¤80.0
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "analyze_agent_result",
       "parameters": {
           "agent_id": "enhanced_real_verilog_agent",
           "result": {"status": "success", "generated_files": ["counter.v"]},
           "task_context": {"task_id": "task_001"},
           "quality_threshold": 80.0
       }
   }
   ```

### 3. **check_task_completion** - ä»»åŠ¡å®Œæˆæ£€æŸ¥
   **åŠŸèƒ½**: æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å®Œæˆå¹¶ç¬¦åˆè¦æ±‚
   **å‚æ•°**:
   - `task_id` (å¿…å¡«, string): ä»»åŠ¡æ ‡è¯†ç¬¦
   - `all_results` (å¿…å¡«, array): æ‰€æœ‰ç›¸å…³ç»“æœåˆ—è¡¨
   - `original_requirements` (å¿…å¡«, string): åŸå§‹éœ€æ±‚æè¿°
   - `completion_criteria` (å¯é€‰, object): å®Œæˆæ ‡å‡†ï¼Œé»˜è®¤{}
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "check_task_completion",
       "parameters": {
           "task_id": "task_001",
           "all_results": [{"status": "success", "files": ["counter.v"]}],
           "original_requirements": "è®¾è®¡ä¸€ä¸ªåä¸ºcounterçš„Verilogæ¨¡å—",
           "completion_criteria": {"require_testbench": true}
       }
   }
   ```

### 4. **query_agent_status** - æ™ºèƒ½ä½“çŠ¶æ€æŸ¥è¯¢
   **åŠŸèƒ½**: æŸ¥è¯¢æ™ºèƒ½ä½“çš„å½“å‰çŠ¶æ€å’Œæ€§èƒ½ä¿¡æ¯
   **å‚æ•°**:
   - `agent_id` (å¿…å¡«, string): æ™ºèƒ½ä½“ID
   - `include_performance` (å¯é€‰, boolean): æ˜¯å¦åŒ…å«æ€§èƒ½æ•°æ®ï¼Œé»˜è®¤true
   - `include_history` (å¯é€‰, boolean): æ˜¯å¦åŒ…å«å†å²è®°å½•ï¼Œé»˜è®¤false
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "query_agent_status",
       "parameters": {
           "agent_id": "enhanced_real_verilog_agent",
           "include_performance": true,
           "include_history": false
       }
   }
   ```

### 5. **identify_task_type** - ä»»åŠ¡ç±»å‹è¯†åˆ«
   **åŠŸèƒ½**: è¯†åˆ«å’Œåˆ†ç±»ç”¨æˆ·ä»»åŠ¡çš„ç±»å‹
   **å‚æ•°**:
   - `user_request` (å¿…å¡«, string): ç”¨æˆ·çš„åŸå§‹è¯·æ±‚
   - `context` (å¯é€‰, object): ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé»˜è®¤{}
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "identify_task_type",
       "parameters": {
           "user_request": "è®¾è®¡ä¸€ä¸ªåä¸ºcounterçš„Verilogæ¨¡å—",
           "context": {}
       }
   }
   ```

### 6. **recommend_agent** - æ™ºèƒ½ä½“æ¨è
   **åŠŸèƒ½**: åŸºäºä»»åŠ¡ç±»å‹æ¨èæœ€åˆé€‚çš„æ™ºèƒ½ä½“
   **å‚æ•°**:
   - `task_type` (å¿…å¡«, string): ä»»åŠ¡ç±»å‹
   - `task_description` (å¿…å¡«, string): ä»»åŠ¡æè¿°
   - `priority` (å¯é€‰, string): ä¼˜å…ˆçº§ï¼Œå¯é€‰å€¼: "low", "medium", "high"ï¼Œé»˜è®¤"medium"
   - `constraints` (å¯é€‰, object): çº¦æŸæ¡ä»¶ï¼Œé»˜è®¤null
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "recommend_agent",
       "parameters": {
           "task_type": "design",
           "task_description": "è®¾è®¡ä¸€ä¸ªåä¸ºcounterçš„Verilogæ¨¡å—",
           "priority": "medium",
           "constraints": {}
       }
   }
   ```

### 7. **provide_final_answer** - æœ€ç»ˆç­”æ¡ˆæä¾›
   **åŠŸèƒ½**: æä¾›ä»»åŠ¡æ‰§è¡Œçš„æœ€ç»ˆç­”æ¡ˆå’Œæ€»ç»“
   **å‚æ•°**:
   - `final_summary` (å¿…å¡«, string): æœ€ç»ˆæ€»ç»“
   - `task_status` (å¿…å¡«, string): ä»»åŠ¡çŠ¶æ€ï¼Œå¯é€‰å€¼: "success", "partial", "failed"
   - `results_summary` (å¯é€‰, object): ç»“æœæ‘˜è¦ï¼Œé»˜è®¤{}
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "provide_final_answer",
       "parameters": {
           "final_summary": "æˆåŠŸè®¾è®¡å¹¶ç”Ÿæˆäº†counteræ¨¡å—",
           "task_status": "success",
           "results_summary": {"generated_files": ["counter.v", "counter_tb.v"]}
       }
   }
   ```

ğŸš¨ **é‡è¦ä»»åŠ¡å®Œæˆè§„åˆ™**:
- **å½“ä»»åŠ¡å®Œæˆæ—¶ï¼Œå¿…é¡»è°ƒç”¨ `provide_final_answer` å·¥å…·**ï¼Œè€Œä¸æ˜¯è¿”å›è¯„ä¼°JSON
- **ç¦æ­¢ç›´æ¥è¿”å›ä»»åŠ¡å®Œæˆè¯„ä¼°**ï¼Œå¿…é¡»é€šè¿‡å·¥å…·è°ƒç”¨å®Œæˆ
- **ä»»åŠ¡å®Œæˆæ¡ä»¶**: æ‰€æœ‰æ™ºèƒ½ä½“æ‰§è¡Œå®Œæˆä¸”ç»“æœè´¨é‡æ»¡è¶³è¦æ±‚

â­ **æ¨èåè°ƒæµç¨‹**:
1. identify_task_type â†’ 2. recommend_agent â†’ 3. assign_task_to_agent 
â†’ 4. analyze_agent_result â†’ 5. check_task_completion â†’ 6. provide_final_answer

ğŸ’¡ **é‡è¦æç¤º**: ä½œä¸ºåè°ƒè€…ï¼Œä¸»è¦è´Ÿè´£ä»»åŠ¡åˆ†é…å’Œç»“æœåˆ†æ
"""
        
        else:
            # æœªçŸ¥agentç±»å‹ï¼Œè¿”å›é€šç”¨æŒ‡å¯¼
            return """
**ğŸ› ï¸ é€šç”¨å·¥å…·ä½¿ç”¨æŒ‡å¯¼**

âš ï¸ **é‡è¦æç¤º**: æœªè¯†åˆ«çš„æ™ºèƒ½ä½“ç±»å‹ï¼Œè¯·ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„å·¥å…·è°ƒç”¨æ–¹å¼ã€‚

ğŸ“‹ **åŸºæœ¬åŸåˆ™**:
- ä¸¥æ ¼æŒ‰ç…§å·¥å…·çš„JSON schemaè°ƒç”¨
- ç¡®ä¿å‚æ•°ç±»å‹å’Œæ ¼å¼æ­£ç¡®
- é¿å…è°ƒç”¨ä¸å­˜åœ¨çš„å·¥å…·
- è¯¦ç»†é˜…è¯»å·¥å…·æè¿°å’Œå‚æ•°è¯´æ˜

å¦‚éœ€è·å–å…·ä½“çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼ï¼Œè¯·è°ƒç”¨ get_tool_usage_guide å·¥å…·ã€‚
"""
    
    async def _tool_analyze_agent_result(self, agent_id: str, result: Dict[str, Any],
                                       task_context: Dict[str, Any] = None,
                                       quality_threshold: float = 80.0) -> Dict[str, Any]:
        """åˆ†ææ™ºèƒ½ä½“æ‰§è¡Œç»“æœçš„è´¨é‡å’Œå®Œæ•´æ€§"""
        
        try:
            self.logger.info(f"ğŸ” æ·±åº¦åˆ†ææ™ºèƒ½ä½“ {agent_id} çš„æ‰§è¡Œç»“æœ")
            
            # æ£€æŸ¥æ™ºèƒ½ä½“æ˜¯å¦å­˜åœ¨
            if agent_id not in self.registered_agents:
                return {
                    "success": False,
                    "error": f"æ™ºèƒ½ä½“ä¸å­˜åœ¨: {agent_id}"
                }
            
            # è·å–ä»»åŠ¡ä¸Šä¸‹æ–‡
            task_context_obj = None
            if task_context and "task_id" in task_context:
                task_id = task_context["task_id"]
                if task_id in self.active_tasks:
                    task_context_obj = self.active_tasks[task_id]
            
            # æ‰§è¡Œå¢å¼ºçš„ç»“æœè´¨é‡åˆ†æ
            analysis = self._enhanced_result_quality_analysis(result, task_context, quality_threshold)
            
            # ğŸ”§ ä¿®å¤6: ä»»åŠ¡å¹»è§‰æ£€æµ‹
            hallucination_check = self._detect_task_hallucination(agent_id, result, task_context)
            if hallucination_check["has_hallucination"]:
                self.logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°æ™ºèƒ½ä½“ {agent_id} ä»»åŠ¡å¹»è§‰: {hallucination_check['hallucination_type']}")
                analysis["hallucination_detected"] = True
                analysis["hallucination_details"] = hallucination_check
                analysis["quality_score"] = min(analysis.get("quality_score", 0), 30.0)  # å¤§å¹…é™ä½è´¨é‡åˆ†æ•°
                analysis["completeness"] = "failed"
                analysis["issues"].append(f"ä»»åŠ¡å¹»è§‰: {hallucination_check['description']}")
                analysis["recommendations"].append("é‡æ–°åˆ†é…ä»»åŠ¡ï¼Œæ˜ç¡®èƒ½åŠ›è¾¹ç•Œ")
            else:
                analysis["hallucination_detected"] = False
            
            # æ›´æ–°æ™ºèƒ½ä½“æ€§èƒ½
            self._update_agent_performance(agent_id, result)
            
            # å¦‚æœä»»åŠ¡ä¸Šä¸‹æ–‡å­˜åœ¨ï¼Œè®°å½•åˆ†æç»“æœ
            if task_context_obj:
                task_context_obj.agent_results[agent_id] = {
                    "result": result,
                    "analysis": analysis,
                    "timestamp": time.time()
                }
            
            return {
                "success": True,
                "analysis": analysis,
                "agent_id": agent_id,
                "quality_score": analysis["quality_score"],
                "completeness": analysis["completeness"],
                "issues": analysis["issues"],
                "strengths": analysis["strengths"],
                "recommendations": analysis["recommendations"],
                "next_action": self._determine_enhanced_next_action(analysis, task_context),
                "detailed_metrics": analysis["detailed_metrics"],
                "risk_assessment": analysis["risk_assessment"]
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ç»“æœåˆ†æå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _update_agent_performance(self, agent_id: str, result: Dict[str, Any]):
        """æ›´æ–°æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ ‡"""
        if agent_id not in self.registered_agents:
            return
        
        agent_info = self.registered_agents[agent_id]
        
        # æ›´æ–°æ‰§è¡Œæ—¶é—´
        execution_time = result.get("execution_time", 0)
        if execution_time > 0:
            agent_info.total_execution_time += execution_time
            total_tasks = agent_info.success_count + agent_info.failure_count
            if total_tasks > 0:
                agent_info.average_response_time = agent_info.total_execution_time / total_tasks
        
        # æ›´æ–°æˆåŠŸ/å¤±è´¥ç»Ÿè®¡
        if result.get("success", False):
            agent_info.success_count += 1
            agent_info.last_success_time = time.time()
            agent_info.consecutive_successes += 1
            agent_info.consecutive_failures = 0
        else:
            agent_info.failure_count += 1
            agent_info.last_failure_time = time.time()
            agent_info.consecutive_failures += 1
            agent_info.consecutive_successes = 0
    
    def _enhanced_result_quality_analysis(self, result: Dict[str, Any], 
                                        task_context: Dict[str, Any],
                                        quality_threshold: float) -> Dict[str, Any]:
        """å¢å¼ºçš„ç»“æœè´¨é‡åˆ†æ - åŒ…å«æ–‡ä»¶éªŒè¯ã€å®é™…æ‰§è¡Œæ£€æŸ¥å’Œä»£ç æµ‹è¯•æµç¨‹æ£€æŸ¥"""
        
        analysis = {
            "quality_score": 0.0,
            "completeness": "unknown",
            "issues": [],
            "strengths": [],
            "recommendations": [],
            "detailed_metrics": {},
            "risk_assessment": "low",
            "file_verification": {},
            "actual_execution_check": {},
            "code_testing_workflow": {}
        }
        
        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿resultå‚æ•°æ˜¯å­—å…¸ç±»å‹
        if result is None:
            result = {}
        elif isinstance(result, str):
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºå­—å…¸
            try:
                import json
                result = json.loads(result)
            except:
                result = {"raw_response": result}
        
        # æ£€æŸ¥ç»“æœæ˜¯å¦ä¸ºç©ºæˆ–å¤±è´¥
        if not result or not result.get("success", False):
            analysis["completeness"] = "failed"
            analysis["issues"].append("ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            analysis["recommendations"].append("é‡æ–°åˆ†é…ä»»åŠ¡æˆ–æ›´æ¢æ™ºèƒ½ä½“")
            analysis["risk_assessment"] = "high"
            return analysis
        
        # è·å–åŸå§‹ä»»åŠ¡éœ€æ±‚
        original_request = task_context.get("original_request", "") if task_context else ""
        
        # åˆ†æç»“æœå†…å®¹
        result_content = result.get("result", "") if isinstance(result.get("result"), str) else str(result.get("result", ""))
        if not result_content:
            analysis["completeness"] = "incomplete"
            analysis["issues"].append("ç»“æœå†…å®¹ä¸ºç©º")
            analysis["recommendations"].append("è¦æ±‚æ™ºèƒ½ä½“é‡æ–°æ‰§è¡Œå¹¶æä¾›è¯¦ç»†ç»“æœ")
            analysis["risk_assessment"] = "medium"
            return analysis
        
        # æ‰§è¡Œæ–‡ä»¶éªŒè¯ã€å®é™…æ‰§è¡Œæ£€æŸ¥å’Œä»£ç æµ‹è¯•æµç¨‹æ£€æŸ¥
        file_verification = self._verify_file_generation(result_content, original_request)
        execution_check = self._check_actual_execution(result_content, original_request)
        testing_workflow = self._check_code_testing_workflow(result_content, original_request, task_context)
        
        # ğŸ¯ æ–°å¢ï¼šå†…å®¹ä¸Šä¸‹æ–‡éªŒè¯ - æ£€æŸ¥æ™ºèƒ½ä½“æ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„æ–‡ä»¶å†…å®¹
        content_verification = self._verify_content_context(result_content, task_context)
        
        analysis["file_verification"] = file_verification
        analysis["actual_execution_check"] = execution_check
        analysis["code_testing_workflow"] = testing_workflow
        analysis["content_verification"] = content_verification
        
        # è¯¦ç»†è´¨é‡æŒ‡æ ‡åˆ†æ
        detailed_metrics = self._analyze_detailed_metrics(result_content, result, file_verification, execution_check)
        analysis["detailed_metrics"] = detailed_metrics
        
        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°ï¼ˆåŒ…å«å®é™…æ‰§è¡Œã€æµ‹è¯•æµç¨‹å’Œå†…å®¹éªŒè¯æƒé‡ï¼‰
        quality_score = self._calculate_comprehensive_quality_score(detailed_metrics, file_verification, execution_check, testing_workflow, content_verification)
        analysis["quality_score"] = quality_score
        
        # åˆ†æé—®é¢˜å’Œä¼˜åŠ¿
        analysis["issues"] = self._identify_quality_issues(detailed_metrics, file_verification, execution_check, testing_workflow, original_request, content_verification)
        analysis["strengths"] = self._identify_quality_strengths(detailed_metrics, file_verification, execution_check, testing_workflow, content_verification)
        
        # æ ¹æ®è´¨é‡åˆ†æ•°ã€å®é™…æ‰§è¡Œæƒ…å†µã€æµ‹è¯•æµç¨‹å®Œæ•´æ€§å’Œå†…å®¹éªŒè¯åˆ¤æ–­å®Œæ•´æ€§
        if not file_verification.get("all_required_files_generated", False):
            analysis["completeness"] = "incomplete"
            analysis["risk_assessment"] = "high"
            analysis["issues"].append("æœªå®é™…ç”Ÿæˆæ‰€éœ€æ–‡ä»¶")
        elif not content_verification.get("correct_content_used", False) and content_verification.get("content_mismatch_detected", False):
            analysis["completeness"] = "failed"
            analysis["risk_assessment"] = "high"
            analysis["issues"].append("æ™ºèƒ½ä½“ä½¿ç”¨äº†é”™è¯¯çš„æ–‡ä»¶å†…å®¹ï¼Œå¯èƒ½å‡ºç°å¹»è§‰é—®é¢˜")
        elif not execution_check.get("simulation_actually_executed", False) and "æµ‹è¯•å°" in original_request:
            analysis["completeness"] = "incomplete" 
            analysis["risk_assessment"] = "high"
            analysis["issues"].append("æœªå®é™…æ‰§è¡Œä»¿çœŸéªŒè¯")
        elif testing_workflow.get("workflow_completeness", 0) < 50 and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            analysis["completeness"] = "incomplete"
            analysis["risk_assessment"] = "high"
            analysis["issues"].append("ä»£ç æµ‹è¯•æµç¨‹ä¸å®Œæ•´")
        elif quality_score >= quality_threshold:
            analysis["completeness"] = "complete"
            analysis["risk_assessment"] = "low"
        elif quality_score >= quality_threshold * 0.7:
            analysis["completeness"] = "partial"
            analysis["risk_assessment"] = "medium"
        else:
            analysis["completeness"] = "incomplete"
            analysis["risk_assessment"] = "high"
        
        # ç”Ÿæˆå…·ä½“å»ºè®®
        analysis["recommendations"] = self._generate_enhanced_recommendations(
            detailed_metrics, quality_score, quality_threshold, 
            file_verification, execution_check, testing_workflow, original_request
        )
        
        return analysis
    
    def _verify_file_generation(self, result_content: str, original_request: str) -> Dict[str, Any]:
        """éªŒè¯æ˜¯å¦å®é™…ç”Ÿæˆäº†æ‰€éœ€æ–‡ä»¶"""
        verification = {
            "verilog_file_mentioned": False,
            "testbench_file_mentioned": False,
            "files_actually_written": False,
            "all_required_files_generated": False,
            "missing_files": []
        }
        
        # æ£€æŸ¥æ˜¯å¦æåˆ°äº†Verilogæ–‡ä»¶
        if ".v" in result_content or "module" in result_content.lower():
            verification["verilog_file_mentioned"] = True
        
        # æ£€æŸ¥æ˜¯å¦æåˆ°äº†æµ‹è¯•å°æ–‡ä»¶
        if ("testbench" in result_content.lower() or "tb_" in result_content.lower() or 
            "_tb.v" in result_content or "test" in result_content.lower()):
            verification["testbench_file_mentioned"] = True
        
        # æ£€æŸ¥æ˜¯å¦å®é™…æ‰§è¡Œäº†æ–‡ä»¶å†™å…¥æ“ä½œ
        # é€šè¿‡æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨çš„è¿¹è±¡
        if ("write_file" in result_content.lower() or 
            "æ–‡ä»¶å·²ç”Ÿæˆ" in result_content or "æ–‡ä»¶åˆ›å»º" in result_content or
            "ä¿å­˜åˆ°" in result_content or "å†™å…¥æ–‡ä»¶" in result_content):
            verification["files_actually_written"] = True
        
        # åˆ†æåŸå§‹éœ€æ±‚ï¼Œç¡®å®šéœ€è¦çš„æ–‡ä»¶
        required_files = []
        if "verilog" in original_request.lower() or "æ¨¡å—" in original_request:
            required_files.append("verilog_module")
        if ("æµ‹è¯•å°" in original_request or "testbench" in original_request.lower() or 
            "éªŒè¯" in original_request or "test" in original_request.lower()):
            required_files.append("testbench")
        
        # æ£€æŸ¥æ‰€æœ‰å¿…éœ€æ–‡ä»¶æ˜¯å¦éƒ½ç”Ÿæˆäº†
        missing_files = []
        if "verilog_module" in required_files and not verification["verilog_file_mentioned"]:
            missing_files.append("Verilogæ¨¡å—æ–‡ä»¶")
        if "testbench" in required_files and not verification["testbench_file_mentioned"]:
            missing_files.append("æµ‹è¯•å°æ–‡ä»¶")
        
        verification["missing_files"] = missing_files
        verification["all_required_files_generated"] = (len(missing_files) == 0 and 
                                                       verification["files_actually_written"])
        
        return verification
    
    def _check_actual_execution(self, result_content: str, original_request: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ˜¯å¦å®é™…æ‰§è¡Œäº†æ‰€éœ€æ“ä½œ"""
        execution_check = {
            "simulation_mentioned": False,
            "simulation_actually_executed": False,
            "tools_called": False,
            "concrete_results_provided": False,
            "missing_executions": []
        }
        
        # æ£€æŸ¥æ˜¯å¦æåˆ°äº†ä»¿çœŸ
        if ("ä»¿çœŸ" in result_content or "simulation" in result_content.lower() or 
            "run_simulation" in result_content or "æ‰§è¡Œä»¿çœŸ" in result_content):
            execution_check["simulation_mentioned"] = True
        
        # æ£€æŸ¥æ˜¯å¦å®é™…æ‰§è¡Œäº†ä»¿çœŸï¼ˆé€šè¿‡å…·ä½“è¾“å‡ºåˆ¤æ–­ï¼‰
        if (("ä»¿çœŸç»“æœ" in result_content or "simulation result" in result_content.lower() or
             "æ³¢å½¢" in result_content or "waveform" in result_content.lower() or
             "ä»¿çœŸè¾“å‡º" in result_content or "æ—¶åº" in result_content) and 
            execution_check["simulation_mentioned"]):
            execution_check["simulation_actually_executed"] = True
        
        # æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†å·¥å…·
        if ("å·¥å…·è°ƒç”¨" in result_content or "tool_call" in result_content.lower() or
            "æ‰§è¡Œå·¥å…·" in result_content or "function calling" in result_content.lower()):
            execution_check["tools_called"] = True
        
        # æ£€æŸ¥æ˜¯å¦æä¾›äº†å…·ä½“ç»“æœ
        if ("æ‰§è¡ŒæˆåŠŸ" in result_content and "ç»“æœ" in result_content) or "è¾“å‡º:" in result_content:
            execution_check["concrete_results_provided"] = True
        
        # åˆ†æç¼ºå¤±çš„æ‰§è¡Œé¡¹
        missing_executions = []
        if ("æµ‹è¯•å°" in original_request or "éªŒè¯" in original_request) and not execution_check["simulation_actually_executed"]:
            missing_executions.append("ä»¿çœŸéªŒè¯æ‰§è¡Œ")
        if ("æ–‡ä»¶" in original_request or "ç”Ÿæˆ" in original_request) and not execution_check["tools_called"]:
            missing_executions.append("æ–‡ä»¶ç”Ÿæˆæ“ä½œ")
        
        execution_check["missing_executions"] = missing_executions
        
        return execution_check
    
    def _check_code_testing_workflow(self, result_content: str, original_request: str, task_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ä¸“é—¨æ£€æŸ¥ä»£ç æµ‹è¯•æµç¨‹çš„å®Œæ•´æ€§"""
        testing_workflow = {
            "test_plan_created": False,
            "test_cases_designed": False,
            "testbench_generated": False,
            "simulation_executed": False,
            "test_results_analyzed": False,
            "coverage_analysis_performed": False,
            "test_report_generated": False,
            "workflow_completeness": 0.0,
            "missing_testing_steps": [],
            "testing_quality_score": 0.0,
            "test_coverage_metrics": {},
            "test_execution_details": {}
        }
        
        # 1. æ£€æŸ¥æµ‹è¯•è®¡åˆ’åˆ›å»º
        test_plan_indicators = [
            "æµ‹è¯•è®¡åˆ’", "test plan", "æµ‹è¯•ç­–ç•¥", "test strategy", 
            "æµ‹è¯•ç›®æ ‡", "test objectives", "æµ‹è¯•èŒƒå›´", "test scope"
        ]
        if any(indicator in result_content for indicator in test_plan_indicators):
            testing_workflow["test_plan_created"] = True
        
        # 2. æ£€æŸ¥æµ‹è¯•ç”¨ä¾‹è®¾è®¡
        test_case_indicators = [
            "æµ‹è¯•ç”¨ä¾‹", "test case", "æµ‹è¯•å‘é‡", "test vector",
            "è¾¹ç•Œæµ‹è¯•", "boundary test", "åŠŸèƒ½æµ‹è¯•", "functional test",
            "æ—¶åºæµ‹è¯•", "timing test", "å¼‚å¸¸æµ‹è¯•", "exception test"
        ]
        if any(indicator in result_content for indicator in test_case_indicators):
            testing_workflow["test_cases_designed"] = True
        
        # 3. æ£€æŸ¥æµ‹è¯•å°ç”Ÿæˆ
        testbench_indicators = [
            "testbench", "æµ‹è¯•å°", "tb_", "_tb.v", "initial", "always",
            "test stimulus", "æµ‹è¯•æ¿€åŠ±", "æ—¶é’Ÿç”Ÿæˆ", "clock generation"
        ]
        if any(indicator in result_content for indicator in testbench_indicators):
            testing_workflow["testbench_generated"] = True
        
        # 4. æ£€æŸ¥ä»¿çœŸæ‰§è¡Œ
        simulation_indicators = [
            "ä»¿çœŸæ‰§è¡Œ", "simulation executed", "ä»¿çœŸç»“æœ", "simulation result",
            "æ³¢å½¢è¾“å‡º", "waveform output", "ä»¿çœŸå®Œæˆ", "simulation completed",
            "ä»¿çœŸæ—¶é—´", "simulation time", "ä»¿çœŸå‘¨æœŸ", "simulation cycles"
        ]
        if any(indicator in result_content for indicator in simulation_indicators):
            testing_workflow["simulation_executed"] = True
        
        # 5. æ£€æŸ¥æµ‹è¯•ç»“æœåˆ†æ
        result_analysis_indicators = [
            "æµ‹è¯•ç»“æœåˆ†æ", "test result analysis", "ç»“æœéªŒè¯", "result verification",
            "åŠŸèƒ½æ­£ç¡®æ€§", "functional correctness", "æ—¶åºæ­£ç¡®æ€§", "timing correctness",
            "æµ‹è¯•é€šè¿‡", "test passed", "æµ‹è¯•å¤±è´¥", "test failed"
        ]
        if any(indicator in result_content for indicator in result_analysis_indicators):
            testing_workflow["test_results_analyzed"] = True
        
        # 6. æ£€æŸ¥è¦†ç›–ç‡åˆ†æ
        coverage_indicators = [
            "ä»£ç è¦†ç›–ç‡", "code coverage", "åŠŸèƒ½è¦†ç›–ç‡", "functional coverage",
            "åˆ†æ”¯è¦†ç›–ç‡", "branch coverage", "è¯­å¥è¦†ç›–ç‡", "statement coverage",
            "è¦†ç›–ç‡æŠ¥å‘Š", "coverage report", "è¦†ç›–ç‡ç»Ÿè®¡", "coverage statistics"
        ]
        if any(indicator in result_content for indicator in coverage_indicators):
            testing_workflow["coverage_analysis_performed"] = True
        
        # 7. æ£€æŸ¥æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
        report_indicators = [
            "æµ‹è¯•æŠ¥å‘Š", "test report", "æµ‹è¯•æ€»ç»“", "test summary",
            "æµ‹è¯•ç»“è®º", "test conclusion", "æµ‹è¯•å»ºè®®", "test recommendations"
        ]
        if any(indicator in result_content for indicator in report_indicators):
            testing_workflow["test_report_generated"] = True
        
        # è®¡ç®—å·¥ä½œæµå®Œæ•´æ€§
        completed_steps = sum([
            testing_workflow["test_plan_created"],
            testing_workflow["test_cases_designed"],
            testing_workflow["testbench_generated"],
            testing_workflow["simulation_executed"],
            testing_workflow["test_results_analyzed"],
            testing_workflow["coverage_analysis_performed"],
            testing_workflow["test_report_generated"]
        ])
        testing_workflow["workflow_completeness"] = (completed_steps / 7.0) * 100
        
        # è¯†åˆ«ç¼ºå¤±çš„æµ‹è¯•æ­¥éª¤
        missing_steps = []
        if not testing_workflow["test_plan_created"]:
            missing_steps.append("æµ‹è¯•è®¡åˆ’åˆ›å»º")
        if not testing_workflow["test_cases_designed"]:
            missing_steps.append("æµ‹è¯•ç”¨ä¾‹è®¾è®¡")
        if not testing_workflow["testbench_generated"]:
            missing_steps.append("æµ‹è¯•å°ç”Ÿæˆ")
        if not testing_workflow["simulation_executed"]:
            missing_steps.append("ä»¿çœŸæ‰§è¡Œ")
        if not testing_workflow["test_results_analyzed"]:
            missing_steps.append("æµ‹è¯•ç»“æœåˆ†æ")
        if not testing_workflow["coverage_analysis_performed"]:
            missing_steps.append("è¦†ç›–ç‡åˆ†æ")
        if not testing_workflow["test_report_generated"]:
            missing_steps.append("æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ")
        
        testing_workflow["missing_testing_steps"] = missing_steps
        
        # è®¡ç®—æµ‹è¯•è´¨é‡åˆ†æ•°
        quality_factors = {
            "test_plan_created": 15,
            "test_cases_designed": 20,
            "testbench_generated": 25,
            "simulation_executed": 20,
            "test_results_analyzed": 10,
            "coverage_analysis_performed": 5,
            "test_report_generated": 5
        }
        
        quality_score = sum([
            quality_factors[step] for step, completed in testing_workflow.items() 
            if step in quality_factors and completed
        ])
        testing_workflow["testing_quality_score"] = quality_score
        
        # æå–è¦†ç›–ç‡æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        coverage_metrics = {}
        if "è¦†ç›–ç‡" in result_content or "coverage" in result_content.lower():
            # å°è¯•æå–å…·ä½“çš„è¦†ç›–ç‡æ•°å€¼
            import re
            coverage_patterns = [
                r"ä»£ç è¦†ç›–ç‡[ï¼š:]\s*(\d+(?:\.\d+)?)%",
                r"åŠŸèƒ½è¦†ç›–ç‡[ï¼š:]\s*(\d+(?:\.\d+)?)%",
                r"åˆ†æ”¯è¦†ç›–ç‡[ï¼š:]\s*(\d+(?:\.\d+)?)%",
                r"è¯­å¥è¦†ç›–ç‡[ï¼š:]\s*(\d+(?:\.\d+)?)%",
                r"code coverage[ï¼š:]\s*(\d+(?:\.\d+)?)%",
                r"functional coverage[ï¼š:]\s*(\d+(?:\.\d+)?)%"
            ]
            
            for pattern in coverage_patterns:
                matches = re.findall(pattern, result_content, re.IGNORECASE)
                if matches:
                    coverage_metrics["coverage_percentage"] = float(matches[0])
                    break
        
        testing_workflow["test_coverage_metrics"] = coverage_metrics
        
        # æå–æµ‹è¯•æ‰§è¡Œè¯¦æƒ…
        execution_details = {}
        if testing_workflow["simulation_executed"]:
            # å°è¯•æå–ä»¿çœŸæ—¶é—´ã€å‘¨æœŸç­‰ä¿¡æ¯
            import re
            time_patterns = [
                r"ä»¿çœŸæ—¶é—´[ï¼š:]\s*(\d+(?:\.\d+)?)",
                r"simulation time[ï¼š:]\s*(\d+(?:\.\d+)?)",
                r"ä»¿çœŸå‘¨æœŸ[ï¼š:]\s*(\d+)",
                r"simulation cycles[ï¼š:]\s*(\d+)"
            ]
            
            for pattern in time_patterns:
                matches = re.findall(pattern, result_content, re.IGNORECASE)
                if matches:
                    if "æ—¶é—´" in pattern or "time" in pattern:
                        execution_details["simulation_time"] = float(matches[0])
                    else:
                        execution_details["simulation_cycles"] = int(matches[0])
                    break
        
        testing_workflow["test_execution_details"] = execution_details
        
        return testing_workflow
    
    def _verify_content_context(self, result_content: str, task_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """éªŒè¯æ™ºèƒ½ä½“æ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„æ–‡ä»¶å†…å®¹ï¼Œæ£€æµ‹å¹»è§‰é—®é¢˜"""
        verification = {
            "correct_content_used": True,
            "content_mismatch_detected": False,
            "content_complexity_appropriate": True,
            "evidence_of_hallucination": False,
            "issues": [],
            "recommendations": []
        }
        
        try:
            # è·å–ä»»åŠ¡æ–‡ä»¶ä¸Šä¸‹æ–‡
            task_id = task_context.get("task_id") if task_context else None
            if not task_id:
                verification["issues"].append("æ— æ³•è·å–ä»»åŠ¡IDï¼Œæ— æ³•éªŒè¯å†…å®¹ä¸Šä¸‹æ–‡")
                return verification
            
            task_file_context = get_task_context(task_id)
            if not task_file_context or len(task_file_context) == 0:
                verification["issues"].append("æ²¡æœ‰æ‰¾åˆ°ä»»åŠ¡æ–‡ä»¶ä¸Šä¸‹æ–‡")
                return verification
            
            # è·å–å®é™…çš„è®¾è®¡æ–‡ä»¶å†…å®¹
            actual_design_content = task_file_context.get_primary_design_content()
            if not actual_design_content:
                verification["issues"].append("æ— æ³•è·å–å®é™…è®¾è®¡æ–‡ä»¶å†…å®¹")
                return verification
            
            # æ£€æŸ¥ç»“æœä¸­æ˜¯å¦åŒ…å«äº†å®é™…æ–‡ä»¶çš„ç‰¹å¾å†…å®¹
            # æå–å®é™…æ–‡ä»¶ä¸­çš„å…³é”®ç‰¹å¾ï¼šç«¯å£åã€ä¿¡å·åã€æ¨¡å—åç­‰
            import re
            
            # æå–æ¨¡å—å
            actual_module_match = re.search(r'module\s+(\w+)', actual_design_content)
            actual_module_name = actual_module_match.group(1) if actual_module_match else None
            
            # æå–ç«¯å£å£°æ˜
            actual_ports = re.findall(r'(?:input|output|inout)\s+.*?(\w+)(?:\s*,|\s*\))', actual_design_content)
            actual_ports = [port.strip() for port in actual_ports if port]
            
            # æå–ä¿¡å·å£°æ˜
            actual_signals = re.findall(r'(?:reg|wire)\s+.*?(\w+)', actual_design_content)
            actual_signals = [signal.strip() for signal in actual_signals if signal]
            
            # æ£€æŸ¥ç»“æœå†…å®¹ä¸­æ˜¯å¦åŒ¹é…è¿™äº›ç‰¹å¾
            if actual_module_name:
                if actual_module_name not in result_content:
                    verification["content_mismatch_detected"] = True
                    verification["issues"].append(f"ç»“æœä¸­æœªæåŠå®é™…æ¨¡å—å '{actual_module_name}'")
            
            # æ£€æŸ¥ç«¯å£åŒ¹é…åº¦
            if actual_ports:
                matched_ports = [port for port in actual_ports if port in result_content]
                port_match_ratio = len(matched_ports) / len(actual_ports) if actual_ports else 0
                
                if port_match_ratio < 0.5:  # å°‘äº50%çš„ç«¯å£è¢«æåŠ
                    verification["content_mismatch_detected"] = True
                    verification["issues"].append(f"ç«¯å£åŒ¹é…åº¦è¿‡ä½: {port_match_ratio:.2%}, å®é™…ç«¯å£: {actual_ports[:5]}")
            
            # æ£€æŸ¥å†…å®¹å¤æ‚åº¦æ˜¯å¦é€‚å½“
            actual_line_count = len(actual_design_content.splitlines())
            actual_complexity_score = len(re.findall(r'\b(?:if|case|for|while|always|assign)\b', actual_design_content))
            
            # å¦‚æœç»“æœä¸­æè¿°çš„ä»£ç è¿‡äºç®€å•ï¼Œå¯èƒ½æ˜¯å¹»è§‰
            if "ç®€å•" in result_content and actual_complexity_score > 10:
                verification["content_complexity_appropriate"] = False
                verification["issues"].append(f"å®é™…ä»£ç å¤æ‚åº¦é«˜({actual_complexity_score}ä¸ªæ§åˆ¶ç»“æ„)ï¼Œä½†ç»“æœæè¿°ä¸ºç®€å•")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„å¹»è§‰è¿¹è±¡
            # 1. ç»“æœä¸­å‡ºç°äº†å®é™…æ–‡ä»¶ä¸­ä¸å­˜åœ¨çš„ä¿¡å·å
            result_mentioned_signals = re.findall(r'\b(?:input|output|reg|wire)\s+.*?(\w+)', result_content)
            result_mentioned_signals = [signal.strip() for signal in result_mentioned_signals if signal]
            
            if result_mentioned_signals and actual_signals:
                fictional_signals = [sig for sig in result_mentioned_signals if sig not in actual_design_content]
                if len(fictional_signals) > len(result_mentioned_signals) * 0.3:  # è¶…è¿‡30%æ˜¯è™šæ„ä¿¡å·
                    verification["evidence_of_hallucination"] = True
                    verification["issues"].append(f"æ£€æµ‹åˆ°å¯èƒ½çš„å¹»è§‰ä¿¡å·: {fictional_signals[:3]}")
            
            # 2. æ£€æŸ¥åŠŸèƒ½æè¿°æ˜¯å¦ä¸å®é™…ä»£ç åŒ¹é…
            if "è®¡æ•°å™¨" in result_content:
                if "count" not in actual_design_content.lower() and "cnt" not in actual_design_content.lower():
                    verification["evidence_of_hallucination"] = True
                    verification["issues"].append("æè¿°ä¸ºè®¡æ•°å™¨ä½†å®é™…ä»£ç ä¸­æ— è®¡æ•°ç›¸å…³ä¿¡å·")
            
            # ç»¼åˆè¯„ä¼°
            if verification["content_mismatch_detected"] or verification["evidence_of_hallucination"]:
                verification["correct_content_used"] = False
                verification["recommendations"].extend([
                    "é‡æ–°æ‰§è¡Œä»»åŠ¡ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ–‡ä»¶å†…å®¹",
                    "æ£€æŸ¥æ™ºèƒ½ä½“çš„ä¸Šä¸‹æ–‡ä¼ é€’æœºåˆ¶",
                    "éªŒè¯æ–‡ä»¶è¯»å–å·¥å…·çš„æ­£ç¡®æ€§"
                ])
            
            # è®°å½•éªŒè¯è¯¦æƒ…
            verification["details"] = {
                "actual_module_name": actual_module_name,
                "actual_ports_count": len(actual_ports),
                "actual_signals_count": len(actual_signals),
                "actual_complexity_score": actual_complexity_score,
                "actual_line_count": actual_line_count
            }
            
        except Exception as e:
            self.logger.error(f"å†…å®¹ä¸Šä¸‹æ–‡éªŒè¯å¤±è´¥: {e}")
            verification["issues"].append(f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
            verification["correct_content_used"] = False
        
        return verification
    
    def _identify_quality_issues(self, metrics: Dict[str, Any], file_verification: Dict[str, Any], 
                               execution_check: Dict[str, Any], testing_workflow: Dict[str, Any], original_request: str, content_verification: Dict[str, Any] = None) -> List[str]:
        """è¯†åˆ«è´¨é‡é—®é¢˜"""
        issues = []
        
        # æ–‡ä»¶ç”Ÿæˆé—®é¢˜
        if file_verification.get("missing_files"):
            issues.extend([f"ç¼ºå¤±{file}" for file in file_verification["missing_files"]])
        
        # æ‰§è¡Œé—®é¢˜
        if execution_check.get("missing_executions"):
            issues.extend(execution_check["missing_executions"])
        
        # ä»£ç æµ‹è¯•æµç¨‹é—®é¢˜
        if testing_workflow.get("missing_testing_steps"):
            issues.extend([f"æµ‹è¯•æµç¨‹ç¼ºå¤±: {step}" for step in testing_workflow["missing_testing_steps"]])
        
        if testing_workflow.get("workflow_completeness", 0) < 50 and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            issues.append(f"æµ‹è¯•æµç¨‹å®Œæ•´æ€§ä¸è¶³ ({testing_workflow['workflow_completeness']:.1f}%)")
        
        if testing_workflow.get("testing_quality_score", 0) < 50 and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            issues.append(f"æµ‹è¯•è´¨é‡åˆ†æ•°è¿‡ä½ ({testing_workflow['testing_quality_score']:.1f})")
        
        # è´¨é‡æŒ‡æ ‡é—®é¢˜
        if metrics.get("test_coverage", 0) < 50 and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            issues.append("æµ‹è¯•è¦†ç›–ç‡ä¸è¶³")
        
        if metrics.get("code_quality", 0) < 50:
            issues.append("ä»£ç è´¨é‡éœ€è¦æå‡")
        
        return issues
    
    def _identify_quality_strengths(self, metrics: Dict[str, Any], file_verification: Dict[str, Any], 
                                  execution_check: Dict[str, Any], testing_workflow: Dict[str, Any], content_verification: Dict[str, Any] = None) -> List[str]:
        """è¯†åˆ«è´¨é‡ä¼˜åŠ¿"""
        strengths = []
        
        if file_verification.get("all_required_files_generated"):
            strengths.append("æ‰€æœ‰å¿…éœ€æ–‡ä»¶å·²ç”Ÿæˆ")
        
        if execution_check.get("simulation_actually_executed"):
            strengths.append("ä»¿çœŸéªŒè¯å·²æ‰§è¡Œ")
        
        # ä»£ç æµ‹è¯•æµç¨‹ä¼˜åŠ¿
        if testing_workflow.get("workflow_completeness", 0) >= 80:
            strengths.append(f"æµ‹è¯•æµç¨‹å®Œæ•´æ€§ä¼˜ç§€ ({testing_workflow['workflow_completeness']:.1f}%)")
        
        if testing_workflow.get("testing_quality_score", 0) >= 80:
            strengths.append(f"æµ‹è¯•è´¨é‡åˆ†æ•°ä¼˜ç§€ ({testing_workflow['testing_quality_score']:.1f})")
        
        if testing_workflow.get("test_plan_created"):
            strengths.append("æµ‹è¯•è®¡åˆ’å·²åˆ›å»º")
        
        if testing_workflow.get("test_cases_designed"):
            strengths.append("æµ‹è¯•ç”¨ä¾‹å·²è®¾è®¡")
        
        if testing_workflow.get("testbench_generated"):
            strengths.append("æµ‹è¯•å°å·²ç”Ÿæˆ")
        
        if testing_workflow.get("simulation_executed"):
            strengths.append("ä»¿çœŸå·²æ‰§è¡Œ")
        
        if testing_workflow.get("test_results_analyzed"):
            strengths.append("æµ‹è¯•ç»“æœå·²åˆ†æ")
        
        if testing_workflow.get("coverage_analysis_performed"):
            strengths.append("è¦†ç›–ç‡åˆ†æå·²å®Œæˆ")
        
        if testing_workflow.get("test_report_generated"):
            strengths.append("æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ")
        
        if metrics.get("code_quality", 0) >= 80:
            strengths.append("ä»£ç è´¨é‡ä¼˜ç§€")
        
        if metrics.get("documentation_quality", 0) >= 70:
            strengths.append("æ–‡æ¡£è´¨é‡è‰¯å¥½")
        
        return strengths
    
    def _generate_enhanced_recommendations(self, metrics: Dict[str, Any], quality_score: float, 
                                         quality_threshold: float, file_verification: Dict[str, Any],
                                         execution_check: Dict[str, Any], testing_workflow: Dict[str, Any], original_request: str) -> List[str]:
        """ç”Ÿæˆå¢å¼ºçš„æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºæ–‡ä»¶éªŒè¯çš„å»ºè®®
        if not file_verification.get("all_required_files_generated"):
            if file_verification.get("missing_files"):
                recommendations.append(f"éœ€è¦è°ƒç”¨enhanced_real_code_review_agentç”Ÿæˆç¼ºå¤±çš„æ–‡ä»¶: {', '.join(file_verification['missing_files'])}")
            else:
                recommendations.append("éœ€è¦ä½¿ç”¨write_fileå·¥å…·å®é™…ç”Ÿæˆæ–‡ä»¶ï¼Œè€Œéä»…åœ¨æŠ¥å‘Šä¸­æè¿°")
        
        # åŸºäºæ‰§è¡Œæ£€æŸ¥çš„å»ºè®®
        if not execution_check.get("simulation_actually_executed") and ("æµ‹è¯•å°" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦è°ƒç”¨enhanced_real_code_review_agentæ‰§è¡Œå®é™…çš„ä»¿çœŸéªŒè¯")
        
        # åŸºäºä»£ç æµ‹è¯•æµç¨‹çš„å»ºè®®
        if testing_workflow.get("workflow_completeness", 0) < 50 and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            missing_steps = testing_workflow.get("missing_testing_steps", [])
            if missing_steps:
                recommendations.append(f"éœ€è¦å®Œå–„æµ‹è¯•æµç¨‹ï¼Œç¼ºå¤±æ­¥éª¤: {', '.join(missing_steps)}")
            else:
                recommendations.append("éœ€è¦å»ºç«‹å®Œæ•´çš„ä»£ç æµ‹è¯•æµç¨‹")
        
        if not testing_workflow.get("test_plan_created") and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦åˆ›å»ºè¯¦ç»†çš„æµ‹è¯•è®¡åˆ’ï¼ŒåŒ…æ‹¬æµ‹è¯•ç›®æ ‡ã€ç­–ç•¥å’ŒèŒƒå›´")
        
        if not testing_workflow.get("test_cases_designed") and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦è®¾è®¡å…¨é¢çš„æµ‹è¯•ç”¨ä¾‹ï¼ŒåŒ…æ‹¬åŠŸèƒ½æµ‹è¯•ã€è¾¹ç•Œæµ‹è¯•å’Œå¼‚å¸¸æµ‹è¯•")
        
        if not testing_workflow.get("testbench_generated") and ("æµ‹è¯•å°" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•å°æ–‡ä»¶ï¼ŒåŒ…å«æµ‹è¯•æ¿€åŠ±å’Œç»“æœéªŒè¯")
        
        if not testing_workflow.get("simulation_executed") and ("ä»¿çœŸ" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦å®é™…æ‰§è¡Œä»¿çœŸéªŒè¯ï¼Œå¹¶æä¾›ä»¿çœŸç»“æœå’Œæ³¢å½¢åˆ†æ")
        
        if not testing_workflow.get("test_results_analyzed") and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦å¯¹æµ‹è¯•ç»“æœè¿›è¡Œè¯¦ç»†åˆ†æï¼ŒéªŒè¯åŠŸèƒ½æ­£ç¡®æ€§å’Œæ—¶åºæ­£ç¡®æ€§")
        
        if not testing_workflow.get("coverage_analysis_performed") and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦è¿›è¡Œä»£ç è¦†ç›–ç‡åˆ†æï¼Œç¡®ä¿æµ‹è¯•çš„å®Œæ•´æ€§")
        
        if not testing_workflow.get("test_report_generated") and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Šï¼ŒåŒ…å«æµ‹è¯•æ€»ç»“ã€ç»“è®ºå’Œå»ºè®®")
        
        # åŸºäºæµ‹è¯•è´¨é‡åˆ†æ•°çš„å»ºè®®
        if testing_workflow.get("testing_quality_score", 0) < 50:
            recommendations.append("æµ‹è¯•è´¨é‡éœ€è¦æ˜¾è‘—æå‡ï¼Œå»ºè®®é‡æ–°è®¾è®¡æµ‹è¯•ç­–ç•¥")
        elif testing_workflow.get("testing_quality_score", 0) < 80:
            recommendations.append("æµ‹è¯•è´¨é‡è‰¯å¥½ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
        
        # åŸºäºè´¨é‡åˆ†æ•°çš„å»ºè®®
        if quality_score < quality_threshold:
            if metrics.get("test_coverage", 0) < 50:
                recommendations.append("éœ€è¦å¢å¼ºæµ‹è¯•è¦†ç›–ç‡ï¼Œæ·»åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹")
            if metrics.get("code_quality", 0) < 60:
                recommendations.append("éœ€è¦æ”¹è¿›ä»£ç è´¨é‡ï¼Œå¢åŠ æ³¨é‡Šå’Œä¼˜åŒ–ç»“æ„")
        
        # åä½œå»ºè®®
        if len(recommendations) > 0:
            recommendations.append("å»ºè®®å¯åŠ¨å¤šæ™ºèƒ½ä½“åä½œæ¨¡å¼ä»¥ç¡®ä¿ä»»åŠ¡å®Œæ•´æ€§")
        
        return recommendations
    
    def _analyze_detailed_metrics(self, content: str, result: Dict[str, Any], 
                                 file_verification: Dict[str, Any] = None, 
                                 execution_check: Dict[str, Any] = None) -> Dict[str, Any]:
        """åˆ†æè¯¦ç»†è´¨é‡æŒ‡æ ‡"""
        metrics = {
            "code_quality": 0.0,
            "documentation_quality": 0.0,
            "test_coverage": 0.0,
            "error_handling": 0.0,
            "performance": 0.0,
            "compliance": 0.0
        }
        
        content_lower = content.lower()
        
        # ä»£ç è´¨é‡è¯„ä¼°
        if "module" in content_lower and "endmodule" in content_lower:
            metrics["code_quality"] += 30
        if "//" in content or "/*" in content:
            metrics["code_quality"] += 20
        if "parameter" in content_lower:
            metrics["code_quality"] += 10
        if "always" in content_lower or "assign" in content_lower:
            metrics["code_quality"] += 20
        
        # æ–‡æ¡£è´¨é‡è¯„ä¼°
        if "module" in content_lower and "description" in content_lower:
            metrics["documentation_quality"] += 30
        if "//" in content and len(content.split("//")) > 5:
            metrics["documentation_quality"] += 20
        if "åŠŸèƒ½" in content or "function" in content_lower:
            metrics["documentation_quality"] += 20
        
        # æµ‹è¯•è¦†ç›–è¯„ä¼°
        if "testbench" in content_lower or "test" in content_lower:
            metrics["test_coverage"] += 40
        if "simulation" in content_lower or "ä»¿çœŸ" in content:
            metrics["test_coverage"] += 30
        if "verification" in content_lower or "éªŒè¯" in content:
            metrics["test_coverage"] += 30
        
        # é”™è¯¯å¤„ç†è¯„ä¼°
        if "error" in content_lower and "fix" in content_lower:
            metrics["error_handling"] += 40
        if "exception" in content_lower or "å¼‚å¸¸" in content:
            metrics["error_handling"] += 30
        if "check" in content_lower or "æ£€æŸ¥" in content:
            metrics["error_handling"] += 30
        
        # æ€§èƒ½è¯„ä¼°
        execution_time = result.get("execution_time", 0)
        if execution_time > 0:
            if execution_time < 30:
                metrics["performance"] = 100
            elif execution_time < 60:
                metrics["performance"] = 80
            elif execution_time < 120:
                metrics["performance"] = 60
            else:
                metrics["performance"] = 40
        
        # åˆè§„æ€§è¯„ä¼°
        if "verilog" in content_lower or "systemverilog" in content_lower:
            metrics["compliance"] += 50
        if "ieee" in content_lower or "æ ‡å‡†" in content:
            metrics["compliance"] += 30
        if "synthesis" in content_lower or "ç»¼åˆ" in content:
            metrics["compliance"] += 20
        
        return metrics
    
    def _calculate_comprehensive_quality_score(self, metrics: Dict[str, float], 
                                             file_verification: Dict[str, Any] = None,
                                             execution_check: Dict[str, Any] = None,
                                             testing_workflow: Dict[str, Any] = None,
                                             content_verification: Dict[str, Any] = None) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•° - åŒ…å«æ–‡ä»¶éªŒè¯ã€å®é™…æ‰§è¡Œå’Œä»£ç æµ‹è¯•æµç¨‹æƒé‡"""
        base_weights = {
            "code_quality": 0.20,
            "documentation_quality": 0.10,
            "test_coverage": 0.15,
            "error_handling": 0.10,
            "performance": 0.05,
            "compliance": 0.05
        }
        
        # åŸºç¡€åˆ†æ•°è®¡ç®—
        base_score = 0.0
        for metric, weight in base_weights.items():
            base_score += metrics.get(metric, 0.0) * weight
        
        # æ–‡ä»¶éªŒè¯æƒé‡ (15%)
        file_score = 0.0
        if file_verification:
            if file_verification.get("all_required_files_generated", False):
                file_score = 100.0
            elif file_verification.get("verilog_file_mentioned", False):
                file_score = 50.0  # ä»…æåˆ°ä½†æœªå®é™…ç”Ÿæˆ
            elif file_verification.get("files_actually_written", False):
                file_score = 30.0  # æœ‰å†™å…¥æ“ä½œä½†ä¸å®Œæ•´
        file_weighted_score = file_score * 0.15
        
        # å®é™…æ‰§è¡Œæƒé‡ (15%)
        execution_score = 0.0
        if execution_check:
            if execution_check.get("simulation_actually_executed", False):
                execution_score += 60.0
            elif execution_check.get("simulation_mentioned", False):
                execution_score += 20.0  # ä»…æåˆ°ä½†æœªå®é™…æ‰§è¡Œ
            
            if execution_check.get("tools_called", False):
                execution_score += 30.0
            
            if execution_check.get("concrete_results_provided", False):
                execution_score += 10.0
        execution_weighted_score = execution_score * 0.15
        
        # ä»£ç æµ‹è¯•æµç¨‹æƒé‡ (20%)
        testing_workflow_score = 0.0
        if testing_workflow:
            workflow_completeness = testing_workflow.get("workflow_completeness", 0)
            testing_quality_score = testing_workflow.get("testing_quality_score", 0)
            
            # åŸºäºå·¥ä½œæµå®Œæ•´æ€§çš„åˆ†æ•°
            testing_workflow_score += workflow_completeness * 0.6  # 60%æƒé‡ç»™å®Œæ•´æ€§
            
            # åŸºäºæµ‹è¯•è´¨é‡åˆ†æ•°çš„åˆ†æ•°
            testing_workflow_score += testing_quality_score * 0.4  # 40%æƒé‡ç»™è´¨é‡åˆ†æ•°
            
            # é¢å¤–å¥–åŠ±ï¼šå¦‚æœæ‰€æœ‰å…³é”®æµ‹è¯•æ­¥éª¤éƒ½å®Œæˆ
            if (testing_workflow.get("test_plan_created") and 
                testing_workflow.get("test_cases_designed") and 
                testing_workflow.get("testbench_generated") and 
                testing_workflow.get("simulation_executed")):
                testing_workflow_score += 20.0  # é¢å¤–å¥–åŠ±
        testing_workflow_weighted_score = testing_workflow_score * 0.20
        
        # ç»¼åˆåˆ†æ•°
        total_score = base_score + file_weighted_score + execution_weighted_score + testing_workflow_weighted_score
        
        # ä¸¥æ ¼çš„æƒ©ç½šæœºåˆ¶ï¼šå¦‚æœå…³é”®è¦æ±‚æœªæ»¡è¶³ï¼Œå¤§å¹…é™ä½åˆ†æ•°
        if file_verification and not file_verification.get("all_required_files_generated", False):
            total_score *= 0.6  # é™ä½40%
        
        if execution_check and execution_check.get("missing_executions"):
            total_score *= 0.7  # é™ä½30%
        
        # æµ‹è¯•æµç¨‹æƒ©ç½šæœºåˆ¶
        if testing_workflow and testing_workflow.get("workflow_completeness", 0) < 30:
            total_score *= 0.8  # æµ‹è¯•æµç¨‹ä¸¥é‡ä¸å®Œæ•´ï¼Œé™ä½20%
        
        # ğŸ¯ å†…å®¹éªŒè¯æƒé‡ (15%) - æœ€é‡è¦çš„ä¿®å¤
        content_score = 100.0  # é»˜è®¤æ»¡åˆ†
        if content_verification:
            if not content_verification.get("correct_content_used", True):
                content_score = 0.0  # ä½¿ç”¨äº†é”™è¯¯å†…å®¹ï¼Œä¸¥é‡é—®é¢˜
            elif content_verification.get("content_mismatch_detected", False):
                content_score *= 0.3  # æ£€æµ‹åˆ°å†…å®¹ä¸åŒ¹é…ï¼Œä¸¥é‡é™åˆ†
            elif content_verification.get("evidence_of_hallucination", False):
                content_score *= 0.1  # æ£€æµ‹åˆ°å¹»è§‰ï¼Œå‡ ä¹é›¶åˆ†
            elif not content_verification.get("content_complexity_appropriate", True):
                content_score *= 0.7  # å¤æ‚åº¦ä¸åŒ¹é…ï¼Œä¸­ç­‰é™åˆ†
        
        content_weighted_score = content_score * 0.15
        total_score += content_weighted_score
        
        # ä¸¥å‰çš„å†…å®¹éªŒè¯æƒ©ç½šæœºåˆ¶ - è¿™æ˜¯æœ€å…³é”®çš„ä¿®å¤
        if content_verification:
            if not content_verification.get("correct_content_used", True):
                total_score *= 0.2  # å†…å®¹é”™è¯¯ï¼Œåˆ†æ•°é™è‡³20%
                self.logger.warning("âš ï¸ æ£€æµ‹åˆ°ä¸¥é‡çš„å†…å®¹ä½¿ç”¨é”™è¯¯ï¼Œåˆ†æ•°ä¸¥å‰æƒ©ç½š")
            elif content_verification.get("evidence_of_hallucination", False):
                total_score *= 0.3  # å¹»è§‰é—®é¢˜ï¼Œåˆ†æ•°é™è‡³30%  
                self.logger.warning("âš ï¸ æ£€æµ‹åˆ°å¹»è§‰é—®é¢˜ï¼Œåˆ†æ•°é‡åº¦æƒ©ç½š")
        
        return min(100.0, max(0.0, total_score))
    
    def _generate_specific_recommendations(self, metrics: Dict[str, float], 
                                         quality_score: float, 
                                         threshold: float) -> List[str]:
        """ç”Ÿæˆå…·ä½“æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if quality_score < threshold:
            if metrics["code_quality"] < 50:
                recommendations.append("æé«˜ä»£ç è´¨é‡ï¼šæ·»åŠ æ›´å¤šæ³¨é‡Šï¼Œæ”¹è¿›ä»£ç ç»“æ„")
            if metrics["documentation_quality"] < 40:
                recommendations.append("æ”¹è¿›æ–‡æ¡£ï¼šæ·»åŠ è¯¦ç»†çš„åŠŸèƒ½è¯´æ˜å’Œä½¿ç”¨ç¤ºä¾‹")
            if metrics["test_coverage"] < 60:
                recommendations.append("å¢åŠ æµ‹è¯•è¦†ç›–ï¼šç”Ÿæˆæ›´å…¨é¢çš„testbench")
            if metrics["error_handling"] < 30:
                recommendations.append("åŠ å¼ºé”™è¯¯å¤„ç†ï¼šæ·»åŠ è¾¹ç•Œæ¡ä»¶æ£€æŸ¥å’Œå¼‚å¸¸å¤„ç†")
        
        if quality_score >= threshold:
            recommendations.append("è´¨é‡è¾¾æ ‡ï¼Œå¯ä»¥ç»§ç»­ä¸‹ä¸€æ­¥æˆ–å®Œæˆä»»åŠ¡")
        
        return recommendations
    
    def _determine_enhanced_next_action(self, analysis: Dict[str, Any], 
                                      task_context: Dict[str, Any]) -> str:
        """ç¡®å®šå¢å¼ºçš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨ - æ”¯æŒå¤šæ™ºèƒ½ä½“åä½œå†³ç­–"""
        
        completeness = analysis.get("completeness", "unknown")
        quality_score = analysis.get("quality_score", 0)
        risk_assessment = analysis.get("risk_assessment", "low")
        file_verification = analysis.get("file_verification", {})
        execution_check = analysis.get("actual_execution_check", {})
        testing_workflow = analysis.get("code_testing_workflow", {})
        
        # è·å–åŸå§‹éœ€æ±‚
        original_request = task_context.get("original_request", "") if task_context else ""
        
        # å…ˆæ£€æŸ¥ä¼ ç»Ÿçš„å®Œæ•´æ€§æŒ‡æ ‡
        if completeness == "complete" and quality_score >= 80:
            # å³ä½¿æŠ¥å‘Šå®Œæˆï¼Œä¹Ÿè¦éªŒè¯å®é™…æ‰§è¡Œ
            missing_files = file_verification.get("missing_files", [])
            missing_executions = execution_check.get("missing_executions", [])
            missing_testing_steps = testing_workflow.get("missing_testing_steps", [])
            
            if missing_files or missing_executions or missing_testing_steps:
                # è™½ç„¶æ™ºèƒ½ä½“å£°ç§°å®Œæˆï¼Œä½†å®é™…ç¼ºå¤±å…³é”®é¡¹
                return "continue_iteration"  # éœ€è¦ç»§ç»­è¿­ä»£
            else:
                return "complete_task"  # çœŸæ­£å®Œæˆ
        
        elif completeness == "failed" or risk_assessment == "high":
            return "retry_with_different_agent"
        
        elif completeness == "partial" or quality_score < 70:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤šæ™ºèƒ½ä½“åä½œ
            missing_files = file_verification.get("missing_files", [])
            missing_executions = execution_check.get("missing_executions", [])
            missing_testing_steps = testing_workflow.get("missing_testing_steps", [])
            
            if (missing_files or missing_executions or missing_testing_steps or 
                not file_verification.get("all_required_files_generated", False) or
                testing_workflow.get("workflow_completeness", 0) < 50):
                return "continue_iteration"  # éœ€è¦é¢å¤–çš„æ™ºèƒ½ä½“åä½œ
            else:
                return "improve_result"
        
        else:
            return "continue_iteration"
    
    def _generate_improvement_suggestions(self, analysis: Dict[str, Any], agent_id: str) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®® - åŒ…å«å…·ä½“çš„æ™ºèƒ½ä½“åä½œå»ºè®®å’Œå·¥å…·è°ƒç”¨æŒ‡å¯¼"""
        suggestions = []
        
        # è·å–éªŒè¯ç»“æœ
        file_verification = analysis.get("file_verification", {})
        execution_check = analysis.get("actual_execution_check", {})
        testing_workflow = analysis.get("code_testing_workflow", {})
        
        # åŸºäºæ–‡ä»¶å’Œæ‰§è¡ŒéªŒè¯ç”Ÿæˆå…·ä½“å»ºè®®
        missing_files = file_verification.get("missing_files", [])
        missing_executions = execution_check.get("missing_executions", [])
        missing_testing_steps = testing_workflow.get("missing_testing_steps", [])
        
        if missing_files or missing_executions or missing_testing_steps:
            suggestions.append("éœ€è¦è°ƒç”¨é¢å¤–çš„æ™ºèƒ½ä½“æ¥è¡¥å……ç¼ºå¤±çš„åŠŸèƒ½")
            
            if "æµ‹è¯•å°æ–‡ä»¶" in missing_files or "ä»¿çœŸéªŒè¯æ‰§è¡Œ" in missing_executions:
                suggestions.append("å»ºè®®è°ƒç”¨ enhanced_real_code_review_agent ç”Ÿæˆæµ‹è¯•å°å¹¶æ‰§è¡Œä»¿çœŸ")
                suggestions.append("  - å¯ç”¨å·¥å…·: generate_testbench, run_simulation, use_external_testbench")
                suggestions.append("  - å·¥å…·è°ƒç”¨ç¤ºä¾‹: generate_testbench(module_name='xxx', module_code='...')")
                suggestions.append("  - å·¥å…·è°ƒç”¨ç¤ºä¾‹: run_simulation(module_code='...', testbench_code='...')")
            
            if "Verilogæ¨¡å—æ–‡ä»¶" in missing_files:
                suggestions.append("å»ºè®®é‡æ–°è°ƒç”¨ enhanced_real_verilog_agent ç”Ÿæˆå®Œæ•´çš„Verilogæ¨¡å—")
                suggestions.append("  - å¯ç”¨å·¥å…·: analyze_design_requirements, generate_verilog_code, search_existing_modules")
                suggestions.append("  - å·¥å…·è°ƒç”¨ç¤ºä¾‹: analyze_design_requirements(requirements='...')")
                suggestions.append("  - å·¥å…·è°ƒç”¨ç¤ºä¾‹: generate_verilog_code(module_name='xxx', requirements='...')")
            
            # åŸºäºæµ‹è¯•æµç¨‹ç¼ºå¤±æ­¥éª¤çš„å»ºè®®
            if missing_testing_steps:
                if "æµ‹è¯•è®¡åˆ’åˆ›å»º" in missing_testing_steps:
                    suggestions.append("éœ€è¦åˆ›å»ºè¯¦ç»†çš„æµ‹è¯•è®¡åˆ’ï¼ŒåŒ…æ‹¬æµ‹è¯•ç›®æ ‡ã€ç­–ç•¥å’ŒèŒƒå›´")
                if "æµ‹è¯•ç”¨ä¾‹è®¾è®¡" in missing_testing_steps:
                    suggestions.append("éœ€è¦è®¾è®¡å…¨é¢çš„æµ‹è¯•ç”¨ä¾‹ï¼ŒåŒ…æ‹¬åŠŸèƒ½æµ‹è¯•ã€è¾¹ç•Œæµ‹è¯•å’Œå¼‚å¸¸æµ‹è¯•")
                if "æµ‹è¯•å°ç”Ÿæˆ" in missing_testing_steps:
                    suggestions.append("éœ€è¦ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•å°æ–‡ä»¶ï¼ŒåŒ…å«æµ‹è¯•æ¿€åŠ±å’Œç»“æœéªŒè¯")
                    suggestions.append("  - ä½¿ç”¨ enhanced_real_code_review_agent çš„ generate_testbench å·¥å…·")
                if "ä»¿çœŸæ‰§è¡Œ" in missing_testing_steps:
                    suggestions.append("éœ€è¦å®é™…æ‰§è¡Œä»¿çœŸéªŒè¯ï¼Œå¹¶æä¾›ä»¿çœŸç»“æœå’Œæ³¢å½¢åˆ†æ")
                    suggestions.append("  - ä½¿ç”¨ enhanced_real_code_review_agent çš„ run_simulation å·¥å…·")
                if "æµ‹è¯•ç»“æœåˆ†æ" in missing_testing_steps:
                    suggestions.append("éœ€è¦å¯¹æµ‹è¯•ç»“æœè¿›è¡Œè¯¦ç»†åˆ†æï¼ŒéªŒè¯åŠŸèƒ½æ­£ç¡®æ€§å’Œæ—¶åºæ­£ç¡®æ€§")
                    suggestions.append("  - ä½¿ç”¨ enhanced_real_code_review_agent çš„ analyze_test_failures å·¥å…·")
                if "è¦†ç›–ç‡åˆ†æ" in missing_testing_steps:
                    suggestions.append("éœ€è¦è¿›è¡Œä»£ç è¦†ç›–ç‡åˆ†æï¼Œç¡®ä¿æµ‹è¯•çš„å®Œæ•´æ€§")
                if "æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ" in missing_testing_steps:
                    suggestions.append("éœ€è¦ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Šï¼ŒåŒ…å«æµ‹è¯•æ€»ç»“ã€ç»“è®ºå’Œå»ºè®®")
        
        if not file_verification.get("all_required_files_generated", False):
            suggestions.append("éœ€è¦ç¡®ä¿æ‰€æœ‰æ–‡ä»¶éƒ½è¢«å®é™…ç”Ÿæˆè€Œéä»…åœ¨æŠ¥å‘Šä¸­æè¿°")
        
        if not execution_check.get("simulation_actually_executed", False):
            suggestions.append("éœ€è¦æ‰§è¡Œå®é™…çš„ä»¿çœŸéªŒè¯è€Œéä»…ç”Ÿæˆä»¿çœŸä»£ç ")
        
        # åŸºäºæµ‹è¯•æµç¨‹å®Œæ•´æ€§çš„å»ºè®®
        workflow_completeness = testing_workflow.get("workflow_completeness", 0)
        if workflow_completeness < 50:
            suggestions.append(f"æµ‹è¯•æµç¨‹å®Œæ•´æ€§ä¸è¶³ ({workflow_completeness:.1f}%)ï¼Œéœ€è¦å®Œå–„æµ‹è¯•æµç¨‹")
        elif workflow_completeness < 80:
            suggestions.append(f"æµ‹è¯•æµç¨‹åŸºæœ¬å®Œæ•´ ({workflow_completeness:.1f}%)ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
        
        # æ·»åŠ æ™ºèƒ½ä½“å·¥å…·è°ƒç”¨æŒ‡å¯¼
        suggestions.append("\n=== æ™ºèƒ½ä½“å·¥å…·è°ƒç”¨æŒ‡å¯¼ ===")
        suggestions.append("LLMCoordinatorAgent å¯ç”¨å·¥å…·:")
        suggestions.append("  - assign_task_to_agent: åˆ†é…ä»»åŠ¡ç»™æŒ‡å®šæ™ºèƒ½ä½“")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: assign_task_to_agent(agent_id='enhanced_real_verilog_agent', task_description='...')")
        suggestions.append("  - analyze_agent_result: åˆ†ææ™ºèƒ½ä½“æ‰§è¡Œç»“æœ")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: analyze_agent_result(agent_id='xxx', result={...}, quality_threshold=80)")
        suggestions.append("  - check_task_completion: æ£€æŸ¥ä»»åŠ¡å®ŒæˆçŠ¶æ€")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: check_task_completion(task_id='xxx', all_results={...}, original_requirements='...')")
        suggestions.append("  - query_agent_status: æŸ¥è¯¢æ™ºèƒ½ä½“çŠ¶æ€")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: query_agent_status(agent_id='enhanced_real_verilog_agent', include_performance=True)")
        suggestions.append("  - identify_task_type: è¯†åˆ«ä»»åŠ¡ç±»å‹")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: identify_task_type(user_request='...', context={...})")
        suggestions.append("  - recommend_agent: æ¨èåˆé€‚çš„æ™ºèƒ½ä½“")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: recommend_agent(task_type='design', task_description='...', priority='high')")
        suggestions.append("  - provide_final_answer: æä¾›æœ€ç»ˆç­”æ¡ˆ")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: provide_final_answer(final_summary='...', task_status='success')")
        
        suggestions.append("\nenhanced_real_verilog_agent å¯ç”¨å·¥å…·:")
        suggestions.append("  - analyze_design_requirements: åˆ†æè®¾è®¡éœ€æ±‚")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: analyze_design_requirements(requirements='è®¾è®¡ä¸€ä¸ª8ä½åŠ æ³•å™¨', design_type='combinational')")
        suggestions.append("  - generate_verilog_code: ç”ŸæˆVerilogä»£ç ")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: generate_verilog_code(module_name='adder_8bit', requirements='8ä½åŠ æ³•å™¨', input_ports=[...])")
        suggestions.append("  - search_existing_modules: æœç´¢ç°æœ‰æ¨¡å—")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: search_existing_modules(module_type='arithmetic', functionality='åŠ æ³•å™¨')")
        suggestions.append("  - analyze_code_quality: åˆ†æä»£ç è´¨é‡")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: analyze_code_quality(verilog_code='...', module_name='adder_8bit')")
        suggestions.append("  - validate_design_specifications: éªŒè¯è®¾è®¡è§„æ ¼")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: validate_design_specifications(requirements='...', generated_code='...')")
        suggestions.append("  - generate_design_documentation: ç”Ÿæˆè®¾è®¡æ–‡æ¡£")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: generate_design_documentation(module_name='adder_8bit', verilog_code='...', requirements='...')")
        suggestions.append("  - optimize_verilog_code: ä¼˜åŒ–Verilogä»£ç ")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: optimize_verilog_code(verilog_code='...', optimization_target='area')")
        
        suggestions.append("\nenhanced_real_code_review_agent å¯ç”¨å·¥å…·:")
        suggestions.append("  - generate_testbench: ç”Ÿæˆæµ‹è¯•å°")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: generate_testbench(module_name='adder_8bit', module_code='...', test_scenarios=[...])")
        suggestions.append("  - run_simulation: è¿è¡Œä»¿çœŸ")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: run_simulation(module_code='...', testbench_code='...', simulator='iverilog')")
        suggestions.append("  - use_external_testbench: ä½¿ç”¨å¤–éƒ¨æµ‹è¯•å°")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: use_external_testbench(design_code='...', external_testbench_path='testbench.v', design_module_name='adder_8bit')")
        suggestions.append("  - generate_build_script: ç”Ÿæˆæ„å»ºè„šæœ¬")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: generate_build_script(verilog_files=['design.v'], testbench_files=['tb.v'], script_type='makefile')")
        suggestions.append("  - execute_build_script: æ‰§è¡Œæ„å»ºè„šæœ¬")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: execute_build_script(script_name='Makefile', action='all', timeout=300)")
        suggestions.append("  - analyze_test_failures: åˆ†ææµ‹è¯•å¤±è´¥")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: analyze_test_failures(design_code='...', compilation_errors='...', testbench_code='...')")
        
        suggestions.append("\n=== å·¥å…·å‚æ•°è¯¦ç»†è¯´æ˜ ===")
        suggestions.append("LLMCoordinatorAgent å·¥å…·å‚æ•°:")
        suggestions.append("  assign_task_to_agent:")
        suggestions.append("    - agent_id: æ™ºèƒ½ä½“ID ('enhanced_real_verilog_agent' æˆ– 'enhanced_real_code_review_agent')")
        suggestions.append("    - task_description: è¯¦ç»†ä»»åŠ¡æè¿°")
        suggestions.append("    - expected_output: æœŸæœ›è¾“å‡ºæ ¼å¼")
        suggestions.append("    - task_type: ä»»åŠ¡ç±»å‹ ('design', 'verification', 'analysis', 'debug', 'composite')")
        suggestions.append("    - priority: ä¼˜å…ˆçº§ ('high', 'medium', 'low')")
        
        suggestions.append("  analyze_agent_result:")
        suggestions.append("    - agent_id: æ™ºèƒ½ä½“ID")
        suggestions.append("    - result: æ™ºèƒ½ä½“æ‰§è¡Œç»“æœ")
        suggestions.append("    - task_context: ä»»åŠ¡ä¸Šä¸‹æ–‡")
        suggestions.append("    - quality_threshold: è´¨é‡é˜ˆå€¼ (0-100)")
        
        suggestions.append("\nenhanced_real_verilog_agent å·¥å…·å‚æ•°:")
        suggestions.append("  generate_verilog_code:")
        suggestions.append("    - module_name: æ¨¡å—åç§° (å­—æ¯å¼€å¤´)")
        suggestions.append("    - requirements: è®¾è®¡éœ€æ±‚æè¿°")
        suggestions.append("    - input_ports: è¾“å…¥ç«¯å£åˆ—è¡¨ [{'name': 'clk', 'width': 1}]")
        suggestions.append("    - output_ports: è¾“å‡ºç«¯å£åˆ—è¡¨ [{'name': 'result', 'width': 8}]")
        suggestions.append("    - coding_style: ç¼–ç é£æ ¼ ('behavioral', 'structural', 'rtl', 'mixed')")
        
        suggestions.append("  analyze_design_requirements:")
        suggestions.append("    - requirements: è®¾è®¡éœ€æ±‚æè¿°")
        suggestions.append("    - design_type: è®¾è®¡ç±»å‹ ('combinational', 'sequential', 'mixed', 'custom')")
        suggestions.append("    - complexity_level: å¤æ‚åº¦ ('simple', 'medium', 'complex', 'advanced')")
        
        suggestions.append("\nenhanced_real_code_review_agent å·¥å…·å‚æ•°:")
        suggestions.append("  generate_testbench:")
        suggestions.append("    - module_name: ç›®æ ‡æ¨¡å—åç§°")
        suggestions.append("    - module_code: å®Œæ•´çš„Verilogæ¨¡å—ä»£ç ")
        suggestions.append("    - test_scenarios: æµ‹è¯•åœºæ™¯åˆ—è¡¨")
        suggestions.append("    - clock_period: æ—¶é’Ÿå‘¨æœŸ (ns)")
        suggestions.append("    - simulation_time: ä»¿çœŸæ—¶é—´ (æ—¶é’Ÿå‘¨æœŸæ•°)")
        
        suggestions.append("  run_simulation:")
        suggestions.append("    - module_code: æ¨¡å—ä»£ç å†…å®¹")
        suggestions.append("    - testbench_code: æµ‹è¯•å°ä»£ç å†…å®¹")
        suggestions.append("    - simulator: ä»¿çœŸå™¨ ('iverilog', 'modelsim', 'vivado', 'auto')")
        suggestions.append("    - simulation_options: ä»¿çœŸé€‰é¡¹")
        
        suggestions.append("  analyze_test_failures:")
        suggestions.append("    - design_code: è®¾è®¡ä»£ç ")
        suggestions.append("    - compilation_errors: ç¼–è¯‘é”™è¯¯ä¿¡æ¯")
        suggestions.append("    - simulation_errors: ä»¿çœŸé”™è¯¯ä¿¡æ¯")
        suggestions.append("    - test_assertions: æµ‹è¯•æ–­è¨€å¤±è´¥ä¿¡æ¯")
        suggestions.append("    - testbench_code: æµ‹è¯•å°ä»£ç ")
        suggestions.append("    - iteration_number: å½“å‰è¿­ä»£æ¬¡æ•°")
        
        suggestions.append("\n=== å·¥å…·è°ƒç”¨æœ€ä½³å®è·µ ===")
        suggestions.append("1. ä»»åŠ¡åˆ†é…æµç¨‹:")
        suggestions.append("   - å…ˆä½¿ç”¨ identify_task_type è¯†åˆ«ä»»åŠ¡ç±»å‹")
        suggestions.append("   - ä½¿ç”¨ recommend_agent æ¨èåˆé€‚çš„æ™ºèƒ½ä½“")
        suggestions.append("   - ä½¿ç”¨ assign_task_to_agent åˆ†é…ä»»åŠ¡")
        suggestions.append("   - ä½¿ç”¨ analyze_agent_result åˆ†æç»“æœ")
        suggestions.append("   - ä½¿ç”¨ check_task_completion æ£€æŸ¥å®ŒæˆçŠ¶æ€")
        
        suggestions.append("2. Verilogè®¾è®¡æµç¨‹:")
        suggestions.append("   - ä½¿ç”¨ analyze_design_requirements åˆ†æéœ€æ±‚")
        suggestions.append("   - ä½¿ç”¨ search_existing_modules æœç´¢ç°æœ‰æ¨¡å—")
        suggestions.append("   - ä½¿ç”¨ generate_verilog_code ç”Ÿæˆä»£ç ")
        suggestions.append("   - ä½¿ç”¨ analyze_code_quality åˆ†æè´¨é‡")
        suggestions.append("   - ä½¿ç”¨ validate_design_specifications éªŒè¯è§„æ ¼")
        
        suggestions.append("3. æµ‹è¯•éªŒè¯æµç¨‹:")
        suggestions.append("   - ä½¿ç”¨ generate_testbench ç”Ÿæˆæµ‹è¯•å°")
        suggestions.append("   - ä½¿ç”¨ run_simulation æ‰§è¡Œä»¿çœŸ")
        suggestions.append("   - ä½¿ç”¨ analyze_test_failures åˆ†æå¤±è´¥")
        suggestions.append("   - ä½¿ç”¨ generate_build_script ç”Ÿæˆæ„å»ºè„šæœ¬")
        suggestions.append("   - ä½¿ç”¨ execute_build_script æ‰§è¡Œæ„å»º")
        
        suggestions.append("4. é”™è¯¯å¤„ç†ç­–ç•¥:")
        suggestions.append("   - ç¼–è¯‘é”™è¯¯: æ£€æŸ¥è¯­æ³•å’Œç«¯å£å®šä¹‰")
        suggestions.append("   - ä»¿çœŸé”™è¯¯: æ£€æŸ¥æ—¶åºå’Œé€»è¾‘")
        suggestions.append("   - æµ‹è¯•å¤±è´¥: åˆ†ææ–­è¨€å’ŒæœŸæœ›å€¼")
        suggestions.append("   - è´¨é‡ä¸è¶³: é‡æ–°è®¾è®¡æˆ–ä¼˜åŒ–ä»£ç ")
        
        # æ·»åŠ LLMCoordinatorAgentçš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼
        coordinator_guide = self._generate_coordinator_tool_guide()
        suggestions.extend(coordinator_guide)
        
        # åŸºäºæµ‹è¯•è´¨é‡åˆ†æ•°çš„å»ºè®®
        testing_quality_score = testing_workflow.get("testing_quality_score", 0)
        if testing_quality_score < 50:
            suggestions.append(f"æµ‹è¯•è´¨é‡åˆ†æ•°è¿‡ä½ ({testing_quality_score:.1f})ï¼Œéœ€è¦é‡æ–°è®¾è®¡æµ‹è¯•ç­–ç•¥")
        elif testing_quality_score < 80:
            suggestions.append(f"æµ‹è¯•è´¨é‡è‰¯å¥½ ({testing_quality_score:.1f})ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
        
        # åŸºäºè´¨é‡åˆ†æ•°ç”Ÿæˆå»ºè®®
        if analysis.get("quality_score", 0) < 70:
            suggestions.append("è´¨é‡åˆ†æ•°åä½ï¼Œéœ€è¦æ”¹è¿›æˆ–ä½¿ç”¨å¤šæ™ºèƒ½ä½“åä½œ")
        
        # åŸºäºæ™ºèƒ½ä½“å†å²è¡¨ç°ç”Ÿæˆå»ºè®®
        if agent_id in self.registered_agents:
            agent_info = self.registered_agents[agent_id]
            if agent_info.consecutive_failures > 2:
                suggestions.append("è¯¥æ™ºèƒ½ä½“è¿ç»­å¤±è´¥æ¬¡æ•°è¾ƒå¤šï¼Œå»ºè®®æ›´æ¢æ™ºèƒ½ä½“")
        
        return suggestions
    
    def _generate_coordinator_tool_guide(self) -> List[str]:
        """ç”ŸæˆLLMCoordinatorAgentä¸“ç”¨çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼"""
        guide = []
        
        guide.append("\n=== LLMCoordinatorAgent å·¥å…·è°ƒç”¨æŒ‡å¯¼ ===")
        guide.append("")
        
        guide.append("ã€å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘")
        guide.append("1. assign_task_to_agent - æ™ºèƒ½ä»»åŠ¡åˆ†é…")
        guide.append("   åŠŸèƒ½: å°†ä»»åŠ¡åˆ†é…ç»™æœ€åˆé€‚çš„æ™ºèƒ½ä½“")
        guide.append("   å‚æ•°: agent_id, task_description, expected_output, task_type, priority")
        guide.append("   ç¤ºä¾‹: assign_task_to_agent('enhanced_real_verilog_agent', 'è®¾è®¡8ä½åŠ æ³•å™¨', '', 'design', 'high')")
        guide.append("")
        
        guide.append("2. analyze_agent_result - ç»“æœè´¨é‡åˆ†æ")
        guide.append("   åŠŸèƒ½: æ·±åº¦åˆ†ææ™ºèƒ½ä½“æ‰§è¡Œç»“æœçš„è´¨é‡å’Œå®Œæ•´æ€§")
        guide.append("   å‚æ•°: agent_id, result, task_context, quality_threshold")
        guide.append("   ç¤ºä¾‹: analyze_agent_result('verilog_agent', result_data, context, 80.0)")
        guide.append("")
        
        guide.append("3. check_task_completion - ä»»åŠ¡å®Œæˆæ£€æŸ¥")
        guide.append("   åŠŸèƒ½: æ£€æŸ¥ä»»åŠ¡æ˜¯å¦çœŸæ­£å®Œæˆï¼Œè¯„ä¼°æ•´ä½“è´¨é‡")
        guide.append("   å‚æ•°: task_id, all_results, original_requirements, completion_criteria")
        guide.append("   ç¤ºä¾‹: check_task_completion('task_001', results, 'è®¾è®¡8ä½åŠ æ³•å™¨')")
        guide.append("")
        
        guide.append("4. query_agent_status - æ™ºèƒ½ä½“çŠ¶æ€æŸ¥è¯¢")
        guide.append("   åŠŸèƒ½: æŸ¥è¯¢æ™ºèƒ½ä½“çš„è¯¦ç»†çŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡")
        guide.append("   å‚æ•°: agent_id, include_performance, include_history")
        guide.append("   ç¤ºä¾‹: query_agent_status('enhanced_real_verilog_agent', True, False)")
        guide.append("")
        
        guide.append("5. identify_task_type - ä»»åŠ¡ç±»å‹è¯†åˆ«")
        guide.append("   åŠŸèƒ½: æ™ºèƒ½è¯†åˆ«ä»»åŠ¡ç±»å‹ï¼Œæ”¯æŒè®¾è®¡ã€éªŒè¯ã€åˆ†æç­‰")
        guide.append("   å‚æ•°: user_request, context")
        guide.append("   ç¤ºä¾‹: identify_task_type('è®¾è®¡ä¸€ä¸ªè®¡æ•°å™¨', {})")
        guide.append("")
        
        guide.append("6. recommend_agent - æ™ºèƒ½ä½“æ¨è")
        guide.append("   åŠŸèƒ½: åŸºäºä»»åŠ¡ç‰¹å¾æ¨èæœ€åˆé€‚çš„æ™ºèƒ½ä½“")
        guide.append("   å‚æ•°: task_type, task_description, priority, constraints")
        guide.append("   ç¤ºä¾‹: recommend_agent('design', 'è®¾è®¡ALU', 'high')")
        guide.append("")
        
        guide.append("7. provide_final_answer - æœ€ç»ˆç­”æ¡ˆæä¾›")
        guide.append("   åŠŸèƒ½: å½“ä»»åŠ¡å®Œæˆæ—¶æä¾›æœ€ç»ˆçš„å®Œæ•´ç­”æ¡ˆ")
        guide.append("   å‚æ•°: final_summary, task_status, results_summary")
        guide.append("   ç¤ºä¾‹: provide_final_answer('ä»»åŠ¡å®Œæˆ', 'success', results)")
        guide.append("")
        
        guide.append("8. get_tool_usage_guide - å·¥å…·ä½¿ç”¨æŒ‡å¯¼")
        guide.append("   åŠŸèƒ½: è·å–å®Œæ•´çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼")
        guide.append("   å‚æ•°: agent_type, include_examples, include_best_practices")
        guide.append("   ç¤ºä¾‹: get_tool_usage_guide('coordinator', True, True)")
        guide.append("")
        
        guide.append("ã€ğŸ¯ å¼ºåˆ¶æ‰§è¡Œçš„åè°ƒæµç¨‹ã€‘")
        guide.append("1. **ç¬¬ä¸€æ­¥**: identify_task_type â†’ è¯†åˆ«ä»»åŠ¡ç±»å‹")
        guide.append("2. **ç¬¬äºŒæ­¥**: recommend_agent â†’ æ¨èæœ€åˆé€‚çš„æ™ºèƒ½ä½“")
        guide.append("3. **ç¬¬ä¸‰æ­¥**: assign_task_to_agent â†’ åˆ†é…ä»»åŠ¡ç»™æ¨èæ™ºèƒ½ä½“")
        guide.append("4. **ç¬¬å››æ­¥**: analyze_agent_result â†’ åˆ†ææ‰§è¡Œç»“æœ")
        guide.append("5. **ç¬¬äº”æ­¥**: æ ¹æ®åˆ†æç»“æœå†³å®šæ˜¯å¦éœ€è¦ç»§ç»­è¿­ä»£")
        guide.append("")
        
        guide.append("ã€âš ï¸ é‡è¦è§„åˆ™ã€‘")
        guide.append("- **å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æµç¨‹æ‰§è¡Œï¼Œä¸å¾—è·³è¿‡ä»»ä½•æ­¥éª¤**")
        guide.append("- **æ¨èä»£ç†å·¥å…· recommend_agent æ˜¯å¿…éœ€çš„ï¼Œä¸èƒ½ç›´æ¥è°ƒç”¨ assign_task_to_agent**")
        guide.append("- **æ¯æ¬¡ä»»åŠ¡åˆ†é…å‰éƒ½å¿…é¡»å…ˆè°ƒç”¨æ¨èä»£ç†å·¥å…·**")
        guide.append("- **åˆ†æç»“æœåå†å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨**")
        guide.append("- **ç¡®ä¿ä»»åŠ¡å®Œæˆåå†æä¾›æœ€ç»ˆç­”æ¡ˆ**")
        guide.append("")
        
        guide.append("ã€âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆç¦æ­¢ï¼‰ã€‘")
        guide.append("- ç›´æ¥è°ƒç”¨ assign_task_to_agent è€Œä¸å…ˆè°ƒç”¨ recommend_agent")
        guide.append("- è·³è¿‡ identify_task_type ç›´æ¥æ¨èæ™ºèƒ½ä½“")
        guide.append("- ä¸åˆ†æç»“æœå°±ç»§ç»­åˆ†é…ä»»åŠ¡")
        guide.append("")
        
        guide.append("ã€âœ… æ­£ç¡®ç¤ºä¾‹ã€‘")
        guide.append("- identify_task_type â†’ recommend_agent â†’ assign_task_to_agent")
        guide.append("- æ¯æ¬¡åˆ†é…ä»»åŠ¡å‰éƒ½å…ˆæ¨èæ™ºèƒ½ä½“")
        guide.append("- åˆ†æç»“æœåå†å†³å®šä¸‹ä¸€æ­¥")
        guide.append("")
        
        guide.append("ã€æ³¨æ„äº‹é¡¹ã€‘")
        guide.append("- ä½œä¸ºåè°ƒè€…ï¼Œä¸»è¦è´Ÿè´£ä»»åŠ¡åˆ†é…å’Œç»“æœåˆ†æ")
        guide.append("- å…·ä½“çš„è®¾è®¡å’ŒéªŒè¯å·¥ä½œäº¤ç»™ä¸“é—¨çš„æ™ºèƒ½ä½“")
        guide.append("- ç¡®ä¿ä»»åŠ¡æè¿°æ¸…æ™°ï¼Œä¾¿äºå…¶ä»–æ™ºèƒ½ä½“ç†è§£æ‰§è¡Œ")
        guide.append("- å®šæœŸæ£€æŸ¥ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€å’Œè´¨é‡")
        
        return guide
    
    async def _tool_check_task_completion(self, task_id: str, 
                                        all_results: Dict[str, Any],
                                        original_requirements: str,
                                        completion_criteria: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¢å¼ºçš„ä»»åŠ¡å®Œæˆæ£€æŸ¥"""
        
        try:
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
            if task_id not in self.active_tasks:
                return {
                    "success": False,
                    "error": f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
                }
            
            task_context = self.active_tasks[task_id]
            
            # ğŸ”§ ä¿®å¤ï¼šå¤„ç†all_resultså¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ
            if isinstance(all_results, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                results_dict = {}
                for i, result in enumerate(all_results):
                    if isinstance(result, dict):
                        # å°è¯•ä»ç»“æœä¸­æå–æ™ºèƒ½ä½“ID
                        agent_id = result.get("agent_id", f"agent_{i}")
                        results_dict[agent_id] = result
                    else:
                        results_dict[f"result_{i}"] = result
                all_results = results_dict
                self.logger.info(f"ğŸ¯ å°†åˆ—è¡¨æ ¼å¼çš„all_resultsè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ŒåŒ…å«{len(all_results)}ä¸ªç»“æœ")
            
            # ğŸ†• æ£€æŸ¥æ˜¯å¦ä¸ºå¤åˆä»»åŠ¡ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨å·¥ä½œæµè¯„ä¼°
            composite_task_analysis = self._detect_composite_task(original_requirements)
            is_composite = composite_task_analysis[0]
            
            if is_composite:
                # å¤åˆä»»åŠ¡ä½¿ç”¨å·¥ä½œæµè¯„ä¼°
                completion_analysis = self._evaluate_composite_task_workflow(
                    all_results, original_requirements, task_context, completion_criteria
                )
            else:
                # å•ä¸€ä»»åŠ¡ä½¿ç”¨åŸæœ‰åˆ†æ
                completion_analysis = self._enhanced_task_completion_analysis(
                    all_results, original_requirements, task_context, completion_criteria
                )
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            if completion_analysis["is_completed"]:
                task_context.completion_status = "completed"
                task_context.quality_score = completion_analysis["completion_score"]
                self._update_performance_metrics(task_context, True)
            else:
                task_context.completion_status = "in_progress"
                task_context.quality_score = completion_analysis["completion_score"]
            
            return {
                "success": True,
                "is_completed": completion_analysis["is_completed"],
                "completion_score": completion_analysis["completion_score"],
                "missing_requirements": completion_analysis["missing_requirements"],
                "quality_assessment": completion_analysis["quality_assessment"],
                "detailed_analysis": completion_analysis["detailed_analysis"],
                "next_steps": completion_analysis["next_steps"],
                "performance_metrics": completion_analysis["performance_metrics"]
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ä»»åŠ¡å®Œæˆæ£€æŸ¥å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _enhanced_task_completion_analysis(self, all_results: Dict[str, Any],
                                         original_requirements: str,
                                         task_context: TaskContext,
                                         completion_criteria: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¢å¼ºçš„ä»»åŠ¡å®Œæˆæƒ…å†µåˆ†æ"""
        
        analysis = {
            "is_completed": False,
            "completion_score": 0.0,
            "missing_requirements": [],
            "quality_assessment": "unknown",
            "detailed_analysis": {},
            "next_steps": [],
            "performance_metrics": {}
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
        if not all_results:
            analysis["missing_requirements"].append("æ²¡æœ‰æ‰§è¡Œç»“æœ")
            analysis["next_steps"].append("å¼€å§‹ä»»åŠ¡æ‰§è¡Œ")
            return analysis
        
        # åˆ†æåŸå§‹éœ€æ±‚
        requirements = original_requirements.lower()
        
        # è¯¦ç»†åˆ†æå„é¡¹æŒ‡æ ‡
        detailed_analysis = self._analyze_completion_metrics(all_results, requirements, task_context)
        analysis["detailed_analysis"] = detailed_analysis
        
        # è®¡ç®—å®Œæˆåˆ†æ•°
        completion_score = self._calculate_completion_score(detailed_analysis, completion_criteria)
        analysis["completion_score"] = completion_score
        
        # æ£€æŸ¥ç¼ºå¤±é¡¹
        missing_items = self._identify_missing_requirements(detailed_analysis, requirements)
        analysis["missing_requirements"] = missing_items
        
        # åˆ¤æ–­æ˜¯å¦å®Œæˆ
        is_completed = self._determine_completion_status(completion_score, missing_items, completion_criteria)
        analysis["is_completed"] = is_completed
        
        # è´¨é‡è¯„ä¼°
        analysis["quality_assessment"] = self._assess_overall_quality(detailed_analysis, completion_score)
        
        # ç¡®å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
        analysis["next_steps"] = self._determine_next_steps(is_completed, missing_items, detailed_analysis)
        
        # æ€§èƒ½æŒ‡æ ‡
        analysis["performance_metrics"] = self._calculate_performance_metrics(task_context, all_results)
        
        return analysis
    
    def _analyze_completion_metrics(self, all_results: Dict[str, Any], 
                                  requirements: str,
                                  task_context: TaskContext) -> Dict[str, Any]:
        """åˆ†æå®ŒæˆæŒ‡æ ‡"""
        metrics = {
            "design_complete": False,
            "verification_complete": False,
            "documentation_complete": False,
            "testing_complete": False,
            "quality_checks_passed": False,
            "agent_performance": {},
            "execution_time": 0.0,
            "total_iterations": 0
        }
        
        # ğŸ¯ ä¿®å¤ï¼šå¤„ç†all_resultså¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ
        if isinstance(all_results, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            results_dict = {}
            for i, result in enumerate(all_results):
                if isinstance(result, dict):
                    # å°è¯•ä»ç»“æœä¸­æå–æ™ºèƒ½ä½“ID
                    agent_id = result.get("agent_id", f"agent_{i}")
                    results_dict[agent_id] = result
                else:
                    results_dict[f"result_{i}"] = result
            all_results = results_dict
            self.logger.info(f"ğŸ¯ å°†åˆ—è¡¨æ ¼å¼çš„all_resultsè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ŒåŒ…å«{len(all_results)}ä¸ªç»“æœ")
        
        # æ£€æŸ¥è®¾è®¡å®Œæˆæƒ…å†µ
        if "design" in requirements or "æ¨¡å—" in requirements:
            design_results = []
            for agent_id, result in all_results.items():
                if agent_id == "enhanced_real_verilog_agent":
                    design_results.append(result)
            
            if design_results:
                # ğŸ”§ ä¿®å¤ï¼šæ›´ä¸¥æ ¼çš„è®¾è®¡å®Œæˆæ£€æŸ¥
                for result in design_results:
                    if isinstance(result, dict):
                        # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸçŠ¶æ€
                        if result.get("success", False):
                            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†Verilogæ–‡ä»¶
                            generated_files = result.get("generated_files", [])
                            if any(".v" in file for file in generated_files):
                                metrics["design_complete"] = True
                                break
                            # æ£€æŸ¥ç»“æœå†…å®¹æ˜¯å¦åŒ…å«æ¨¡å—å®šä¹‰
                            result_content = str(result.get("result", ""))
                            if "module" in result_content.lower() and "endmodule" in result_content.lower():
                                metrics["design_complete"] = True
                                break
        
        # æ£€æŸ¥éªŒè¯å®Œæˆæƒ…å†µ
        if "test" in requirements or "éªŒè¯" in requirements or "testbench" in requirements:
            verification_results = []
            for agent_id, result in all_results.items():
                if agent_id == "enhanced_real_code_review_agent":
                    verification_results.append(result)
            
            if verification_results:
                # ğŸ”§ ä¿®å¤ï¼šæ›´ä¸¥æ ¼çš„éªŒè¯å®Œæˆæ£€æŸ¥
                for result in verification_results:
                    if isinstance(result, dict):
                        # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸçŠ¶æ€
                        if result.get("success", False):
                            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†æµ‹è¯•å°æ–‡ä»¶
                            generated_files = result.get("generated_files", [])
                            if any("tb" in file.lower() or "testbench" in file.lower() for file in generated_files):
                                metrics["verification_complete"] = True
                                break
                            # æ£€æŸ¥ç»“æœå†…å®¹æ˜¯å¦åŒ…å«æµ‹è¯•å°
                            result_content = str(result.get("result", ""))
                            if "testbench" in result_content.lower() or "simulation" in result_content.lower():
                                metrics["verification_complete"] = True
                                break
        
        # æ£€æŸ¥æ–‡æ¡£å®Œæˆæƒ…å†µ
        doc_results = []
        for result in all_results.values():
            if "documentation" in str(result).lower() or "æ–‡æ¡£" in str(result):
                doc_results.append(result)
        
        metrics["documentation_complete"] = len(doc_results) > 0
        
        # æ£€æŸ¥æµ‹è¯•å®Œæˆæƒ…å†µ
        test_results = []
        for result in all_results.values():
            if "test" in str(result).lower() or "ä»¿çœŸ" in str(result):
                test_results.append(result)
        
        metrics["testing_complete"] = len(test_results) > 0
        
        # æ£€æŸ¥è´¨é‡æ£€æŸ¥
        quality_results = []
        for result in all_results.values():
            if "quality" in str(result).lower() or "è´¨é‡" in str(result):
                quality_results.append(result)
        
        metrics["quality_checks_passed"] = len(quality_results) > 0
        
        # æ™ºèƒ½ä½“æ€§èƒ½åˆ†æ
        for agent_id, result in all_results.items():
            if isinstance(result, dict):
                execution_time = result.get("execution_time", 0)
                metrics["agent_performance"][agent_id] = {
                    "execution_time": execution_time,
                    "success": result.get("success", False),
                    "quality_score": result.get("quality_score", 0)
                }
                metrics["execution_time"] += execution_time
        
        metrics["total_iterations"] = task_context.iteration_count
        
        return metrics
    
    def _calculate_completion_score(self, detailed_analysis: Dict[str, Any],
                                  completion_criteria: Dict[str, Any] = None) -> float:
        """è®¡ç®—å®Œæˆåˆ†æ•°"""
        score = 0.0
        
        # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®å®Œæˆæ ‡å‡†è°ƒæ•´æƒé‡
        if completion_criteria:
            # å¦‚æœæœ‰ç‰¹å®šçš„å®Œæˆæ ‡å‡†ï¼Œè°ƒæ•´æƒé‡
            if completion_criteria.get("require_testbench", False):
                weights = {
                    "design_complete": 0.40,
                    "verification_complete": 0.40,
                    "documentation_complete": 0.10,
                    "testing_complete": 0.05,
                    "quality_checks_passed": 0.05
                }
            elif completion_criteria.get("require_verification", False):
                weights = {
                    "design_complete": 0.35,
                    "verification_complete": 0.35,
                    "documentation_complete": 0.15,
                    "testing_complete": 0.10,
                    "quality_checks_passed": 0.05
                }
            else:
                weights = {
                    "design_complete": 0.35,
                    "verification_complete": 0.30,
                    "documentation_complete": 0.15,
                    "testing_complete": 0.15,
                    "quality_checks_passed": 0.05
                }
        else:
            # é»˜è®¤æƒé‡
            weights = {
                "design_complete": 0.35,
                "verification_complete": 0.30,
                "documentation_complete": 0.15,
                "testing_complete": 0.15,
                "quality_checks_passed": 0.05
            }
        
        # åº”ç”¨æƒé‡
        for metric, weight in weights.items():
            if detailed_analysis.get(metric, False):
                score += weight * 100
        
        # è€ƒè™‘æ™ºèƒ½ä½“æ€§èƒ½
        agent_performance = detailed_analysis.get("agent_performance", {})
        if agent_performance:
            avg_quality = sum(
                perf.get("quality_score", 0) for perf in agent_performance.values()
            ) / len(agent_performance)
            score += avg_quality * 0.1  # 10%æƒé‡ç»™è´¨é‡åˆ†æ•°
        
        # ğŸ”§ ä¿®å¤ï¼šè€ƒè™‘æ‰§è¡Œæ—¶é—´æ•ˆç‡
        execution_time = detailed_analysis.get("execution_time", 0)
        if execution_time > 0:
            # æ‰§è¡Œæ—¶é—´è¶ŠçŸ­ï¼Œåˆ†æ•°è¶Šé«˜ï¼ˆæœ€å¤šåŠ 5åˆ†ï¼‰
            time_bonus = min(5.0, 100.0 / execution_time)
            score += time_bonus
        
        return min(100.0, score)
    
    def _identify_missing_requirements(self, detailed_analysis: Dict[str, Any],
                                     requirements: str) -> List[str]:
        """è¯†åˆ«ç¼ºå¤±çš„éœ€æ±‚"""
        missing = []
        
        # æ£€æŸ¥è®¾è®¡éœ€æ±‚
        if ("design" in requirements or "æ¨¡å—" in requirements) and not detailed_analysis.get("design_complete", False):
            missing.append("ç¼ºå°‘Verilogæ¨¡å—è®¾è®¡")
        
        # æ£€æŸ¥éªŒè¯éœ€æ±‚
        if ("test" in requirements or "éªŒè¯" in requirements or "testbench" in requirements) and not detailed_analysis.get("verification_complete", False):
            missing.append("ç¼ºå°‘æµ‹è¯•å°å’ŒéªŒè¯")
        
        # æ£€æŸ¥æ–‡æ¡£éœ€æ±‚
        if not detailed_analysis.get("documentation_complete", False):
            missing.append("ç¼ºå°‘è®¾è®¡æ–‡æ¡£")
        
        # æ£€æŸ¥æµ‹è¯•éœ€æ±‚
        if not detailed_analysis.get("testing_complete", False):
            missing.append("ç¼ºå°‘æµ‹è¯•æ‰§è¡Œ")
        
        # æ£€æŸ¥è´¨é‡éœ€æ±‚
        if not detailed_analysis.get("quality_checks_passed", False):
            missing.append("ç¼ºå°‘è´¨é‡æ£€æŸ¥")
        
        return missing
    
    def _determine_completion_status(self, completion_score: float,
                                   missing_requirements: List[str],
                                   completion_criteria: Dict[str, Any] = None) -> bool:
        """ç¡®å®šå®ŒæˆçŠ¶æ€ - ä¿®å¤ç‰ˆæœ¬"""
        
        # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
        self.logger.info(f"ğŸ” ä»»åŠ¡å®ŒæˆçŠ¶æ€æ£€æŸ¥:")
        self.logger.info(f"   å®Œæˆåˆ†æ•°: {completion_score}")
        self.logger.info(f"   ç¼ºå¤±é¡¹æ•°é‡: {len(missing_requirements)}")
        self.logger.info(f"   ç¼ºå¤±é¡¹: {missing_requirements}")
        self.logger.info(f"   å®Œæˆæ ‡å‡†: {completion_criteria}")
        
        # ä½¿ç”¨è‡ªå®šä¹‰å®Œæˆæ ‡å‡†
        if completion_criteria:
            required_score = completion_criteria.get("required_score", 80.0)
            max_missing_items = completion_criteria.get("max_missing_items", 0)
            
            # ğŸ”§ ä¿®å¤ï¼šæ›´ä¸¥æ ¼çš„å®Œæˆæ ‡å‡†
            if completion_criteria.get("require_testbench", False):
                # å¦‚æœéœ€è¦æµ‹è¯•å°ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«æµ‹è¯•å°ç›¸å…³ç»“æœ
                has_testbench = any("testbench" in req.lower() or "æµ‹è¯•å°" in req for req in missing_requirements)
                if has_testbench:
                    self.logger.info(f"âŒ ä»»åŠ¡æœªå®Œæˆ: ç¼ºå°‘æµ‹è¯•å°")
                    return False
            
            if completion_criteria.get("require_verification", False):
                # å¦‚æœéœ€è¦éªŒè¯ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«éªŒè¯ç›¸å…³ç»“æœ
                has_verification = any("verification" in req.lower() or "éªŒè¯" in req for req in missing_requirements)
                if has_verification:
                    self.logger.info(f"âŒ ä»»åŠ¡æœªå®Œæˆ: ç¼ºå°‘éªŒè¯")
                    return False
            
            is_completed = (completion_score >= required_score and 
                          len(missing_requirements) <= max_missing_items)
            self.logger.info(f"ğŸ” è‡ªå®šä¹‰æ ‡å‡†æ£€æŸ¥ç»“æœ: {is_completed}")
            return is_completed
        
        # ğŸ”§ ä¿®å¤ï¼šé»˜è®¤å®Œæˆæ ‡å‡† - é’ˆå¯¹Verilogè®¾è®¡ä»»åŠ¡çš„ç‰¹æ®Šé€»è¾‘
        # å¯¹äºVerilogè®¾è®¡ä»»åŠ¡ï¼Œå¿…é¡»åŒ…å«è®¾è®¡å’ŒéªŒè¯ä¸¤ä¸ªé˜¶æ®µ
        if len(missing_requirements) > 0:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å…³é”®ç¼ºå¤±é¡¹
            critical_missing = [
                "ç¼ºå°‘Verilogæ¨¡å—è®¾è®¡",
                "ç¼ºå°‘æµ‹è¯•å°å’ŒéªŒè¯",
                "ç¼ºå°‘è®¾è®¡æ–‡æ¡£"
            ]
            
            for missing in missing_requirements:
                if any(critical in missing for critical in critical_missing):
                    self.logger.info(f"âŒ ä»»åŠ¡æœªå®Œæˆ: å‘ç°å…³é”®ç¼ºå¤±é¡¹ '{missing}'")
                    return False
        
        # ğŸ”§ ä¿®å¤ï¼šæ›´ä¸¥æ ¼çš„å®Œæˆæ¡ä»¶
        is_completed = completion_score >= 80.0 and len(missing_requirements) == 0
        self.logger.info(f"ğŸ” é»˜è®¤æ ‡å‡†æ£€æŸ¥ç»“æœ: {is_completed}")
        return is_completed
    
    def _assess_overall_quality(self, detailed_analysis: Dict[str, Any],
                              completion_score: float) -> str:
        """è¯„ä¼°æ•´ä½“è´¨é‡"""
        if completion_score >= 90:
            return "excellent"
        elif completion_score >= 80:
            return "good"
        elif completion_score >= 70:
            return "fair"
        elif completion_score >= 60:
            return "poor"
        else:
            return "very_poor"
    
    def _determine_next_steps(self, is_completed: bool,
                            missing_requirements: List[str],
                            detailed_analysis: Dict[str, Any]) -> List[str]:
        """ç¡®å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
        next_steps = []
        
        if is_completed:
            next_steps.append("ä»»åŠ¡å®Œæˆï¼Œå¯ä»¥äº¤ä»˜ç»“æœ")
        else:
            # æ ¹æ®ç¼ºå¤±é¡¹ç¡®å®šä¸‹ä¸€æ­¥
            if "ç¼ºå°‘Verilogæ¨¡å—è®¾è®¡" in missing_requirements:
                next_steps.append("åˆ†é…è®¾è®¡ä»»åŠ¡ç»™enhanced_real_verilog_agent")
            
            if "ç¼ºå°‘æµ‹è¯•å°å’ŒéªŒè¯" in missing_requirements:
                next_steps.append("åˆ†é…éªŒè¯ä»»åŠ¡ç»™enhanced_real_code_review_agent")
            
            if "ç¼ºå°‘è®¾è®¡æ–‡æ¡£" in missing_requirements:
                next_steps.append("è¦æ±‚æ™ºèƒ½ä½“ç”Ÿæˆè®¾è®¡æ–‡æ¡£")
            
            if "ç¼ºå°‘æµ‹è¯•æ‰§è¡Œ" in missing_requirements:
                next_steps.append("æ‰§è¡Œæµ‹è¯•éªŒè¯")
            
            if "ç¼ºå°‘è´¨é‡æ£€æŸ¥" in missing_requirements:
                next_steps.append("è¿›è¡Œä»£ç è´¨é‡æ£€æŸ¥")
        
        return next_steps
    
    def _calculate_performance_metrics(self, task_context: TaskContext,
                                     all_results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            "total_execution_time": 0.0,
            "average_execution_time": 0.0,
            "success_rate": 0.0,
            "agent_utilization": {},
            "iteration_efficiency": 0.0
        }
        
        # ğŸ¯ ä¿®å¤ï¼šå¤„ç†all_resultså¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ
        if isinstance(all_results, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            results_dict = {}
            for i, result in enumerate(all_results):
                if isinstance(result, dict):
                    agent_id = result.get("agent_id", f"agent_{i}")
                    results_dict[agent_id] = result
                else:
                    results_dict[f"result_{i}"] = result
            all_results = results_dict
            self.logger.info(f"ğŸ¯ æ€§èƒ½æŒ‡æ ‡è®¡ç®—ä¸­å°†åˆ—è¡¨æ ¼å¼çš„all_resultsè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ŒåŒ…å«{len(all_results)}ä¸ªç»“æœ")
        
        # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
        total_time = 0.0
        success_count = 0
        total_count = len(all_results)
        
        for result in all_results.values():
            if isinstance(result, dict):
                execution_time = result.get("execution_time", 0)
                total_time += execution_time
                
                if result.get("success", False):
                    success_count += 1
        
        metrics["total_execution_time"] = total_time
        metrics["average_execution_time"] = total_time / total_count if total_count > 0 else 0
        metrics["success_rate"] = success_count / total_count if total_count > 0 else 0
        
        # è®¡ç®—è¿­ä»£æ•ˆç‡
        if task_context.iteration_count > 0:
            # ğŸ¯ ä¿®å¤ï¼šä½¿ç”¨task_contextä¸­çš„quality_scoreè€Œä¸æ˜¯æœªå®šä¹‰çš„completion_score
            metrics["iteration_efficiency"] = task_context.quality_score / task_context.iteration_count
        
        # æ™ºèƒ½ä½“åˆ©ç”¨ç‡
        for agent_id, agent_info in self.registered_agents.items():
            if agent_id in all_results:
                metrics["agent_utilization"][agent_id] = {
                    "tasks_assigned": 1,
                    "success_rate": 1.0 if all_results[agent_id].get("success", False) else 0.0,
                    "average_time": all_results[agent_id].get("execution_time", 0)
                }
        
        return metrics
    
    def _update_performance_metrics(self, task_context: TaskContext, success: bool):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics["total_tasks"] += 1
        
        if success:
            self.performance_metrics["successful_tasks"] += 1
        else:
            self.performance_metrics["failed_tasks"] += 1
        
        # æ›´æ–°å¹³å‡å®Œæˆæ—¶é—´
        execution_time = time.time() - task_context.start_time
        total_tasks = self.performance_metrics["total_tasks"]
        current_avg = self.performance_metrics["average_completion_time"]
        
        self.performance_metrics["average_completion_time"] = (
            (current_avg * (total_tasks - 1) + execution_time) / total_tasks
        )
    
    async def _tool_query_agent_status(self, agent_id: str,
                                     include_performance: bool = True,
                                     include_history: bool = False) -> Dict[str, Any]:
        """å¢å¼ºçš„æ™ºèƒ½ä½“çŠ¶æ€æŸ¥è¯¢"""
        
        try:
            if agent_id not in self.registered_agents:
                return {
                    "success": False,
                    "error": f"æ™ºèƒ½ä½“ä¸å­˜åœ¨: {agent_id}"
                }
            
            agent_info = self.registered_agents[agent_id]
            
            # åŸºç¡€çŠ¶æ€ä¿¡æ¯
            status_info = {
                "success": True,
                "agent_id": agent_id,
                "status": agent_info.status.value,
                "capabilities": [cap.value for cap in agent_info.capabilities],
                "specialty": agent_info.specialty,
                "conversation_id": agent_info.conversation_id,
                "last_used": agent_info.last_used
            }
            
            # æ€§èƒ½æŒ‡æ ‡
            if include_performance:
                performance_metrics = self._calculate_agent_performance_metrics(agent_info)
                status_info["performance_metrics"] = performance_metrics
            
            # å†å²è®°å½•
            if include_history:
                history_data = self._get_agent_history(agent_id)
                status_info["history"] = history_data
            
            # å¥åº·çŠ¶æ€è¯„ä¼°
            health_assessment = self._assess_agent_health(agent_info)
            status_info["health_assessment"] = health_assessment
            
            # æ¨èä»»åŠ¡ç±»å‹
            recommended_tasks = self._get_recommended_task_types(agent_info)
            status_info["recommended_tasks"] = recommended_tasks
            
            return status_info
            
        except Exception as e:
            self.logger.error(f"âŒ æŸ¥è¯¢æ™ºèƒ½ä½“çŠ¶æ€å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _calculate_agent_performance_metrics(self, agent_info: AgentInfo) -> Dict[str, Any]:
        """è®¡ç®—æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ ‡"""
        total_tasks = agent_info.success_count + agent_info.failure_count
        
        metrics = {
            "total_tasks": total_tasks,
            "success_count": agent_info.success_count,
            "failure_count": agent_info.failure_count,
            "success_rate": agent_info.success_count / total_tasks if total_tasks > 0 else 0.0,
            "average_response_time": agent_info.average_response_time,
            "total_execution_time": agent_info.total_execution_time,
            "consecutive_successes": agent_info.consecutive_successes,
            "consecutive_failures": agent_info.consecutive_failures,
            "last_success_time": agent_info.last_success_time,
            "last_failure_time": agent_info.last_failure_time
        }
        
        # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
        if agent_info.last_success_time and agent_info.last_failure_time:
            if agent_info.last_success_time > agent_info.last_failure_time:
                metrics["recent_trend"] = "improving"
            else:
                metrics["recent_trend"] = "declining"
        elif agent_info.last_success_time:
            metrics["recent_trend"] = "stable_success"
        elif agent_info.last_failure_time:
            metrics["recent_trend"] = "stable_failure"
        else:
            metrics["recent_trend"] = "unknown"
        
        # è®¡ç®—å¯é æ€§è¯„åˆ†
        reliability_score = self._calculate_reliability_score(agent_info)
        metrics["reliability_score"] = reliability_score
        
        return metrics
    
    def _calculate_reliability_score(self, agent_info: AgentInfo) -> float:
        """è®¡ç®—å¯é æ€§è¯„åˆ†"""
        total_tasks = agent_info.success_count + agent_info.failure_count
        if total_tasks == 0:
            return 0.0
        
        # åŸºç¡€æˆåŠŸç‡
        base_score = agent_info.success_count / total_tasks * 100
        
        # è¿ç»­æˆåŠŸå¥–åŠ±
        consecutive_bonus = min(10.0, agent_info.consecutive_successes * 2.0)
        
        # å“åº”æ—¶é—´å¥–åŠ±
        time_bonus = 0.0
        if agent_info.average_response_time > 0:
            if agent_info.average_response_time < 30:
                time_bonus = 10.0
            elif agent_info.average_response_time < 60:
                time_bonus = 5.0
        
        # è¿ç»­å¤±è´¥æƒ©ç½š
        consecutive_penalty = min(20.0, agent_info.consecutive_failures * 5.0)
        
        final_score = base_score + consecutive_bonus + time_bonus - consecutive_penalty
        return max(0.0, min(100.0, final_score))
    
    def _get_agent_history(self, agent_id: str) -> Dict[str, Any]:
        """è·å–æ™ºèƒ½ä½“å†å²è®°å½•"""
        # è¿™é‡Œå¯ä»¥ä»æ•°æ®åº“æˆ–æ—¥å¿—ä¸­è·å–æ›´è¯¦ç»†çš„å†å²è®°å½•
        # ç›®å‰è¿”å›åŸºæœ¬çš„å†å²ç»Ÿè®¡
        return {
            "total_executions": 0,
            "recent_performance": [],
            "common_issues": [],
            "improvement_trend": "stable"
        }
    
    def _assess_agent_health(self, agent_info: AgentInfo) -> Dict[str, Any]:
        """è¯„ä¼°æ™ºèƒ½ä½“å¥åº·çŠ¶æ€"""
        health = {
            "status": "healthy",
            "score": 100.0,
            "issues": [],
            "recommendations": []
        }
        
        # æ£€æŸ¥è¿ç»­å¤±è´¥
        if agent_info.consecutive_failures > 3:
            health["status"] = "warning"
            health["score"] -= 30
            health["issues"].append(f"è¿ç»­å¤±è´¥{agent_info.consecutive_failures}æ¬¡")
            health["recommendations"].append("å»ºè®®æ£€æŸ¥æ™ºèƒ½ä½“é…ç½®æˆ–æ›´æ¢æ™ºèƒ½ä½“")
        
        # æ£€æŸ¥æˆåŠŸç‡
        total_tasks = agent_info.success_count + agent_info.failure_count
        if total_tasks > 0:
            success_rate = agent_info.success_count / total_tasks
            if success_rate < 0.5:
                health["status"] = "critical"
                health["score"] -= 50
                health["issues"].append(f"æˆåŠŸç‡è¿‡ä½: {success_rate:.1%}")
                health["recommendations"].append("å»ºè®®é‡æ–°é…ç½®æˆ–æ›´æ¢æ™ºèƒ½ä½“")
            elif success_rate < 0.7:
                health["status"] = "warning"
                health["score"] -= 20
                health["issues"].append(f"æˆåŠŸç‡åä½: {success_rate:.1%}")
                health["recommendations"].append("å»ºè®®ä¼˜åŒ–æ™ºèƒ½ä½“é…ç½®")
        
        # æ£€æŸ¥å“åº”æ—¶é—´
        if agent_info.average_response_time > 120:
            health["status"] = "warning"
            health["score"] -= 15
            health["issues"].append(f"å“åº”æ—¶é—´è¿‡é•¿: {agent_info.average_response_time:.1f}ç§’")
            health["recommendations"].append("å»ºè®®ä¼˜åŒ–æ€§èƒ½æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
        
        # æ£€æŸ¥é•¿æœŸæœªä½¿ç”¨
        if agent_info.last_used:
            time_since_last_use = time.time() - agent_info.last_used
            if time_since_last_use > 3600:  # 1å°æ—¶
                health["issues"].append(f"é•¿æœŸæœªä½¿ç”¨: {time_since_last_use/3600:.1f}å°æ—¶")
        
        health["score"] = max(0.0, health["score"])
        
        return health
    
    def _get_recommended_task_types(self, agent_info: AgentInfo) -> List[str]:
        """è·å–æ¨èçš„ä»»åŠ¡ç±»å‹"""
        recommended = []
        
        # åŸºäºæ™ºèƒ½ä½“IDæ¨è
        if agent_info.agent_id == "enhanced_real_verilog_agent":
            recommended.extend(["design", "code_generation", "module_implementation"])
        elif agent_info.agent_id == "enhanced_real_code_review_agent":
            recommended.extend(["verification", "analysis", "debug", "testbench_generation"])
        
        # åŸºäºå†å²è¡¨ç°æ¨è
        if agent_info.consecutive_successes > 2:
            recommended.append("high_priority_tasks")
        
        if agent_info.average_response_time < 30:
            recommended.append("time_sensitive_tasks")
        
        return list(set(recommended))  # å»é‡
    
    async def _collect_agent_conversations(self, task_context: TaskContext):
        """æ”¶é›†å·²åˆ†é…æ™ºèƒ½ä½“çš„å¯¹è¯å†å²åˆ°ä»»åŠ¡ä¸Šä¸‹æ–‡"""
        try:
            for agent_id, agent_result in task_context.agent_results.items():
                # è·å–æ™ºèƒ½ä½“å®ä¾‹
                agent_instance = None
                
                # ä»æ³¨å†Œçš„æ™ºèƒ½ä½“ä¸­æŸ¥æ‰¾
                for registered_agent_id, registered_agent in self.registered_agents.items():
                    if registered_agent_id == agent_id or registered_agent.agent_id == agent_id:
                        agent_instance = registered_agent
                        break
                
                if agent_instance and hasattr(agent_instance, 'conversation_history'):
                    self.logger.info(f"ğŸ“¥ æ”¶é›†æ™ºèƒ½ä½“ {agent_id} çš„å¯¹è¯å†å²: {len(agent_instance.conversation_history)} æ¡æ¶ˆæ¯")
                    
                    # å°†æ™ºèƒ½ä½“çš„å¯¹è¯å†å²åˆå¹¶åˆ°ä»»åŠ¡ä¸Šä¸‹æ–‡
                    for msg in agent_instance.conversation_history:
                        # é¿å…é‡å¤æ·»åŠ ç›¸åŒçš„æ¶ˆæ¯
                        msg_signature = f"{msg.get('role', '')}_{msg.get('agent_id', '')}_{hash(msg.get('content', ''))}"
                        existing_signatures = [
                            f"{existing_msg.get('role', '')}_{existing_msg.get('agent_id', '')}_{hash(existing_msg.get('content', ''))}"
                            for existing_msg in task_context.conversation_history
                        ]
                        
                        if msg_signature not in existing_signatures:
                            # æ ‡è®°æ¶ˆæ¯æ¥æºå¹¶æ·»åŠ åˆ°ä»»åŠ¡ä¸Šä¸‹æ–‡
                            msg_copy = msg.copy()
                            msg_copy['source_agent'] = agent_id
                            msg_copy['collected_timestamp'] = time.time()
                            task_context.conversation_history.append(msg_copy)
                else:
                    self.logger.warning(f"âš ï¸ æ— æ³•æ‰¾åˆ°æ™ºèƒ½ä½“ {agent_id} çš„å®ä¾‹æˆ–å¯¹è¯å†å²")
                    
        except Exception as e:
            self.logger.error(f"âŒ æ”¶é›†æ™ºèƒ½ä½“å¯¹è¯å†å²å¤±è´¥: {str(e)}")
    
    def _collect_final_result(self, task_context: TaskContext, 
                            coordination_result: str) -> Dict[str, Any]:
        """æ”¶é›†æœ€ç»ˆç»“æœ"""
        
        # ğŸ†• è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_execution_time = time.time() - task_context.start_time
        performance_metrics = {
            "total_execution_time": total_execution_time,
            "average_tool_execution_time": sum(t.get("execution_time", 0) or 0 for t in task_context.tool_executions) / max(len(task_context.tool_executions), 1),
            "total_file_operations": len(task_context.file_operations),
            "total_workflow_stages": len(task_context.workflow_stages),
            "success_rate": len([t for t in task_context.tool_executions if t.get("success", True)]) / max(len(task_context.tool_executions), 1)
        }
        
        # æ›´æ–°TaskContextçš„æ€§èƒ½æŒ‡æ ‡
        task_context.update_performance_metrics(performance_metrics)
        
        return {
            "success": True,
            "task_id": task_context.task_id,
            "coordination_result": coordination_result,
            "agent_results": task_context.agent_results,
            "execution_summary": {
                "total_iterations": task_context.iteration_count,
                "assigned_agents": list(task_context.agent_results.keys()),
                "execution_time": total_execution_time
            },
            "conversation_history": task_context.conversation_history,
            # ğŸ†• åŒ…å«å®Œæ•´çš„TaskContextæ•°æ®ç”¨äºGradioå¯è§†åŒ–
            "task_context": {
                "tool_executions": task_context.tool_executions,
                "agent_interactions": task_context.agent_interactions,
                "performance_metrics": task_context.performance_metrics,
                "workflow_stages": task_context.workflow_stages,
                "file_operations": task_context.file_operations,
                "execution_timeline": task_context.execution_timeline,
                "llm_conversations": task_context.llm_conversations,
                "data_collection_summary": task_context.get_data_collection_summary()
            }
        }
    
    def get_registered_agents(self) -> Dict[str, AgentInfo]:
        """è·å–å·²æ³¨å†Œçš„æ™ºèƒ½ä½“"""
        return self.registered_agents
    
    def get_registered_tools(self) -> List[Dict[str, Any]]:
        """è·å–å·²æ³¨å†Œçš„å·¥å…·"""
        return self.enhanced_tools if hasattr(self, 'enhanced_tools') else [].copy()
    
    def get_active_tasks(self) -> Dict[str, TaskContext]:
        """è·å–æ´»è·ƒä»»åŠ¡"""
        return self.active_tasks.copy()
    
    async def _run_coordination_loop(self, task_context: TaskContext, initial_result: str, 
                                   conversation_id: str, max_iterations: int) -> Dict[str, Any]:
        """è¿è¡ŒæŒç»­åè°ƒå¾ªç¯ï¼Œç›‘å¬æ™ºèƒ½ä½“ç»“æœå¹¶ç»§ç»­åè°ƒ"""
        self.logger.info(f"ğŸ”„ å¼€å§‹æŒç»­åè°ƒå¾ªç¯ - ä»»åŠ¡ID: {task_context.task_id}")
        self.logger.info(f"ğŸ” å½“å‰æ™ºèƒ½ä½“ç»“æœæ•°é‡: {len(task_context.agent_results)}")
        self.logger.info(f"ğŸ” æ™ºèƒ½ä½“ç»“æœé”®: {list(task_context.agent_results.keys())}")
        
        # ğŸ” æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ™ºèƒ½ä½“ç»“æœï¼Œå¦‚æœæ²¡æœ‰åˆ™å¼ºåˆ¶åˆ†é…ä»»åŠ¡
        if not task_context.agent_results:
            self.logger.warning("âš ï¸ æ²¡æœ‰æ™ºèƒ½ä½“ç»“æœï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶åˆ†é…ä»»åŠ¡")
            
            # æ£€æŸ¥å¯¹è¯å†å²ä¸­æ˜¯å¦æœ‰assign_task_to_agentè°ƒç”¨
            assign_task_found = False
            for msg in task_context.conversation_history:
                if msg.get("role") == "assistant":
                    content = msg.get("content", "")
                    if "assign_task_to_agent" in content:
                        assign_task_found = True
                        break
            
            if not assign_task_found:
                self.logger.warning("âš ï¸ å¯¹è¯å†å²ä¸­æ²¡æœ‰æ‰¾åˆ°assign_task_to_agentè°ƒç”¨ï¼Œå¼ºåˆ¶åˆ†é…ä»»åŠ¡")
                forced_result = await self._force_assign_task(task_context.original_request, task_context)
                if forced_result.get("success", False):
                    self.logger.info("âœ… å¼ºåˆ¶åˆ†é…ä»»åŠ¡æˆåŠŸï¼Œç»§ç»­åè°ƒå¾ªç¯")
                else:
                    self.logger.error("âŒ å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¤±è´¥")
                    return {
                        "success": False,
                        "error": "å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¤±è´¥",
                        "task_id": task_context.task_id,
                        "debug_info": forced_result
                    }
        
        # ç­‰å¾…æ‰€æœ‰åˆ†é…çš„æ™ºèƒ½ä½“å®Œæˆ
        await self._wait_for_agents_completion(task_context)
        
        # æ”¶é›†æ™ºèƒ½ä½“æ‰§è¡Œç»“æœ
        await self._collect_agent_conversations(task_context)
        
        # ğŸ†• æ£€æŸ¥æ˜¯å¦ä¸ºå¤åˆä»»åŠ¡ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨ä¸²è¡Œå·¥ä½œæµç®¡ç†
        task_type_result = await self._tool_identify_task_type(task_context.original_request)
        if task_type_result.get("success") and task_type_result.get("is_composite"):
            self.logger.info("ğŸ”€ æ£€æµ‹åˆ°å¤åˆä»»åŠ¡ï¼Œå¯ç”¨ä¸²è¡Œå·¥ä½œæµç®¡ç†")
            workflow_result = self._manage_serial_workflow(task_context, task_type_result)
            
            # å¦‚æœå·¥ä½œæµå®Œæˆï¼Œæä¾›æœ€ç»ˆç­”æ¡ˆ
            if workflow_result.get("workflow_complete"):
                self.logger.info("âœ… ä¸²è¡Œå·¥ä½œæµå·²å®Œæˆ")
                final_answer_result = await self._tool_provide_final_answer(
                    final_summary="å¤åˆä»»åŠ¡å·²æŒ‰ä¸²è¡Œå·¥ä½œæµå®Œæˆï¼šè®¾è®¡â†’å®¡æŸ¥â†’éªŒè¯",
                    task_status="success",
                    results_summary={
                        "workflow_stages": len(task_context.workflow_stages),
                        "agents_involved": list(task_context.agent_results.keys()),
                        "files_generated": sum(len(result.get("generated_files", [])) for result in task_context.agent_results.values())
                    }
                )
                return self._collect_final_result(task_context, str(final_answer_result))
            
            # å¦‚æœéœ€è¦æ‰§è¡Œä¸‹ä¸€ä¸ªå·¥ä½œæµé˜¶æ®µ
            next_action = workflow_result.get("next_action")
            if next_action and next_action.get("type") == "assign_task":
                # æ‰§è¡Œä»»åŠ¡åˆ†é…
                assign_result = await self._tool_assign_task_to_agent(
                    agent_id=next_action["agent_id"],
                    task_description=next_action["description"],
                    task_type=next_action["task_type"],
                    priority="high"
                )
                
                if assign_result.get("success"):
                    self.logger.info(f"âœ… å·¥ä½œæµé˜¶æ®µå·²åˆ†é…ç»™ {next_action['agent_id']}")
                    # é€’å½’è°ƒç”¨å·¥ä½œæµç»§ç»­
                    return await self._run_coordination_loop(task_context, initial_result, conversation_id, max_iterations)
                else:
                    self.logger.error(f"âŒ å·¥ä½œæµé˜¶æ®µåˆ†é…å¤±è´¥: {assign_result.get('error')}")
                    return {
                        "success": False,
                        "error": f"å·¥ä½œæµé˜¶æ®µåˆ†é…å¤±è´¥: {assign_result.get('error')}",
                        "task_id": task_context.task_id
                    }
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­åè°ƒ
        continuation_needed = await self._check_coordination_continuation(task_context)
        
        if continuation_needed:
            self.logger.info(f"ğŸ”„ éœ€è¦ç»§ç»­åè°ƒ - åˆ†ææ™ºèƒ½ä½“ç»“æœå¹¶å†³å®šä¸‹ä¸€æ­¥")
            
            # æ„å»ºç»§ç»­åè°ƒçš„ä»»åŠ¡æè¿°
            continuation_task = await self._build_continuation_task(task_context)
            
            # ç»§ç»­æ‰§è¡Œåè°ƒï¼ˆåˆ†æç»“æœ + ä¸‹ä¸€æ­¥å†³ç­–ï¼‰
            continuation_result = await self.process_with_function_calling(
                user_request=continuation_task,
                max_iterations=max_iterations,
                conversation_id=f"{conversation_id}_continuation",
                preserve_context=True,
                enable_self_continuation=True,
                max_self_iterations=2
            )
            
            # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æ‰§è¡Œäº†ä»»åŠ¡å®Œæˆæ£€æŸ¥
            if "check_task_completion" in continuation_result:
                self.logger.info(f"ğŸ” æ£€æµ‹åˆ°ä»»åŠ¡å®Œæˆæ£€æŸ¥ï¼Œåˆ†æç»“æœ...")
                # è§£æä»»åŠ¡å®Œæˆæ£€æŸ¥çš„ç»“æœ
                completion_check_result = self._parse_task_completion_check(continuation_result)
                
                if completion_check_result:
                    is_completed = completion_check_result.get("is_completed", False)
                    completion_score = completion_check_result.get("completion_score", 0.0)
                    missing_requirements = completion_check_result.get("missing_requirements", [])
                    
                    self.logger.info(f"ğŸ” ä»»åŠ¡å®Œæˆæ£€æŸ¥ç»“æœ: is_completed={is_completed}, score={completion_score}")
                    self.logger.info(f"ğŸ” ç¼ºå¤±é¡¹: {missing_requirements}")
                    
                    # ğŸ”§ ä¿®å¤5: å¼ºåˆ¶å®Œæˆæ£€æŸ¥ - ç¦æ­¢åœ¨ä»»åŠ¡æœªå®Œæˆæ—¶è°ƒç”¨provide_final_answer
                    if not is_completed or completion_score < 60.0:
                        self.logger.warning(f"âš ï¸ ä»»åŠ¡æœªå®Œæˆ (å®Œæˆåˆ†æ•°: {completion_score})ï¼Œç¦æ­¢ç»“æŸä»»åŠ¡")
                        
                        # å¼ºåˆ¶é˜»æ­¢provide_final_answerè°ƒç”¨
                        continuation_result = self._block_premature_completion(
                            continuation_result, completion_check_result
                        )
                        
                        self.logger.info(f"ğŸ”„ ä»»åŠ¡æœªå®Œæˆï¼Œå¼ºåˆ¶ç»§ç»­åè°ƒå¾ªç¯")
                        # è®°å½•ç»§ç»­åè°ƒçš„ç»“æœ
                        task_context.add_conversation_message(
                            role="assistant",
                            content=continuation_result,
                            agent_id=self.agent_id,
                            metadata={"type": "coordination_continuation", "task_stage": "incomplete_task_blocked"}
                        )
                        
                        # é€’å½’è°ƒç”¨ï¼Œç›´åˆ°æ‰€æœ‰åè°ƒå®Œæˆ
                        return await self._run_coordination_loop(task_context, continuation_result, conversation_id, max_iterations)
                    else:
                        self.logger.info(f"âœ… ä»»åŠ¡å®Œæˆæ£€æŸ¥é€šè¿‡ (åˆ†æ•°: {completion_score})ï¼Œå…è®¸ç»“æŸä»»åŠ¡")
                        # è®°å½•ç»§ç»­åè°ƒçš„ç»“æœ
                        task_context.add_conversation_message(
                            role="assistant",
                            content=continuation_result,
                            agent_id=self.agent_id,
                            metadata={"type": "coordination_continuation", "task_stage": "completion"}
                        )
                        return self._collect_final_result(task_context, continuation_result)
                else:
                    self.logger.warning(f"âš ï¸ æ— æ³•è§£æä»»åŠ¡å®Œæˆæ£€æŸ¥ç»“æœï¼Œç»§ç»­åè°ƒ")
                    # ç»§ç»­åè°ƒå¾ªç¯
                    task_context.add_conversation_message(
                        role="assistant",
                        content=continuation_result,
                        agent_id=self.agent_id,
                        metadata={"type": "coordination_continuation", "task_stage": "parse_error"}
                    )
                    return await self._run_coordination_loop(task_context, continuation_result, conversation_id, max_iterations)
            else:
                # æ²¡æœ‰ä»»åŠ¡å®Œæˆæ£€æŸ¥ï¼Œè®°å½•ç»§ç»­åè°ƒçš„ç»“æœ
                task_context.add_conversation_message(
                    role="assistant",
                    content=continuation_result,
                    agent_id=self.agent_id,
                    metadata={"type": "coordination_continuation", "task_stage": "analysis"}
                )
                
                # é€’å½’è°ƒç”¨ï¼Œç›´åˆ°æ‰€æœ‰åè°ƒå®Œæˆ
                return await self._run_coordination_loop(task_context, continuation_result, conversation_id, max_iterations)
        else:
            self.logger.info(f"âœ… åè°ƒå¾ªç¯å®Œæˆ - ä»»åŠ¡ID: {task_context.task_id}")
            return self._collect_final_result(task_context, initial_result)
    
    async def _wait_for_agents_completion(self, task_context: TaskContext):
        """ç­‰å¾…æ‰€æœ‰åˆ†é…çš„æ™ºèƒ½ä½“å®Œæˆä»»åŠ¡"""
        max_wait_time = 300  # æœ€å¤§ç­‰å¾…5åˆ†é’Ÿ
        check_interval = 2   # æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡
        
        start_time = time.time()
        
        while time.time() - start_time < max_wait_time:
            # æ£€æŸ¥æ‰€æœ‰åˆ†é…çš„æ™ºèƒ½ä½“æ˜¯å¦å®Œæˆ
            all_completed = True
            for agent_id in task_context.agent_results.keys():
                if agent_id in self.registered_agents:
                    agent_info = self.registered_agents[agent_id]
                    if agent_info.status != AgentStatus.IDLE:
                        all_completed = False
                        break
            
            if all_completed:
                self.logger.info(f"âœ… æ‰€æœ‰æ™ºèƒ½ä½“æ‰§è¡Œå®Œæˆ")
                break
                
            # çŸ­æš‚ç­‰å¾…åé‡æ–°æ£€æŸ¥
            await asyncio.sleep(check_interval)
        
        if time.time() - start_time >= max_wait_time:
            self.logger.warning(f"â° ç­‰å¾…æ™ºèƒ½ä½“å®Œæˆè¶…æ—¶")
    
    async def _check_coordination_continuation(self, task_context: TaskContext) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­åè°ƒ - ä¿®å¤ç‰ˆæœ¬"""
        # ğŸ”§ æ–°å¢ï¼šæ£€æŸ¥å¯¹è¯å†å²ä¸­æ˜¯å¦æœ‰ä»»åŠ¡å®ŒæˆæŒ‡ç¤ºå™¨
        if self._has_task_completion_in_history(task_context):
            self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: å¯¹è¯å†å²ä¸­æœ‰ä»»åŠ¡å®ŒæˆæŒ‡ç¤ºå™¨ï¼Œæ— éœ€ç»§ç»­")
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ™ºèƒ½ä½“æ‰§è¡Œç»“æœéœ€è¦åˆ†æ
        if not task_context.agent_results:
            self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: æ²¡æœ‰æ™ºèƒ½ä½“ç»“æœï¼Œéœ€è¦ç»§ç»­åè°ƒ")
            return True  # ğŸ†• ä¿®æ”¹ï¼šå¦‚æœæ²¡æœ‰æ™ºèƒ½ä½“ç»“æœï¼Œè¯´æ˜è¿˜æ²¡æœ‰åˆ†é…ä»»åŠ¡ï¼Œéœ€è¦ç»§ç»­åè°ƒ
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å¿…éœ€çš„é˜¶æ®µéƒ½å®Œæˆäº†
        completed_agents = set(task_context.agent_results.keys())
        self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: å·²å®Œæˆæ™ºèƒ½ä½“: {completed_agents}")
        
        # ğŸ”§ ä¿®å¤ï¼šæ›´ä¸¥æ ¼çš„Verilogè®¾è®¡ä»»åŠ¡æ£€æŸ¥
        # å¯¹äºVerilogè®¾è®¡ä»»åŠ¡ï¼Œå¿…é¡»åŒ…å«è®¾è®¡å’ŒéªŒè¯ä¸¤ä¸ªé˜¶æ®µ
        if "enhanced_real_verilog_agent" in completed_agents:
            # æ£€æŸ¥Verilogæ™ºèƒ½ä½“çš„ç»“æœæ˜¯å¦æˆåŠŸ
            verilog_result = task_context.agent_results.get("enhanced_real_verilog_agent", {})
            if isinstance(verilog_result, dict) and verilog_result.get("success", False):
                self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: Verilogè®¾è®¡æ™ºèƒ½ä½“å·²å®Œæˆ")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“
                if "enhanced_real_code_review_agent" not in completed_agents:
                    self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: éœ€è¦ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“è¿›è¡ŒéªŒè¯")
                    return True
                else:
                    # æ£€æŸ¥ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“çš„ç»“æœ
                    review_result = task_context.agent_results.get("enhanced_real_code_review_agent", {})
                    if isinstance(review_result, dict) and review_result.get("success", False):
                        self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“å·²å®Œæˆ")
                    else:
                        self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“æœªæˆåŠŸå®Œæˆï¼Œéœ€è¦é‡è¯•")
                        return True
            else:
                self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: Verilogè®¾è®¡æ™ºèƒ½ä½“æœªæˆåŠŸå®Œæˆï¼Œéœ€è¦é‡è¯•")
                return True
        
        # å¦‚æœä¸¤ä¸ªæ™ºèƒ½ä½“éƒ½å®Œæˆäº†ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æœ€ç»ˆç­”æ¡ˆ
        if len(completed_agents) >= 2:
            # æ£€æŸ¥æ˜¯å¦å·²ç»æä¾›äº†æœ€ç»ˆç­”æ¡ˆ
            for msg in task_context.conversation_history:
                if msg.get("metadata", {}).get("type") == "final_answer":
                    self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: å·²æœ‰æœ€ç»ˆç­”æ¡ˆï¼Œæ— éœ€ç»§ç»­")
                    return False
            self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: éœ€è¦æä¾›æœ€ç»ˆç­”æ¡ˆ")
            return True
        
        # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–éœ€è¦å¤„ç†çš„æ™ºèƒ½ä½“
        if len(completed_agents) == 0:
            self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: æ²¡æœ‰æ™ºèƒ½ä½“å®Œæˆï¼Œéœ€è¦ç»§ç»­åè°ƒ")
            return True
        
        self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ")
        return False
    
    async def _build_continuation_task(self, task_context: TaskContext) -> str:
        """æ„å»ºç»§ç»­åè°ƒçš„ä»»åŠ¡æè¿°"""
        # è·å–æ™ºèƒ½ä½“æ‰§è¡Œç»“æœ
        results_summary = []
        for agent_id, result_info in task_context.agent_results.items():
            if isinstance(result_info, dict) and 'response' in result_info:
                results_summary.append(f"- {agent_id}: {result_info.get('response', 'Unknown')[:2000]}...")
        
        results_text = "\n".join(results_summary) if results_summary else "æ— æ™ºèƒ½ä½“ç»“æœ"
        
        # ğŸ”§ ä¿®æ­£3: å¼ºåˆ¶æ‰§è¡Œè®¾è®¡â†’éªŒè¯ä¸¤é˜¶æ®µæµç¨‹
        completed_agents = set(task_context.agent_results.keys())
        workflow_stage = self._determine_workflow_stage(completed_agents)
        
        # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·æ‰§è¡Œå¤±è´¥çš„æƒ…å†µ
        failed_tools = []
        for tool_exec in task_context.tool_executions:
            if not tool_exec.get("success", True):
                failed_tools.append(f"- {tool_exec.get('tool_name')}: {tool_exec.get('error', 'Unknown error')}")
        
        failed_tools_text = "\n".join(failed_tools) if failed_tools else "æ— å¤±è´¥çš„å·¥å…·è°ƒç”¨"
        
        # æ ¹æ®å·¥ä½œæµé˜¶æ®µå’Œå¤±è´¥æƒ…å†µç”Ÿæˆä¸åŒçš„åè°ƒæŒ‡å¯¼
        if workflow_stage == "initial" and failed_tools:
            # ğŸ”§ ä¿®å¤ï¼šå¤„ç†åˆå§‹é˜¶æ®µå·¥å…·è°ƒç”¨å¤±è´¥çš„æƒ…å†µ
            coordination_guide = f"""
**ğŸš¨ ç´§æ€¥ä¿®å¤ - å·¥å…·è°ƒç”¨å¤±è´¥**:
æ£€æµ‹åˆ°ä»¥ä¸‹å·¥å…·è°ƒç”¨å¤±è´¥ï¼š
{failed_tools_text}

**ä¿®å¤ç­–ç•¥**:
1. å¦‚æœ `assign_task_to_agent` å¤±è´¥ï¼Œå¿…é¡»é‡æ–°è°ƒç”¨è¯¥å·¥å…·
2. ä¸è¦è°ƒç”¨ `analyze_agent_result`ï¼Œå› ä¸ºæ²¡æœ‰å¯åˆ†æçš„ç»“æœ
3. ç¡®ä¿ `assign_task_to_agent` å‚æ•°æ­£ç¡®ï¼š
   - agent_id: "enhanced_real_verilog_agent"
   - task_description: å®Œæ•´çš„ä»»åŠ¡æè¿°
   - ä¸è¦åŒ…å« task_id å‚æ•°ï¼ˆè¯¥å·¥å…·ä¸æ”¯æŒæ­¤å‚æ•°ï¼‰

**é‡è¦**: å¿…é¡»å…ˆæˆåŠŸåˆ†é…ä»»åŠ¡ï¼Œç„¶åæ‰èƒ½åˆ†æç»“æœ"""
        
        elif workflow_stage == "design_completed":
            coordination_guide = """
**ğŸš¨ å¼ºåˆ¶å·¥ä½œæµç¨‹ - è®¾è®¡é˜¶æ®µå·²å®Œæˆ**:
1. é¦–å…ˆè°ƒç”¨ `analyze_agent_result` åˆ†æVerilogè®¾è®¡æ™ºèƒ½ä½“çš„ç»“æœ
2. **å¼ºåˆ¶ä¸‹ä¸€æ­¥**: æ— è®ºåˆ†æç»“æœå¦‚ä½•ï¼Œéƒ½å¿…é¡»è¿›å…¥éªŒè¯é˜¶æ®µ
3. å¿…é¡»è°ƒç”¨ `assign_task_to_agent` åˆ†é…ä»»åŠ¡ç»™ `enhanced_real_code_review_agent`
4. ä»»åŠ¡æè¿°åº”åŒ…å«ï¼šå®¡æŸ¥è®¾è®¡ä»£ç ã€ç”Ÿæˆæµ‹è¯•å°ã€æ‰§è¡Œä»¿çœŸéªŒè¯

**é‡è¦**: è¿™æ˜¯å¼ºåˆ¶çš„ä¸¤é˜¶æ®µæµç¨‹ï¼Œè®¾è®¡å®Œæˆåå¿…é¡»è¿›è¡ŒéªŒè¯"""
        
        elif workflow_stage == "verification_completed":
            coordination_guide = """
**ğŸ¯ å·¥ä½œæµç¨‹ - éªŒè¯é˜¶æ®µå·²å®Œæˆ**:
1. é¦–å…ˆè°ƒç”¨ `analyze_agent_result` åˆ†æä»£ç å®¡æŸ¥æ™ºèƒ½ä½“çš„ç»“æœ
2. è°ƒç”¨ `check_task_completion` æ£€æŸ¥æ•´ä½“ä»»åŠ¡å®ŒæˆçŠ¶æ€
3. å¦‚æœä»»åŠ¡å®Œæ•´ï¼Œè°ƒç”¨ `provide_final_answer` æä¾›æœ€ç»ˆç­”æ¡ˆ"""
        
        else:
            # é»˜è®¤æƒ…å†µ
            coordination_guide = """
**åè°ƒè¦æ±‚**:
1. é¦–å…ˆè°ƒç”¨ `analyze_agent_result` åˆ†æå·²å®Œæˆæ™ºèƒ½ä½“çš„ç»“æœè´¨é‡
2. åŸºäºåˆ†æç»“æœå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨:
   - å¦‚æœæ˜¯è®¾è®¡é˜¶æ®µå®Œæˆï¼Œå¿…é¡»åˆ†é…ç»™ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“è¿›è¡ŒéªŒè¯
   - å¦‚æœæ˜¯éªŒè¯é˜¶æ®µå®Œæˆï¼Œè°ƒç”¨ `check_task_completion`
   - å¦‚æœä¸¤ä¸ªé˜¶æ®µéƒ½å®Œæˆï¼Œè°ƒç”¨ `provide_final_answer`
   - å¦‚æœè´¨é‡ä¸æ»¡è¶³è¦æ±‚ï¼Œé‡æ–°åˆ†é…ç»™åŒä¸€æ™ºèƒ½ä½“æ”¹è¿›"""
        
        return f"""ğŸ”„ åè°ƒä»»åŠ¡ç»§ç»­ - åˆ†ææ™ºèƒ½ä½“æ‰§è¡Œç»“æœå¹¶å†³å®šä¸‹ä¸€æ­¥

**å½“å‰ä»»åŠ¡çŠ¶æ€**:
- ä»»åŠ¡ID: {task_context.task_id}
- å·²å®Œæˆçš„æ™ºèƒ½ä½“: {list(task_context.agent_results.keys())}
- å·¥ä½œæµé˜¶æ®µ: {workflow_stage}
- æ‰§è¡Œç»“æœæ‘˜è¦:
{results_text}

**å·¥å…·æ‰§è¡ŒçŠ¶æ€**:
- å¤±è´¥çš„å·¥å…·è°ƒç”¨:
{failed_tools_text}

**å½“å‰é˜¶æ®µ**: ç»“æœåˆ†æä¸ä¸‹ä¸€æ­¥å†³ç­–

{coordination_guide}

è¯·ä¸¥æ ¼æŒ‰ç…§åè°ƒæµç¨‹æ‰§è¡Œï¼Œå¿…é¡»è°ƒç”¨ç›¸åº”çš„åˆ†æå’Œå†³ç­–å·¥å…·ã€‚
"""
    
    def _determine_workflow_stage(self, completed_agents: Set[str]) -> str:
        """ç¡®å®šå½“å‰å·¥ä½œæµé˜¶æ®µ"""
        
        if "enhanced_real_verilog_agent" in completed_agents and "enhanced_real_code_review_agent" not in completed_agents:
            # Verilogè®¾è®¡å®Œæˆï¼Œä½†ä»£ç å®¡æŸ¥æœªå®Œæˆ
            return "design_completed"
        elif "enhanced_real_code_review_agent" in completed_agents:
            # ä»£ç å®¡æŸ¥å·²å®Œæˆï¼ˆå¯èƒ½ä¸¤ä¸ªéƒ½å®Œæˆäº†ï¼‰
            return "verification_completed"
        elif len(completed_agents) == 0:
            # è¿˜æ²¡æœ‰æ™ºèƒ½ä½“å®Œæˆ
            return "initial"
        else:
            # å…¶ä»–æƒ…å†µ
            return "unknown"
    
    # =============================================================================
    # å®ç°æŠ½è±¡æ–¹æ³•
    # =============================================================================
    
    async def _call_llm_for_function_calling(self, conversation: List[Dict[str, str]]) -> str:
        """è°ƒç”¨LLMè¿›è¡ŒFunction Calling - åŒ…å«æ ¼å¼ä¿®å¤"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„LLMé€šä¿¡æ¨¡å—
            result = await self.llm_manager.call_llm_for_function_calling(
                conversation, 
                system_prompt_builder=self._build_enhanced_system_prompt
            )
            
            # ğŸ”§ åº”ç”¨æ ¼å¼ä¿®å¤ - ä¿®å¤åè°ƒå™¨çš„å·¥å…·è°ƒç”¨æ ¼å¼é—®é¢˜
            result = self._fix_tool_call_format(result)
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Function Callingè°ƒç”¨å¤±è´¥: {e}")
            return f"é”™è¯¯: {str(e)}"
    
    async def _call_llm_traditional(self, conversation: List[Dict[str, str]]) -> str:
        """ä¼ ç»ŸLLMè°ƒç”¨æ–¹æ³•"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„LLMé€šä¿¡æ¨¡å—
            return await self.llm_manager.call_llm_traditional(
                conversation,
                system_prompt_builder=self._build_enhanced_system_prompt
            )
            
        except Exception as e:
            self.logger.error(f"âŒ ä¼ ç»ŸLLMè°ƒç”¨å¤±è´¥: {e}")
            return f"é”™è¯¯: {str(e)}"
    
    def get_capabilities(self) -> Set[AgentCapability]:
        """è·å–æ™ºèƒ½ä½“èƒ½åŠ›"""
        return {
            AgentCapability.TASK_COORDINATION,
            AgentCapability.INTELLIGENT_ROUTING,
            AgentCapability.CONTEXT_MANAGEMENT,
            AgentCapability.DECISION_MAKING
        }
    
    def get_specialty_description(self) -> str:
        """è·å–æ™ºèƒ½ä½“ä¸“ä¸šæè¿°"""
        return "åŸºäºLLMçš„æ™ºèƒ½åè°ƒæ™ºèƒ½ä½“ï¼Œè´Ÿè´£ä»»åŠ¡åˆ†æå’Œæ™ºèƒ½ä½“åˆ†é…"
    
    async def execute_enhanced_task(self, enhanced_prompt: str,
                                  original_message: TaskMessage,
                                  file_contents: Dict[str, Dict]) -> Dict[str, Any]:
        """æ‰§è¡Œå¢å¼ºä»»åŠ¡"""
        task_id = original_message.task_id
        self.logger.info(f"ğŸ¯ å¼€å§‹æ‰§è¡Œåè°ƒä»»åŠ¡: {task_id}")
        
        try:
            # ä½¿ç”¨åè°ƒä»»åŠ¡æ‰§è¡Œ
            result = await self.coordinate_task(
                user_request=enhanced_prompt,
                conversation_id=original_message.task_id,
                max_iterations=10
            )
            
            return {
                "success": result.get("success", False),
                "task_id": task_id,
                "response": result.get("coordination_result", ""),
                "agent_results": result.get("agent_results", {}),
                "execution_summary": result.get("execution_summary", {})
            }
            
        except Exception as e:
            self.logger.error(f"âŒ åè°ƒä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {task_id} - {str(e)}")
            return {
                "success": False,
                "task_id": task_id,
                "error": f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            } 

    async def _tool_identify_task_type(self, user_request: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¢å¼ºçš„æ™ºèƒ½ä»»åŠ¡ç±»å‹è¯†åˆ« - æ”¯æŒå¤åˆä»»åŠ¡åˆ†è§£"""
        try:
            self.logger.info(f"ğŸ” è¯†åˆ«ä»»åŠ¡ç±»å‹: {user_request[:100]}...")
            
            # ğŸ”§ ä¿®å¤1: æ£€æµ‹å¤åˆä»»åŠ¡ç‰¹å¾
            is_composite, composite_analysis = self._detect_composite_task(user_request)
            
            if is_composite:
                self.logger.info(f"ğŸ¯ æ£€æµ‹åˆ°å¤åˆä»»åŠ¡ï¼Œåˆ†è§£ä¸º: {composite_analysis['subtasks']}")
                return {
                    "success": True,
                    "task_type": "composite",
                    "confidence": composite_analysis["confidence"],
                    "priority": self._determine_task_priority(user_request, TaskType.COMPOSITE).value,
                    "analysis": composite_analysis["analysis"],
                    "subtasks": composite_analysis["subtasks"],
                    "execution_order": composite_analysis["execution_order"],
                    "keywords": composite_analysis.get("keywords", []),
                    "requires_task_decomposition": True
                }
            
            # ä½¿ç”¨æ¨¡å¼åŒ¹é…è¯†åˆ«å•ä¸€ä»»åŠ¡ç±»å‹
            task_type = self._classify_task_by_patterns(user_request)
            
            # ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æ
            llm_analysis = await self._analyze_task_with_llm(user_request, task_type)
            
            # åˆå¹¶ç»“æœ
            final_task_type = llm_analysis.get("task_type", task_type)
            confidence = llm_analysis.get("confidence", 0.7)
            
            # ç¡®å®šä¼˜å…ˆçº§
            priority = self._determine_task_priority(user_request, final_task_type)
            
            return {
                "success": True,
                "task_type": final_task_type.value if isinstance(final_task_type, TaskType) else final_task_type,
                "confidence": confidence,
                "priority": priority.value if isinstance(priority, TaskPriority) else priority,
                "analysis": llm_analysis.get("analysis", ""),
                "keywords": llm_analysis.get("keywords", []),
                "suggested_agent": self._get_suggested_agent(final_task_type),
                "requires_task_decomposition": False
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ä»»åŠ¡ç±»å‹è¯†åˆ«å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "task_type": "unknown",
                "confidence": 0.0,
                "priority": "medium"
            }
    
    def _detect_composite_task(self, user_request: str) -> Tuple[bool, Dict[str, Any]]:
        """æ£€æµ‹å’Œåˆ†æå¤åˆä»»åŠ¡"""
        request_lower = user_request.lower()
        
        # å®šä¹‰å¤åˆä»»åŠ¡æ£€æµ‹æ¨¡å¼
        composite_patterns = [
            # è®¾è®¡ + æµ‹è¯•æ¨¡å¼
            {
                "patterns": [r"è®¾è®¡.*ç”Ÿæˆ.*æµ‹è¯•", r"è®¾è®¡.*éªŒè¯", r"ç”Ÿæˆ.*testbench", r"ç”Ÿæˆ.*æµ‹è¯•å°"],
                "subtasks": ["design", "verification"],
                "analysis": "è®¾è®¡+éªŒè¯å¤åˆä»»åŠ¡ï¼šéœ€è¦å…ˆå®ŒæˆVerilogæ¨¡å—è®¾è®¡ï¼Œå†ç”Ÿæˆæµ‹è¯•å°è¿›è¡ŒéªŒè¯"
            },
            # å®Œæ•´å¼€å‘æµç¨‹
            {
                "patterns": [r"å®Œæ•´.*å¼€å‘", r"ç«¯åˆ°ç«¯.*è®¾è®¡", r"å…¨æµç¨‹.*å®ç°"],
                "subtasks": ["design", "verification", "analysis"],
                "analysis": "å…¨æµç¨‹å¼€å‘ä»»åŠ¡ï¼šåŒ…å«è®¾è®¡ã€éªŒè¯å’Œåˆ†æçš„å®Œæ•´å¼€å‘å‘¨æœŸ"
            },
            # è®¾è®¡ + ä»¿çœŸ
            {
                "patterns": [r"è®¾è®¡.*ä»¿çœŸ", r"å®ç°.*æµ‹è¯•", r"æ¨¡å—.*éªŒè¯"],
                "subtasks": ["design", "verification"],
                "analysis": "è®¾è®¡+ä»¿çœŸä»»åŠ¡ï¼šå…ˆè®¾è®¡æ¨¡å—ï¼Œç„¶åè¿›è¡Œä»¿çœŸéªŒè¯"
            }
        ]
        
        # ğŸ”§ æ ¸å¿ƒå¤åˆä»»åŠ¡æ£€æµ‹ï¼šåŒ…å«æµ‹è¯•å°è¦æ±‚çš„è®¾è®¡ä»»åŠ¡
        testbench_keywords = ["testbench", "æµ‹è¯•å°", "éªŒè¯", "ä»¿çœŸ", "æµ‹è¯•"]
        design_keywords = ["è®¾è®¡", "ç”Ÿæˆ", "å®ç°", "åˆ›å»º", "æ¨¡å—", "ä»£ç "]
        
        has_testbench_req = any(keyword in request_lower for keyword in testbench_keywords)
        has_design_req = any(keyword in request_lower for keyword in design_keywords)
        
        if has_design_req and has_testbench_req:
            self.logger.info("ğŸ¯ æ£€æµ‹åˆ°æ ¸å¿ƒé—®é¢˜ï¼šè®¾è®¡ä»»åŠ¡åŒ…å«æµ‹è¯•å°è¦æ±‚")
            return True, {
                "confidence": 0.95,
                "subtasks": ["design", "verification"],
                "execution_order": ["design", "verification"],
                "analysis": "æ£€æµ‹åˆ°è®¾è®¡+æµ‹è¯•å°å¤åˆä»»åŠ¡ã€‚æ ¹æ®æ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•Œï¼Œå°†è‡ªåŠ¨åˆ†è§£ä¸ºï¼š1) è®¾è®¡ä»»åŠ¡ï¼ˆenhanced_real_verilog_agentï¼‰2) éªŒè¯ä»»åŠ¡ï¼ˆenhanced_real_code_review_agentï¼‰",
                "keywords": ["composite_task", "capability_boundary_conflict"],
                "decomposition_reason": "èƒ½åŠ›è¾¹ç•Œå†²çªï¼šè®¾è®¡æ™ºèƒ½ä½“æ˜ç¡®ç¦æ­¢æµ‹è¯•å°ç”Ÿæˆ"
            }
        
        # æ£€æŸ¥å…¶ä»–å¤åˆä»»åŠ¡æ¨¡å¼
        for pattern_config in composite_patterns:
            for pattern in pattern_config["patterns"]:
                if re.search(pattern, request_lower):
                    return True, {
                        "confidence": 0.8,
                        "subtasks": pattern_config["subtasks"],
                        "execution_order": pattern_config["subtasks"],
                        "analysis": pattern_config["analysis"],
                        "keywords": ["composite_task"],
                        "decomposition_reason": "å¤åˆä»»åŠ¡æ¨¡å¼åŒ¹é…"
                    }
        
        return False, {}
    
    def _classify_task_by_patterns(self, user_request: str) -> TaskType:
        """ä½¿ç”¨æ¨¡å¼åŒ¹é…åˆ†ç±»ä»»åŠ¡"""
        request_lower = user_request.lower()
        
        # è®¡ç®—æ¯ç§ä»»åŠ¡ç±»å‹çš„åŒ¹é…åˆ†æ•°
        scores = {}
        for task_type, patterns in self.task_patterns.items():
            score = 0
            for pattern in patterns:
                if re.search(pattern, request_lower):
                    score += 1
            scores[task_type] = score
        
        # æ‰¾åˆ°æœ€é«˜åˆ†æ•°çš„ä»»åŠ¡ç±»å‹
        if scores:
            max_score = max(scores.values())
            if max_score > 0:
                for task_type, score in scores.items():
                    if score == max_score:
                        return task_type
        
        return TaskType.UNKNOWN
    
    async def _analyze_task_with_llm(self, user_request: str, initial_task_type: TaskType) -> Dict[str, Any]:
        """ä½¿ç”¨LLMæ·±åº¦åˆ†æä»»åŠ¡"""
        try:
            analysis_prompt = f"""
è¯·æ·±åº¦åˆ†æä»¥ä¸‹ç”¨æˆ·è¯·æ±‚ï¼Œè¯†åˆ«ä»»åŠ¡ç±»å‹å’Œç‰¹å¾ï¼š

ç”¨æˆ·è¯·æ±‚: {user_request}
åˆæ­¥åˆ†ç±»: {initial_task_type.value}

è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œåˆ†æï¼š
1. ä»»åŠ¡ç±»å‹ï¼ˆdesign/verification/analysis/debug/compositeï¼‰
2. ä»»åŠ¡å¤æ‚åº¦ï¼ˆsimple/medium/complexï¼‰
3. å…³é”®ç‰¹å¾å’Œéœ€æ±‚
4. å»ºè®®çš„æ™ºèƒ½ä½“é€‰æ‹©
5. ç½®ä¿¡åº¦è¯„åˆ†ï¼ˆ0-1ï¼‰

è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚
"""
            
            # ä½¿ç”¨ç»Ÿä¸€çš„LLMç®¡ç†å™¨è¿›è¡Œè°ƒç”¨
            conversation = [
                {"role": "system", "content": "ä½ æ˜¯ä»»åŠ¡åˆ†æä¸“å®¶ï¼Œè¯·æä¾›å‡†ç¡®çš„ä»»åŠ¡ç±»å‹è¯†åˆ«ã€‚"},
                {"role": "user", "content": analysis_prompt}
            ]
            
            response = await self.llm_manager.call_llm_for_function_calling(conversation)
            
            # å°è¯•è§£æJSONå“åº”
            try:
                analysis = json.loads(response)
                return analysis
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸºæœ¬åˆ†æ
                return {
                    "task_type": initial_task_type,
                    "confidence": 0.7,
                    "analysis": response,
                    "keywords": []
                }
                
        except Exception as e:
            self.logger.error(f"âŒ LLMä»»åŠ¡åˆ†æå¤±è´¥: {str(e)}")
            return {
                "task_type": initial_task_type,
                "confidence": 0.5,
                "analysis": f"åˆ†æå¤±è´¥: {str(e)}",
                "keywords": []
            }
    
    def _determine_task_priority(self, user_request: str, task_type: TaskType) -> TaskPriority:
        """ç¡®å®šä»»åŠ¡ä¼˜å…ˆçº§"""
        request_lower = user_request.lower()
        
        # é«˜ä¼˜å…ˆçº§å…³é”®è¯
        high_priority_keywords = [
            "ç´§æ€¥", "urgent", "ç«‹å³", "immediately", "é‡è¦", "important",
            "å…³é”®", "critical", "ä¼˜å…ˆ", "priority", "å¿«é€Ÿ", "quick"
        ]
        
        # ä½ä¼˜å…ˆçº§å…³é”®è¯
        low_priority_keywords = [
            "ä¸æ€¥", "not urgent", "æ…¢æ…¢", "slowly", "å¯é€‰", "optional",
            "æ¬¡è¦", "secondary", "ä½ä¼˜å…ˆçº§", "low priority"
        ]
        
        # æ£€æŸ¥ä¼˜å…ˆçº§å…³é”®è¯
        for keyword in high_priority_keywords:
            if keyword in request_lower:
                return TaskPriority.HIGH
        
        for keyword in low_priority_keywords:
            if keyword in request_lower:
                return TaskPriority.LOW
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹ç¡®å®šé»˜è®¤ä¼˜å…ˆçº§
        if task_type == TaskType.DEBUG:
            return TaskPriority.HIGH  # è°ƒè¯•ä»»åŠ¡é€šå¸¸ä¼˜å…ˆçº§è¾ƒé«˜
        elif task_type == TaskType.ANALYSIS:
            return TaskPriority.LOW   # åˆ†æä»»åŠ¡é€šå¸¸ä¼˜å…ˆçº§è¾ƒä½
        
        return TaskPriority.MEDIUM
    
    def _get_suggested_agent(self, task_type: TaskType) -> str:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è·å–å»ºè®®çš„æ™ºèƒ½ä½“"""
        if task_type == TaskType.DESIGN:
            return "enhanced_real_verilog_agent"
        elif task_type in [TaskType.VERIFICATION, TaskType.ANALYSIS, TaskType.DEBUG]:
            return "enhanced_real_code_review_agent"
        else:
            return "enhanced_real_verilog_agent"  # é»˜è®¤é€‰æ‹©è®¾è®¡æ™ºèƒ½ä½“
    
    async def _tool_recommend_agent(self, task_type: str, task_description: str,
                                  priority: str = "medium", constraints: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¢å¼ºçš„æ™ºèƒ½ä½“æ¨è - æ”¯æŒå¤åˆä»»åŠ¡å’Œèƒ½åŠ›è¾¹ç•ŒéªŒè¯"""
        try:
            self.logger.info(f"ğŸ¤– æ¨èæ™ºèƒ½ä½“: {task_type} - {priority}")
            
            # ğŸ”§ ä¿®å¤2: å¤„ç†å¤åˆä»»åŠ¡æ¨è
            if task_type == "composite":
                return await self._recommend_for_composite_task(task_description, priority, constraints)
            
            # ğŸ”§ ä¿®å¤3: èƒ½åŠ›è¾¹ç•ŒéªŒè¯
            capability_check = self._verify_agent_capability_boundary(task_type, task_description)
            if not capability_check["valid"]:
                self.logger.warning(f"âš ï¸ èƒ½åŠ›è¾¹ç•Œå†²çª: {capability_check['conflict_reason']}")
                return {
                    "success": False,
                    "error": f"èƒ½åŠ›è¾¹ç•Œå†²çª: {capability_check['conflict_reason']}",
                    "suggested_action": capability_check.get("suggested_action", "è¯·é‡æ–°åˆ†è§£ä»»åŠ¡"),
                    "requires_task_decomposition": True
                }
            
            # è·å–å¯ç”¨æ™ºèƒ½ä½“
            available_agents = self._get_available_agents()
            
            if not available_agents:
                return {
                    "success": False,
                    "error": "æ²¡æœ‰å¯ç”¨çš„æ™ºèƒ½ä½“"
                }
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹è¿‡æ»¤æ™ºèƒ½ä½“
            suitable_agents = self._filter_agents_by_task_type(available_agents, task_type)
            
            if not suitable_agents:
                return {
                    "success": False,
                    "error": f"æ²¡æœ‰é€‚åˆä»»åŠ¡ç±»å‹ '{task_type}' çš„æ™ºèƒ½ä½“"
                }
            
            # è®¡ç®—æ™ºèƒ½ä½“è¯„åˆ†
            agent_scores = self._calculate_agent_scores(suitable_agents, task_type, priority, constraints)
            
            # é€‰æ‹©æœ€é«˜åˆ†çš„æ™ºèƒ½ä½“
            best_agent = max(agent_scores.items(), key=lambda x: x[1])
            
            return {
                "success": True,
                "recommended_agent": best_agent[0],
                "score": best_agent[1],
                "all_scores": agent_scores,
                "reasoning": self._generate_recommendation_reasoning(best_agent[0], best_agent[1], task_type),
                "alternative_agents": self._get_alternative_agents(agent_scores, best_agent[0])
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æ™ºèƒ½ä½“æ¨èå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _recommend_for_composite_task(self, task_description: str, priority: str, constraints: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸ºå¤åˆä»»åŠ¡æ¨èæ™ºèƒ½ä½“æ‰§è¡Œåºåˆ—"""
        try:
            # é‡æ–°åˆ†æä»»åŠ¡ä»¥è·å–å­ä»»åŠ¡ä¿¡æ¯
            is_composite, composite_analysis = self._detect_composite_task(task_description)
            
            if not is_composite:
                return {
                    "success": False,
                    "error": "ä»»åŠ¡æœªè¢«è¯†åˆ«ä¸ºå¤åˆä»»åŠ¡"
                }
            
            subtasks = composite_analysis.get("subtasks", [])
            execution_order = composite_analysis.get("execution_order", subtasks)
            
            # ä¸ºæ¯ä¸ªå­ä»»åŠ¡æ¨èæ™ºèƒ½ä½“
            agent_sequence = []
            for i, subtask_type in enumerate(execution_order):
                agent_recommendation = await self._tool_recommend_agent(
                    task_type=subtask_type,
                    task_description=f"å­ä»»åŠ¡{i+1}: {subtask_type}éƒ¨åˆ† - {task_description}",
                    priority=priority,
                    constraints=constraints
                )
                
                if agent_recommendation.get("success", False):
                    agent_sequence.append({
                        "subtask_type": subtask_type,
                        "recommended_agent": agent_recommendation["recommended_agent"],
                        "score": agent_recommendation["score"],
                        "execution_order": i + 1
                    })
                else:
                    self.logger.error(f"âŒ å­ä»»åŠ¡ {subtask_type} æ™ºèƒ½ä½“æ¨èå¤±è´¥")
            
            if not agent_sequence:
                return {
                    "success": False,
                    "error": "å¤åˆä»»åŠ¡çš„æ‰€æœ‰å­ä»»åŠ¡æ¨èéƒ½å¤±è´¥"
                }
            
            # è¿”å›ç¬¬ä¸€ä¸ªè¦æ‰§è¡Œçš„æ™ºèƒ½ä½“ï¼ˆä¸²è¡Œæ‰§è¡Œï¼‰
            first_agent = agent_sequence[0]
            
            return {
                "success": True,
                "recommended_agent": first_agent["recommended_agent"],
                "score": first_agent["score"],
                "task_type": "composite_first_stage",
                "agent_sequence": agent_sequence,
                "execution_plan": {
                    "total_stages": len(agent_sequence),
                    "current_stage": 1,
                    "next_stages": agent_sequence[1:] if len(agent_sequence) > 1 else []
                },
                "reasoning": f"å¤åˆä»»åŠ¡åˆ†è§£ä¸º{len(agent_sequence)}ä¸ªé˜¶æ®µï¼Œå½“å‰æ‰§è¡Œç¬¬1é˜¶æ®µï¼š{first_agent['subtask_type']}"
            }
            
        except Exception as e:
            self.logger.error(f"âŒ å¤åˆä»»åŠ¡æ¨èå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _verify_agent_capability_boundary(self, task_type: str, task_description: str) -> Dict[str, Any]:
        """éªŒè¯æ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•Œ"""
        task_description_lower = task_description.lower()
        
        # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šæ£€æµ‹è®¾è®¡æ™ºèƒ½ä½“çš„èƒ½åŠ›è¾¹ç•Œè¿å
        if task_type == "design":
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¦æ­¢çš„æµ‹è¯•å°ç›¸å…³è¦æ±‚
            testbench_forbidden_keywords = ["testbench", "æµ‹è¯•å°", "ç”Ÿæˆ.*æµ‹è¯•", "ä»¿çœŸéªŒè¯", "ç”Ÿæˆ.*éªŒè¯"]
            
            for keyword in testbench_forbidden_keywords:
                if re.search(keyword, task_description_lower):
                    return {
                        "valid": False,
                        "conflict_reason": f"è®¾è®¡æ™ºèƒ½ä½“æ˜ç¡®ç¦æ­¢æµ‹è¯•å°ç”Ÿæˆï¼Œä½†ä»»åŠ¡åŒ…å«'{keyword}'è¦æ±‚",
                        "suggested_action": "å°†ä»»åŠ¡åˆ†è§£ä¸ºè®¾è®¡ä»»åŠ¡å’ŒéªŒè¯ä»»åŠ¡",
                        "agent_capability_note": "enhanced_real_verilog_agentç»ä¸è´Ÿè´£æµ‹è¯•å°(testbench)ç”Ÿæˆ"
                    }
        
        # æ£€æŸ¥éªŒè¯æ™ºèƒ½ä½“çš„èƒ½åŠ›è¾¹ç•Œ
        elif task_type == "verification":
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è®¾è®¡ä»»åŠ¡è¦æ±‚
            design_keywords = ["è®¾è®¡æ¨¡å—", "ç”Ÿæˆä»£ç ", "å®ç°åŠŸèƒ½", "åˆ›å»ºç”µè·¯"]
            
            for keyword in design_keywords:
                if re.search(keyword, task_description_lower):
                    return {
                        "valid": False,
                        "conflict_reason": f"éªŒè¯æ™ºèƒ½ä½“ä¸è´Ÿè´£è®¾è®¡å·¥ä½œï¼Œä½†ä»»åŠ¡åŒ…å«'{keyword}'è¦æ±‚",
                        "suggested_action": "å°†è®¾è®¡éƒ¨åˆ†åˆ†é…ç»™è®¾è®¡æ™ºèƒ½ä½“",
                        "agent_capability_note": "enhanced_real_code_review_agentä¸“æ³¨äºæµ‹è¯•å’ŒéªŒè¯"
                    }
        
        # æ‰€æœ‰æ£€æŸ¥éƒ½é€šè¿‡
        return {
            "valid": True,
            "capability_match": "ä»»åŠ¡ä¸æ™ºèƒ½ä½“èƒ½åŠ›åŒ¹é…"
        }
    
    def _verify_task_assignment_boundary(self, agent_id: str, task_type: str, task_description: str) -> Dict[str, Any]:
        """éªŒè¯ä»»åŠ¡åˆ†é…çš„èƒ½åŠ›è¾¹ç•Œ"""
        task_description_lower = task_description.lower()
        
        # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šè®¾è®¡æ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•Œæ£€æŸ¥
        if agent_id == "enhanced_real_verilog_agent":
            # æ£€æµ‹è®¾è®¡æ™ºèƒ½ä½“ç¦æ­¢çš„æµ‹è¯•å°ç›¸å…³ä»»åŠ¡
            forbidden_patterns = [
                (r"testbench|æµ‹è¯•å°", "æ˜ç¡®ç¦æ­¢æµ‹è¯•å°ç”Ÿæˆ"),
                (r"ç”Ÿæˆ.*æµ‹è¯•", "ä¸è´Ÿè´£ç”Ÿæˆæµ‹è¯•ç›¸å…³å†…å®¹"),
                (r"ä»¿çœŸ.*éªŒè¯", "ä¸è´Ÿè´£ä»¿çœŸéªŒè¯å·¥ä½œ"),
                (r"éªŒè¯.*åŠŸèƒ½", "ä¸è´Ÿè´£åŠŸèƒ½éªŒè¯"),
                (r"è¿è¡Œ.*ä»¿çœŸ", "ä¸è´Ÿè´£è¿è¡Œä»¿çœŸ"),
                (r"æµ‹è¯•.*ç”¨ä¾‹", "ä¸è´Ÿè´£æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ")
            ]
            
            for pattern, reason in forbidden_patterns:
                if re.search(pattern, task_description_lower):
                    return {
                        "valid": False,
                        "reason": f"è®¾è®¡æ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•Œå†²çª: {reason}",
                        "suggested_action": "å°†æµ‹è¯•éªŒè¯éƒ¨åˆ†åˆ†é…ç»™ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“(enhanced_real_code_review_agent)",
                        "agent_capabilities": {
                            "allowed": ["Verilogæ¨¡å—è®¾è®¡", "ä»£ç ç”Ÿæˆ", "åŠŸèƒ½å®ç°", "ä»£ç è´¨é‡åˆ†æ"],
                            "forbidden": ["æµ‹è¯•å°ç”Ÿæˆ", "ä»¿çœŸéªŒè¯", "æµ‹è¯•æ‰§è¡Œ"]
                        },
                        "conflict_details": {
                            "matched_pattern": pattern,
                            "conflict_reason": reason,
                            "agent_policy": "è®¾è®¡æ™ºèƒ½ä½“System Promptæ˜ç¡®å£°æ˜'ç»ä¸è´Ÿè´£æµ‹è¯•å°(testbench)ç”Ÿæˆ'"
                        }
                    }
        
        # ğŸ”§ ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•Œæ£€æŸ¥
        elif agent_id == "enhanced_real_code_review_agent":
            # æ£€æµ‹å®¡æŸ¥æ™ºèƒ½ä½“ä¸åº”æ‰¿æ‹…çš„çº¯è®¾è®¡ä»»åŠ¡
            design_only_patterns = [
                (r"è®¾è®¡.*æ¨¡å—", "ä¸è´Ÿè´£ä»é›¶å¼€å§‹çš„æ¨¡å—è®¾è®¡"),
                (r"å®ç°.*åŠŸèƒ½", "ä¸è´Ÿè´£åŠŸèƒ½å®ç°ï¼Œåªè´Ÿè´£éªŒè¯"),
                (r"åˆ›å»º.*ç”µè·¯", "ä¸è´Ÿè´£ç”µè·¯åˆ›å»º")
            ]
            
            for pattern, reason in design_only_patterns:
                if re.search(pattern, task_description_lower) and not re.search(r"æµ‹è¯•|éªŒè¯|testbench", task_description_lower):
                    return {
                        "valid": False,
                        "reason": f"ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•Œè­¦å‘Š: {reason}",
                        "suggested_action": "çº¯è®¾è®¡ä»»åŠ¡åº”åˆ†é…ç»™è®¾è®¡æ™ºèƒ½ä½“(enhanced_real_verilog_agent)",
                        "agent_capabilities": {
                            "primary": ["æµ‹è¯•å°ç”Ÿæˆ", "ä»£ç å®¡æŸ¥", "ä»¿çœŸéªŒè¯", "è´¨é‡åˆ†æ"],
                            "secondary": ["è®¾è®¡éªŒè¯", "ä»£ç ä¼˜åŒ–å»ºè®®"]
                        },
                        "conflict_details": {
                            "matched_pattern": pattern,
                            "conflict_reason": reason,
                            "recommendation": "å…ˆç”±è®¾è®¡æ™ºèƒ½ä½“å®Œæˆè®¾è®¡ï¼Œå†ç”±å®¡æŸ¥æ™ºèƒ½ä½“éªŒè¯"
                        }
                    }
        
        # é€šç”¨ä»»åŠ¡ç±»å‹å’Œæ™ºèƒ½ä½“åŒ¹é…æ£€æŸ¥
        agent_task_compatibility = {
            "enhanced_real_verilog_agent": ["design", "analysis"],
            "enhanced_real_code_review_agent": ["verification", "review", "test"]
        }
        
        compatible_tasks = agent_task_compatibility.get(agent_id, [])
        if task_type not in compatible_tasks and task_type != "composite":
            return {
                "valid": False,
                "reason": f"ä»»åŠ¡ç±»å‹ '{task_type}' ä¸æ™ºèƒ½ä½“ '{agent_id}' ä¸å…¼å®¹",
                "suggested_action": f"è¯·é€‰æ‹©é€‚åˆ '{task_type}' ä»»åŠ¡çš„æ™ºèƒ½ä½“",
                "agent_capabilities": {
                    "compatible_task_types": compatible_tasks
                },
                "conflict_details": {
                    "task_type": task_type,
                    "agent_id": agent_id,
                    "compatibility_matrix": agent_task_compatibility
                }
            }
        
        # æ‰€æœ‰æ£€æŸ¥éƒ½é€šè¿‡
        return {
            "valid": True,
            "verification_passed": f"ä»»åŠ¡åˆ†é…ç»™ {agent_id} é€šè¿‡èƒ½åŠ›è¾¹ç•Œæ£€æŸ¥"
        }
    
    def _get_available_agents(self) -> List[Tuple[str, AgentInfo]]:
        """è·å–å¯ç”¨æ™ºèƒ½ä½“åˆ—è¡¨"""
        available = []
        for agent_id, agent_info in self.registered_agents.items():
            if agent_info.status == AgentStatus.IDLE:
                available.append((agent_id, agent_info))
        return available
    
    def _filter_agents_by_task_type(self, agents: List[Tuple[str, AgentInfo]], task_type: str) -> List[Tuple[str, AgentInfo]]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è¿‡æ»¤æ™ºèƒ½ä½“"""
        filtered = []
        
        for agent_id, agent_info in agents:
            # æ£€æŸ¥æ™ºèƒ½ä½“æ˜¯å¦é€‚åˆè¯¥ä»»åŠ¡ç±»å‹
            if self._is_agent_suitable_for_task(agent_id, task_type):
                filtered.append((agent_id, agent_info))
        
        return filtered
    
    def _is_agent_suitable_for_task(self, agent_id: str, task_type: str) -> bool:
        """æ£€æŸ¥æ™ºèƒ½ä½“æ˜¯å¦é€‚åˆç‰¹å®šä»»åŠ¡ç±»å‹"""
        if task_type == "design":
            return agent_id == "enhanced_real_verilog_agent"
        elif task_type in ["verification", "analysis", "debug"]:
            return agent_id == "enhanced_real_code_review_agent"
        elif task_type == "composite":
            return True  # å¤åˆä»»åŠ¡å¯ä»¥ä½¿ç”¨ä»»ä½•æ™ºèƒ½ä½“
        else:
            return True  # æœªçŸ¥ä»»åŠ¡ç±»å‹å…è®¸ä½¿ç”¨ä»»ä½•æ™ºèƒ½ä½“
    
    def _calculate_agent_scores(self, agents: List[Tuple[str, AgentInfo]], task_type: str,
                              priority: str, constraints: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—æ™ºèƒ½ä½“è¯„åˆ†"""
        scores = {}
        
        for agent_id, agent_info in agents:
            score = 0.0
            
            # åŸºç¡€åˆ†æ•°
            score += 50.0
            
            # æˆåŠŸç‡åˆ†æ•°ï¼ˆæƒé‡ï¼š30%ï¼‰
            total_tasks = agent_info.success_count + agent_info.failure_count
            if total_tasks > 0:
                success_rate = agent_info.success_count / total_tasks
                score += success_rate * 30.0
            
            # å“åº”æ—¶é—´åˆ†æ•°ï¼ˆæƒé‡ï¼š20%ï¼‰
            if agent_info.average_response_time > 0:
                # å“åº”æ—¶é—´è¶ŠçŸ­ï¼Œåˆ†æ•°è¶Šé«˜
                time_score = max(0, 20.0 - (agent_info.average_response_time / 10.0))
                score += time_score
            
            # è¿ç»­æˆåŠŸåˆ†æ•°ï¼ˆæƒé‡ï¼š10%ï¼‰
            consecutive_bonus = min(10.0, agent_info.consecutive_successes * 2.0)
            score += consecutive_bonus
            
            # ä¼˜å…ˆçº§åŒ¹é…åˆ†æ•°
            if priority == "high" and agent_info.average_response_time < 30:
                score += 5.0
            
            # ä»»åŠ¡ç±»å‹åŒ¹é…åˆ†æ•°
            if task_type in [task_type.value for task_type in agent_info.preferred_task_types]:
                score += 10.0
            
            # é»‘åå•æƒ©ç½š
            if task_type in [task_type.value for task_type in agent_info.blacklisted_task_types]:
                score -= 20.0
            
            scores[agent_id] = max(0.0, score)
        
        return scores
    
    def _generate_recommendation_reasoning(self, agent_id: str, score: float, task_type: str) -> str:
        """ç”Ÿæˆæ¨èç†ç”±"""
        agent_info = self.registered_agents.get(agent_id)
        if not agent_info:
            return f"æ¨è {agent_id}ï¼Œè¯„åˆ†: {score:.1f}"
        
        reasons = []
        
        # æˆåŠŸç‡
        total_tasks = agent_info.success_count + agent_info.failure_count
        if total_tasks > 0:
            success_rate = agent_info.success_count / total_tasks
            reasons.append(f"å†å²æˆåŠŸç‡: {success_rate:.1%}")
        
        # å“åº”æ—¶é—´
        if agent_info.average_response_time > 0:
            reasons.append(f"å¹³å‡å“åº”æ—¶é—´: {agent_info.average_response_time:.1f}ç§’")
        
        # è¿ç»­æˆåŠŸ
        if agent_info.consecutive_successes > 0:
            reasons.append(f"è¿ç»­æˆåŠŸ: {agent_info.consecutive_successes}æ¬¡")
        
        # ä»»åŠ¡ç±»å‹åŒ¹é…
        if task_type in [task_type.value for task_type in agent_info.preferred_task_types]:
            reasons.append("ä»»åŠ¡ç±»å‹åŒ¹é…")
        
        return f"æ¨è {agent_id} (è¯„åˆ†: {score:.1f})ï¼Œç†ç”±: {', '.join(reasons)}"
    
    def _get_alternative_agents(self, agent_scores: Dict[str, float], best_agent: str) -> List[str]:
        """è·å–å¤‡é€‰æ™ºèƒ½ä½“"""
        alternatives = []
        best_score = agent_scores.get(best_agent, 0)
        
        for agent_id, score in agent_scores.items():
            if agent_id != best_agent and score >= best_score * 0.8:  # åˆ†æ•°ä¸ä½äºæœ€ä½³æ™ºèƒ½ä½“çš„80%
                alternatives.append(agent_id)
        
        return alternatives 

    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿæ€§èƒ½æ‘˜è¦"""
        summary = {
            "total_tasks": self.performance_metrics["total_tasks"],
            "successful_tasks": self.performance_metrics["successful_tasks"],
            "failed_tasks": self.performance_metrics["failed_tasks"],
            "success_rate": 0.0,
            "average_completion_time": self.performance_metrics["average_completion_time"],
            "agent_performance": {},
            "system_health": "healthy",
            "recommendations": []
        }
        
        # è®¡ç®—æˆåŠŸç‡
        if summary["total_tasks"] > 0:
            summary["success_rate"] = summary["successful_tasks"] / summary["total_tasks"]
        
        # æ™ºèƒ½ä½“æ€§èƒ½ç»Ÿè®¡
        for agent_id, agent_info in self.registered_agents.items():
            total_tasks = agent_info.success_count + agent_info.failure_count
            success_rate = agent_info.success_count / total_tasks if total_tasks > 0 else 0.0
            
            summary["agent_performance"][agent_id] = {
                "total_tasks": total_tasks,
                "success_rate": success_rate,
                "average_response_time": agent_info.average_response_time,
                "reliability_score": self._calculate_reliability_score(agent_info),
                "status": agent_info.status.value
            }
        
        # ç³»ç»Ÿå¥åº·è¯„ä¼°
        system_health = self._assess_system_health()
        summary["system_health"] = system_health["status"]
        summary["recommendations"] = system_health["recommendations"]
        
        return summary
    
    def _assess_system_health(self) -> Dict[str, Any]:
        """è¯„ä¼°ç³»ç»Ÿæ•´ä½“å¥åº·çŠ¶æ€"""
        health = {
            "status": "healthy",
            "issues": [],
            "recommendations": []
        }
        
        # æ£€æŸ¥æ€»ä½“æˆåŠŸç‡
        total_tasks = self.performance_metrics["total_tasks"]
        if total_tasks > 0:
            success_rate = self.performance_metrics["successful_tasks"] / total_tasks
            if success_rate < 0.5:
                health["status"] = "critical"
                health["issues"].append(f"ç³»ç»ŸæˆåŠŸç‡è¿‡ä½: {success_rate:.1%}")
                health["recommendations"].append("å»ºè®®æ£€æŸ¥æ™ºèƒ½ä½“é…ç½®å’Œç³»ç»Ÿè®¾ç½®")
            elif success_rate < 0.7:
                health["status"] = "warning"
                health["issues"].append(f"ç³»ç»ŸæˆåŠŸç‡åä½: {success_rate:.1%}")
                health["recommendations"].append("å»ºè®®ä¼˜åŒ–ä»»åŠ¡åˆ†é…ç­–ç•¥")
        
        # æ£€æŸ¥æ™ºèƒ½ä½“çŠ¶æ€
        idle_agents = 0
        working_agents = 0
        for agent_info in self.registered_agents.values():
            if agent_info.status == AgentStatus.IDLE:
                idle_agents += 1
            elif agent_info.status == AgentStatus.WORKING:
                working_agents += 1
        
        if idle_agents == 0:
            health["status"] = "warning"
            health["issues"].append("æ‰€æœ‰æ™ºèƒ½ä½“éƒ½åœ¨å·¥ä½œï¼Œå¯èƒ½å­˜åœ¨è´Ÿè½½è¿‡é‡")
            health["recommendations"].append("è€ƒè™‘æ·»åŠ æ›´å¤šæ™ºèƒ½ä½“æˆ–ä¼˜åŒ–ä»»åŠ¡åˆ†é…")
        
        if working_agents == 0 and total_tasks > 0:
            health["status"] = "warning"
            health["issues"].append("æ²¡æœ‰æ™ºèƒ½ä½“åœ¨å·¥ä½œï¼Œå¯èƒ½å­˜åœ¨é…ç½®é—®é¢˜")
            health["recommendations"].append("æ£€æŸ¥æ™ºèƒ½ä½“æ³¨å†Œå’ŒçŠ¶æ€")
        
        return health
    
    async def emergency_recovery(self, task_id: str = None) -> Dict[str, Any]:
        """ç´§æ€¥æ¢å¤åŠŸèƒ½"""
        try:
            recovery_result = {
                "success": True,
                "recovered_tasks": [],
                "reset_agents": [],
                "actions_taken": []
            }
            
            # é‡ç½®æ‰€æœ‰æ™ºèƒ½ä½“çŠ¶æ€
            for agent_id, agent_info in self.registered_agents.items():
                if agent_info.status == AgentStatus.WORKING:
                    agent_info.status = AgentStatus.IDLE
                    recovery_result["reset_agents"].append(agent_id)
                    recovery_result["actions_taken"].append(f"é‡ç½®æ™ºèƒ½ä½“ {agent_id} çŠ¶æ€")
            
            # æ¢å¤ç‰¹å®šä»»åŠ¡æˆ–æ‰€æœ‰ä»»åŠ¡
            if task_id:
                if task_id in self.active_tasks:
                    task = self.active_tasks[task_id]
                    task.assigned_agent = None
                    task.current_stage = "recovered"
                    task.retry_count += 1
                    recovery_result["recovered_tasks"].append(task_id)
                    recovery_result["actions_taken"].append(f"æ¢å¤ä»»åŠ¡ {task_id}")
            else:
                # æ¢å¤æ‰€æœ‰æ´»è·ƒä»»åŠ¡
                for tid, task in self.active_tasks.items():
                    task.assigned_agent = None
                    task.current_stage = "recovered"
                    task.retry_count += 1
                    recovery_result["recovered_tasks"].append(tid)
                recovery_result["actions_taken"].append("æ¢å¤æ‰€æœ‰æ´»è·ƒä»»åŠ¡")
            
            self.logger.info(f"ğŸš¨ ç´§æ€¥æ¢å¤å®Œæˆ: {len(recovery_result['recovered_tasks'])} ä¸ªä»»åŠ¡, {len(recovery_result['reset_agents'])} ä¸ªæ™ºèƒ½ä½“")
            
            return recovery_result
            
        except Exception as e:
            self.logger.error(f"âŒ ç´§æ€¥æ¢å¤å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def get_system_status_report(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š"""
        report = {
            "timestamp": time.time(),
            "system_info": {
                "coordinator_id": self.agent_id,
                "version": "enhanced_v2.0",
                "uptime": time.time() - self.start_time if hasattr(self, 'start_time') else 0
            },
            "agent_status": {},
            "task_status": {},
            "performance_metrics": self.get_performance_summary(),
            "system_health": self._assess_system_health(),
            "recommendations": []
        }
        
        # æ™ºèƒ½ä½“çŠ¶æ€
        for agent_id, agent_info in self.registered_agents.items():
            report["agent_status"][agent_id] = {
                "status": agent_info.status.value,
                "specialty": agent_info.specialty,
                "capabilities": [cap.value for cap in agent_info.capabilities],
                "performance": self._calculate_agent_performance_metrics(agent_info),
                "health": self._assess_agent_health(agent_info)
            }
        
        # ä»»åŠ¡çŠ¶æ€
        for task_id, task in self.active_tasks.items():
            report["task_status"][task_id] = {
                "stage": task.current_stage,
                "assigned_agent": task.assigned_agent,
                "iteration_count": task.iteration_count,
                "retry_count": task.retry_count,
                "completion_status": task.completion_status,
                "quality_score": task.quality_score,
                "execution_time": time.time() - task.start_time
            }
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        
        # åŸºäºæ€§èƒ½æŒ‡æ ‡çš„å»ºè®®
        if report["performance_metrics"]["success_rate"] < 0.7:
            recommendations.append("ç³»ç»ŸæˆåŠŸç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–ä»»åŠ¡åˆ†é…ç­–ç•¥")
        
        # åŸºäºæ™ºèƒ½ä½“çŠ¶æ€çš„å»ºè®®
        idle_count = sum(1 for agent in self.registered_agents.values() if agent.status == AgentStatus.IDLE)
        if idle_count == 0:
            recommendations.append("æ‰€æœ‰æ™ºèƒ½ä½“éƒ½åœ¨å·¥ä½œï¼Œè€ƒè™‘æ·»åŠ æ›´å¤šæ™ºèƒ½ä½“")
        
        # åŸºäºä»»åŠ¡çŠ¶æ€çš„å»ºè®®
        stuck_tasks = [tid for tid, task in self.active_tasks.items() if task.retry_count > 3]
        if stuck_tasks:
            recommendations.append(f"å‘ç° {len(stuck_tasks)} ä¸ªå¡ä½çš„ä»»åŠ¡ï¼Œå»ºè®®è¿›è¡Œç´§æ€¥æ¢å¤")
        
        report["recommendations"] = recommendations
        
        return report
    
    def optimize_agent_allocation(self) -> Dict[str, Any]:
        """ä¼˜åŒ–æ™ºèƒ½ä½“åˆ†é…ç­–ç•¥"""
        optimization_result = {
            "success": True,
            "optimizations": [],
            "agent_recommendations": {}
        }
        
        # åˆ†ææ™ºèƒ½ä½“æ€§èƒ½
        for agent_id, agent_info in self.registered_agents.items():
            recommendations = []
            
            # åŸºäºæˆåŠŸç‡çš„ä¼˜åŒ–
            total_tasks = agent_info.success_count + agent_info.failure_count
            if total_tasks > 0:
                success_rate = agent_info.success_count / total_tasks
                if success_rate < 0.5:
                    recommendations.append("æˆåŠŸç‡è¿‡ä½ï¼Œå»ºè®®æ£€æŸ¥é…ç½®æˆ–é‡æ–°è®­ç»ƒ")
                elif success_rate < 0.7:
                    recommendations.append("æˆåŠŸç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–promptæˆ–å‚æ•°")
            
            # åŸºäºå“åº”æ—¶é—´çš„ä¼˜åŒ–
            if agent_info.average_response_time > 60:
                recommendations.append("å“åº”æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½æˆ–æ£€æŸ¥ç½‘ç»œ")
            
            # åŸºäºè¿ç»­å¤±è´¥çš„ä¼˜åŒ–
            if agent_info.consecutive_failures > 2:
                recommendations.append("è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œå»ºè®®é‡å¯æˆ–æ›´æ¢æ™ºèƒ½ä½“")
            
            if recommendations:
                optimization_result["agent_recommendations"][agent_id] = recommendations
                optimization_result["optimizations"].extend(recommendations)
        
        # è´Ÿè½½å‡è¡¡ä¼˜åŒ–
        working_agents = [aid for aid, info in self.registered_agents.items() if info.status == AgentStatus.WORKING]
        idle_agents = [aid for aid, info in self.registered_agents.items() if info.status == AgentStatus.IDLE]
        
        if len(working_agents) > len(idle_agents):
            optimization_result["optimizations"].append("å·¥ä½œæ™ºèƒ½ä½“è¿‡å¤šï¼Œå»ºè®®ä¼˜åŒ–ä»»åŠ¡åˆ†é…ç­–ç•¥")
        
        return optimization_result
    
    async def _tool_provide_final_answer(self, final_summary: str, task_status: str, results_summary: Dict = None) -> Dict[str, Any]:
        """è¿™æ˜¯ä¸€ä¸ªè™šæ‹Ÿå·¥å…·ï¼Œå®ƒçš„ä½œç”¨æ˜¯æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡ºï¼Œå¹¶æ ‡è®°ä»»åŠ¡æµç¨‹çš„ç»“æŸã€‚"""
        self.logger.info(f"ğŸ ä»»åŠ¡å®Œæˆï¼Œæä¾›æœ€ç»ˆç­”æ¡ˆ: {final_summary[:100]}...")
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ä¸ªå‡½æ•°å¯ä»¥ç›´æ¥è¿”å›å…¶è¾“å…¥ï¼Œå› ä¸ºå®ƒçš„ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†è¢«LLMè°ƒç”¨
        return {
            "success": True,
            "final_answer_provided": True,
            "summary": final_summary,
            "status": task_status,
            "results": results_summary or {}
        }
    
    def _parse_task_completion_check(self, continuation_result: str) -> Dict[str, Any]:
        """è§£æä»»åŠ¡å®Œæˆæ£€æŸ¥ç»“æœ"""
        try:
            json_response = self._extract_json_from_response(continuation_result)
            if json_response:
                import json
                completion_data = json.loads(json_response)
                
                # æŸ¥æ‰¾check_task_completionå·¥å…·è°ƒç”¨ç»“æœ
                if "tool_calls" in completion_data:
                    for tool_call in completion_data["tool_calls"]:
                        if tool_call.get("tool_name") == "check_task_completion":
                            # è¿™æ˜¯å·¥å…·è°ƒç”¨ï¼Œä¸æ˜¯ç»“æœ
                            continue
                
                # æŸ¥æ‰¾å·¥å…·æ‰§è¡Œç»“æœ
                if "is_completed" in completion_data:
                    return completion_data
                
                # æŸ¥æ‰¾åµŒå¥—çš„ç»“æœç»“æ„
                for key, value in completion_data.items():
                    if isinstance(value, dict) and "is_completed" in value:
                        return value
                        
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ è§£æä»»åŠ¡å®Œæˆæ£€æŸ¥ç»“æœå¤±è´¥: {str(e)}")
            return None
    
    def _block_premature_completion(self, continuation_result: str, completion_check_result: Dict[str, Any]) -> str:
        """é˜»æ­¢è¿‡æ—©çš„ä»»åŠ¡å®Œæˆ"""
        try:
            # å¦‚æœåŒ…å«provide_final_answerè°ƒç”¨ï¼Œåˆ é™¤å®ƒ
            json_response = self._extract_json_from_response(continuation_result)
            if json_response:
                import json
                data = json.loads(json_response)
                
                if "tool_calls" in data and isinstance(data["tool_calls"], list):
                    # è¿‡æ»¤æ‰provide_final_answerè°ƒç”¨
                    filtered_calls = [
                        call for call in data["tool_calls"] 
                        if call.get("tool_name") != "provide_final_answer"
                    ]
                    
                    if len(filtered_calls) != len(data["tool_calls"]):
                        self.logger.warning(f"ğŸš« å·²é˜»æ­¢è¿‡æ—©çš„provide_final_answerè°ƒç”¨")
                        
                        # æ·»åŠ å¼ºåˆ¶ç»§ç»­åè°ƒçš„æŒ‡ä»¤
                        missing_reqs = completion_check_result.get("missing_requirements", [])
                        completion_score = completion_check_result.get("completion_score", 0)
                        
                        forced_continuation = {
                            "tool_calls": [{
                                "tool_name": "assign_task_to_agent",
                                "parameters": {
                                    "agent_id": "enhanced_real_code_review_agent",
                                    "task_description": f"å®Œæˆç¼ºå¤±çš„ä»»åŠ¡é¡¹: {', '.join(missing_reqs[:3])}ã€‚å½“å‰å®Œæˆåˆ†æ•°ä»…{completion_score}ï¼Œéœ€è¦ç”Ÿæˆæµ‹è¯•å°å’Œè¿›è¡ŒéªŒè¯ã€‚",
                                    "task_type": "verification",
                                    "priority": "high"
                                }
                            }]
                        }
                        
                        return f"```json\n{json.dumps(forced_continuation, ensure_ascii=False, indent=2)}\n```"
            
            # å¦‚æœæ— æ³•ä¿®æ”¹ï¼Œè¿”å›åŸå§‹ç»“æœä½†æ·»åŠ è­¦å‘Š
            return continuation_result + "\n\nâš ï¸ è­¦å‘Šï¼šä»»åŠ¡æœªå®Œæˆï¼Œä¸å…è®¸ç»“æŸã€‚"
            
        except Exception as e:
            self.logger.error(f"âŒ é˜»æ­¢è¿‡æ—©å®Œæˆå¤±è´¥: {str(e)}")
            return continuation_result
    
    def _detect_task_hallucination(self, agent_id: str, result: Dict[str, Any], task_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ£€æµ‹æ™ºèƒ½ä½“ä»»åŠ¡å¹»è§‰"""
        hallucination_details = {
            "has_hallucination": False,
            "hallucination_type": "none",
            "description": "",
            "confidence": 0.0,
            "evidence": [],
            "suggested_recovery": ""
        }
        
        try:
            # ğŸ”§ æ ¸å¿ƒå¹»è§‰æ£€æµ‹ï¼šè®¾è®¡æ™ºèƒ½ä½“å£°ç§°ç”Ÿæˆæµ‹è¯•å°
            if agent_id == "enhanced_real_verilog_agent":
                claimed_files = result.get("generated_files", [])
                result_text = str(result).lower()
                
                # æ£€æµ‹æµ‹è¯•å°ç”Ÿæˆå¹»è§‰
                testbench_indicators = [
                    "testbench", "æµ‹è¯•å°", "_tb.v", "counter_tb.v", 
                    "test_bench", "verification", "ä»¿çœŸéªŒè¯"
                ]
                
                has_testbench_claim = False
                evidence = []
                
                # æ£€æŸ¥å£°ç§°çš„æ–‡ä»¶åˆ—è¡¨
                for file_name in claimed_files:
                    if any(indicator in file_name.lower() for indicator in testbench_indicators):
                        has_testbench_claim = True
                        evidence.append(f"å£°ç§°ç”Ÿæˆæ–‡ä»¶: {file_name}")
                
                # æ£€æŸ¥ç»“æœæè¿°æ–‡æœ¬
                for indicator in testbench_indicators:
                    if indicator in result_text:
                        has_testbench_claim = True
                        evidence.append(f"æè¿°ä¸­æåŠ: {indicator}")
                
                if has_testbench_claim:
                    hallucination_details.update({
                        "has_hallucination": True,
                        "hallucination_type": "capability_boundary_violation",
                        "description": "è®¾è®¡æ™ºèƒ½ä½“è¿åèƒ½åŠ›è¾¹ç•Œï¼Œå£°ç§°ç”Ÿæˆäº†æ˜ç¡®ç¦æ­¢çš„æµ‹è¯•å°æ–‡ä»¶",
                        "confidence": 0.95,
                        "evidence": evidence,
                        "suggested_recovery": "é‡æ–°åˆ†é…æµ‹è¯•å°ç”Ÿæˆä»»åŠ¡ç»™ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“"
                    })
                    return hallucination_details
            
            # æ£€æµ‹æ–‡ä»¶å­˜åœ¨æ€§å¹»è§‰
            if "generated_files" in result:
                claimed_files = result["generated_files"]
                if isinstance(claimed_files, list) and claimed_files:
                    non_existent_files = []
                    
                    for file_path in claimed_files:
                        if isinstance(file_path, str):
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦çœŸå®å­˜åœ¨
                            from pathlib import Path
                            if not Path(file_path).exists():
                                # æ£€æŸ¥æ˜¯å¦åœ¨å®éªŒç›®å½•ä¸­
                                experiment_paths = [
                                    f"experiments/{file_path}",
                                    f"logs/*/artifacts/{file_path}",
                                    f"designs/{file_path}",
                                    f"testbenches/{file_path}"
                                ]
                                
                                file_found = False
                                for exp_path in experiment_paths:
                                    if any(Path(".").glob(exp_path)):
                                        file_found = True
                                        break
                                
                                if not file_found:
                                    non_existent_files.append(file_path)
                    
                    if non_existent_files:
                        hallucination_details.update({
                            "has_hallucination": True,
                            "hallucination_type": "file_existence_hallucination",
                            "description": f"å£°ç§°ç”Ÿæˆäº†ä¸å­˜åœ¨çš„æ–‡ä»¶: {', '.join(non_existent_files)}",
                            "confidence": 0.8,
                            "evidence": [f"æ–‡ä»¶ä¸å­˜åœ¨: {f}" for f in non_existent_files],
                            "suggested_recovery": "é‡æ–°æ‰§è¡Œä»»åŠ¡ï¼Œç¡®ä¿å®é™…ç”Ÿæˆæ–‡ä»¶"
                        })
                        return hallucination_details
            
            # æ£€æµ‹åŠŸèƒ½å®Œæˆåº¦å¹»è§‰
            claimed_status = result.get("status", "").lower()
            claimed_quality = result.get("code_quality", "").lower()
            
            if claimed_status == "success" and claimed_quality in ["high", "excellent"]:
                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…çš„æˆåŠŸæŒ‡æ ‡
                success_indicators = [
                    result.get("generated_files"),
                    result.get("file_references"),
                    result.get("output_files")
                ]
                
                has_concrete_output = any(
                    indicator and len(indicator) > 0 
                    for indicator in success_indicators 
                    if indicator is not None
                )
                
                if not has_concrete_output:
                    hallucination_details.update({
                        "has_hallucination": True,
                        "hallucination_type": "success_status_hallucination", 
                        "description": "å£°ç§°ä»»åŠ¡æˆåŠŸå®Œæˆä½†ç¼ºä¹å…·ä½“çš„è¾“å‡ºè¯æ®",
                        "confidence": 0.6,
                        "evidence": ["å£°ç§°çŠ¶æ€ä¸ºsuccess", "å£°ç§°é«˜è´¨é‡", "ä½†æ— å…·ä½“è¾“å‡ºæ–‡ä»¶"],
                        "suggested_recovery": "è¦æ±‚æ™ºèƒ½ä½“æä¾›å…·ä½“çš„æˆæœè¯æ®"
                    })
                    return hallucination_details
            
            # é»˜è®¤ï¼šæ— å¹»è§‰æ£€æµ‹åˆ°
            return hallucination_details
            
        except Exception as e:
            self.logger.error(f"âŒ å¹»è§‰æ£€æµ‹å¤±è´¥: {str(e)}")
            return hallucination_details
    
    def _manage_serial_workflow(self, task_context: TaskContext, task_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç®¡ç†ä¸²è¡Œå·¥ä½œæµ: è®¾è®¡â†’å®¡æŸ¥â†’éªŒè¯"""
        
        # æ£€æŸ¥å½“å‰å·¥ä½œæµé˜¶æ®µ
        current_stage = self._determine_workflow_stage(task_context, task_analysis)
        self.logger.info(f"ğŸ”„ å½“å‰å·¥ä½œæµé˜¶æ®µ: {current_stage}")
        
        workflow_plan = {
            "current_stage": current_stage,
            "next_action": None,
            "stage_completed": False,
            "workflow_complete": False,
            "quality_gate_passed": False
        }
        
        if current_stage == "initial":
            # å¼€å§‹è®¾è®¡é˜¶æ®µ
            workflow_plan.update({
                "next_action": {
                    "type": "assign_task",
                    "agent_id": "enhanced_real_verilog_agent", 
                    "task_type": "design",
                    "description": "æ‰§è¡ŒVerilogæ¨¡å—è®¾è®¡ä»»åŠ¡ï¼ˆä»…è®¾è®¡ï¼Œä¸åŒ…æ‹¬æµ‹è¯•å°ï¼‰"
                },
                "stage_name": "design_stage",
                "expected_outputs": ["Verilogè®¾è®¡æ–‡ä»¶", "è®¾è®¡æ–‡æ¡£"]
            })
            
        elif current_stage == "design_completed":
            # è®¾è®¡å®Œæˆï¼Œè¿›å…¥å®¡æŸ¥/éªŒè¯é˜¶æ®µ
            # é¦–å…ˆæ£€æŸ¥è®¾è®¡è´¨é‡é—¨æ§
            design_quality = self._evaluate_design_quality_gate(task_context)
            
            if design_quality["passed"]:
                workflow_plan.update({
                    "next_action": {
                        "type": "assign_task",
                        "agent_id": "enhanced_real_code_review_agent",
                        "task_type": "verification", 
                        "description": "å¯¹è®¾è®¡è¿›è¡Œä»£ç å®¡æŸ¥å¹¶ç”Ÿæˆæµ‹è¯•å°éªŒè¯åŠŸèƒ½",
                        "context_files": design_quality.get("design_files", [])
                    },
                    "stage_name": "verification_stage",
                    "expected_outputs": ["æµ‹è¯•å°æ–‡ä»¶", "éªŒè¯æŠ¥å‘Š", "è´¨é‡åˆ†ææŠ¥å‘Š"],
                    "quality_gate_passed": True
                })
            else:
                # è®¾è®¡è´¨é‡ä¸è¿‡å…³ï¼Œé‡æ–°è®¾è®¡
                workflow_plan.update({
                    "next_action": {
                        "type": "retry_design",
                        "agent_id": "enhanced_real_verilog_agent",
                        "task_type": "design",
                        "description": f"é‡æ–°è®¾è®¡ï¼Œè§£å†³è´¨é‡é—®é¢˜: {', '.join(design_quality.get('issues', []))}"
                    },
                    "stage_name": "design_retry",
                    "quality_gate_passed": False,
                    "retry_reason": design_quality.get("issues", [])
                })
        
        elif current_stage == "verification_completed":
            # éªŒè¯å®Œæˆï¼Œæ£€æŸ¥æ•´ä½“å®Œæˆåº¦
            verification_quality = self._evaluate_verification_quality_gate(task_context)
            
            if verification_quality["passed"]:
                workflow_plan.update({
                    "next_action": {
                        "type": "provide_final_answer",
                        "description": "æ‰€æœ‰é˜¶æ®µå®Œæˆï¼Œæä¾›æœ€ç»ˆç­”æ¡ˆ"
                    },
                    "stage_name": "completion",
                    "workflow_complete": True,
                    "quality_gate_passed": True
                })
            else:
                # éªŒè¯ä¸å®Œæ•´ï¼Œé‡æ–°éªŒè¯
                workflow_plan.update({
                    "next_action": {
                        "type": "retry_verification", 
                        "agent_id": "enhanced_real_code_review_agent",
                        "task_type": "verification",
                        "description": f"é‡æ–°éªŒè¯ï¼Œè¡¥å……ç¼ºå¤±é¡¹: {', '.join(verification_quality.get('missing_items', []))}"
                    },
                    "stage_name": "verification_retry",
                    "quality_gate_passed": False,
                    "retry_reason": verification_quality.get("missing_items", [])
                })
        
        return workflow_plan
    
    def _determine_workflow_stage(self, task_context: TaskContext, task_analysis: Dict[str, Any]) -> str:
        """ç¡®å®šå½“å‰å·¥ä½œæµé˜¶æ®µ"""
        
        # æ£€æŸ¥æ™ºèƒ½ä½“æ‰§è¡Œå†å²
        agent_results = task_context.agent_results
        
        if not agent_results:
            return "initial"
        
        has_design_results = "enhanced_real_verilog_agent" in agent_results
        has_verification_results = "enhanced_real_code_review_agent" in agent_results
        
        if has_verification_results:
            return "verification_completed"
        elif has_design_results:
            return "design_completed" 
        else:
            return "initial"
    
    def _evaluate_design_quality_gate(self, task_context: TaskContext) -> Dict[str, Any]:
        """è¯„ä¼°è®¾è®¡é˜¶æ®µè´¨é‡é—¨æ§"""
        
        design_results = task_context.agent_results.get("enhanced_real_verilog_agent")
        if not design_results:
            return {
                "passed": False,
                "issues": ["è®¾è®¡ç»“æœä¸å­˜åœ¨"],
                "design_files": []
            }
        
        result = design_results.get("result", {})
        analysis = design_results.get("analysis", {})
        
        # è´¨é‡é—¨æ§æ ‡å‡†
        quality_gates = {
            "has_generated_files": len(result.get("generated_files", [])) > 0,
            "status_success": result.get("status") == "success",
            "quality_acceptable": analysis.get("quality_score", 0) >= 60.0,
            "no_hallucination": not analysis.get("hallucination_detected", False)
        }
        
        issues = []
        for gate, passed in quality_gates.items():
            if not passed:
                issues.append(f"è´¨é‡é—¨æ§å¤±è´¥: {gate}")
        
        return {
            "passed": all(quality_gates.values()),
            "issues": issues,
            "design_files": result.get("generated_files", []),
            "quality_score": analysis.get("quality_score", 0),
            "quality_gates": quality_gates
        }
    
    def _evaluate_verification_quality_gate(self, task_context: TaskContext) -> Dict[str, Any]:
        """è¯„ä¼°éªŒè¯é˜¶æ®µè´¨é‡é—¨æ§"""
        
        verification_results = task_context.agent_results.get("enhanced_real_code_review_agent")
        if not verification_results:
            return {
                "passed": False,
                "missing_items": ["éªŒè¯ç»“æœä¸å­˜åœ¨"]
            }
        
        result = verification_results.get("result", {})
        analysis = verification_results.get("analysis", {})
        
        # éªŒè¯é—¨æ§æ ‡å‡†
        verification_gates = {
            "has_testbench": any("test" in f.lower() or "_tb" in f.lower() for f in result.get("generated_files", [])),
            "status_success": result.get("status") == "success", 
            "quality_acceptable": analysis.get("quality_score", 0) >= 70.0,
            "verification_complete": result.get("verification", "") == "completed"
        }
        
        missing_items = []
        for gate, passed in verification_gates.items():
            if not passed:
                missing_items.append(f"éªŒè¯é—¨æ§å¤±è´¥: {gate}")
        
        return {
            "passed": all(verification_gates.values()),
            "missing_items": missing_items,
            "verification_files": result.get("generated_files", []),
            "quality_score": analysis.get("quality_score", 0),
            "verification_gates": verification_gates
        }

    def _extract_design_file_path_from_previous_results(self) -> Optional[str]:
        """ä»ä¹‹å‰çš„æ™ºèƒ½ä½“ç»“æœä¸­æå–è®¾è®¡æ–‡ä»¶è·¯å¾„ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰"""
        try:
            self.logger.debug("ğŸ” å¼€å§‹ä»è®¾è®¡æ™ºèƒ½ä½“ç»“æœä¸­æå–æ–‡ä»¶è·¯å¾„")
            found_files = []
            
            # éå†æ‰€æœ‰æ´»è·ƒä»»åŠ¡
            for task_id, task in self.active_tasks.items():
                self.logger.debug(f"ğŸ” æ£€æŸ¥ä»»åŠ¡: {task_id}")
                
                # éå†ä»»åŠ¡ä¸­çš„æ™ºèƒ½ä½“ç»“æœ
                for agent_id, agent_result in task.agent_results.items():
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è®¾è®¡æ™ºèƒ½ä½“çš„ç»“æœ
                    if agent_id == "enhanced_real_verilog_agent" and agent_result.get("success", False):
                        self.logger.debug(f"ğŸ” æ‰¾åˆ°è®¾è®¡æ™ºèƒ½ä½“æˆåŠŸç»“æœ: {agent_id}")
                        
                        # æ–¹æ³•1ï¼šç›´æ¥ä» agent_result ä¸­è·å– design_file_path
                        if "design_file_path" in agent_result:
                            path = agent_result["design_file_path"]
                            found_files.append(path)
                            self.logger.info(f"ğŸ“ æ–¹æ³•1æ‰¾åˆ°è®¾è®¡æ–‡ä»¶: {path}")
                        
                        # æ–¹æ³•2ï¼šä» response ä¸­è§£ææ–‡ä»¶è·¯å¾„
                        response = agent_result.get("response", "")
                        if isinstance(response, dict):
                            if "file_path" in response:
                                path = response["file_path"]
                                found_files.append(path)
                                self.logger.info(f"ğŸ“ æ–¹æ³•2æ‰¾åˆ°è®¾è®¡æ–‡ä»¶: {path}")
                            
                            # æ£€æŸ¥ generated_files å­—æ®µ
                            if "generated_files" in response:
                                files = response["generated_files"]
                                if isinstance(files, list):
                                    for file in files:
                                        if isinstance(file, str) and file.endswith('.v'):
                                            found_files.append(file)
                                            self.logger.info(f"ğŸ“ ä»generated_filesæ‰¾åˆ°: {file}")
                        
                        # æ–¹æ³•3ï¼šä» response å­—ç¬¦ä¸²ä¸­æå–æ–‡ä»¶è·¯å¾„
                        if isinstance(response, str):
                            import re
                            
                            # å¢å¼ºçš„æ–‡ä»¶è·¯å¾„æå–æ¨¡å¼
                            path_patterns = [
                                # åŒ¹é…å®Œæ•´çš„æ–‡ä»¶è·¯å¾„ï¼ˆå¸¦ç›®å½•ï¼‰
                                r'(/[^"\s]+/[^"\s]*\.v)',
                                # åŒ¹é…JSONæ ¼å¼çš„file_path
                                r'"file_path"\s*:\s*"([^"]+\.v)"',
                                # åŒ¹é…å†™å…¥æ–‡ä»¶çš„æ—¥å¿—
                                r'å†™å…¥æ–‡ä»¶:\s*([^\s\n,}]+\.v)',
                                r'æˆåŠŸå†™å…¥æ–‡ä»¶:\s*([^\s\n,}]+\.v)',
                                # åŒ¹é…ç”Ÿæˆæ–‡ä»¶çš„æè¿°
                                r'ç”Ÿæˆ.*?æ–‡ä»¶.*?:\s*([^\s\n,}]+\.v)',
                                r'ä¿å­˜.*?æ–‡ä»¶.*?:\s*([^\s\n,}]+\.v)',
                                # åŒ¹é…æ–‡ä»¶åæ¨¡å¼
                                r'([a-zA-Z0-9_]+(?:_optimized)?\.v)',
                            ]
                            
                            for pattern in path_patterns:
                                matches = re.findall(pattern, response, re.IGNORECASE)
                                for match in matches:
                                    path = match.strip('"\'')
                                    if path.endswith('.v') and path not in found_files:
                                        found_files.append(path)
                                        self.logger.info(f"ğŸ“ æ–¹æ³•3æ‰¾åˆ°è®¾è®¡æ–‡ä»¶: {path}")
            
            # æ–¹æ³•4ï¼šä»å®éªŒç›®å½•ä¸­æŸ¥æ‰¾æœ€è¿‘ç”Ÿæˆçš„æ–‡ä»¶
            if not found_files:
                recent_files = self._find_recent_design_files()
                found_files.extend(recent_files)
            
            # é€‰æ‹©æœ€ä½³æ–‡ä»¶è·¯å¾„
            if found_files:
                # ä¼˜å…ˆé€‰æ‹©optimizedç‰ˆæœ¬ï¼Œå…¶æ¬¡é€‰æ‹©æœ€æ–°çš„
                best_file = self._select_best_design_file(found_files)
                self.logger.info(f"âœ… é€‰æ‹©æœ€ä½³è®¾è®¡æ–‡ä»¶: {best_file}")
                return best_file
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»ä»»åŠ¡åˆ†é…ä¸­æŸ¥æ‰¾
            for task_id, task in self.active_tasks.items():
                for assignment in task.agent_assignments:
                    if assignment.get("design_file_path"):
                        path = assignment["design_file_path"]
                        self.logger.info(f"ğŸ“ ä»ä»»åŠ¡åˆ†é…ä¸­æ‰¾åˆ°: {path}")
                        return path
            
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ä¹‹å‰çš„è®¾è®¡æ–‡ä»¶è·¯å¾„")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ æå–è®¾è®¡æ–‡ä»¶è·¯å¾„æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None
    
    def _validate_design_file_before_distribution(self, design_content: str, file_path: str, target_agent_id: str):
        """åœ¨åˆ†å‘è®¾è®¡æ–‡ä»¶ç»™æ™ºèƒ½ä½“ä¹‹å‰éªŒè¯å…¶å®Œæ•´æ€§"""
        try:
            from core.code_consistency_checker import get_consistency_checker
            checker = get_consistency_checker()
            
            # éªŒè¯ä»£ç å®Œæ•´æ€§
            expected_features = ["parameterized", "width_parameter", "enable_input", "reset_input"]
            validation_result = checker.validate_code_parameter(design_content, expected_features)
            
            # è®°å½•éªŒè¯ç»“æœ
            if validation_result['valid']:
                module_info = validation_result.get('module_info')
                if module_info:
                    signature = module_info.get_signature()
                    self.logger.info(f"âœ… [åè°ƒå™¨â†’{target_agent_id}] è®¾è®¡æ–‡ä»¶éªŒè¯é€šè¿‡: {signature}")
                    self.logger.info(f"âœ… [åè°ƒå™¨â†’{target_agent_id}] æ–‡ä»¶: {file_path}")
            else:
                issues = validation_result.get('issues', [])
                self.logger.error(f"âŒ [åè°ƒå™¨â†’{target_agent_id}] è®¾è®¡æ–‡ä»¶éªŒè¯å¤±è´¥: {issues}")
                
                # è®°å½•è¯¦ç»†ä¿¡æ¯
                module_info = validation_result.get('module_info')
                if module_info:
                    signature = module_info.get_signature()
                    self.logger.error(f"âŒ [åè°ƒå™¨â†’{target_agent_id}] æ–‡ä»¶: {file_path}")
                    self.logger.error(f"âŒ [åè°ƒå™¨â†’{target_agent_id}] ä»£ç ç­¾å: {signature}")
                    self.logger.error(f"âŒ [åè°ƒå™¨â†’{target_agent_id}] ä»£ç è¡Œæ•°: {module_info.line_count}")
                
                # ğŸš¨ é‡è¦ï¼šå¦‚æœåˆ†å‘çš„ä»£ç ä¸å®Œæ•´ï¼Œè¿™ä¼šå¯¼è‡´ä¸‹æ¸¸æ™ºèƒ½ä½“äº§ç”Ÿä¸åŒ¹é…çš„ç»“æœ
                self.logger.error(f"ğŸš¨ [åè°ƒå™¨] è­¦å‘Šï¼šæ­£åœ¨åˆ†å‘ä¸å®Œæ•´çš„è®¾è®¡ä»£ç ç»™{target_agent_id}ï¼Œè¿™å¯èƒ½å¯¼è‡´ä¸Šä¸‹æ–‡ä¼ é€’é—®é¢˜")
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ [åè°ƒå™¨â†’{target_agent_id}] è®¾è®¡æ–‡ä»¶éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
    
    def _validate_inter_agent_context_consistency(self, task_file_context, current_agent_id: str):
        """éªŒè¯æ™ºèƒ½ä½“é—´çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªVerilogæ–‡ä»¶éœ€è¦éªŒè¯ä¸€è‡´æ€§
            consistency_results = task_file_context.validate_all_files_consistency()
            
            if consistency_results and not consistency_results.get('error'):
                # æœ‰ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
                for comparison_key, result in consistency_results.items():
                    if not result['consistent']:
                        self.logger.error(f"âŒ [åè°ƒå™¨] æ™ºèƒ½ä½“é—´ä¸Šä¸‹æ–‡ä¸ä¸€è‡´: {comparison_key}")
                        self.logger.error(f"âŒ [åè°ƒå™¨] ä¸ä¸€è‡´é—®é¢˜: {result['issues']}")
                        self.logger.error(f"âŒ [åè°ƒå™¨] ä¿®å¤å»ºè®®: {result['recommendations']}")
                        self.logger.error(f"ğŸš¨ [åè°ƒå™¨] å½“å‰æ™ºèƒ½ä½“{current_agent_id}å¯èƒ½ä¼šæ¥æ”¶åˆ°é”™è¯¯çš„ä¸Šä¸‹æ–‡")
                    else:
                        self.logger.info(f"âœ… [åè°ƒå™¨] æ™ºèƒ½ä½“é—´ä¸Šä¸‹æ–‡ä¸€è‡´æ€§éªŒè¯é€šè¿‡: {comparison_key} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ [åè°ƒå™¨] æ™ºèƒ½ä½“é—´ä¸Šä¸‹æ–‡ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {str(e)}")

    def _extract_module_name_from_verilog(self, verilog_code: str) -> str:
        """ä»Verilogä»£ç ä¸­æå–æ¨¡å—åï¼ˆä¸review agentä¿æŒä¸€è‡´çš„å®ç°ï¼‰"""
        import re
        
        # ğŸ”§ ä½¿ç”¨ä¸review agentç›¸åŒçš„æ¨¡å¼ï¼Œç¡®ä¿ä¸€è‡´æ€§
        module_patterns = [
            # æ¨¡å¼1: å¸¦å‚æ•°çš„æ¨¡å— module name #(...)(
            r'module\s+(\w+)\s*#\s*\([^)]*\)\s*\(',
            # æ¨¡å¼2: ä¸å¸¦å‚æ•°çš„æ¨¡å— module name(
            r'module\s+(\w+)\s*\(',
            # æ¨¡å¼3: å¤æ‚å‚æ•°æ¨¡å— module name #(...) (å¤šè¡Œ)
            r'module\s+(\w+)\s*#[^(]*\([^)]*\)\s*\(',
        ]
        
        self.logger.debug(f"ğŸ” åè°ƒå™¨æå–æ¨¡å—åï¼Œä»£ç é•¿åº¦: {len(verilog_code)} å­—ç¬¦")
        
        for i, pattern in enumerate(module_patterns):
            match = re.search(pattern, verilog_code, re.IGNORECASE | re.DOTALL)
            if match:
                module_name = match.group(1)
                self.logger.info(f"âœ… åè°ƒå™¨ä½¿ç”¨æ¨¡å¼ {i+1} æˆåŠŸæå–æ¨¡å—å: {module_name}")
                return module_name
        
        # å¦‚æœæ‰€æœ‰æ¨¡å¼éƒ½æ²¡æœ‰åŒ¹é…ï¼Œå°è¯•ç®€å•çš„å›é€€æ–¹æ¡ˆ
        module_keyword_match = re.search(r'module\s+(\w+)', verilog_code, re.IGNORECASE)
        if module_keyword_match:
            fallback_name = module_keyword_match.group(1)
            self.logger.warning(f"ğŸ”„ åè°ƒå™¨ä½¿ç”¨å›é€€æ–¹æ¡ˆæå–åˆ°æ¨¡å—å: {fallback_name}")
            return fallback_name
        
        # å¦‚æœå®Œå…¨æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›é»˜è®¤åç§°
        self.logger.error("âŒ åè°ƒå™¨å®Œå…¨æ— æ³•æå–æ¨¡å—å")
        return "unknown_module"
    
    def _find_recent_design_files(self) -> List[str]:
        """ä»å®éªŒç›®å½•ä¸­æŸ¥æ‰¾æœ€è¿‘ç”Ÿæˆçš„è®¾è®¡æ–‡ä»¶"""
        import os
        import glob
        from pathlib import Path
        
        try:
            design_files = []
            
            # æŸ¥æ‰¾å½“å‰å®éªŒç›®å½•
            for task_id, task in self.active_tasks.items():
                if hasattr(task, 'experiment_path') and task.experiment_path:
                    experiment_path = task.experiment_path
                    self.logger.debug(f"ğŸ” æœç´¢å®éªŒç›®å½•: {experiment_path}")
                    
                    # æŸ¥æ‰¾designså­ç›®å½•
                    designs_dir = os.path.join(experiment_path, "designs")
                    if os.path.exists(designs_dir):
                        verilog_files = glob.glob(os.path.join(designs_dir, "*.v"))
                        verilog_files.sort(key=os.path.getmtime, reverse=True)  # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
                        design_files.extend(verilog_files)
                        self.logger.info(f"ğŸ“ ä»å®éªŒç›®å½•æ‰¾åˆ° {len(verilog_files)} ä¸ªVerilogæ–‡ä»¶")
                    
                    # æŸ¥æ‰¾artifactså­ç›®å½•
                    artifacts_dir = os.path.join(experiment_path, "artifacts") 
                    if os.path.exists(artifacts_dir):
                        verilog_files = glob.glob(os.path.join(artifacts_dir, "*.v"))
                        verilog_files.sort(key=os.path.getmtime, reverse=True)
                        design_files.extend(verilog_files)
                        self.logger.info(f"ğŸ“ ä»artifactsç›®å½•æ‰¾åˆ° {len(verilog_files)} ä¸ªVerilogæ–‡ä»¶")
            
            return design_files[:5]  # è¿”å›æœ€æ–°çš„5ä¸ªæ–‡ä»¶
            
        except Exception as e:
            self.logger.error(f"âŒ æŸ¥æ‰¾æœ€è¿‘è®¾è®¡æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def _select_best_design_file(self, found_files: List[str]) -> str:
        """ä»æ‰¾åˆ°çš„æ–‡ä»¶åˆ—è¡¨ä¸­é€‰æ‹©æœ€ä½³çš„è®¾è®¡æ–‡ä»¶"""
        import os
        
        if not found_files:
            return None
        
        # ä¼˜å…ˆçº§è§„åˆ™
        priority_keywords = ['optimized', 'final', 'enhanced', 'improved']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¼˜åŒ–ç‰ˆæœ¬
        for keyword in priority_keywords:
            for file in found_files:
                if keyword in os.path.basename(file).lower():
                    self.logger.info(f"ğŸ¯ é€‰æ‹©ä¼˜å…ˆæ–‡ä»¶ï¼ˆåŒ…å«'{keyword}'ï¼‰: {file}")
                    return file
        
        # å¦‚æœæ²¡æœ‰ä¼˜å…ˆæ–‡ä»¶ï¼Œé€‰æ‹©è·¯å¾„æœ€å®Œæ•´çš„ï¼ˆåŒ…å«å®Œæ•´ç›®å½•ç»“æ„ï¼‰
        complete_paths = [f for f in found_files if '/' in f and f.startswith('/')]
        if complete_paths:
            # é€‰æ‹©è·¯å¾„æœ€é•¿çš„ï¼ˆé€šå¸¸æ˜¯æœ€å®Œæ•´çš„ï¼‰
            best_path = max(complete_paths, key=len)
            self.logger.info(f"ğŸ¯ é€‰æ‹©æœ€å®Œæ•´è·¯å¾„: {best_path}")
            return best_path
        
        # æœ€åé€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡ä»¶
        best_file = found_files[0]
        self.logger.info(f"ğŸ¯ é€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡ä»¶: {best_file}")
        return best_file
    
    def _load_design_file_to_context(self, design_file_path: str, task_file_context, agent_id: str):
        """åŠ è½½è®¾è®¡æ–‡ä»¶åˆ°ä»»åŠ¡ä¸Šä¸‹æ–‡"""
        from pathlib import Path
        import hashlib
        
        try:
            # è¿™é‡Œä¸éœ€è¦è®¾ç½®task_contextï¼Œå› ä¸ºæˆ‘ä»¬ç›´æ¥å°†æ–‡ä»¶æ·»åŠ åˆ°TaskFileContextä¸­
            self.logger.info(f"ğŸ“ ä½¿ç”¨è®¾è®¡æ–‡ä»¶è·¯å¾„: {design_file_path}")
            self.logger.debug(f"ğŸ§  è®¾è®¡æ–‡ä»¶è·¯å¾„åˆ†é… - æ™ºèƒ½ä½“: {agent_id}, æ–‡ä»¶: {design_file_path}")
            
            if Path(design_file_path).exists():
                with open(design_file_path, 'r', encoding='utf-8') as f:
                    design_content = f.read()
                
                # ä¸Šä¸‹æ–‡è°ƒè¯•æ—¥å¿—
                content_checksum = hashlib.md5(design_content.encode('utf-8')).hexdigest()[:8]
                self.logger.debug(f"ğŸ§  æ–‡ä»¶å†…å®¹è¯»å– - æ–‡ä»¶: {design_file_path}, é•¿åº¦: {len(design_content)}, æ ¡éªŒ: {content_checksum}")
                
                # ğŸ”§ æ–°å¢ï¼šéªŒè¯è®¾è®¡æ–‡ä»¶å®Œæ•´æ€§
                self._validate_design_file_before_distribution(design_content, design_file_path, agent_id)
                
                # æå–å®é™…æ¨¡å—å
                actual_module_name = self._extract_module_name_from_verilog(design_content)
                if actual_module_name and actual_module_name != "unknown_module":
                    self.logger.info(f"ğŸ¯ æå–åˆ°å®é™…æ¨¡å—å: {actual_module_name}")
                    module_metadata = {"actual_module_name": actual_module_name}
                else:
                    self.logger.warning("âš ï¸ æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„æ¨¡å—å")
                    module_metadata = {}
                
                task_file_context.add_file(
                    file_path=design_file_path,
                    content=design_content,
                    is_primary_design=True,
                    metadata=module_metadata
                )
                self.logger.info(f"ğŸ“„ è®¾è®¡æ–‡ä»¶å·²åŠ è½½åˆ°ä¸Šä¸‹æ–‡: {design_file_path} ({len(design_content)} å­—ç¬¦), æ¨¡å—å: {actual_module_name}")
                self.logger.debug(f"ğŸ§  æ–‡ä»¶åŠ è½½å®Œæˆ - ä¸Šä¸‹æ–‡æ–‡ä»¶æ€»æ•°: {len(task_file_context)}, ä¸»è®¾è®¡æ–‡ä»¶: {task_file_context.primary_design_file}")
                
                # ğŸ”§ æ–°å¢ï¼šåŒæ—¶æ›´æ–°å…¨å±€æ–‡ä»¶ä¸Šä¸‹æ–‡
                self._update_global_file_context(
                    file_path=design_file_path,
                    content=design_content,
                    module_name=actual_module_name,
                    task_id=task_file_context.task_id
                )
                
                return True
                
            else:
                self.logger.warning(f"âš ï¸ è®¾è®¡æ–‡ä»¶ä¸å­˜åœ¨: {design_file_path}")
                self.logger.debug(f"ğŸ§  æ–‡ä»¶ä¸å­˜åœ¨ - æ™ºèƒ½ä½“: {agent_id}, æ–‡ä»¶: {design_file_path}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ è¯»å–è®¾è®¡æ–‡ä»¶å¤±è´¥ {design_file_path}: {e}")
            self.logger.debug(f"ğŸ§  æ–‡ä»¶è¯»å–å¤±è´¥ - æ™ºèƒ½ä½“: {agent_id}, æ–‡ä»¶: {design_file_path}, é”™è¯¯: {str(e)}")
            return False
    
    def _inherit_file_context_from_previous_tasks(self, current_task_context) -> int:
        """ä»ä¹‹å‰çš„ä»»åŠ¡ä¸­ç»§æ‰¿æ–‡ä»¶ä¸Šä¸‹æ–‡"""
        from pathlib import Path
        
        try:
            inherited_count = 0
            
            # éå†æ‰€æœ‰æ´»è·ƒä»»åŠ¡ï¼ŒæŸ¥æ‰¾è®¾è®¡æ™ºèƒ½ä½“çš„æ–‡ä»¶è¾“å‡º
            for task_id, task in self.active_tasks.items():
                # è·³è¿‡å½“å‰ä»»åŠ¡
                if current_task_context.task_id == task_id:
                    continue
                
                self.logger.debug(f"ğŸ” æ£€æŸ¥ä»»åŠ¡ {task_id} çš„æ–‡ä»¶ä¸Šä¸‹æ–‡")
                
                # æ–¹æ³•1ï¼šä»ä»»åŠ¡çš„æ–‡ä»¶ä¸Šä¸‹æ–‡ä¸­ç»§æ‰¿
                if hasattr(task, 'task_file_context') and task.task_file_context:
                    source_context = task.task_file_context
                    for file_path, file_content in source_context.files.items():
                        if file_content.file_type.value in ['verilog', 'systemverilog']:
                            # éªŒè¯æ–‡ä»¶ä»ç„¶å­˜åœ¨
                            if Path(file_path).exists():
                                current_task_context.add_file(
                                    file_path=file_path,
                                    content=file_content.content,
                                    file_type=file_content.file_type,
                                    is_primary_design=file_content.is_primary_design,
                                    metadata=file_content.metadata
                                )
                                inherited_count += 1
                                self.logger.info(f"ğŸ“„ ä»ä»»åŠ¡ {task_id} ç»§æ‰¿æ–‡ä»¶: {file_path}")
                
                # æ–¹æ³•2ï¼šä»æ™ºèƒ½ä½“ç»“æœä¸­æŸ¥æ‰¾æ–‡ä»¶è·¯å¾„å¹¶è¯»å–
                for agent_id, agent_result in task.agent_results.items():
                    if agent_id == "enhanced_real_verilog_agent" and agent_result.get("success", False):
                        # å°è¯•æå–æ–‡ä»¶è·¯å¾„
                        file_paths = self._extract_file_paths_from_agent_result(agent_result)
                        for file_path in file_paths:
                            if Path(file_path).exists() and file_path.endswith('.v'):
                                try:
                                    with open(file_path, 'r', encoding='utf-8') as f:
                                        content = f.read()
                                    
                                    # æå–æ¨¡å—åä½œä¸ºå…ƒæ•°æ®
                                    module_name = self._extract_module_name_from_verilog(content)
                                    metadata = {"actual_module_name": module_name} if module_name != "unknown_module" else {}
                                    
                                    current_task_context.add_file(
                                        file_path=file_path,
                                        content=content,
                                        is_primary_design=True,
                                        metadata=metadata
                                    )
                                    inherited_count += 1
                                    self.logger.info(f"ğŸ“„ ä»æ™ºèƒ½ä½“ç»“æœç»§æ‰¿æ–‡ä»¶: {file_path}")
                                    
                                except Exception as e:
                                    self.logger.error(f"âŒ è¯»å–ç»§æ‰¿æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            
            if inherited_count > 0:
                self.logger.info(f"âœ… æˆåŠŸç»§æ‰¿äº† {inherited_count} ä¸ªè®¾è®¡æ–‡ä»¶")
            else:
                self.logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¯ç»§æ‰¿çš„è®¾è®¡æ–‡ä»¶")
            
            return inherited_count
            
        except Exception as e:
            self.logger.error(f"âŒ ç»§æ‰¿æ–‡ä»¶ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return 0
    
    def _extract_file_paths_from_agent_result(self, agent_result: dict) -> List[str]:
        """ä»æ™ºèƒ½ä½“ç»“æœä¸­æå–æ‰€æœ‰æ–‡ä»¶è·¯å¾„"""
        import re
        
        file_paths = []
        
        try:
            # ä»ç›´æ¥çš„å­—æ®µä¸­æå–
            if "file_path" in agent_result:
                file_paths.append(agent_result["file_path"])
            
            if "design_file_path" in agent_result:
                file_paths.append(agent_result["design_file_path"])
            
            # ä»responseä¸­æå–
            response = agent_result.get("response", "")
            if isinstance(response, str):
                # ä½¿ç”¨å¢å¼ºçš„è·¯å¾„æå–æ¨¡å¼
                path_patterns = [
                    r'(/[^"\s]+/[^"\s]*\.v)',
                    r'"file_path"\s*:\s*"([^"]+\.v)"',
                    r'å†™å…¥æ–‡ä»¶:\s*([^\s\n,}]+\.v)',
                    r'æˆåŠŸå†™å…¥æ–‡ä»¶:\s*([^\s\n,}]+\.v)',
                ]
                
                for pattern in path_patterns:
                    matches = re.findall(pattern, response, re.IGNORECASE)
                    file_paths.extend([match.strip('"\'') for match in matches])
            
            elif isinstance(response, dict):
                if "generated_files" in response:
                    files = response["generated_files"]
                    if isinstance(files, list):
                        file_paths.extend([f for f in files if isinstance(f, str) and f.endswith('.v')])
            
            # å»é‡
            unique_paths = list(set(file_paths))
            self.logger.debug(f"ğŸ” ä»æ™ºèƒ½ä½“ç»“æœæå–åˆ° {len(unique_paths)} ä¸ªæ–‡ä»¶è·¯å¾„")
            
            return unique_paths
            
        except Exception as e:
            self.logger.error(f"âŒ æå–æ–‡ä»¶è·¯å¾„å¤±è´¥: {e}")
            return []
    
    def _update_global_file_context(self, file_path: str, content: str, module_name: str = None, task_id: str = None):
        """æ›´æ–°å…¨å±€æ–‡ä»¶ä¸Šä¸‹æ–‡"""
        import os
        from datetime import datetime
        
        try:
            file_info = {
                "file_path": file_path,
                "content": content,
                "module_name": module_name,
                "task_id": task_id,
                "timestamp": datetime.now().isoformat(),
                "file_size": len(content),
                "checksum": hashlib.md5(content.encode('utf-8')).hexdigest()[:8],
                "file_exists": os.path.exists(file_path) if file_path else False
            }
            
            self.global_file_context[file_path] = file_info
            self.logger.debug(f"ğŸ—‚ï¸ æ›´æ–°å…¨å±€æ–‡ä»¶ä¸Šä¸‹æ–‡: {file_path}")
            
            # åŒæ—¶æ›´æ–°å¯¹è¯æ–‡ä»¶å†å²
            if task_id:
                if task_id not in self.conversation_file_history:
                    self.conversation_file_history[task_id] = {}
                self.conversation_file_history[task_id][file_path] = file_info
                
        except Exception as e:
            self.logger.error(f"âŒ æ›´æ–°å…¨å±€æ–‡ä»¶ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
    
    def _get_latest_design_files(self, count: int = 3) -> List[Dict]:
        """è·å–æœ€æ–°çš„è®¾è®¡æ–‡ä»¶ä¿¡æ¯"""
        try:
            # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè¿”å›æœ€æ–°çš„æ–‡ä»¶
            sorted_files = sorted(
                self.global_file_context.values(),
                key=lambda x: x.get("timestamp", ""),
                reverse=True
            )
            
            # ç­›é€‰Verilogè®¾è®¡æ–‡ä»¶
            design_files = [
                f for f in sorted_files 
                if f.get("file_path", "").endswith('.v') and 
                   f.get("module_name") and 
                   f.get("module_name") != "unknown_module"
            ]
            
            return design_files[:count]
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–æœ€æ–°è®¾è®¡æ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def _find_file_by_module_name(self, module_name: str) -> Optional[Dict]:
        """æ ¹æ®æ¨¡å—åæŸ¥æ‰¾æ–‡ä»¶"""
        try:
            for file_path, file_info in self.global_file_context.items():
                if file_info.get("module_name") == module_name:
                    self.logger.info(f"ğŸ” æ ¹æ®æ¨¡å—å '{module_name}' æ‰¾åˆ°æ–‡ä»¶: {file_path}")
                    return file_info
            
            self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å—åä¸º '{module_name}' çš„æ–‡ä»¶")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ æ ¹æ®æ¨¡å—åæŸ¥æ‰¾æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def _inherit_global_file_context(self, task_file_context, max_files: int = 5) -> int:
        """å°†å…¨å±€æ–‡ä»¶ä¸Šä¸‹æ–‡ç»§æ‰¿åˆ°ä»»åŠ¡æ–‡ä»¶ä¸Šä¸‹æ–‡ä¸­"""
        try:
            inherited_count = 0
            latest_files = self._get_latest_design_files(max_files)
            
            for file_info in latest_files:
                file_path = file_info.get("file_path")
                content = file_info.get("content")
                module_name = file_info.get("module_name")
                
                if file_path and content:
                    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨äºä»»åŠ¡ä¸Šä¸‹æ–‡ä¸­
                    if file_path not in task_file_context.files:
                        metadata = {"actual_module_name": module_name} if module_name else {}
                        
                        task_file_context.add_file(
                            file_path=file_path,
                            content=content,
                            is_primary_design=(inherited_count == 0),  # ç¬¬ä¸€ä¸ªæ–‡ä»¶è®¾ä¸ºä¸»è®¾è®¡æ–‡ä»¶
                            metadata=metadata
                        )
                        inherited_count += 1
                        self.logger.info(f"ğŸ“„ ä»å…¨å±€ä¸Šä¸‹æ–‡ç»§æ‰¿æ–‡ä»¶: {file_path} (æ¨¡å—: {module_name})")
            
            if inherited_count > 0:
                self.logger.info(f"âœ… ä»å…¨å±€ä¸Šä¸‹æ–‡ç»§æ‰¿äº† {inherited_count} ä¸ªè®¾è®¡æ–‡ä»¶")
            
            return inherited_count
            
        except Exception as e:
            self.logger.error(f"âŒ ç»§æ‰¿å…¨å±€æ–‡ä»¶ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return 0
    
    def _evaluate_composite_task_workflow(self, all_results: Dict[str, Any], 
                                        original_requirements: str, 
                                        task_context: TaskContext,
                                        completion_criteria: Dict[str, Any] = None) -> Dict[str, Any]:
        """è¯„ä¼°å¤åˆä»»åŠ¡çš„å·¥ä½œæµå®Œæˆæƒ…å†µï¼Œä½¿ç”¨è´¨é‡é—¨æ§"""
        
        # è¯„ä¼°å„ä¸ªé˜¶æ®µçš„è´¨é‡é—¨æ§
        design_quality = self._evaluate_design_quality_gate(task_context)
        verification_quality = self._evaluate_verification_quality_gate(task_context)
        
        # è®¡ç®—æ€»ä½“å®Œæˆåº¦
        design_weight = 0.6  # è®¾è®¡é˜¶æ®µæƒé‡
        verification_weight = 0.4  # éªŒè¯é˜¶æ®µæƒé‡
        
        design_score = design_quality.get("quality_score", 0) if design_quality.get("passed") else 0
        verification_score = verification_quality.get("quality_score", 0) if verification_quality.get("passed") else 0
        
        overall_quality_score = (design_score * design_weight) + (verification_score * verification_weight)
        
        # æ£€æŸ¥å¿…è¦çš„é˜¶æ®µæ˜¯å¦å®Œæˆ
        has_design = "enhanced_real_verilog_agent" in all_results
        has_verification = "enhanced_real_code_review_agent" in all_results
        
        design_successful = has_design and all_results["enhanced_real_verilog_agent"].get("success", False)
        verification_successful = has_verification and all_results["enhanced_real_code_review_agent"].get("success", False)
        
        # å†³å®šä»»åŠ¡æ˜¯å¦å®Œæˆ
        is_completed = design_successful and verification_successful and design_quality.get("passed") and verification_quality.get("passed")
        completion_score = 100 if is_completed else (60 if design_successful and verification_successful else (40 if design_successful else 10))
        
        # æ”¶é›†ç¼ºå¤±é¡¹
        missing_requirements = []
        if not design_successful:
            missing_requirements.append("è®¾è®¡é˜¶æ®µæœªæˆåŠŸå®Œæˆ")
        if not verification_successful:
            missing_requirements.append("éªŒè¯é˜¶æ®µæœªæˆåŠŸå®Œæˆ")
        if not design_quality.get("passed"):
            missing_requirements.extend(design_quality.get("issues", []))
        if not verification_quality.get("passed"):
            missing_requirements.extend(verification_quality.get("missing_items", []))
        
        # ç¡®å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
        next_steps = []
        if not design_successful:
            next_steps.append("éœ€è¦å®ŒæˆVerilogè®¾è®¡ä»»åŠ¡")
        elif not verification_successful:
            next_steps.append("éœ€è¦å®Œæˆä»£ç å®¡æŸ¥å’ŒéªŒè¯ä»»åŠ¡")
        elif not design_quality.get("passed"):
            next_steps.append("éœ€è¦é‡æ–°è®¾è®¡ï¼Œè§£å†³è´¨é‡é—¨æ§é—®é¢˜")
        elif not verification_quality.get("passed"):
            next_steps.append("éœ€è¦é‡æ–°éªŒè¯ï¼Œè¡¥å……ç¼ºå¤±çš„éªŒè¯é¡¹")
        else:
            next_steps.append("ä»»åŠ¡å·²å®Œæˆï¼Œå¯ä»¥æä¾›æœ€ç»ˆç­”æ¡ˆ")
        
        # è´¨é‡è¯„ä¼°
        if overall_quality_score >= 80:
            quality_assessment = "excellent"
        elif overall_quality_score >= 60:
            quality_assessment = "good" 
        elif overall_quality_score >= 40:
            quality_assessment = "fair"
        else:
            quality_assessment = "poor"
        
        return {
            "is_completed": is_completed,
            "completion_score": overall_quality_score,
            "missing_requirements": missing_requirements,
            "quality_assessment": quality_assessment,
            "next_steps": next_steps,
            "detailed_analysis": {
                "workflow_type": "composite_task",
                "design_stage": {
                    "completed": design_successful,
                    "quality_passed": design_quality.get("passed"),
                    "quality_score": design_score,
                    "issues": design_quality.get("issues", [])
                },
                "verification_stage": {
                    "completed": verification_successful,
                    "quality_passed": verification_quality.get("passed"),
                    "quality_score": verification_score,
                    "missing_items": verification_quality.get("missing_items", [])
                },
                "overall_workflow_score": overall_quality_score
            },
            "performance_metrics": {
                "total_agents_used": len(all_results),
                "workflow_stages_completed": len(task_context.workflow_stages),
                "total_execution_time": task_context.performance_metrics.get("total_execution_time", 0)
            }
        }
    
    async def _tool_get_tool_usage_guide(self, include_examples: bool = True,
                                       include_best_practices: bool = True) -> Dict[str, Any]:
        """è·å–LLMCoordinatorAgentä¸“ç”¨çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼"""
        try:
            guide = self._generate_coordinator_tool_guide()
            
            return {
                "success": True,
                "guide": guide,
                "agent_type": "LLMCoordinatorAgent",
                "include_examples": include_examples,
                "include_best_practices": include_best_practices,
                "total_tools": 8,  # LLMCoordinatorAgentæœ‰8ä¸ªå·¥å…·
                "message": "æˆåŠŸç”ŸæˆLLMCoordinatorAgentçš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼"
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆå·¥å…·ä½¿ç”¨æŒ‡å¯¼å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "message": "ç”Ÿæˆå·¥å…·ä½¿ç”¨æŒ‡å¯¼æ—¶å‘ç”Ÿé”™è¯¯"
            }
    
    def _check_assign_task_called(self, result: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†assign_task_to_agentå·¥å…·"""
        if not isinstance(result, str) or not result.strip():
            return False
        
        # æå–JSONå†…å®¹
        json_content = self._extract_json_from_response(result.strip())
        if not json_content:
            return False
            
        try:
            data = json.loads(json_content)
            
            if "tool_calls" in data and isinstance(data["tool_calls"], list):
                for call in data["tool_calls"]:
                    if isinstance(call, dict) and call.get("tool_name") == "assign_task_to_agent":
                        self.logger.info(f"âœ… æ£€æµ‹åˆ°assign_task_to_agentè°ƒç”¨")
                        return True
            
            return False
        except json.JSONDecodeError:
            return False
    
    async def _force_assign_task(self, user_request: str, task_context: TaskContext) -> Dict[str, Any]:
        """å¼ºåˆ¶åˆ†é…ä»»åŠ¡ç»™æ™ºèƒ½ä½“"""
        try:
            self.logger.info(f"ğŸš¨ å¼ºåˆ¶åˆ†é…ä»»åŠ¡: {user_request[:100]}...")
            
            # è®°å½•å¼ºåˆ¶åˆ†é…äº‹ä»¶
            task_context.add_conversation_message(
                role="system",
                content="æ£€æµ‹åˆ°LLMæœªæ­£ç¡®è°ƒç”¨æ¨èä»£ç†å·¥å…·ï¼Œå¯åŠ¨å¼ºåˆ¶åˆ†é…æœºåˆ¶",
                agent_id=self.agent_id,
                metadata={"type": "force_assignment", "reason": "missing_recommend_agent"}
            )
            
            # åˆ†æä»»åŠ¡ç±»å‹
            task_analysis = await self._tool_identify_task_type(user_request)
            if not task_analysis.get("success", False):
                return {"success": False, "error": "ä»»åŠ¡ç±»å‹è¯†åˆ«å¤±è´¥"}
            
            task_type = task_analysis.get("task_type", "design")
            
            # è®°å½•ä»»åŠ¡ç±»å‹è¯†åˆ«ç»“æœ
            task_context.add_conversation_message(
                role="system",
                content=f"å¼ºåˆ¶è¯†åˆ«ä»»åŠ¡ç±»å‹: {task_type}",
                agent_id=self.agent_id,
                metadata={"type": "task_type_identification", "task_type": task_type}
            )
            
            # æ¨èæ™ºèƒ½ä½“
            agent_recommendation = await self._tool_recommend_agent(
                task_type=task_type,
                task_description=user_request,
                priority="medium"
            )
            
            if not agent_recommendation.get("success", False):
                return {"success": False, "error": "æ™ºèƒ½ä½“æ¨èå¤±è´¥"}
            
            recommended_agent = agent_recommendation.get("recommended_agent", "enhanced_real_verilog_agent")
            recommendation_score = agent_recommendation.get("score", 0)
            
            # è®°å½•æ™ºèƒ½ä½“æ¨èç»“æœ
            task_context.add_conversation_message(
                role="system",
                content=f"å¼ºåˆ¶æ¨èæ™ºèƒ½ä½“: {recommended_agent} (è¯„åˆ†: {recommendation_score:.1f})",
                agent_id=self.agent_id,
                metadata={"type": "agent_recommendation", "agent_id": recommended_agent, "score": recommendation_score}
            )
            
            # å¼ºåˆ¶åˆ†é…ä»»åŠ¡
            assign_result = await self._tool_assign_task_to_agent(
                agent_id=recommended_agent,
                task_description=user_request,
                expected_output="ç”Ÿæˆå®Œæ•´çš„Verilogä»£ç æ–‡ä»¶",
                task_type=task_type,
                priority="medium"
            )
            
            # è®°å½•ä»»åŠ¡åˆ†é…ç»“æœ
            if assign_result.get("success", False):
                self.logger.info(f"âœ… å¼ºåˆ¶åˆ†é…ä»»åŠ¡æˆåŠŸ: {recommended_agent}")
                task_context.add_conversation_message(
                    role="system",
                    content=f"å¼ºåˆ¶åˆ†é…ä»»åŠ¡æˆåŠŸ: {recommended_agent}",
                    agent_id=self.agent_id,
                    metadata={"type": "task_assignment", "agent_id": recommended_agent, "success": True}
                )
                return {"success": True, "agent_id": recommended_agent, "result": assign_result}
            else:
                self.logger.error(f"âŒ å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¤±è´¥: {assign_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                task_context.add_conversation_message(
                    role="system",
                    content=f"å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¤±è´¥: {assign_result.get('error')}",
                    agent_id=self.agent_id,
                    metadata={"type": "task_assignment", "success": False, "error": assign_result.get('error')}
                )
                return {"success": False, "error": assign_result.get("error", "åˆ†é…ä»»åŠ¡å¤±è´¥")}
                
        except Exception as e:
            self.logger.error(f"âŒ å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¼‚å¸¸: {str(e)}")
            return {"success": False, "error": str(e)}