#!/usr/bin/env python3
"""
åŸºäºLLMçš„åè°ƒæ™ºèƒ½ä½“ - å¼ºåŒ–ç‰ˆæœ¬

Enhanced LLM-Driven Coordinator Agent
"""

import asyncio
import json
import time
import logging
import re
from typing import Dict, Any, List, Optional, Set, Tuple
from dataclasses import dataclass, field
from pathlib import Path
from datetime import datetime
from enum import Enum

from core.schema_system.enhanced_base_agent import EnhancedBaseAgent
from core.base_agent import TaskMessage
from core.enums import AgentCapability, AgentStatus
from config.config import FrameworkConfig
from llm_integration.enhanced_llm_client import EnhancedLLMClient
from core.enhanced_logging_config import get_agent_logger
from core.llm_communication import UnifiedLLMClientManager, SystemPromptBuilder, CallType


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    DESIGN = "design"
    VERIFICATION = "verification"
    ANALYSIS = "analysis"
    DEBUG = "debug"
    COMPOSITE = "composite"
    UNKNOWN = "unknown"


class TaskPriority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§æšä¸¾"""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class AgentInfo:
    """æ™ºèƒ½ä½“ä¿¡æ¯ - å¢å¼ºç‰ˆæœ¬"""
    agent_id: str
    agent_instance: EnhancedBaseAgent
    capabilities: Set[AgentCapability]
    specialty: str
    status: AgentStatus = AgentStatus.IDLE
    conversation_id: Optional[str] = None
    last_used: Optional[float] = None
    success_count: int = 0
    failure_count: int = 0
    total_execution_time: float = 0.0
    average_response_time: float = 0.0
    last_success_time: Optional[float] = None
    last_failure_time: Optional[float] = None
    consecutive_successes: int = 0
    consecutive_failures: int = 0
    preferred_task_types: Set[TaskType] = field(default_factory=set)
    blacklisted_task_types: Set[TaskType] = field(default_factory=set)


@dataclass
class TaskContext:
    """ä»»åŠ¡ä¸Šä¸‹æ–‡ - å¢å¼ºç‰ˆæœ¬"""
    task_id: str
    original_request: str
    task_type: TaskType = TaskType.UNKNOWN
    priority: TaskPriority = TaskPriority.MEDIUM
    current_stage: str = "initial"
    assigned_agent: Optional[str] = None
    agent_results: Dict[str, Any] = field(default_factory=dict)
    conversation_history: List[Dict[str, str]] = field(default_factory=list)
    start_time: float = field(default_factory=time.time)
    iteration_count: int = 0
    max_iterations: int = 10
    external_testbench_path: Optional[str] = None
    quality_score: float = 0.0
    completion_status: str = "pending"
    error_history: List[str] = field(default_factory=list)
    retry_count: int = 0
    max_retries: int = 3
    stage_history: List[Dict[str, Any]] = field(default_factory=list)
    agent_assignments: List[Dict[str, Any]] = field(default_factory=list)
    design_file_path: Optional[str] = None
    # ğŸ†• æ·»åŠ å®éªŒè·¯å¾„å­—æ®µï¼Œä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„å®éªŒç›®å½•
    experiment_path: Optional[str] = None
    
    # ğŸ†• å¢å¼ºæ•°æ®æ”¶é›†å­—æ®µ - ç”¨äºGradioå¯è§†åŒ–
    tool_executions: List[Dict[str, Any]] = field(default_factory=list)
    agent_interactions: List[Dict[str, Any]] = field(default_factory=list) 
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    workflow_stages: List[Dict[str, Any]] = field(default_factory=list)
    file_operations: List[Dict[str, Any]] = field(default_factory=list)
    execution_timeline: List[Dict[str, Any]] = field(default_factory=list)
    # ğŸ†• æ–°å¢ï¼šLLMå¯¹è¯è®°å½•
    llm_conversations: List[Dict[str, Any]] = field(default_factory=list)
    
    def add_conversation_message(self, role: str, content: str, agent_id: str = None, 
                               tool_info: Dict[str, Any] = None, metadata: Dict[str, Any] = None):
        """æ·»åŠ å¯¹è¯æ¶ˆæ¯åˆ°å†å²è®°å½•"""
        # ğŸ”§ å®‰å…¨å¤„ç†contentå‚æ•°ï¼Œç¡®ä¿å®ƒæ˜¯å­—ç¬¦ä¸²
        if content is None:
            safe_content = ""
        elif hasattr(content, '__await__'):  # æ£€æŸ¥æ˜¯å¦ä¸ºåç¨‹å¯¹è±¡
            safe_content = f"[åç¨‹å¯¹è±¡: {type(content).__name__}]"
        else:
            safe_content = str(content)
        
        message = {
            "timestamp": time.time(),
            "role": role,
            "content": safe_content,
            "agent_id": agent_id or "unknown",
        }
        
        if tool_info:
            message["tool_info"] = tool_info
        if metadata:
            message["metadata"] = metadata
            
        self.conversation_history.append(message)
        
        # è®°å½•æ—¥å¿—
        import logging
        logger = logging.getLogger("TaskContext")
        logger.info(f"ğŸ“ è®°å½•å¯¹è¯æ¶ˆæ¯: {role} - {agent_id or 'unknown'} - é•¿åº¦: {len(safe_content)}")
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """è·å–å¯¹è¯ç»Ÿè®¡æ‘˜è¦"""
        agents_involved = list(set(msg.get('agent_id', 'unknown') for msg in self.conversation_history))
        message_types = {}
        
        for msg in self.conversation_history:
            role = msg.get('role', 'unknown')
            message_types[role] = message_types.get(role, 0) + 1
        
        return {
            "total_messages": len(self.conversation_history),
            "agents_involved": agents_involved,
            "message_types": message_types,
            "conversation_duration": time.time() - self.start_time if self.conversation_history else 0
        }
    
    def get_agent_conversation_count(self, agent_id: str) -> int:
        """è·å–ç‰¹å®šæ™ºèƒ½ä½“çš„å¯¹è¯æ¶ˆæ¯æ•°é‡"""
        return len([msg for msg in self.conversation_history if msg.get('agent_id') == agent_id])
    
    def get_tool_calls_summary(self) -> Dict[str, Any]:
        """è·å–å·¥å…·è°ƒç”¨ç»Ÿè®¡æ‘˜è¦"""
        tool_calls = [msg for msg in self.conversation_history if msg.get('tool_info')]
        
        tool_names = []
        successful_calls = 0
        
        for msg in tool_calls:
            tool_info = msg.get('tool_info', {})
            if 'tool_name' in tool_info:
                tool_names.append(tool_info['tool_name'])
            if tool_info.get('success', False):
                successful_calls += 1
        
        return {
            "total_tool_calls": len(tool_calls),
            "successful_calls": successful_calls,
            "failure_rate": (len(tool_calls) - successful_calls) / max(len(tool_calls), 1),
            "unique_tools_used": list(set(tool_names)),
            "tool_usage_count": {name: tool_names.count(name) for name in set(tool_names)}
        }
    
    def add_tool_execution(self, tool_name: str, parameters: Dict[str, Any], 
                          agent_id: str, success: bool = True, 
                          result: Any = None, error: str = None, 
                          execution_time: float = 0.0):
        """è®°å½•å·¥å…·è°ƒç”¨æ‰§è¡Œ"""
        tool_execution = {
            "timestamp": time.time(),
            "tool_name": tool_name,
            "parameters": parameters,
            "agent_id": agent_id,
            "success": success,
            "result": str(result)[:500] if result else None,  # é™åˆ¶ç»“æœé•¿åº¦
            "error": error,
            "execution_time": execution_time
        }
        self.tool_executions.append(tool_execution)
        
        # åŒæ—¶è®°å½•åˆ°æ‰§è¡Œæ—¶é—´çº¿
        self.execution_timeline.append({
            "timestamp": time.time(),
            "event_type": "tool_execution",
            "agent_id": agent_id,
            "tool_name": tool_name,
            "success": success,
            "duration": execution_time
        })
    
    def add_file_operation(self, operation_type: str, file_path: str, 
                          agent_id: str, success: bool = True, 
                          file_size: int = 0, error: str = None):
        """è®°å½•æ–‡ä»¶æ“ä½œ"""
        file_operation = {
            "timestamp": time.time(),
            "operation_type": operation_type,  # read, write, create, delete
            "file_path": file_path,
            "agent_id": agent_id,
            "success": success,
            "file_size": file_size,
            "error": error
        }
        self.file_operations.append(file_operation)
        
        # åŒæ—¶è®°å½•åˆ°æ‰§è¡Œæ—¶é—´çº¿
        self.execution_timeline.append({
            "timestamp": time.time(),
            "event_type": "file_operation",
            "agent_id": agent_id,
            "operation_type": operation_type,
            "file_path": file_path,
            "success": success
        })
    
    def add_workflow_stage(self, stage_name: str, description: str, 
                          agent_id: str = None, duration: float = 0.0, 
                          success: bool = True, metadata: Dict[str, Any] = None):
        """è®°å½•å·¥ä½œæµé˜¶æ®µ"""
        stage = {
            "timestamp": time.time(),
            "stage_name": stage_name,
            "description": description,
            "agent_id": agent_id,
            "duration": duration,
            "success": success,
            "metadata": metadata or {}
        }
        self.workflow_stages.append(stage)
        
        # åŒæ—¶è®°å½•åˆ°æ‰§è¡Œæ—¶é—´çº¿
        self.execution_timeline.append({
            "timestamp": time.time(),
            "event_type": "workflow_stage",
            "stage_name": stage_name,
            "agent_id": agent_id,
            "success": success,
            "duration": duration
        })
    
    def update_performance_metrics(self, metrics: Dict[str, Any]):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics.update(metrics)
    
    def get_data_collection_summary(self) -> Dict[str, Any]:
        """è·å–æ•°æ®æ”¶é›†æ‘˜è¦"""
        return {
            "tool_executions": {
                "total": len(self.tool_executions),
                "successful": len([t for t in self.tool_executions if t.get("success", True)]),
                "failed": len([t for t in self.tool_executions if not t.get("success", True)]),
                "unique_tools": list(set(t.get("tool_name") for t in self.tool_executions)),
                "total_execution_time": sum(t.get("execution_time", 0) or 0 for t in self.tool_executions)
            },
            "file_operations": {
                "total": len(self.file_operations),
                "successful": len([f for f in self.file_operations if f.get("success", True)]),
                "failed": len([f for f in self.file_operations if not f.get("success", True)]),
                "operation_types": list(set(f.get("operation_type") for f in self.file_operations)),
                "total_file_size": sum(f.get("file_size", 0) or 0 for f in self.file_operations)
            },
            "workflow_stages": {
                "total": len(self.workflow_stages),
                "successful": len([w for w in self.workflow_stages if w.get("success", True)]),
                "failed": len([w for w in self.workflow_stages if not w.get("success", True)]),
                "total_duration": sum(w.get("duration", 0) or 0 for w in self.workflow_stages)
            },
            "agent_interactions": {
                "total": len(self.agent_interactions),
                "unique_agents": list(set(i.get("target_agent_id") for i in self.agent_interactions)),
                "successful": len([i for i in self.agent_interactions if i.get("success", True)]),
                "failed": len([i for i in self.agent_interactions if not i.get("success", True)])
            },
            "execution_timeline": {
                "total_events": len(self.execution_timeline),
                "event_types": list(set(e.get("event_type") for e in self.execution_timeline))
            },
            "llm_conversations": {
                "total": len(self.llm_conversations),
                "successful": len([l for l in self.llm_conversations if l.get("success", True)]),
                "failed": len([l for l in self.llm_conversations if not l.get("success", True)]),
                "unique_agents": list(set(l.get("agent_id") for l in self.llm_conversations)),
                "unique_models": list(set(l.get("model_name") for l in self.llm_conversations)),
                "total_duration": sum(l.get("duration", 0) or 0 for l in self.llm_conversations),
                "first_calls": len([l for l in self.llm_conversations if l.get("is_first_call", False)]),
                "total_tokens": sum(l.get("total_tokens", 0) or 0 for l in self.llm_conversations)
            }
        }
    
    def add_llm_conversation(self, agent_id: str, conversation_id: str,
                           system_prompt: str, user_message: str, 
                           assistant_response: str, model_name: str = None,
                           duration: float = 0.0, success: bool = True,
                           error_info: str = None, is_first_call: bool = False,
                           temperature: float = None, max_tokens: int = None,
                           prompt_tokens: int = None, completion_tokens: int = None,
                           total_tokens: int = None):
        """è®°å½•LLMå¯¹è¯"""
        # å®‰å…¨å¤„ç†å¯èƒ½ä¸ºNoneçš„å­—ç¬¦ä¸²å‚æ•°
        safe_system_prompt = system_prompt or ""
        safe_user_message = user_message or ""
        safe_assistant_response = assistant_response or ""
        
        llm_conversation = {
            "timestamp": time.time(),
            "agent_id": agent_id,
            "conversation_id": conversation_id,
            "system_prompt": safe_system_prompt[:1000] + ("..." if len(safe_system_prompt) > 1000 else ""),  # é™åˆ¶é•¿åº¦
            "user_message": safe_user_message[:2000] + ("..." if len(safe_user_message) > 2000 else ""),  # é™åˆ¶é•¿åº¦
            "assistant_response": safe_assistant_response[:2000] + ("..." if len(safe_assistant_response) > 2000 else ""),  # é™åˆ¶é•¿åº¦
            "model_name": model_name,
            "duration": duration,
            "success": success,
            "error_info": error_info,
            "is_first_call": is_first_call,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens
        }
        self.llm_conversations.append(llm_conversation)
        
        # åŒæ—¶è®°å½•åˆ°æ‰§è¡Œæ—¶é—´çº¿
        self.execution_timeline.append({
            "timestamp": time.time(),
            "event_type": "llm_conversation",
            "agent_id": agent_id,
            "conversation_id": conversation_id,
            "model_name": model_name,
            "success": success,
            "duration": duration,
            "is_first_call": is_first_call
        })


class LLMCoordinatorAgent(EnhancedBaseAgent):
    """
    åŸºäºLLMçš„åè°ƒæ™ºèƒ½ä½“ - å¼ºåŒ–ç‰ˆæœ¬
    
    ç‰¹ç‚¹ï¼š
    1. æ™ºèƒ½ä»»åŠ¡ç±»å‹è¯†åˆ«å’Œåˆ†ç±»
    2. åŸºäºå†å²è¡¨ç°çš„æ™ºèƒ½ä½“é€‰æ‹©
    3. ä¸¥æ ¼çš„ä»»åŠ¡èŒè´£åˆ†ç¦»æ§åˆ¶
    4. å¢å¼ºçš„ç»“æœè´¨é‡åˆ†æ
    5. æ™ºèƒ½é”™è¯¯æ¢å¤å’Œé‡è¯•æœºåˆ¶
    6. è¯¦ç»†çš„æ‰§è¡Œç›‘æ§å’Œæ—¥å¿—
    """
    
    def __init__(self, config: FrameworkConfig = None):
        # åˆå§‹åŒ–é…ç½®
        self.config = config or FrameworkConfig.from_env()
        
        super().__init__(
            agent_id="llm_coordinator_agent",
            role="coordinator",
            capabilities={
                AgentCapability.TASK_COORDINATION,
                AgentCapability.WORKFLOW_MANAGEMENT,
                AgentCapability.SPECIFICATION_ANALYSIS,
                AgentCapability.CODE_REVIEW,
                AgentCapability.QUALITY_ANALYSIS,
                AgentCapability.SYSTEM_MONITORING
            },
            config=self.config
        )
        
        # è®°å½•å¯åŠ¨æ—¶é—´
        self.start_time = time.time()
        
        # åˆå§‹åŒ–LLMé€šä¿¡æ¨¡å—
        self.llm_manager = UnifiedLLMClientManager(
            agent_id="llm_coordinator_agent",
            role="coordinator",
            config=self.config
        )
        self.prompt_builder = SystemPromptBuilder()
        
        # è®¾ç½®ä¸“ç”¨æ—¥å¿—å™¨
        self.agent_logger = get_agent_logger('LLMCoordinatorAgent')
        
        # æ³¨å†Œçš„æ™ºèƒ½ä½“
        self.registered_agents: Dict[str, AgentInfo] = {}
        
        # ä»»åŠ¡ä¸Šä¸‹æ–‡ç®¡ç†
        self.active_tasks: Dict[str, TaskContext] = {}
        
        # ä»»åŠ¡ç±»å‹è¯†åˆ«å™¨
        self.task_patterns = self._initialize_task_patterns()
        
        # åè°ƒå·¥å…·
        self._register_enhanced_coordination_tools()
        
        # æ€§èƒ½ç›‘æ§
        self.performance_metrics = {
            "total_tasks": 0,
            "successful_tasks": 0,
            "failed_tasks": 0,
            "average_completion_time": 0.0,
            "agent_utilization": {}
        }
        
        self.logger.debug("ğŸ§  LLMåè°ƒæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_task_patterns(self) -> Dict[TaskType, List[str]]:
        """åˆå§‹åŒ–ä»»åŠ¡ç±»å‹è¯†åˆ«æ¨¡å¼"""
        return {
            TaskType.DESIGN: [
                r"è®¾è®¡.*æ¨¡å—", r"ç”Ÿæˆ.*ä»£ç ", r"å®ç°.*åŠŸèƒ½", r"åˆ›å»º.*ç”µè·¯",
                r"design.*module", r"generate.*code", r"implement.*function",
                r"verilog.*module", r"hdl.*design", r"circuit.*design",
                r"æ¨¡å—è®¾è®¡", r"ä»£ç ç”Ÿæˆ", r"åŠŸèƒ½å®ç°", r"ç”µè·¯è®¾è®¡"
            ],
            TaskType.VERIFICATION: [
                r"æµ‹è¯•.*éªŒè¯", r"ä»¿çœŸ.*æµ‹è¯•", r"ç”Ÿæˆ.*testbench", r"éªŒè¯.*åŠŸèƒ½",
                r"test.*verification", r"simulation.*test", r"generate.*testbench",
                r"verify.*function", r"testbench.*generation", r"simulation.*verification",
                r"ä»£ç å®¡æŸ¥", r"è´¨é‡åˆ†æ", r"é”™è¯¯æ£€æŸ¥", r"åŠŸèƒ½éªŒè¯"
            ],
            TaskType.ANALYSIS: [
                r"åˆ†æ.*ä»£ç ", r"è´¨é‡.*è¯„ä¼°", r"æ€§èƒ½.*åˆ†æ", r"ä»£ç .*å®¡æŸ¥",
                r"analyze.*code", r"quality.*assessment", r"performance.*analysis",
                r"code.*review", r"static.*analysis", r"dynamic.*analysis",
                r"ä»£ç åˆ†æ", r"è´¨é‡è¯„ä¼°", r"æ€§èƒ½åˆ†æ", r"é™æ€åˆ†æ"
            ],
            TaskType.DEBUG: [
                r"è°ƒè¯•.*é”™è¯¯", r"ä¿®å¤.*é—®é¢˜", r"é”™è¯¯.*åˆ†æ", r"é—®é¢˜.*è§£å†³",
                r"debug.*error", r"fix.*issue", r"error.*analysis", r"problem.*solving",
                r"bug.*fix", r"error.*correction", r"issue.*resolution",
                r"é”™è¯¯ä¿®å¤", r"é—®é¢˜è°ƒè¯•", r"bugä¿®å¤", r"é”™è¯¯åˆ†æ"
            ]
        }
    
    def _register_enhanced_coordination_tools(self):
        """æ³¨å†Œå¢å¼ºçš„åè°ƒå·¥å…·"""
        
        # 1. æ™ºèƒ½ä»»åŠ¡åˆ†é…å·¥å…·
        self.register_enhanced_tool(
            name="assign_task_to_agent",
            func=self._tool_assign_task_to_agent,
            description="æ™ºèƒ½åˆ†é…ä»»åŠ¡ç»™æœ€åˆé€‚çš„æ™ºèƒ½ä½“ï¼ŒåŸºäºä»»åŠ¡ç±»å‹ã€æ™ºèƒ½ä½“èƒ½åŠ›å’Œå†å²è¡¨ç°",
            security_level="high",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "agent_id": {
                        "type": "string",
                        "description": "ç›®æ ‡æ™ºèƒ½ä½“IDï¼ˆå¿…é¡»æ˜¯enhanced_real_verilog_agentæˆ–enhanced_real_code_review_agentï¼‰",
                        "enum": ["enhanced_real_verilog_agent", "enhanced_real_code_review_agent"]
                    },
                    "task_description": {
                        "type": "string",
                        "description": "è¯¦ç»†çš„ä»»åŠ¡æè¿°å’Œè¦æ±‚",
                        "minLength": 10,
                        "maxLength": 5000
                    },
                    "expected_output": {
                        "type": "string",
                        "description": "æœŸæœ›çš„è¾“å‡ºæ ¼å¼å’Œå†…å®¹",
                        "default": "",
                        "maxLength": 2000
                    },
                    "task_type": {
                        "type": "string",
                        "enum": ["design", "verification", "analysis", "debug", "composite"],
                        "description": "ä»»åŠ¡ç±»å‹åˆ†ç±»"
                    },
                    "priority": {
                        "type": "string",
                        "enum": ["high", "medium", "low"],
                        "default": "medium",
                        "description": "ä»»åŠ¡ä¼˜å…ˆçº§"
                    }
                },
                "required": ["agent_id", "task_description"]
            }
        )
        
        # 2. å¢å¼ºç»“æœåˆ†æå·¥å…·
        self.register_enhanced_tool(
            name="analyze_agent_result",
            func=self._tool_analyze_agent_result,
            description="æ·±åº¦åˆ†ææ™ºèƒ½ä½“æ‰§è¡Œç»“æœï¼Œè¯„ä¼°è´¨é‡ã€å®Œæ•´æ€§å’Œä¸‹ä¸€æ­¥è¡ŒåŠ¨",
            security_level="high",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "agent_id": {
                        "type": "string",
                        "description": "æ‰§è¡Œä»»åŠ¡çš„æ™ºèƒ½ä½“ID"
                    },
                    "result": {
                        "type": "object",
                        "description": "æ™ºèƒ½ä½“æ‰§è¡Œç»“æœçš„è¯¦ç»†ä¿¡æ¯"
                    },
                    "task_context": {
                        "type": "object",
                        "description": "å½“å‰ä»»åŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯",
                        "default": {}
                    },
                    "quality_threshold": {
                        "type": "number",
                        "minimum": 0,
                        "maximum": 100,
                        "default": 80,
                        "description": "è´¨é‡è¯„ä¼°é˜ˆå€¼"
                    }
                },
                "required": ["agent_id", "result"]
            }
        )
        
        # 3. æ™ºèƒ½ä»»åŠ¡å®Œæˆæ£€æŸ¥å·¥å…·
        self.register_enhanced_tool(
            name="check_task_completion",
            func=self._tool_check_task_completion,
            description="æ™ºèƒ½æ£€æŸ¥ä»»åŠ¡å®ŒæˆçŠ¶æ€ï¼Œè¯„ä¼°æ•´ä½“è´¨é‡å’Œç¼ºå¤±é¡¹",
            security_level="high",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "task_id": {
                        "type": "string",
                        "description": "ä»»åŠ¡ID"
                    },
                    "all_results": {
                        "type": "object",
                        "description": "æ‰€æœ‰æ™ºèƒ½ä½“çš„æ‰§è¡Œç»“æœæ±‡æ€»"
                    },
                    "original_requirements": {
                        "type": "string",
                        "description": "åŸå§‹ä»»åŠ¡éœ€æ±‚æè¿°"
                    },
                    "completion_criteria": {
                        "type": "object",
                        "description": "å®Œæˆæ ‡å‡†å®šä¹‰",
                        "default": {}
                    }
                },
                "required": ["task_id", "all_results", "original_requirements"]
            }
        )
        
        # 4. æ™ºèƒ½ä½“çŠ¶æ€å’Œæ€§èƒ½æŸ¥è¯¢å·¥å…·
        self.register_enhanced_tool(
            name="query_agent_status",
            func=self._tool_query_agent_status,
            description="æŸ¥è¯¢æ™ºèƒ½ä½“è¯¦ç»†çŠ¶æ€ã€æ€§èƒ½æŒ‡æ ‡å’Œå†å²è¡¨ç°",
            security_level="normal",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "agent_id": {
                        "type": "string",
                        "description": "æ™ºèƒ½ä½“ID",
                        "enum": ["enhanced_real_verilog_agent", "enhanced_real_code_review_agent"]
                    },
                    "include_performance": {
                        "type": "boolean",
                        "default": True,
                        "description": "æ˜¯å¦åŒ…å«æ€§èƒ½æŒ‡æ ‡"
                    },
                    "include_history": {
                        "type": "boolean",
                        "default": False,
                        "description": "æ˜¯å¦åŒ…å«å†å²è®°å½•"
                    }
                },
                "required": ["agent_id"]
            }
        )
        
        # 5. æ–°å¢ï¼šæ™ºèƒ½ä»»åŠ¡ç±»å‹è¯†åˆ«å·¥å…·
        self.register_enhanced_tool(
            name="identify_task_type",
            func=self._tool_identify_task_type,
            description="æ™ºèƒ½è¯†åˆ«ä»»åŠ¡ç±»å‹ï¼Œä¸ºä»»åŠ¡åˆ†é…æä¾›å†³ç­–æ”¯æŒ",
            security_level="normal",
            category="analysis",
            schema={
                "type": "object",
                "properties": {
                    "user_request": {
                        "type": "string",
                        "description": "ç”¨æˆ·è¯·æ±‚å†…å®¹",
                        "minLength": 5,
                        "maxLength": 10000
                    },
                    "context": {
                        "type": "object",
                        "description": "ä»»åŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯",
                        "default": {}
                    }
                },
                "required": ["user_request"]
            }
        )
        
        # 6. æ–°å¢ï¼šæ™ºèƒ½ä½“é€‰æ‹©æ¨èå·¥å…·
        self.register_enhanced_tool(
            name="recommend_agent",
            func=self._tool_recommend_agent,
            description="åŸºäºä»»åŠ¡ç‰¹å¾å’Œæ™ºèƒ½ä½“èƒ½åŠ›æ¨èæœ€åˆé€‚çš„æ™ºèƒ½ä½“",
            security_level="normal",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "task_type": {
                        "type": "string",
                        "enum": ["design", "verification", "analysis", "debug", "composite"],
                        "description": "ä»»åŠ¡ç±»å‹"
                    },
                    "task_description": {
                        "type": "string",
                        "description": "ä»»åŠ¡æè¿°",
                        "minLength": 10
                    },
                    "priority": {
                        "type": "string",
                        "enum": ["high", "medium", "low"],
                        "default": "medium",
                        "description": "ä»»åŠ¡ä¼˜å…ˆçº§"
                    },
                    "constraints": {
                        "type": "object",
                        "description": "ä»»åŠ¡çº¦æŸæ¡ä»¶",
                        "default": {}
                    }
                },
                "required": ["task_type", "task_description"]
            }
        )
        
        # 7. æ–°å¢ï¼šæœ€ç»ˆç­”æ¡ˆå·¥å…·
        self.register_enhanced_tool(
            name="provide_final_answer",
            func=self._tool_provide_final_answer,
            description="å½“æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆï¼Œè°ƒç”¨æ­¤å·¥å…·æ¥ç”Ÿæˆå¹¶æä¾›æœ€ç»ˆçš„ã€å®Œæ•´çš„ç­”æ¡ˆç»™ç”¨æˆ·ã€‚",
            security_level="normal",
            category="coordination",
            schema={
                "type": "object",
                "properties": {
                    "final_summary": {
                        "type": "string",
                        "description": "å¯¹æ•´ä¸ªä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹å’Œæœ€ç»ˆç»“æœçš„è¯¦ç»†æ€»ç»“ã€‚"
                    },
                    "task_status": {
                        "type": "string",
                        "description": "ä»»åŠ¡çš„æœ€ç»ˆçŠ¶æ€ (ä¾‹å¦‚ï¼š'æˆåŠŸ', 'å¤±è´¥', 'éƒ¨åˆ†å®Œæˆ')."
                    },
                    "results_summary": {
                        "type": "object",
                        "description": "æ‰€æœ‰æ™ºèƒ½ä½“äº§ç”Ÿçš„ç»“æœçš„ç®€è¦æ±‡æ€»ã€‚"
                    }
                },
                "required": ["final_summary", "task_status"]
            }
        )
        
        # 8. æ–°å¢ï¼šå·¥å…·ä½¿ç”¨æŒ‡å¯¼å·¥å…·
        self.register_enhanced_tool(
            name="get_tool_usage_guide",
            func=self._tool_get_tool_usage_guide,
            description="è·å–LLMCoordinatorAgentçš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼ï¼ŒåŒ…æ‹¬å¯ç”¨å·¥å…·ã€å‚æ•°è¯´æ˜ã€è°ƒç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µã€‚",
            security_level="normal",
            category="help",
            schema={
                "type": "object",
                "properties": {
                    "include_examples": {
                        "type": "boolean",
                        "default": True,
                        "description": "æ˜¯å¦åŒ…å«è°ƒç”¨ç¤ºä¾‹"
                    },
                    "include_best_practices": {
                        "type": "boolean",
                        "default": True,
                        "description": "æ˜¯å¦åŒ…å«æœ€ä½³å®è·µ"
                    }
                },
                "additionalProperties": False
            }
        )
    
    async def _build_enhanced_system_prompt(self) -> str:
        """æ„å»ºæ”¯æŒåŠ¨æ€å†³ç­–å’Œå¤šæ™ºèƒ½ä½“åä½œçš„ç³»ç»Ÿæç¤ºè¯"""
        
        # ä½¿ç”¨SystemPromptBuilderæ„å»ºç³»ç»Ÿæç¤ºè¯
        return await self.prompt_builder.build_system_prompt(
            role="coordinator",
            call_type=CallType.FUNCTION_CALLING,
            agent_id=self.agent_id,
            capabilities=self._capabilities,
            metadata={
                "has_tools": hasattr(self, 'enhanced_tools') and bool(self.enhanced_tools),
                "tools_count": len(self.enhanced_tools) if hasattr(self, 'enhanced_tools') else 0
            }
        )
    async def register_agent(self, agent: EnhancedBaseAgent):
        """æ³¨å†Œæ™ºèƒ½ä½“"""
        agent_info = AgentInfo(
            agent_id=agent.agent_id,
            agent_instance=agent,
            capabilities=agent.get_capabilities(),
            specialty=agent.get_specialty_description()
        )
        
        self.registered_agents[agent.agent_id] = agent_info
        self.logger.info(f"âœ… æ³¨å†Œæ™ºèƒ½ä½“: {agent.agent_id} ({agent_info.specialty})")
    
    async def coordinate_task(self, user_request: str, 
                            conversation_id: str = None,
                            max_iterations: int = 10,
                            external_testbench_path: str = None) -> Dict[str, Any]:
        """
        åè°ƒä»»åŠ¡æ‰§è¡Œ
        
        Args:
            user_request: ç”¨æˆ·è¯·æ±‚
            conversation_id: å¯¹è¯ID
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            external_testbench_path: å¤–éƒ¨æä¾›çš„testbenchæ–‡ä»¶è·¯å¾„
            
        Returns:
            åè°ƒç»“æœ
        """
        self.logger.info(f"ğŸš€ å¼€å§‹åè°ƒä»»åŠ¡: {user_request[:100]}...")
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = f"task_{int(time.time())}"
        
        # ğŸ†• åˆ›å»ºç‹¬ç«‹çš„å®éªŒç›®å½•
        from core.experiment_manager import ExperimentManager
        experiment_manager = ExperimentManager()
        
        # ğŸ¯ ä¼˜åŒ–ï¼šä½¿ç”¨æ™ºèƒ½å®éªŒåç§°ç”Ÿæˆ
        experiment_name = experiment_manager.generate_experiment_name(user_request, task_type="design")
        
        # åˆ›å»ºå®éªŒ
        experiment_info = experiment_manager.create_experiment(
            experiment_name=experiment_name,
            task_description=user_request,
            metadata={
                "task_id": task_id,
                "created_by": "llm_coordinator_agent",
                "conversation_id": conversation_id
            }
        )
        
        # åˆ›å»ºä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«å®éªŒè·¯å¾„
        task_context = TaskContext(
            task_id=task_id,
            original_request=user_request,
            max_iterations=max_iterations,
            experiment_path=experiment_info.workspace_path
        )
        
        # ğŸ†• è®°å½•åˆå§‹ç”¨æˆ·è¯·æ±‚åˆ°å¯¹è¯å†å²
        task_context.add_conversation_message(
            role="user",
            content=user_request,
            agent_id="user",
            metadata={"task_id": task_id, "conversation_id": conversation_id}
        )
        
        # å¦‚æœæä¾›äº†å¤–éƒ¨testbenchï¼Œæ·»åŠ åˆ°ä»»åŠ¡ä¸Šä¸‹æ–‡
        if external_testbench_path:
            task_context.external_testbench_path = external_testbench_path
            self.logger.info(f"ğŸ“ ä½¿ç”¨å¤–éƒ¨testbench: {external_testbench_path}")
            # è®°å½•å¤–éƒ¨testbenchä¿¡æ¯
            task_context.add_conversation_message(
                role="system",
                content=f"ä½¿ç”¨å¤–éƒ¨testbenchæ–‡ä»¶: {external_testbench_path}",
                agent_id=self.agent_id,
                metadata={"type": "external_testbench"}
            )
        
        self.active_tasks[task_id] = task_context
        
        # ğŸ†• è®¾ç½®ä»»åŠ¡ä¸Šä¸‹æ–‡åˆ°å½“å‰å®ä¾‹ï¼Œç”¨äºåç»­å¯¹è¯è®°å½•
        self.current_task_context = task_context
        
        try:
            # æ„å»ºåè°ƒä»»åŠ¡
            coordination_task = self._build_coordination_task(user_request, task_context)
            
            # ğŸ†• è®°å½•ç³»ç»Ÿæç¤ºï¼ˆåè°ƒä»»åŠ¡ï¼‰åˆ°å¯¹è¯å†å²
            task_context.add_conversation_message(
                role="system",
                content=coordination_task,
                agent_id=self.agent_id,
                metadata={"type": "coordination_task", "task_stage": "initial"}
            )
            
            # ä½¿ç”¨Function Callingæ‰§è¡Œåè°ƒ
            result = await self.process_with_function_calling(
                user_request=coordination_task,
                max_iterations=max_iterations,
                conversation_id=conversation_id,
                preserve_context=True,
                enable_self_continuation=True,
                max_self_iterations=3
            )
            
            # ğŸ”§ æ–°å¢ï¼šæ ¼å¼ä¿®å¤ - å¦‚æœLLMè¾“å‡ºäº†é”™è¯¯æ ¼å¼ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤
            result = self._fix_tool_call_format(result)
            
            # ğŸ†• è®°å½•åè°ƒå™¨çš„å“åº”åˆ°å¯¹è¯å†å²
            task_context.add_conversation_message(
                role="assistant",
                content=result,
                agent_id=self.agent_id,
                metadata={"type": "coordination_response", "task_stage": "initial"}
            )
            
            # ğŸ” æ£€æŸ¥æ˜¯å¦å®é™…è°ƒç”¨äº†å·¥å…·
            self.logger.info(f"ğŸ” æ£€æŸ¥å·¥å…·è°ƒç”¨: ç»“æœé•¿åº¦={len(result)}, å†…å®¹é¢„è§ˆ={result[:100]}...")
            tools_executed = self._has_executed_tools(result)
            self.logger.info(f"ğŸ” å·¥å…·è°ƒç”¨æ£€æŸ¥ç»“æœ: {tools_executed}")
            
            if not tools_executed:
                self.logger.warning("âš ï¸ åè°ƒæ™ºèƒ½ä½“æ²¡æœ‰è°ƒç”¨ä»»ä½•å·¥å…·ï¼Œå¼ºåˆ¶é‡æ–°æ‰§è¡Œ")
                self.logger.info(f"ğŸ” åŸå§‹ç»“æœå†…å®¹: {result[:200]}...")
                
                # å¼ºåˆ¶é‡æ–°æ‰§è¡Œï¼Œä½¿ç”¨æ›´æ˜ç¡®çš„æŒ‡ä»¤
                forced_task = self._build_forced_coordination_task(user_request, task_context)
                self.logger.info(f"ğŸš¨ å¼ºåˆ¶é‡æ–°æ‰§è¡Œï¼Œä»»åŠ¡é•¿åº¦: {len(forced_task)} å­—ç¬¦")
                
                # ä½¿ç”¨æ›´ä¸¥æ ¼çš„å‚æ•°è¿›è¡Œå¼ºåˆ¶é‡æ–°æ‰§è¡Œ
                result = await self.process_with_function_calling(
                    user_request=forced_task,
                    max_iterations=1,  # é™åˆ¶ä¸º1æ¬¡è¿­ä»£ï¼Œå¼ºåˆ¶ç«‹å³æ‰§è¡Œ
                    conversation_id=f"{conversation_id}_forced",
                    preserve_context=False,  # ä¸ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œé‡æ–°å¼€å§‹
                    enable_self_continuation=False,  # ç¦ç”¨è‡ªä¸»ç»§ç»­
                    max_self_iterations=0  # ç¦ç”¨è‡ªæˆ‘ç»§ç»­
                )
                
                # å†æ¬¡æ£€æŸ¥æ˜¯å¦æ‰§è¡Œäº†å·¥å…·
                if not self._has_executed_tools(result):
                    self.logger.error("âŒ å¼ºåˆ¶é‡æ–°æ‰§è¡Œåä»æœªè°ƒç”¨å·¥å…·ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯")
                    self.logger.error(f"ğŸ” å¼ºåˆ¶æ‰§è¡Œç»“æœ: {result[:200]}...")
                    return {
                        "success": False,
                        "error": "åè°ƒæ™ºèƒ½ä½“æ— æ³•æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®",
                        "task_id": task_id,
                        "debug_info": {
                            "original_result": result[:500],
                            "forced_result": result[:500],
                            "tool_detection_failed": True
                        }
                    }
            
            # ğŸ” æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†assign_task_to_agentå·¥å…·
            self.logger.info(f"ğŸ” æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†assign_task_to_agentå·¥å…·...")
            assign_task_called = self._check_assign_task_called(result)
            self.logger.info(f"ğŸ” assign_task_to_agentè°ƒç”¨æ£€æŸ¥: {assign_task_called}")
            
            if not assign_task_called:
                self.logger.warning("âš ï¸ æ²¡æœ‰è°ƒç”¨assign_task_to_agentå·¥å…·ï¼Œå¼ºåˆ¶è°ƒç”¨")
                # å¼ºåˆ¶è°ƒç”¨assign_task_to_agent
                forced_assign_result = await self._force_assign_task(user_request, task_context)
                if not forced_assign_result.get("success", False):
                    self.logger.error("âŒ å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¤±è´¥")
                    return {
                        "success": False,
                        "error": "å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¤±è´¥",
                        "task_id": task_id,
                        "debug_info": forced_assign_result
                    }
            
            # ğŸ”„ å¼€å§‹æŒç»­åè°ƒå¾ªç¯ - ç›‘å¬æ™ºèƒ½ä½“ç»“æœå¹¶ç»§ç»­åè°ƒ
            final_result = await self._run_coordination_loop(task_context, result, conversation_id, max_iterations)
            
            # ğŸ†• è®°å½•ä»»åŠ¡å®Œæˆ
            task_context.add_conversation_message(
                role="system",
                content=f"ä»»åŠ¡åè°ƒå®Œæˆï¼Œä»»åŠ¡ID: {task_id}",
                agent_id=self.agent_id,
                metadata={"type": "task_completion", "success": True}
            )
            
            self.logger.info(f"âœ… ä»»åŠ¡åè°ƒå®Œæˆ: {task_id}, å¯¹è¯å†å²é•¿åº¦: {len(task_context.conversation_history)}")
            return final_result
            
        except Exception as e:
            self.logger.error(f"âŒ ä»»åŠ¡åè°ƒå¤±è´¥: {str(e)}")
            
            # ğŸ†• è®°å½•é”™è¯¯åˆ°å¯¹è¯å†å²
            if 'task_context' in locals():
                task_context.add_conversation_message(
                    role="error",
                    content=f"ä»»åŠ¡åè°ƒå¤±è´¥: {str(e)}",
                    agent_id=self.agent_id,
                    metadata={"type": "task_error", "error_type": type(e).__name__}
                )
            
            return {
                "success": False,
                "error": str(e),
                "task_id": task_id
            }
        finally:
            # æ¸…ç†ä»»åŠ¡ä¸Šä¸‹æ–‡
            if task_id in self.active_tasks:
                del self.active_tasks[task_id]
    
    def _fix_tool_call_format(self, result: str) -> str:
        """ä¿®å¤å·¥å…·è°ƒç”¨æ ¼å¼ - å°†é”™è¯¯çš„å•å·¥å…·æ ¼å¼è½¬æ¢ä¸ºæ­£ç¡®çš„tool_callsæ•°ç»„æ ¼å¼"""
        if not isinstance(result, str) or not result.strip():
            return result
        
        # æå–JSONå†…å®¹
        json_content = self._extract_json_from_response(result.strip())
        if not json_content:
            return result
        
        try:
            data = json.loads(json_content)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯çš„å•ä¸ªå·¥å…·æ ¼å¼
            if "tool_name" in data and "parameters" in data and "tool_calls" not in data:
                self.logger.warning(f"ğŸ”§ æ£€æµ‹åˆ°é”™è¯¯çš„å•å·¥å…·æ ¼å¼ï¼Œè‡ªåŠ¨ä¿®å¤ä¸ºtool_callsæ•°ç»„æ ¼å¼")
                
                # è½¬æ¢ä¸ºæ­£ç¡®çš„æ ¼å¼
                fixed_data = {
                    "tool_calls": [
                        {
                            "tool_name": data["tool_name"],
                            "parameters": data["parameters"]
                        }
                    ]
                }
                
                # ç”Ÿæˆä¿®å¤åçš„å“åº”
                fixed_json = json.dumps(fixed_data, ensure_ascii=False, indent=2)
                fixed_result = f"```json\n{fixed_json}\n```"
                
                self.logger.info(f"âœ… å·²ä¿®å¤å·¥å…·è°ƒç”¨æ ¼å¼ï¼š{data['tool_name']}")
                return fixed_result
            
        except json.JSONDecodeError:
            self.logger.debug("JSONè§£æå¤±è´¥ï¼Œä¿æŒåŸå§‹æ ¼å¼")
        
        return result

    def _has_executed_tools(self, result: str) -> bool:
        """æ£€æŸ¥å¹¶ä¿®å¤LLMçš„å›å¤ï¼Œç¡®ä¿æ˜¯æœ‰æ•ˆçš„å·¥å…·è°ƒç”¨JSONã€‚"""
        if not isinstance(result, str) or not result.strip():
            self.logger.debug(f"ğŸ” å·¥å…·æ£€æµ‹å¤±è´¥: ç»“æœä¸ºç©ºæˆ–éå­—ç¬¦ä¸²ç±»å‹")
            return False
        
        # æå–JSONå†…å®¹ï¼Œæ”¯æŒmarkdownä»£ç å—æ ¼å¼
        json_content = self._extract_json_from_response(result.strip())
        if not json_content:
            self.logger.debug(f"ğŸ” å·¥å…·æ£€æµ‹å¤±è´¥: æ— æ³•æå–JSONå†…å®¹")
            return False
            
        try:
            data = json.loads(json_content)
            self.logger.debug(f"ğŸ” è§£æJSONæˆåŠŸ: {list(data.keys())}")
            
            # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯çš„å•ä¸ªå·¥å…·æ ¼å¼ï¼Œå¹¶è‡ªåŠ¨è½¬æ¢ä¸ºæ­£ç¡®çš„tool_callsæ ¼å¼
            if "tool_name" in data and "parameters" in data and "tool_calls" not in data:
                self.logger.warning(f"ğŸ”§ æ£€æµ‹åˆ°é”™è¯¯çš„å•å·¥å…·æ ¼å¼ï¼Œè‡ªåŠ¨è½¬æ¢ä¸ºtool_callsæ•°ç»„æ ¼å¼")
                # è½¬æ¢ä¸ºæ­£ç¡®çš„æ ¼å¼
                fixed_data = {
                    "tool_calls": [
                        {
                            "tool_name": data["tool_name"],
                            "parameters": data["parameters"]
                        }
                    ]
                }
                # æ›´æ–°resultï¼ˆç”¨äºåç»­å¤„ç†ï¼‰
                fixed_json = json.dumps(fixed_data, ensure_ascii=False, indent=2)
                # è¿™é‡Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥ä¿®æ”¹ä¼ å…¥çš„resultï¼Œä½†å¯ä»¥è®°å½•é—®é¢˜
                self.logger.info(f"ğŸ”§ ä¿®å¤åçš„æ ¼å¼: {fixed_json}")
                return True  # æ ¼å¼å¯ä»¥ä¿®å¤ï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆçš„å·¥å…·è°ƒç”¨
            
            # æ£€æŸ¥æ­£ç¡®çš„tool_callsæ ¼å¼
            if "tool_calls" in data and isinstance(data["tool_calls"], list) and len(data["tool_calls"]) > 0:
                # è¿›ä¸€æ­¥æ£€æŸ¥tool_callsåˆ—è¡¨ä¸­çš„å…ƒç´ æ˜¯å¦åˆæ³•
                call = data["tool_calls"][0]
                if "tool_name" in call and "parameters" in call:
                    tool_name = call["tool_name"]
                    self.logger.info(f"âœ… æ£€æµ‹åˆ°æœ‰æ•ˆå·¥å…·è°ƒç”¨: {tool_name}")
                    return True
                else:
                    self.logger.debug(f"ğŸ” å·¥å…·è°ƒç”¨æ ¼å¼ä¸å®Œæ•´: {call}")
            else:
                self.logger.debug(f"ğŸ” æœªæ‰¾åˆ°æœ‰æ•ˆçš„tool_calls: {data}")
            return False
        except json.JSONDecodeError as e:
            self.logger.debug(f"ğŸ” JSONè§£æå¤±è´¥: {e}")
            return False
    
    def _extract_json_from_response(self, response: str) -> str:
        """ä»å“åº”ä¸­æå–JSONå†…å®¹ï¼Œæ”¯æŒmarkdownä»£ç å—æ ¼å¼"""
        if not response:
            return ""
        
        # å¦‚æœç›´æ¥ä»¥{å¼€å¤´ï¼Œç›´æ¥è¿”å›
        if response.startswith('{'):
            return response
        
        # å¤„ç†markdownä»£ç å—æ ¼å¼
        import re
        
        # åŒ¹é… ```json ... ``` æ ¼å¼
        json_block_pattern = r'```json\s*(.*?)\s*```'
        match = re.search(json_block_pattern, response, re.DOTALL)
        if match:
            return match.group(1).strip()
        
        # åŒ¹é… ``` ... ``` æ ¼å¼ï¼ˆå¯èƒ½æ²¡æœ‰è¯­è¨€æ ‡è¯†ï¼‰
        code_block_pattern = r'```\s*(.*?)\s*```'
        match = re.search(code_block_pattern, response, re.DOTALL)
        if match:
            content = match.group(1).strip()
            # æ£€æŸ¥æ˜¯å¦æ˜¯JSONæ ¼å¼
            if content.startswith('{') and content.endswith('}'):
                return content
        
        # å°è¯•æŸ¥æ‰¾JSONå¯¹è±¡
        json_pattern = r'\{.*?\}'
        match = re.search(json_pattern, response, re.DOTALL)
        if match:
            return match.group(0)
        
        return ""
    
    def _build_forced_coordination_task(self, user_request: str, task_context: TaskContext) -> str:
        """æ„å»ºä¸€ä¸ªæåº¦å¼ºåˆ¶çš„åè°ƒä»»åŠ¡ï¼Œåªè¦æ±‚è°ƒç”¨ç¬¬ä¸€ä¸ªå·¥å…·ã€‚"""
        
        # è·å–ç¬¬ä¸€ä¸ªå¿…é¡»è°ƒç”¨çš„å·¥å…·ä¿¡æ¯
        first_tool_schema = self.get_tool_schema("identify_task_type")
        
        # Escape quotes in user_request
        escaped_user_request = user_request.replace('"', '\\"')
        
        # è·å–LLMCoordinatorAgentçš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
        coordinator_guide_simple = """
**ğŸ› ï¸ æ­£ç¡®çš„å·¥å…·è°ƒç”¨æ–¹å¼**:
1. identify_task_type - è¯†åˆ«ä»»åŠ¡ç±»å‹
2. recommend_agent - æ¨èæ™ºèƒ½ä½“
3. assign_task_to_agent - åˆ†é…ä»»åŠ¡ç»™æ™ºèƒ½ä½“

âš ï¸ **ä¸¥ç¦ç›´æ¥è°ƒç”¨æ™ºèƒ½ä½“åç§°ä½œä¸ºå·¥å…·**:
âŒ enhanced_real_verilog_agent
âŒ enhanced_real_code_review_agent
âœ… ä½¿ç”¨ assign_task_to_agent å·¥å…·æ¥åˆ†é…ä»»åŠ¡
"""

        return f"""
# ğŸš¨ğŸš¨ğŸš¨ å¼ºåˆ¶æŒ‡ä»¤ ğŸš¨ğŸš¨ğŸš¨
ä½ å¿…é¡»ç«‹å³è°ƒç”¨ `assign_task_to_agent` å·¥å…·æ¥åˆ†é…ä»»åŠ¡ã€‚

**ç”¨æˆ·éœ€æ±‚**:
{user_request}

{coordinator_guide_simple}

# âœ… å”¯ä¸€æ­£ç¡®çš„å·¥å…·è°ƒç”¨æ ¼å¼:
```json
{{
    "tool_calls": [
        {{
            "tool_name": "assign_task_to_agent",
            "parameters": {{
                "agent_id": "enhanced_real_verilog_agent",
                "task_description": "æ ¹æ®ç”¨æˆ·éœ€æ±‚è®¾è®¡Verilogæ¨¡å—ï¼Œä¸“æ³¨äºæ¨¡å—è®¾è®¡å’Œä»£ç å®ç°",
                "expected_output": "ç”Ÿæˆå®Œæ•´çš„Verilogä»£ç æ–‡ä»¶",
                "task_type": "design",
                "priority": "medium"
            }}
        }}
    ]
}}
```

# ğŸš¨ğŸš¨ğŸš¨ ä¸¥æ ¼è¦æ±‚ ğŸš¨ğŸš¨ğŸš¨
- âœ… åªèƒ½è°ƒç”¨ `assign_task_to_agent` å·¥å…·
- âŒ ç»å¯¹ç¦æ­¢è°ƒç”¨æ™ºèƒ½ä½“åç§°ä½œä¸ºå·¥å…·
- âŒ ç»å¯¹ç¦æ­¢ä½¿ç”¨ enhanced_real_verilog_agent ä½œä¸º tool_name
- âŒ ç»å¯¹ç¦æ­¢ä½¿ç”¨ enhanced_real_code_review_agent ä½œä¸º tool_name
- âŒ ä¸è¦ç”Ÿæˆä»»ä½•å…¶ä»–å†…å®¹
- âŒ ä¸è¦ç”Ÿæˆä»»ä½•æè¿°æ€§æ–‡æœ¬
- âŒ ä¸è¦è§£é‡Šä½ çš„ç­–ç•¥
- âŒ ä¸è¦åˆ†æä»»åŠ¡
- âŒ ä¸è¦ä½¿ç”¨markdownæ ¼å¼
- âŒ ä¸è¦ç”Ÿæˆè¡¨æ ¼
- âœ… åªç”Ÿæˆå·¥å…·è°ƒç”¨JSON

âš¡ ç«‹å³æ‰§è¡Œ: å¤åˆ¶ä¸Šé¢çš„JSONï¼Œä¸è¦ä¿®æ”¹ä»»ä½•å†…å®¹ï¼Œç«‹å³ç”Ÿæˆã€‚
"""
    
    def _build_coordination_task(self, user_request: str, task_context: TaskContext) -> str:
        """æ„å»ºåè°ƒä»»åŠ¡æè¿°"""
        
        # æ„å»ºå¯ç”¨æ™ºèƒ½ä½“ä¿¡æ¯
        agents_info = []
        for agent_id, agent_info in self.registered_agents.items():
            capabilities = ", ".join([cap.value for cap in agent_info.capabilities])
            agents_info.append(f"- {agent_id}: {agent_info.specialty} (èƒ½åŠ›: {capabilities})")
        
        agents_section = "\n".join(agents_info) if agents_info else "æš‚æ— å¯ç”¨æ™ºèƒ½ä½“"
        
        # æ„å»ºå¤–éƒ¨testbenchä¿¡æ¯
        external_testbench_info = ""
        if hasattr(task_context, 'external_testbench_path') and task_context.external_testbench_path:
            external_testbench_info = f"""

**ğŸ“ å¤–éƒ¨Testbench**:
- è·¯å¾„: {task_context.external_testbench_path}
- è¯´æ˜: ç”¨æˆ·å·²æä¾›testbenchæ–‡ä»¶ï¼Œå®¡æŸ¥æ™ºèƒ½ä½“åº”ç›´æ¥ä½¿ç”¨æ­¤æ–‡ä»¶è¿›è¡Œæµ‹è¯•ï¼Œæ— éœ€ç”Ÿæˆæ–°çš„testbench
- å·¥ä½œæ¨¡å¼: å®¡æŸ¥æ™ºèƒ½ä½“ä¸“æ³¨äºä»£ç å®¡æŸ¥ã€æµ‹è¯•æ‰§è¡Œå’Œé—®é¢˜ä¿®å¤ï¼Œè·³è¿‡testbenchç”Ÿæˆæ­¥éª¤"""

        # è·å–LLMCoordinatorAgentçš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼
        coordinator_tool_guide = self._get_agent_specific_tool_guide("llm_coordinator_agent")

        return f"""
ğŸ§  åè°ƒä»»åŠ¡

**ç”¨æˆ·éœ€æ±‚**:
{user_request}

**ä»»åŠ¡ID**: {task_context.task_id}
**å½“å‰é˜¶æ®µ**: {task_context.current_stage}
**å·²æ‰§è¡Œè¿­ä»£**: {task_context.iteration_count}/{task_context.max_iterations}

**å¯ç”¨æ™ºèƒ½ä½“**:
{agents_section}
{external_testbench_info}

**ä»»åŠ¡ä¸Šä¸‹æ–‡**:
- å¼€å§‹æ—¶é—´: {datetime.fromtimestamp(task_context.start_time).strftime('%Y-%m-%d %H:%M:%S')}
- å·²åˆ†é…æ™ºèƒ½ä½“: {task_context.assigned_agent or 'æ— '}
- æ‰§è¡Œç»“æœ: {len(task_context.agent_results)} ä¸ªç»“æœ

**ğŸ¯ å¼ºåˆ¶æ‰§è¡Œçš„åè°ƒæµç¨‹**:
1. **ç¬¬ä¸€æ­¥**: è°ƒç”¨ `identify_task_type` è¯†åˆ«ä»»åŠ¡ç±»å‹
2. **ç¬¬äºŒæ­¥**: è°ƒç”¨ `recommend_agent` æ¨èæœ€åˆé€‚çš„æ™ºèƒ½ä½“
3. **ç¬¬ä¸‰æ­¥**: è°ƒç”¨ `assign_task_to_agent` åˆ†é…ä»»åŠ¡ç»™æ¨èæ™ºèƒ½ä½“
4. **ç¬¬å››æ­¥**: è°ƒç”¨ `analyze_agent_result` åˆ†ææ‰§è¡Œç»“æœ
5. **ç¬¬äº”æ­¥**: æ ¹æ®åˆ†æç»“æœå†³å®šæ˜¯å¦éœ€è¦ç»§ç»­è¿­ä»£

**âš ï¸ é‡è¦æé†’**:
- å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æµç¨‹æ‰§è¡Œï¼Œä¸å¾—è·³è¿‡ä»»ä½•æ­¥éª¤
- æ¨èä»£ç†å·¥å…· `recommend_agent` æ˜¯å¿…éœ€çš„ï¼Œä¸èƒ½ç›´æ¥è°ƒç”¨ `assign_task_to_agent`
- æ¯æ¬¡ä»»åŠ¡åˆ†é…å‰éƒ½å¿…é¡»å…ˆè°ƒç”¨æ¨èä»£ç†å·¥å…·

{coordinator_tool_guide}

**æ‰§è¡Œè¦æ±‚**:
1. ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°å·¥å…·ä½¿ç”¨æŒ‡å¯¼è¿›è¡Œæ“ä½œ
2. ç»å¯¹ç¦æ­¢ç›´æ¥è°ƒç”¨æ™ºèƒ½ä½“åç§°ä½œä¸ºå·¥å…·
3. å¿…é¡»ä½¿ç”¨ assign_task_to_agent å·¥å…·æ¥åˆ†é…ä»»åŠ¡
4. å¿…é¡»ä½¿ç”¨ recommend_agent å·¥å…·æ¥æ¨èæ™ºèƒ½ä½“
5. æŒ‰ç…§æ¨èçš„åè°ƒæµç¨‹æ‰§è¡Œ

è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚å’Œå¯ç”¨æ™ºèƒ½ä½“èƒ½åŠ›ï¼Œåˆ¶å®šæœ€ä¼˜çš„æ‰§è¡Œç­–ç•¥å¹¶å¼€å§‹åè°ƒã€‚
"""
    
    async def _tool_assign_task_to_agent(self, agent_id: str, task_description: str,
                                       expected_output: str = "",
                                       task_type: str = "composite",
                                       priority: str = "medium",
                                       design_file_path: str = None) -> Dict[str, Any]:
        """æ™ºèƒ½åˆ†é…ä»»åŠ¡ç»™æœ€åˆé€‚çš„æ™ºèƒ½ä½“"""
        
        try:
            self.logger.info(f"ğŸ¯ åˆ†é…ä»»åŠ¡ç»™æ™ºèƒ½ä½“: {agent_id}")
            self.logger.info(f"ğŸ“‹ ä»»åŠ¡æè¿°: {task_description[:100]}...")
            
            # æ£€æŸ¥æ™ºèƒ½ä½“æ˜¯å¦å­˜åœ¨
            if agent_id not in self.registered_agents:
                return {
                    "success": False,
                    "error": f"æ™ºèƒ½ä½“ {agent_id} æœªæ³¨å†Œ",
                    "available_agents": list(self.registered_agents.keys())
                }
            
            agent_info = self.registered_agents[agent_id]
            agent = agent_info.agent_instance
            
            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ æ™ºèƒ½ä½“å¥åº·æ£€æŸ¥
            if hasattr(agent_info, 'failure_count') and agent_info.failure_count >= 3:
                return {
                    "success": False,
                    "error": f"æ™ºèƒ½ä½“ {agent_id} è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œå·²æš‚æ—¶ç¦ç”¨",
                    "agent_status": "disabled",
                    "failure_count": agent_info.failure_count
                }
            
            # æ£€æŸ¥æ™ºèƒ½ä½“çŠ¶æ€
            if agent_info.status == AgentStatus.BUSY:
                return {
                    "success": False,
                    "error": f"æ™ºèƒ½ä½“ {agent_id} å½“å‰å¿™ç¢Œä¸­",
                    "agent_status": agent_info.status.value
                }
            
            # åˆ›å»ºä»»åŠ¡ä¸Šä¸‹æ–‡
            task_id = f"task_{int(time.time())}"
            
            # ğŸ¯ ä¿®å¤ï¼šTaskTypeéªŒè¯è¿‡äºæ­»æ¿çš„é—®é¢˜
            # æ”¯æŒæ›´å¤šä»»åŠ¡ç±»å‹ï¼ŒåŒ…æ‹¬"review"ç­‰
            try:
                task_type_enum = TaskType(task_type)
            except ValueError:
                # å¦‚æœtask_typeä¸åœ¨æšä¸¾ä¸­ï¼Œæ˜ å°„åˆ°åˆé€‚çš„ç±»å‹
                task_type_mapping = {
                    "review": TaskType.VERIFICATION,
                    "test": TaskType.VERIFICATION,
                    "verify": TaskType.VERIFICATION,
                    "analyze": TaskType.ANALYSIS,
                    "debug": TaskType.DEBUG,
                    "design": TaskType.DESIGN,
                    "composite": TaskType.COMPOSITE
                }
                task_type_enum = task_type_mapping.get(task_type.lower(), TaskType.COMPOSITE)
                self.logger.info(f"ğŸ¯ ä»»åŠ¡ç±»å‹æ˜ å°„: '{task_type}' -> {task_type_enum.value}")
            
            task_context = TaskContext(
                task_id=task_id,
                original_request=task_description,
                task_type=task_type_enum,
                priority=TaskPriority(priority),
                current_stage=f"assigned_to_{agent_id}",
                assigned_agent=agent_id,
                start_time=time.time()
            )
            
            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®è®¾ç½®å®éªŒè·¯å¾„
            current_experiment_path = None
            
            # 1. é¦–å…ˆå°è¯•ä»å½“å‰æ´»è·ƒä»»åŠ¡ä¸­è·å–å®éªŒè·¯å¾„
            for task in self.active_tasks.values():
                if hasattr(task, 'experiment_path') and task.experiment_path:
                    current_experiment_path = task.experiment_path
                    self.logger.info(f"ğŸ§ª ä»æ´»è·ƒä»»åŠ¡ä¸­è·å–å®éªŒè·¯å¾„: {current_experiment_path}")
                    break
            
            # 2. å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»å®éªŒç®¡ç†å™¨è·å–
            if not current_experiment_path:
                try:
                    from core.experiment_manager import get_experiment_manager
                    exp_manager = get_experiment_manager()
                    
                    if exp_manager and exp_manager.current_experiment_path:
                        current_experiment_path = exp_manager.current_experiment_path
                        self.logger.info(f"ğŸ§ª ä»å®éªŒç®¡ç†å™¨è·å–å®éªŒè·¯å¾„: {current_experiment_path}")
                except (ImportError, Exception) as e:
                    self.logger.debug(f"å®éªŒç®¡ç†å™¨ä¸å¯ç”¨: {e}")
            
            # 3. å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
            if not current_experiment_path:
                current_experiment_path = "./file_workspace"
                self.logger.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°å®éªŒè·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {current_experiment_path}")
            
            # è®¾ç½®å®éªŒè·¯å¾„
            task_context.experiment_path = current_experiment_path
            self.logger.info(f"âœ… è®¾ç½®ä»»åŠ¡å®éªŒè·¯å¾„: {current_experiment_path}")
            
            # ğŸ”§ æ–°å¢ï¼šå¦‚æœæä¾›äº† design_file_pathï¼Œç›´æ¥ä½¿ç”¨
            if design_file_path:
                task_context.design_file_path = design_file_path
                self.logger.info(f"ğŸ“ ä½¿ç”¨æä¾›çš„è®¾è®¡æ–‡ä»¶è·¯å¾„: {design_file_path}")
            else:
                # ğŸ”§ æ–°å¢ï¼šå°è¯•ä»ä¹‹å‰çš„æ™ºèƒ½ä½“ç»“æœä¸­æå–è®¾è®¡æ–‡ä»¶è·¯å¾„
                design_file_path = self._extract_design_file_path_from_previous_results()
                if design_file_path:
                    task_context.design_file_path = design_file_path
                    self.logger.info(f"ğŸ“ ä»ä¹‹å‰ç»“æœä¸­æå–è®¾è®¡æ–‡ä»¶è·¯å¾„: {design_file_path}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯åç»­è°ƒç”¨ï¼ˆé€šè¿‡å¯¹è¯å†å²åˆ¤æ–­ï¼‰
            is_follow_up_call = False
            if hasattr(agent, 'conversation_history') and agent.conversation_history:
                # æ£€æŸ¥å¯¹è¯å†å²ä¸­æ˜¯å¦å·²ç»æœ‰ç›¸åŒçš„ä»»åŠ¡æè¿°
                for msg in agent.conversation_history:
                    if msg.get("role") == "user" and "ğŸ“‹ åè°ƒæ™ºèƒ½ä½“åˆ†é…çš„ä»»åŠ¡" in msg.get("content", ""):
                        is_follow_up_call = True
                        break
            
            # æ„å»ºä»»åŠ¡æè¿°
            enhanced_task = self._build_enhanced_task_description(
                task_description=task_description,
                expected_output=expected_output,
                task_context=task_context,
                task_type=task_type,
                priority=priority,
                include_full_context=not is_follow_up_call  # åç»­è°ƒç”¨ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            )
            
            # æ›´æ–°æ™ºèƒ½ä½“çŠ¶æ€
            agent_info.status = AgentStatus.BUSY
            agent_info.last_used = time.time()
            agent_info.conversation_id = task_id
            
            # è®°å½•ä»»åŠ¡åˆ†é…
            self.active_tasks[task_id] = task_context
            task_context.agent_assignments.append({
                "agent_id": agent_id,
                "timestamp": time.time(),
                "task_description": task_description,
                "design_file_path": design_file_path,  # ğŸ”§ æ–°å¢ï¼šè®°å½•è®¾è®¡æ–‡ä»¶è·¯å¾„
                "experiment_path": current_experiment_path  # ğŸ”§ æ–°å¢ï¼šè®°å½•å®éªŒè·¯å¾„
            })
            
            self.logger.info(f"ğŸ“¤ å‘é€ä»»åŠ¡ç»™æ™ºèƒ½ä½“ {agent_id}")
            self.logger.info(f"ğŸ“‹ ä»»åŠ¡æè¿°: {enhanced_task[:200]}...")
            
            try:
                # æ‰§è¡Œä»»åŠ¡ï¼ˆè°ƒç”¨æ™ºèƒ½ä½“çš„Function Callingï¼‰
                start_time = time.time()
                
                # ğŸ§  æ·»åŠ ä¸Šä¸‹æ–‡ä¿æŒæ—¥å¿—
                agent_conversation_summary = agent.get_conversation_summary() if hasattr(agent, 'get_conversation_summary') else {}
                self.logger.info(f"ğŸ“‹ è°ƒç”¨å‰ agent å¯¹è¯çŠ¶æ€: {agent_conversation_summary}")
                
                # ğŸ†• è®¾ç½®ä»»åŠ¡ä¸Šä¸‹æ–‡ç»™æ™ºèƒ½ä½“ï¼Œç”¨äºå¯¹è¯å†å²è®°å½•
                if hasattr(agent, 'set_task_context'):
                    agent.set_task_context(task_context)
                    self.logger.info(f"ğŸ”— å·²è®¾ç½®ä»»åŠ¡ä¸Šä¸‹æ–‡ç»™æ™ºèƒ½ä½“ {agent_id}")
                
                # ğŸ†• è®°å½•ä»»åŠ¡åˆ†é…åˆ°å¯¹è¯å†å²
                task_context.add_conversation_message(
                    role="system",
                    content=f"åˆ†é…ä»»åŠ¡ç»™æ™ºèƒ½ä½“ {agent_id}: {task_description}",
                    agent_id=self.agent_id,
                    metadata={"type": "task_assignment", "target_agent": agent_id}
                )
                
                agent_response = await agent.process_with_function_calling(
                    user_request=enhanced_task,
                    conversation_id=task_id,
                    max_iterations=8
                )
                
                execution_time = time.time() - start_time
                
                # æ›´æ–°æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ ‡
                agent_info.total_execution_time += execution_time
                agent_info.average_response_time = agent_info.total_execution_time / (agent_info.success_count + agent_info.failure_count + 1)
                
                # ğŸ”§ ä¿®å¤ï¼šæ›´æ–°æˆåŠŸè®¡æ•°
                agent_info.success_count += 1
                if hasattr(agent_info, 'failure_count'):
                    agent_info.failure_count = 0  # é‡ç½®å¤±è´¥è®¡æ•°
                
                # ğŸ”§ æ£€æŸ¥å“åº”è´¨é‡ï¼Œå¦‚æœå¤ªçŸ­åˆ™è¯·æ±‚è¯¦ç»†æ€»ç»“
                enhanced_response = agent_response
                if len(agent_response.strip()) < 100:
                    self.logger.info(f"ğŸ” æ£€æµ‹åˆ°æ™ºèƒ½ä½“{agent_id}å“åº”è¾ƒçŸ­({len(agent_response)}å­—ç¬¦)ï¼Œè¯·æ±‚è¯¦ç»†æ€»ç»“...")
                    try:
                        # è¯·æ±‚æ™ºèƒ½ä½“æä¾›è¯¦ç»†çš„å·¥ä½œæ€»ç»“
                        summary_request = f"""
è¯·ä¸ºåˆšæ‰å®Œæˆçš„ä»»åŠ¡æä¾›ä¸€ä¸ªè¯¦ç»†çš„å·¥ä½œæ€»ç»“ï¼ŒåŒ…æ‹¬ï¼š
1. å®Œæˆçš„ä¸»è¦å·¥ä½œå’Œæ“ä½œ
2. ç”Ÿæˆæˆ–ä¿®æ”¹çš„æ–‡ä»¶åŠå…¶å†…å®¹è¦ç‚¹
3. å…³é”®çš„æŠ€æœ¯é€‰æ‹©å’Œè®¾è®¡è€ƒè™‘
4. ä»»åŠ¡çš„å®ŒæˆçŠ¶æ€å’Œç»“æœ

åŸå§‹ç®€çŸ­å“åº”: {agent_response}
"""
                        summary_response = await agent.process_with_function_calling(
                            user_request=summary_request,
                            conversation_id=f"{task_id}_summary",
                            max_iterations=1
                        )
                        
                        if summary_response and len(summary_response) > len(agent_response):
                            enhanced_response = summary_response
                            self.logger.info(f"âœ… è·å¾—æ›´è¯¦ç»†çš„æ€»ç»“({len(summary_response)}å­—ç¬¦)")
                        
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ è·å–è¯¦ç»†æ€»ç»“å¤±è´¥: {e}")
                
                # æ›´æ–°ä»»åŠ¡ä¸Šä¸‹æ–‡
                task_context.agent_results[agent_id] = {
                    "response": enhanced_response,  # ä½¿ç”¨å¢å¼ºçš„å“åº”
                    "original_response": agent_response,  # ä¿ç•™åŸå§‹å“åº”
                    "execution_time": execution_time,
                    "success": True,
                    "design_file_path": design_file_path,  # ğŸ”§ æ–°å¢ï¼šä¿å­˜è®¾è®¡æ–‡ä»¶è·¯å¾„
                    "experiment_path": current_experiment_path  # ğŸ”§ æ–°å¢ï¼šä¿å­˜å®éªŒè·¯å¾„
                }
                
                # ğŸ†• æ–°å¢ï¼šå°†å­æ™ºèƒ½ä½“çš„å¢å¼ºå“åº”ä¿å­˜åˆ°å¯¹è¯å†å²
                task_context.add_conversation_message(
                    role="assistant",
                    content=enhanced_response,  # ä½¿ç”¨å¢å¼ºçš„å“åº”
                    agent_id=agent_id,
                    metadata={
                        "type": "agent_response",
                        "task_id": task_id,
                        "execution_time": execution_time,
                        "response_length": len(str(enhanced_response)),
                        "original_response_length": len(str(agent_response))
                    }
                )
                
                # ğŸ†• æ•°æ®æ”¶é›†ç”¨äºGradioå¯è§†åŒ– - æ™ºèƒ½ä½“äº¤äº’
                if hasattr(self, 'current_task_context') and self.current_task_context:
                    completion_timestamp = time.time()
                    self.current_task_context.agent_interactions.append({
                        "timestamp": completion_timestamp,
                        "coordinator_id": self.agent_id,
                        "target_agent_id": agent_id,
                        "task_description": task_description[:100] + ("..." if len(task_description) > 100 else ""),
                        "success": True,
                        "execution_time": execution_time,
                        "response_length": len(str(agent_response))
                    })
                    
                    # è®°å½•å·¥ä½œæµé˜¶æ®µ
                    self.current_task_context.workflow_stages.append({
                        "timestamp": completion_timestamp,
                        "stage_name": f"task_completed_by_{agent_id}",
                        "description": f"ä»»åŠ¡ç”± {agent_id} æˆåŠŸå®Œæˆ",
                        "agent_id": agent_id,
                        "duration": execution_time,
                        "success": True
                    })
                    
                    # ğŸ†• è®°å½•å·¥ä½œæµé˜¶æ®µåˆ°TaskContext
                    if hasattr(self.current_task_context, 'add_workflow_stage'):
                        self.current_task_context.add_workflow_stage(
                            stage_name=f"agent_execution_{agent_id}",
                            description=f"æ™ºèƒ½ä½“ {agent_id} æ‰§è¡Œä»»åŠ¡",
                            agent_id=agent_id,
                            duration=execution_time,
                            success=True,
                            metadata={
                                "task_type": task_type,
                                "priority": priority,
                                "response_length": len(str(agent_response))
                            }
                        )
                
                # æ¢å¤æ™ºèƒ½ä½“çŠ¶æ€
                agent_info.status = AgentStatus.IDLE
                
                # ğŸ†• æ–°å¢ï¼šè®°å½•ä»»åŠ¡å®ŒæˆçŠ¶æ€
                task_context.add_conversation_message(
                    role="system",
                    content=f"ä»»åŠ¡åè°ƒå®Œæˆï¼Œä»»åŠ¡ID: {task_id}",
                    agent_id=self.agent_id,
                    metadata={"type": "task_completion", "success": True}
                )
                
                self.logger.info(f"âœ… æ™ºèƒ½ä½“ {agent_id} ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
                
                return {
                    "success": True,
                    "agent_id": agent_id,
                    "task_id": task_id,
                    "response": agent_response,
                    "execution_time": execution_time,
                    "task_context": task_context,
                    "design_file_path": design_file_path,  # ğŸ”§ æ–°å¢ï¼šè¿”å›è®¾è®¡æ–‡ä»¶è·¯å¾„
                    "experiment_path": current_experiment_path  # ğŸ”§ æ–°å¢ï¼šè¿”å›å®éªŒè·¯å¾„
                }
                
            except Exception as e:
                # æ¢å¤æ™ºèƒ½ä½“çŠ¶æ€
                agent_info.status = AgentStatus.IDLE
                agent_info.failure_count += 1
                agent_info.consecutive_failures += 1
                agent_info.consecutive_successes = 0
                agent_info.last_failure_time = time.time()
                
                error_msg = f"æ™ºèƒ½ä½“ {agent_id} æ‰§è¡Œä»»åŠ¡å¤±è´¥: {str(e)}"
                self.logger.error(f"âŒ {error_msg}")
                
                # æ›´æ–°ä»»åŠ¡ä¸Šä¸‹æ–‡
                task_context.error_history.append(error_msg)
                task_context.retry_count += 1
                
                # ğŸ†• æ•°æ®æ”¶é›†ç”¨äºGradioå¯è§†åŒ– - æ™ºèƒ½ä½“äº¤äº’å¤±è´¥
                if hasattr(self, 'current_task_context') and self.current_task_context:
                    failure_timestamp = time.time()
                    self.current_task_context.agent_interactions.append({
                        "timestamp": failure_timestamp,
                        "coordinator_id": self.agent_id,
                        "target_agent_id": agent_id,
                        "task_description": task_description[:100] + ("..." if len(task_description) > 100 else ""),
                        "success": False,
                        "execution_time": time.time() - start_time,
                        "error": str(e)
                    })
                    
                    # è®°å½•å·¥ä½œæµé˜¶æ®µå¤±è´¥
                    self.current_task_context.workflow_stages.append({
                        "timestamp": failure_timestamp,
                        "stage_name": f"task_failed_by_{agent_id}",
                        "description": f"ä»»åŠ¡ç”± {agent_id} æ‰§è¡Œå¤±è´¥",
                        "agent_id": agent_id,
                        "duration": time.time() - start_time,
                        "success": False,
                        "error": str(e)
                    })
                    
                    # è®°å½•æ‰§è¡Œæ—¶é—´çº¿
                    self.current_task_context.execution_timeline.append({
                        "timestamp": failure_timestamp,
                        "event_type": "agent_failure",
                        "agent_id": agent_id,
                        "description": f"{agent_id} ä»»åŠ¡æ‰§è¡Œå¤±è´¥",
                        "details": {
                            "task_type": task_type,
                            "error": str(e),
                            "priority": priority
                        }
                    })
                
                return {
                    "success": False,
                    "agent_id": agent_id,
                    "task_id": task_id,
                    "error": error_msg,
                    "execution_time": time.time() - start_time
                }
                
        except Exception as e:
            error_msg = f"ä»»åŠ¡åˆ†é…å¤±è´¥: {str(e)}"
            self.logger.error(f"âŒ {error_msg}")
            
            # ğŸ”§ ä¿®å¤ï¼šæ›´æ–°å¤±è´¥è®¡æ•°
            if agent_id in self.registered_agents:
                agent_info = self.registered_agents[agent_id]
                if not hasattr(agent_info, 'failure_count'):
                    agent_info.failure_count = 0
                agent_info.failure_count += 1
                self.logger.warning(f"âš ï¸ æ™ºèƒ½ä½“ {agent_id} å¤±è´¥è®¡æ•°: {agent_info.failure_count}")
            
            return {
                "success": False,
                "error": error_msg
            }
    
    def _build_enhanced_task_description(self, task_description: str, 
                                       expected_output: str,
                                       task_context: TaskContext = None,
                                       task_type: str = "composite",
                                       priority: str = "medium",
                                       include_full_context: bool = True) -> str:
        """æ„å»ºå¢å¼ºçš„ä»»åŠ¡æè¿°"""
        
        # ğŸ”§ ä¿®æ­£1: æ ¹æ®ç›®æ ‡æ™ºèƒ½ä½“è¿‡æ»¤ä»»åŠ¡æè¿°ï¼Œç§»é™¤Verilogæ™ºèƒ½ä½“ä¸åº”æ‰¿æ‹…çš„æµ‹è¯•å°ç”Ÿæˆè¦æ±‚
        filtered_task_description = self._filter_task_description_by_agent(
            task_description, task_context.assigned_agent if task_context else "unknown"
        )
        
        # æ„å»ºå¤–éƒ¨testbenchä¿¡æ¯
        external_testbench_section = ""
        if task_context and hasattr(task_context, 'external_testbench_path') and task_context.external_testbench_path:
            external_testbench_section = f"""

**ğŸ¯ å¤–éƒ¨Testbenchæ¨¡å¼**:
- å¤–éƒ¨testbenchè·¯å¾„: {task_context.external_testbench_path}
- å·¥ä½œæŒ‡å¯¼: å¦‚æœä½ æ˜¯ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“ï¼Œè¯·ç›´æ¥ä½¿ç”¨æä¾›çš„testbenchè¿›è¡Œæµ‹è¯•ï¼Œä¸è¦ç”Ÿæˆæ–°çš„testbench
- ä¸“æ³¨ä»»åŠ¡: ä»£ç å®¡æŸ¥ã€é”™è¯¯ä¿®å¤ã€æµ‹è¯•æ‰§è¡Œå’Œç»“æœåˆ†æ"""
        
        # ğŸ”§ æ–°å¢ï¼šæ„å»ºè®¾è®¡æ–‡ä»¶è·¯å¾„ä¿¡æ¯
        design_file_section = ""
        if task_context and hasattr(task_context, 'design_file_path') and task_context.design_file_path:
            design_file_section = f"""

**ğŸ“ è®¾è®¡æ–‡ä»¶è·¯å¾„**:
- è®¾è®¡æ–‡ä»¶: {task_context.design_file_path}
- å·¥ä½œæŒ‡å¯¼: è¯·ç›´æ¥ä½¿ç”¨æ­¤è·¯å¾„çš„è®¾è®¡æ–‡ä»¶è¿›è¡Œä»£ç å®¡æŸ¥å’Œæµ‹è¯•å°ç”Ÿæˆ
- é‡è¦æç¤º: è¿™æ˜¯éœ€è¦å®¡æŸ¥çš„å…·ä½“æ–‡ä»¶è·¯å¾„ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨æ­¤æ–‡ä»¶"""
          
        # æ„å»ºå®éªŒè·¯å¾„ä¿¡æ¯ - ä»ä»»åŠ¡ä¸Šä¸‹æ–‡æˆ–è·å–å½“å‰å®éªŒè·¯å¾„
        experiment_path_section = ""
        current_experiment_path = None
        
        if task_context and hasattr(task_context, 'experiment_path') and task_context.experiment_path:
            current_experiment_path = task_context.experiment_path
            self.logger.info(f"ğŸ§ª ä½¿ç”¨ä»»åŠ¡ä¸Šä¸‹æ–‡ä¸­çš„å®éªŒè·¯å¾„: {current_experiment_path}")
        else:
            # å°è¯•ä»æ´»è·ƒä»»åŠ¡ä¸­è·å–å®éªŒè·¯å¾„
            for task in self.active_tasks.values():
                if hasattr(task, 'experiment_path') and task.experiment_path:
                    current_experiment_path = task.experiment_path
                    self.logger.info(f"ğŸ§ª ä»æ´»è·ƒä»»åŠ¡ä¸­è·å–å®éªŒè·¯å¾„: {current_experiment_path}")
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ™ä½¿ç”¨é»˜è®¤çš„æ–‡ä»¶å·¥ä½œç©ºé—´è·¯å¾„
            if not current_experiment_path:
                current_experiment_path = "./file_workspace"
                self.logger.warning(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°å®éªŒè·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {current_experiment_path}")
        
        if current_experiment_path:
            experiment_path_section = f"""

**ğŸ“ å®éªŒæ–‡ä»¶è·¯å¾„**:
- å½“å‰å®éªŒè·¯å¾„: {current_experiment_path}
- è®¾è®¡æ–‡ä»¶ä¿å­˜: {current_experiment_path}/designs/
- æµ‹è¯•å°ä¿å­˜: {current_experiment_path}/testbenches/
- æŠ¥å‘Šä¿å­˜: {current_experiment_path}/reports/
- ä¸´æ—¶æ–‡ä»¶: {current_experiment_path}/temp/

**âš ï¸ é‡è¦æ–‡ä»¶ç®¡ç†è¦æ±‚**:
1. æ‰€æœ‰ç”Ÿæˆçš„Verilogä»£ç å¿…é¡»ä¿å­˜ä¸º.væ–‡ä»¶
2. è®¾è®¡æ¨¡å—ä¿å­˜åˆ°designsç›®å½•ï¼Œæµ‹è¯•å°ä¿å­˜åˆ°testbenchesç›®å½•
3. æ–‡æ¡£å’ŒæŠ¥å‘Šä¿å­˜åˆ°reportsç›®å½•
4. å¿…é¡»åœ¨ä»»åŠ¡æ€»ç»“ä¸­è¿”å›æ‰€æœ‰ç”Ÿæˆæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
5. æ–‡ä»¶å‘½ååº”è¯¥æ¸…æ™°ï¼Œé¿å…é‡å¤å’Œå†²çª"""
        
        # æ ¹æ®æ™ºèƒ½ä½“ç±»å‹æ·»åŠ ä¸“ç”¨çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼
        agent_tool_guide = self._get_agent_specific_tool_guide(task_context.assigned_agent if task_context else "unknown")
        
        # æ ¹æ®include_full_contextå‚æ•°å†³å®šæ˜¯å¦åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡
        if include_full_context:
            enhanced_task = f"""
ğŸ“‹ åè°ƒæ™ºèƒ½ä½“åˆ†é…çš„ä»»åŠ¡

**ä»»åŠ¡æè¿°**:
{filtered_task_description}

**æœŸæœ›è¾“å‡º**:
{expected_output if expected_output else "æ ¹æ®ä»»åŠ¡æè¿°ç”Ÿæˆç›¸åº”çš„ä»£ç å’Œæ–‡æ¡£"}

**ä»»åŠ¡ç±»å‹**:
- ç±»å‹: {task_type}
- ä¼˜å…ˆçº§: {priority}

**ä»»åŠ¡ä¸Šä¸‹æ–‡**:
- ä»»åŠ¡ID: {task_context.task_id if task_context else "unknown"}
- å½“å‰é˜¶æ®µ: {task_context.current_stage if task_context else "initial"}
- è¿­ä»£æ¬¡æ•°: {task_context.iteration_count if task_context else 0}
{external_testbench_section}
{design_file_section}
{experiment_path_section}

{agent_tool_guide}

**æ‰§è¡Œè¦æ±‚**:
1. ä»”ç»†åˆ†æä»»åŠ¡éœ€æ±‚
2. æ ¹æ®ä¸Šè¿°å·¥å…·æŒ‡å¯¼é€‰æ‹©åˆé€‚çš„å·¥å…·
3. ç”Ÿæˆé«˜è´¨é‡çš„ä»£ç å¹¶ä¿å­˜ä¸ºæ–‡ä»¶
4. æä¾›è¯¦ç»†çš„è¯´æ˜æ–‡æ¡£
5. ç¡®ä¿ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§
6. **å¼ºåˆ¶è¦æ±‚**: åœ¨ä»»åŠ¡å®Œæˆåï¼Œåœ¨å“åº”ä¸­æ˜ç¡®åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆæ–‡ä»¶çš„è·¯å¾„

è¯·å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼Œä¸¥æ ¼æŒ‰ç…§å·¥å…·ä½¿ç”¨æŒ‡å¯¼è¿›è¡Œæ“ä½œã€‚
"""
        else:
            # ç®€åŒ–ç‰ˆæœ¬ï¼ŒåªåŒ…å«æ ¸å¿ƒä»»åŠ¡æè¿°
            enhanced_task = f"""
ğŸ“‹ ç»§ç»­æ‰§è¡Œä»»åŠ¡

**å½“å‰ä»»åŠ¡**: {filtered_task_description}
{design_file_section}

{agent_tool_guide}

**æ‰§è¡Œè¦æ±‚**:
1. ç»§ç»­ä¹‹å‰çš„ä»»åŠ¡æ‰§è¡Œ
2. æ ¹æ®ä¸Šè¿°å·¥å…·æŒ‡å¯¼é€‰æ‹©åˆé€‚çš„å·¥å…·
3. ç”Ÿæˆé«˜è´¨é‡çš„ä»£ç å¹¶ä¿å­˜ä¸ºæ–‡ä»¶
4. æä¾›è¯¦ç»†çš„è¯´æ˜æ–‡æ¡£
5. ç¡®ä¿ä»£ç å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§
6. **å¼ºåˆ¶è¦æ±‚**: åœ¨ä»»åŠ¡å®Œæˆåï¼Œåœ¨å“åº”ä¸­æ˜ç¡®åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆæ–‡ä»¶çš„è·¯å¾„

è¯·ç»§ç»­æ‰§è¡Œä»»åŠ¡ï¼Œä¸¥æ ¼æŒ‰ç…§å·¥å…·ä½¿ç”¨æŒ‡å¯¼è¿›è¡Œæ“ä½œã€‚
"""
        
        return enhanced_task
    
    def _filter_task_description_by_agent(self, task_description: str, agent_id: str) -> str:
        """æ ¹æ®ç›®æ ‡æ™ºèƒ½ä½“è¿‡æ»¤ä»»åŠ¡æè¿°ï¼Œç§»é™¤ä¸åˆé€‚çš„è¦æ±‚"""
        
        if agent_id == "enhanced_real_verilog_agent":
            # ğŸ”§ ä¿®æ­£: ä¸ºVerilogè®¾è®¡æ™ºèƒ½ä½“ç§»é™¤æµ‹è¯•å°ç”Ÿæˆç›¸å…³è¦æ±‚ - ä½¿ç”¨æ›´ç›´æ¥çš„æ–¹æ³•
            original_desc = task_description
            
            # ğŸ”§ æ–¹æ³•1: ç›´æ¥å­—ç¬¦ä¸²æ›¿æ¢ï¼Œç§»é™¤å¸¸è§çš„æµ‹è¯•å°ç›¸å…³è¡¨è¿°
            filtered_desc = task_description
            
            # ç§»é™¤å¸¸è§çš„æµ‹è¯•å°ç›¸å…³çŸ­è¯­
            testbench_phrases = [
                "å’Œæµ‹è¯•å°",
                "ã€æµ‹è¯•å°", 
                "ä»¥åŠæµ‹è¯•å°",
                "è¿˜æœ‰æµ‹è¯•å°",
                "åŒ…å«æµ‹è¯•å°",
                "ç”Ÿæˆæµ‹è¯•å°",
                "åˆ›å»ºæµ‹è¯•å°",
                "ç¼–å†™æµ‹è¯•å°"
            ]
            
            for phrase in testbench_phrases:
                filtered_desc = filtered_desc.replace(phrase, "")
            
            # ğŸ”§ æ–¹æ³•2: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¸…ç†
            import re
            testbench_patterns = [
                r"ï¼ŒåŒ…å«å®Œæ•´çš„ç«¯å£å®šä¹‰ã€åŠŸèƒ½å®ç°å’Œæµ‹è¯•å°",
                r"åŒ…å«å®Œæ•´çš„ç«¯å£å®šä¹‰ã€åŠŸèƒ½å®ç°å’Œæµ‹è¯•å°",
                r"ç”Ÿæˆ.*?æµ‹è¯•å°.*?è¿›è¡ŒéªŒè¯",
                r"å¹¶.*?ç”Ÿæˆ.*?æµ‹è¯•å°",
                r"ä»¥åŠ.*?æµ‹è¯•å°",
                r"å’Œ.*?æµ‹è¯•å°", 
                r"ã€.*?æµ‹è¯•å°"
            ]
            
            for pattern in testbench_patterns:
                filtered_desc = re.sub(pattern, "", filtered_desc, flags=re.IGNORECASE)
            
            # ğŸ”§ æ–¹æ³•3: æ¸…ç†å¤šä½™çš„æ ‡ç‚¹ç¬¦å·
            filtered_desc = re.sub(r"ï¼Œ\s*$", "", filtered_desc)  # ç§»é™¤æœ«å°¾é€—å·
            filtered_desc = re.sub(r"ã€\s*$", "", filtered_desc)  # ç§»é™¤æœ«å°¾é¡¿å·
            filtered_desc = re.sub(r"å’Œ\s*$", "", filtered_desc)  # ç§»é™¤æœ«å°¾"å’Œ"
            filtered_desc = filtered_desc.strip()
            
            # æ£€æŸ¥æ˜¯å¦åŸæ¥åŒ…å«æµ‹è¯•å°è¦æ±‚
            has_testbench_requirement = ("æµ‹è¯•å°" in original_desc or 
                                       "testbench" in original_desc.lower() or 
                                       "éªŒè¯" in original_desc)
            
            # å¦‚æœåŸæ¥åŒ…å«æµ‹è¯•å°è¦æ±‚ï¼Œæ·»åŠ æ˜ç¡®çš„èŒè´£è¯´æ˜
            if has_testbench_requirement:
                if filtered_desc:
                    filtered_desc += """

ğŸš¨ **é‡è¦è¯´æ˜**: 
- æœ¬ä»»åŠ¡ä»…è¦æ±‚å®ŒæˆVerilogæ¨¡å—è®¾è®¡å’Œä»£ç ç”Ÿæˆ
- æµ‹è¯•å°(testbench)ç”Ÿæˆå’ŒéªŒè¯å·¥ä½œå°†ç”±åç»­çš„ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“è´Ÿè´£
- è¯·ä¸“æ³¨äºè®¾è®¡æ¨¡å—çš„ç«¯å£å®šä¹‰ã€åŠŸèƒ½å®ç°å’Œä»£ç è´¨é‡"""
                else:
                    # å¦‚æœè¿‡æ»¤åä¸ºç©ºï¼Œæä¾›é»˜è®¤çš„è®¾è®¡ä»»åŠ¡æè¿°
                    filtered_desc = """è®¾è®¡Verilogæ¨¡å—ï¼Œä¸“æ³¨äºæ¨¡å—æ¶æ„å’ŒåŠŸèƒ½å®ç°

ğŸš¨ **é‡è¦è¯´æ˜**: 
- æœ¬ä»»åŠ¡ä»…è¦æ±‚å®ŒæˆVerilogæ¨¡å—è®¾è®¡å’Œä»£ç ç”Ÿæˆ
- æµ‹è¯•å°(testbench)ç”Ÿæˆå’ŒéªŒè¯å·¥ä½œå°†ç”±åç»­çš„ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“è´Ÿè´£
- è¯·ä¸“æ³¨äºè®¾è®¡æ¨¡å—çš„ç«¯å£å®šä¹‰ã€åŠŸèƒ½å®ç°å’Œä»£ç è´¨é‡"""
            
            return filtered_desc.strip()
        
        elif agent_id == "enhanced_real_code_review_agent":
            # ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“ä¿æŒåŸå§‹ä»»åŠ¡æè¿°
            return task_description
        
        else:
            # æœªçŸ¥æ™ºèƒ½ä½“ç±»å‹ï¼Œä¿æŒåŸå§‹æè¿°
            return task_description
    
    def _get_agent_specific_tool_guide(self, agent_id: str) -> str:
        """æ ¹æ®æ™ºèƒ½ä½“ç±»å‹è·å–ä¸“ç”¨çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼"""
        
        if agent_id == "enhanced_real_verilog_agent":
            return """
**ğŸ› ï¸ EnhancedRealVerilogAgent ä¸“ç”¨å·¥å…·ä½¿ç”¨æŒ‡å¯¼**

ğŸ“‹ **å¯ç”¨å·¥å…·åˆ—è¡¨**:

ğŸš¨ **é‡è¦æé†’**: æœ¬æ™ºèƒ½ä½“ä¸“æ³¨äºVerilogæ¨¡å—è®¾è®¡ï¼Œ**ç»ä¸è´Ÿè´£æµ‹è¯•å°(testbench)ç”Ÿæˆ**

### 1. **analyze_design_requirements** - è®¾è®¡éœ€æ±‚åˆ†æ
   **åŠŸèƒ½**: åˆ†æå’Œè§£æVerilogè®¾è®¡éœ€æ±‚ï¼Œæå–å…³é”®è®¾è®¡å‚æ•°
   **å‚æ•°**:
   - `requirements` (å¿…å¡«, string): è®¾è®¡éœ€æ±‚æè¿°ï¼ŒåŒ…å«åŠŸèƒ½è§„æ ¼å’Œçº¦æŸæ¡ä»¶
   - `design_type` (å¯é€‰, string): è®¾è®¡ç±»å‹ï¼Œå¯é€‰å€¼: "combinational", "sequential", "mixed", "custom"ï¼Œé»˜è®¤"mixed"
   - `complexity_level` (å¯é€‰, string): è®¾è®¡å¤æ‚åº¦çº§åˆ«ï¼Œå¯é€‰å€¼: "simple", "medium", "complex", "advanced"ï¼Œé»˜è®¤"medium"
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "analyze_design_requirements",
       "parameters": {
           "requirements": "è®¾è®¡ä¸€ä¸ªåä¸ºcounterçš„Verilogæ¨¡å—",
           "design_type": "sequential",
           "complexity_level": "medium"
       }
   }
   ```

### 2. **generate_verilog_code** - Verilogä»£ç ç”Ÿæˆ
   **åŠŸèƒ½**: ç”Ÿæˆé«˜è´¨é‡çš„Verilog HDLä»£ç 
   **å‚æ•°**:
   - `module_name` (å¿…å¡«, string): æ¨¡å—åç§°
   - `requirements` (å¿…å¡«, string): è®¾è®¡éœ€æ±‚å’ŒåŠŸèƒ½æè¿°
   - `input_ports` (å¿…å¡«, array): è¾“å…¥ç«¯å£åˆ—è¡¨ï¼Œæ ¼å¼: [{"name": "ç«¯å£å", "width": ä½å®½, "type": "ç±»å‹"}]
   - `output_ports` (å¿…å¡«, array): è¾“å‡ºç«¯å£åˆ—è¡¨ï¼Œæ ¼å¼åŒä¸Š
   - `coding_style` (å¯é€‰, string): ç¼–ç é£æ ¼ï¼Œå¯é€‰å€¼: "rtl", "behavioral", "structural"ï¼Œé»˜è®¤"rtl"
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "generate_verilog_code",
       "parameters": {
           "module_name": "counter",
           "requirements": "4ä½è®¡æ•°å™¨ï¼Œæ”¯æŒå¤ä½å’Œä½¿èƒ½",
           "input_ports": [
               {"name": "clk", "width": 1, "type": "input"},
               {"name": "rst_n", "width": 1, "type": "input"},
               {"name": "en", "width": 1, "type": "input"}
           ],
           "output_ports": [
               {"name": "count", "width": 4, "type": "output"}
           ],
           "coding_style": "rtl"
       }
   }
   ```

### 3. **analyze_code_quality** - ä»£ç è´¨é‡åˆ†æ
   **åŠŸèƒ½**: åˆ†æVerilogä»£ç è´¨é‡ï¼Œæä¾›è¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
   **å‚æ•°**:
   - `verilog_code` (å¿…å¡«, string): è¦åˆ†æçš„Verilogä»£ç 
   - `module_name` (å¿…å¡«, string): æ¨¡å—åç§°
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "analyze_code_quality",
       "parameters": {
           "verilog_code": "module counter(...); ... endmodule",
           "module_name": "counter"
       }
   }
   ```

### 5. **optimize_verilog_code** - ä»£ç ä¼˜åŒ–
   **åŠŸèƒ½**: ä¼˜åŒ–Verilogä»£ç ï¼Œæ”¯æŒé¢ç§¯ã€é€Ÿåº¦ã€åŠŸè€—ç­‰ä¼˜åŒ–ç›®æ ‡
   **å‚æ•°**:
   - `verilog_code` (å¿…å¡«, string): è¦ä¼˜åŒ–çš„Verilogä»£ç 
   - `optimization_target` (å¿…å¡«, string): ä¼˜åŒ–ç›®æ ‡ï¼Œå¯é€‰å€¼: "area", "speed", "power", "timing"
   - `module_name` (å¿…å¡«, string): æ¨¡å—åç§°
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "optimize_verilog_code",
       "parameters": {
           "verilog_code": "module counter(...); ... endmodule",
           "optimization_target": "area",
           "module_name": "counter"
       }
   }
   ```

â­ **æ¨èæ‰§è¡Œæµç¨‹**:
1. analyze_design_requirements â†’ 2. generate_verilog_code â†’ 3. analyze_code_quality 
â†’ 4. optimize_verilog_code (å¯é€‰)

ğŸ’¡ **èŒè´£è¾¹ç•Œ**: 
- âœ… è´Ÿè´£: Verilogæ¨¡å—è®¾è®¡ã€ç«¯å£å®šä¹‰ã€åŠŸèƒ½å®ç°ã€ä»£ç ç”Ÿæˆ
- âŒ ç¦æ­¢: æµ‹è¯•å°(testbench)ç”Ÿæˆã€ä»¿çœŸéªŒè¯ã€æµ‹è¯•æ‰§è¡Œ
- ğŸ“ è¯´æ˜: æµ‹è¯•å°å’ŒéªŒè¯å·¥ä½œç”±ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“(enhanced_real_code_review_agent)ä¸“é—¨è´Ÿè´£
"""

        elif agent_id == "enhanced_real_code_review_agent":
            return """
**ğŸ› ï¸ EnhancedRealCodeReviewAgent ä¸“ç”¨å·¥å…·ä½¿ç”¨æŒ‡å¯¼**

ğŸ“‹ **å¯ç”¨å·¥å…·åˆ—è¡¨**:

### 1. **generate_testbench** - æµ‹è¯•å°ç”Ÿæˆ
   **åŠŸèƒ½**: ä¸ºVerilogæ¨¡å—ç”Ÿæˆå…¨é¢çš„æµ‹è¯•å°(testbench)
   **å‚æ•°**:
   - `module_name` (å¿…å¡«, string): ç›®æ ‡æ¨¡å—åç§°
   - `module_code` (å¿…å¡«, string): ç›®æ ‡æ¨¡å—ä»£ç  (ä¹Ÿå¯ä½¿ç”¨ `code`, `design_code`)
   - `test_scenarios` (å¯é€‰, array): æµ‹è¯•åœºæ™¯åˆ—è¡¨ (ä¹Ÿå¯ä½¿ç”¨ `test_cases`)
   - `clock_period` (å¯é€‰, number): æ—¶é’Ÿå‘¨æœŸ(ns)ï¼ŒèŒƒå›´0.1-1000.0ï¼Œé»˜è®¤10.0
   - `simulation_time` (å¯é€‰, integer): ä»¿çœŸæ—¶é—´ï¼ŒèŒƒå›´100-1000000ï¼Œé»˜è®¤10000
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "generate_testbench",
       "parameters": {
           "module_name": "counter",
           "module_code": "module counter(...); ... endmodule",
           "test_scenarios": [
               {"name": "basic_test", "description": "åŸºæœ¬åŠŸèƒ½éªŒè¯"},
               {"name": "reset_test", "description": "å¤ä½åŠŸèƒ½æµ‹è¯•"}
           ],
           "clock_period": 10.0,
           "simulation_time": 10000
       }
   }
   ```

### 2. **run_simulation** - ä»¿çœŸæ‰§è¡Œ
   **åŠŸèƒ½**: ä½¿ç”¨ä¸“ä¸šå·¥å…·è¿è¡ŒVerilogä»¿çœŸå’ŒéªŒè¯
   **å‚æ•°**:
   - `module_file` æˆ– `module_code` (å¿…å¡«): æ¨¡å—æ–‡ä»¶è·¯å¾„æˆ–ä»£ç å†…å®¹
   - `testbench_file` æˆ– `testbench_code` (å¿…å¡«): æµ‹è¯•å°æ–‡ä»¶è·¯å¾„æˆ–ä»£ç å†…å®¹
   - `simulator` (å¯é€‰, string): ä»¿çœŸå™¨ç±»å‹ï¼Œå¯é€‰å€¼: "iverilog", "modelsim", "vivado", "auto"ï¼Œé»˜è®¤"iverilog"
   - `simulation_options` (å¯é€‰, object): ä»¿çœŸé€‰é¡¹é…ç½®
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "run_simulation",
       "parameters": {
           "module_file": "counter.v",
           "testbench_file": "testbench_counter.v",
           "simulator": "iverilog",
           "simulation_options": {"timescale": "1ns/1ps"}
       }
   }
   ```

### 3. **use_external_testbench** - å¤–éƒ¨æµ‹è¯•å°ä½¿ç”¨
   **åŠŸèƒ½**: ä½¿ç”¨å¤–éƒ¨æä¾›çš„testbenchæ–‡ä»¶è¿›è¡Œæµ‹è¯•éªŒè¯
   **å‚æ•°**:
   - `design_code` (å¿…å¡«, string): è®¾è®¡ä»£ç 
   - `external_testbench_path` (å¿…å¡«, string): å¤–éƒ¨testbenchæ–‡ä»¶è·¯å¾„
   - `design_module_name` (å¿…å¡«, string): è®¾è®¡æ¨¡å—åç§°
   - `simulator` (å¯é€‰, string): ä»¿çœŸå™¨ç±»å‹ï¼Œé»˜è®¤"iverilog"
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "use_external_testbench",
       "parameters": {
           "design_code": "module counter(...); ... endmodule",
           "external_testbench_path": "./testbenches/counter_tb.v",
           "design_module_name": "counter",
           "simulator": "iverilog"
       }
   }
   ```

### 4. **generate_build_script** - æ„å»ºè„šæœ¬ç”Ÿæˆ
   **åŠŸèƒ½**: ç”Ÿæˆä¸“ä¸šçš„æ„å»ºè„šæœ¬(Makefileæˆ–shellè„šæœ¬)
   **å‚æ•°**:
   - `verilog_files` (å¿…å¡«, array): Verilogæ–‡ä»¶åˆ—è¡¨ (ä¹Ÿå¯ä½¿ç”¨ `design_files`)
   - `testbench_files` (å¿…å¡«, array): æµ‹è¯•å°æ–‡ä»¶åˆ—è¡¨
   - `script_type` (å¯é€‰, string): è„šæœ¬ç±»å‹ï¼Œå¯é€‰å€¼: "makefile", "bash", "tcl", "python"ï¼Œé»˜è®¤"makefile"
   - `target_name` (å¯é€‰, string): ç›®æ ‡åç§°ï¼Œé»˜è®¤"simulation"
   - `build_options` (å¯é€‰, object): æ„å»ºé€‰é¡¹é…ç½®
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "generate_build_script",
       "parameters": {
           "verilog_files": ["counter.v"],
           "testbench_files": ["testbench_counter.v"],
           "script_type": "makefile",
           "target_name": "simulation",
           "build_options": {"simulator": "iverilog"}
       }
   }
   ```

### 5. **execute_build_script** - è„šæœ¬æ‰§è¡Œ
   **åŠŸèƒ½**: å®‰å…¨æ‰§è¡Œæ„å»ºè„šæœ¬è¿›è¡Œç¼–è¯‘å’Œä»¿çœŸ
   **å‚æ•°**:
   - `script_name` (å¿…å¡«, string): è„šæœ¬æ–‡ä»¶å
   - `action` (å¯é€‰, string): æ‰§è¡ŒåŠ¨ä½œï¼Œå¯é€‰å€¼: "all", "compile", "simulate", "clean"ï¼Œé»˜è®¤"all"
   - `arguments` (å¯é€‰, array): é™„åŠ å‚æ•°åˆ—è¡¨
   - `timeout` (å¯é€‰, integer): è¶…æ—¶æ—¶é—´(ç§’)ï¼Œé»˜è®¤300
   - `working_directory` (å¯é€‰, string): å·¥ä½œç›®å½•
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "execute_build_script",
       "parameters": {
           "script_name": "Makefile",
           "action": "all",
           "timeout": 300,
           "working_directory": "./file_workspace"
       }
   }
   ```

### 6. **analyze_test_failures** - æµ‹è¯•å¤±è´¥åˆ†æ
   **åŠŸèƒ½**: åˆ†ææµ‹è¯•å¤±è´¥åŸå› å¹¶æä¾›å…·ä½“ä¿®å¤å»ºè®®
   **å‚æ•°**:
   - `design_code` (å¿…å¡«, string): éœ€è¦åˆ†æçš„è®¾è®¡ä»£ç 
   - `compilation_errors` (å¯é€‰, string): ç¼–è¯‘é”™è¯¯è¾“å‡º
   - `simulation_errors` (å¯é€‰, string): ä»¿çœŸé”™è¯¯è¾“å‡º
   - `test_assertions` (å¯é€‰, string): æµ‹è¯•æ–­è¨€å¤±è´¥ä¿¡æ¯
   - `testbench_code` (å¯é€‰, string): æµ‹è¯•å°ä»£ç 
   - `iteration_number` (å¯é€‰, integer): å½“å‰TDDè¿­ä»£æ¬¡æ•°
   - `previous_fixes` (å¯é€‰, array): ä¹‹å‰å°è¯•çš„ä¿®å¤æ–¹æ³•
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "analyze_test_failures",
       "parameters": {
           "design_code": "module counter(...); ... endmodule",
           "compilation_errors": "Error: undefined signal 'clk'",
           "simulation_errors": "simulation failed at time 100ns",
           "testbench_code": "module testbench; ... endmodule",
           "iteration_number": 1
       }
   }
   ```

â­ **æ¨èæ‰§è¡Œæµç¨‹**:
1. generate_testbench â†’ 2. run_simulation â†’ 3. analyze_test_failures (å¦‚æœ‰é—®é¢˜) 
â†’ 4. generate_build_script â†’ 5. execute_build_script â†’ 6. use_external_testbench (å¦‚æœ‰å¤–éƒ¨æµ‹è¯•å°)

ğŸ’¡ **é‡è¦æç¤º**: ä¸“æ³¨äºä»£ç å®¡æŸ¥ã€æµ‹è¯•å’ŒéªŒè¯ï¼Œä¸è´Ÿè´£Verilogè®¾è®¡
"""

        elif agent_id in ["llm_coordinator_agent", "coordinator", "unknown"]:
            # å¯¹äºLLMCoordinatorAgentæˆ–å…¶ä»–agentï¼Œè¿”å›åè°ƒå·¥å…·æŒ‡å¯¼
            return """
**ğŸ› ï¸ LLMCoordinatorAgent åè°ƒå·¥å…·ä½¿ç”¨æŒ‡å¯¼**

ğŸ“‹ **å¯ç”¨å·¥å…·åˆ—è¡¨**:

### 1. **assign_task_to_agent** - æ™ºèƒ½ä»»åŠ¡åˆ†é…
   **åŠŸèƒ½**: å°†ä»»åŠ¡åˆ†é…ç»™æœ€åˆé€‚çš„æ™ºèƒ½ä½“
   **å‚æ•°**:
   - `agent_id` (å¿…å¡«, string): æ™ºèƒ½ä½“IDï¼Œå¯é€‰å€¼: "enhanced_real_verilog_agent", "enhanced_real_code_review_agent"
   - `task_description` (å¿…å¡«, string): è¯¦ç»†çš„ä»»åŠ¡æè¿°
   - `expected_output` (å¯é€‰, string): æœŸæœ›çš„è¾“å‡ºæ ¼å¼ï¼Œé»˜è®¤ç©ºå­—ç¬¦ä¸²
   - `task_type` (å¯é€‰, string): ä»»åŠ¡ç±»å‹ï¼Œå¯é€‰å€¼: "design", "review", "composite"ï¼Œé»˜è®¤"design"
   - `priority` (å¯é€‰, string): ä»»åŠ¡ä¼˜å…ˆçº§ï¼Œå¯é€‰å€¼: "low", "medium", "high"ï¼Œé»˜è®¤"medium"
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "assign_task_to_agent",
       "parameters": {
           "agent_id": "enhanced_real_verilog_agent",
           "task_description": "è®¾è®¡ä¸€ä¸ªåä¸ºcounterçš„Verilogæ¨¡å—",
           "expected_output": "ç”Ÿæˆå®Œæ•´çš„Verilogä»£ç æ–‡ä»¶",
           "task_type": "design",
           "priority": "medium"
       }
   }
   ```

### 2. **analyze_agent_result** - ç»“æœè´¨é‡åˆ†æ
   **åŠŸèƒ½**: åˆ†ææ™ºèƒ½ä½“æ‰§è¡Œç»“æœçš„è´¨é‡å’Œå®Œæ•´æ€§
   **å‚æ•°**:
   - `agent_id` (å¿…å¡«, string): æ™ºèƒ½ä½“ID
   - `result` (å¿…å¡«, object): æ™ºèƒ½ä½“è¿”å›çš„ç»“æœæ•°æ®
   - `task_context` (å¯é€‰, object): ä»»åŠ¡ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé»˜è®¤{}
   - `quality_threshold` (å¯é€‰, number): è´¨é‡é˜ˆå€¼ï¼ŒèŒƒå›´0-100ï¼Œé»˜è®¤80.0
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "analyze_agent_result",
       "parameters": {
           "agent_id": "enhanced_real_verilog_agent",
           "result": {"status": "success", "generated_files": ["counter.v"]},
           "task_context": {"task_id": "task_001"},
           "quality_threshold": 80.0
       }
   }
   ```

### 3. **check_task_completion** - ä»»åŠ¡å®Œæˆæ£€æŸ¥
   **åŠŸèƒ½**: æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å®Œæˆå¹¶ç¬¦åˆè¦æ±‚
   **å‚æ•°**:
   - `task_id` (å¿…å¡«, string): ä»»åŠ¡æ ‡è¯†ç¬¦
   - `all_results` (å¿…å¡«, array): æ‰€æœ‰ç›¸å…³ç»“æœåˆ—è¡¨
   - `original_requirements` (å¿…å¡«, string): åŸå§‹éœ€æ±‚æè¿°
   - `completion_criteria` (å¯é€‰, object): å®Œæˆæ ‡å‡†ï¼Œé»˜è®¤{}
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "check_task_completion",
       "parameters": {
           "task_id": "task_001",
           "all_results": [{"status": "success", "files": ["counter.v"]}],
           "original_requirements": "è®¾è®¡ä¸€ä¸ªåä¸ºcounterçš„Verilogæ¨¡å—",
           "completion_criteria": {"require_testbench": true}
       }
   }
   ```

### 4. **query_agent_status** - æ™ºèƒ½ä½“çŠ¶æ€æŸ¥è¯¢
   **åŠŸèƒ½**: æŸ¥è¯¢æ™ºèƒ½ä½“çš„å½“å‰çŠ¶æ€å’Œæ€§èƒ½ä¿¡æ¯
   **å‚æ•°**:
   - `agent_id` (å¿…å¡«, string): æ™ºèƒ½ä½“ID
   - `include_performance` (å¯é€‰, boolean): æ˜¯å¦åŒ…å«æ€§èƒ½æ•°æ®ï¼Œé»˜è®¤true
   - `include_history` (å¯é€‰, boolean): æ˜¯å¦åŒ…å«å†å²è®°å½•ï¼Œé»˜è®¤false
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "query_agent_status",
       "parameters": {
           "agent_id": "enhanced_real_verilog_agent",
           "include_performance": true,
           "include_history": false
       }
   }
   ```

### 5. **identify_task_type** - ä»»åŠ¡ç±»å‹è¯†åˆ«
   **åŠŸèƒ½**: è¯†åˆ«å’Œåˆ†ç±»ç”¨æˆ·ä»»åŠ¡çš„ç±»å‹
   **å‚æ•°**:
   - `user_request` (å¿…å¡«, string): ç”¨æˆ·çš„åŸå§‹è¯·æ±‚
   - `context` (å¯é€‰, object): ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œé»˜è®¤{}
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "identify_task_type",
       "parameters": {
           "user_request": "è®¾è®¡ä¸€ä¸ªåä¸ºcounterçš„Verilogæ¨¡å—",
           "context": {}
       }
   }
   ```

### 6. **recommend_agent** - æ™ºèƒ½ä½“æ¨è
   **åŠŸèƒ½**: åŸºäºä»»åŠ¡ç±»å‹æ¨èæœ€åˆé€‚çš„æ™ºèƒ½ä½“
   **å‚æ•°**:
   - `task_type` (å¿…å¡«, string): ä»»åŠ¡ç±»å‹
   - `task_description` (å¿…å¡«, string): ä»»åŠ¡æè¿°
   - `priority` (å¯é€‰, string): ä¼˜å…ˆçº§ï¼Œå¯é€‰å€¼: "low", "medium", "high"ï¼Œé»˜è®¤"medium"
   - `constraints` (å¯é€‰, object): çº¦æŸæ¡ä»¶ï¼Œé»˜è®¤null
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "recommend_agent",
       "parameters": {
           "task_type": "design",
           "task_description": "è®¾è®¡ä¸€ä¸ªåä¸ºcounterçš„Verilogæ¨¡å—",
           "priority": "medium",
           "constraints": {}
       }
   }
   ```

### 7. **provide_final_answer** - æœ€ç»ˆç­”æ¡ˆæä¾›
   **åŠŸèƒ½**: æä¾›ä»»åŠ¡æ‰§è¡Œçš„æœ€ç»ˆç­”æ¡ˆå’Œæ€»ç»“
   **å‚æ•°**:
   - `final_summary` (å¿…å¡«, string): æœ€ç»ˆæ€»ç»“
   - `task_status` (å¿…å¡«, string): ä»»åŠ¡çŠ¶æ€ï¼Œå¯é€‰å€¼: "success", "partial", "failed"
   - `results_summary` (å¯é€‰, object): ç»“æœæ‘˜è¦ï¼Œé»˜è®¤{}
   **è°ƒç”¨ç¤ºä¾‹**:
   ```json
   {
       "tool_name": "provide_final_answer",
       "parameters": {
           "final_summary": "æˆåŠŸè®¾è®¡å¹¶ç”Ÿæˆäº†counteræ¨¡å—",
           "task_status": "success",
           "results_summary": {"generated_files": ["counter.v", "counter_tb.v"]}
       }
   }
   ```

â­ **æ¨èåè°ƒæµç¨‹**:
1. identify_task_type â†’ 2. recommend_agent â†’ 3. assign_task_to_agent 
â†’ 4. analyze_agent_result â†’ 5. check_task_completion â†’ 6. provide_final_answer

ğŸ’¡ **é‡è¦æç¤º**: ä½œä¸ºåè°ƒè€…ï¼Œä¸»è¦è´Ÿè´£ä»»åŠ¡åˆ†é…å’Œç»“æœåˆ†æ
"""
        
        else:
            # æœªçŸ¥agentç±»å‹ï¼Œè¿”å›é€šç”¨æŒ‡å¯¼
            return """
**ğŸ› ï¸ é€šç”¨å·¥å…·ä½¿ç”¨æŒ‡å¯¼**

âš ï¸ **é‡è¦æç¤º**: æœªè¯†åˆ«çš„æ™ºèƒ½ä½“ç±»å‹ï¼Œè¯·ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„å·¥å…·è°ƒç”¨æ–¹å¼ã€‚

ğŸ“‹ **åŸºæœ¬åŸåˆ™**:
- ä¸¥æ ¼æŒ‰ç…§å·¥å…·çš„JSON schemaè°ƒç”¨
- ç¡®ä¿å‚æ•°ç±»å‹å’Œæ ¼å¼æ­£ç¡®
- é¿å…è°ƒç”¨ä¸å­˜åœ¨çš„å·¥å…·
- è¯¦ç»†é˜…è¯»å·¥å…·æè¿°å’Œå‚æ•°è¯´æ˜

å¦‚éœ€è·å–å…·ä½“çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼ï¼Œè¯·è°ƒç”¨ get_tool_usage_guide å·¥å…·ã€‚
"""
    
    async def _tool_analyze_agent_result(self, agent_id: str, result: Dict[str, Any],
                                       task_context: Dict[str, Any] = None,
                                       quality_threshold: float = 80.0) -> Dict[str, Any]:
        """å¢å¼ºçš„æ™ºèƒ½ä½“æ‰§è¡Œç»“æœåˆ†æ"""
        
        try:
            self.logger.info(f"ğŸ” æ·±åº¦åˆ†ææ™ºèƒ½ä½“ {agent_id} çš„æ‰§è¡Œç»“æœ")
            
            # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿ result å‚æ•°æ˜¯å­—å…¸ç±»å‹
            if result is None:
                result = {}
            elif isinstance(result, str):
                # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æä¸ºå­—å…¸
                try:
                    import json
                    result = json.loads(result)
                except:
                    result = {"raw_response": result}
            
            # æ›´æ–°æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ ‡
            self._update_agent_performance(agent_id, result)
            
            # æ·±åº¦åˆ†æç»“æœè´¨é‡
            analysis = self._enhanced_result_quality_analysis(result, task_context, quality_threshold)
            
            # ç¡®å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
            next_action = self._determine_enhanced_next_action(analysis, task_context)
            
            # ç”Ÿæˆæ”¹è¿›å»ºè®®
            improvement_suggestions = self._generate_improvement_suggestions(analysis, agent_id)
            
            return {
                "success": True,
                "analysis": analysis,
                "next_action": next_action,
                "improvement_suggestions": improvement_suggestions,
                "agent_id": agent_id,
                "quality_threshold": quality_threshold
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ç»“æœåˆ†æå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "agent_id": agent_id
            }
    
    def _update_agent_performance(self, agent_id: str, result: Dict[str, Any]):
        """æ›´æ–°æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ ‡"""
        if agent_id not in self.registered_agents:
            return
        
        agent_info = self.registered_agents[agent_id]
        
        # æ›´æ–°æ‰§è¡Œæ—¶é—´
        execution_time = result.get("execution_time", 0)
        if execution_time > 0:
            agent_info.total_execution_time += execution_time
            total_tasks = agent_info.success_count + agent_info.failure_count
            if total_tasks > 0:
                agent_info.average_response_time = agent_info.total_execution_time / total_tasks
        
        # æ›´æ–°æˆåŠŸ/å¤±è´¥ç»Ÿè®¡
        if result.get("success", False):
            agent_info.success_count += 1
            agent_info.last_success_time = time.time()
            agent_info.consecutive_successes += 1
            agent_info.consecutive_failures = 0
        else:
            agent_info.failure_count += 1
            agent_info.last_failure_time = time.time()
            agent_info.consecutive_failures += 1
            agent_info.consecutive_successes = 0
    
    def _enhanced_result_quality_analysis(self, result: Dict[str, Any], 
                                        task_context: Dict[str, Any],
                                        quality_threshold: float) -> Dict[str, Any]:
        """å¢å¼ºçš„ç»“æœè´¨é‡åˆ†æ - åŒ…å«æ–‡ä»¶éªŒè¯ã€å®é™…æ‰§è¡Œæ£€æŸ¥å’Œä»£ç æµ‹è¯•æµç¨‹æ£€æŸ¥"""
        
        analysis = {
            "quality_score": 0.0,
            "completeness": "unknown",
            "issues": [],
            "strengths": [],
            "recommendations": [],
            "detailed_metrics": {},
            "risk_assessment": "low",
            "file_verification": {},
            "actual_execution_check": {},
            "code_testing_workflow": {}
        }
        
        # æ£€æŸ¥ç»“æœæ˜¯å¦ä¸ºç©º
        if not result or not result.get("success", False):
            analysis["completeness"] = "failed"
            analysis["issues"].append("ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
            analysis["recommendations"].append("é‡æ–°åˆ†é…ä»»åŠ¡æˆ–æ›´æ¢æ™ºèƒ½ä½“")
            analysis["risk_assessment"] = "high"
            return analysis
        
        # è·å–åŸå§‹ä»»åŠ¡éœ€æ±‚
        original_request = task_context.get("original_request", "") if task_context else ""
        
        # åˆ†æç»“æœå†…å®¹
        result_content = result.get("result", "") if isinstance(result.get("result"), str) else str(result.get("result", ""))
        if not result_content:
            analysis["completeness"] = "incomplete"
            analysis["issues"].append("ç»“æœå†…å®¹ä¸ºç©º")
            analysis["recommendations"].append("è¦æ±‚æ™ºèƒ½ä½“é‡æ–°æ‰§è¡Œå¹¶æä¾›è¯¦ç»†ç»“æœ")
            analysis["risk_assessment"] = "medium"
            return analysis
        
        # æ‰§è¡Œæ–‡ä»¶éªŒè¯ã€å®é™…æ‰§è¡Œæ£€æŸ¥å’Œä»£ç æµ‹è¯•æµç¨‹æ£€æŸ¥
        file_verification = self._verify_file_generation(result_content, original_request)
        execution_check = self._check_actual_execution(result_content, original_request)
        testing_workflow = self._check_code_testing_workflow(result_content, original_request, task_context)
        
        analysis["file_verification"] = file_verification
        analysis["actual_execution_check"] = execution_check
        analysis["code_testing_workflow"] = testing_workflow
        
        # è¯¦ç»†è´¨é‡æŒ‡æ ‡åˆ†æ
        detailed_metrics = self._analyze_detailed_metrics(result_content, result, file_verification, execution_check)
        analysis["detailed_metrics"] = detailed_metrics
        
        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°ï¼ˆåŒ…å«å®é™…æ‰§è¡Œå’Œæµ‹è¯•æµç¨‹æƒé‡ï¼‰
        quality_score = self._calculate_comprehensive_quality_score(detailed_metrics, file_verification, execution_check, testing_workflow)
        analysis["quality_score"] = quality_score
        
        # åˆ†æé—®é¢˜å’Œä¼˜åŠ¿
        analysis["issues"] = self._identify_quality_issues(detailed_metrics, file_verification, execution_check, testing_workflow, original_request)
        analysis["strengths"] = self._identify_quality_strengths(detailed_metrics, file_verification, execution_check, testing_workflow)
        
        # æ ¹æ®è´¨é‡åˆ†æ•°ã€å®é™…æ‰§è¡Œæƒ…å†µå’Œæµ‹è¯•æµç¨‹å®Œæ•´æ€§åˆ¤æ–­å®Œæ•´æ€§
        if not file_verification.get("all_required_files_generated", False):
            analysis["completeness"] = "incomplete"
            analysis["risk_assessment"] = "high"
            analysis["issues"].append("æœªå®é™…ç”Ÿæˆæ‰€éœ€æ–‡ä»¶")
        elif not execution_check.get("simulation_actually_executed", False) and "æµ‹è¯•å°" in original_request:
            analysis["completeness"] = "incomplete" 
            analysis["risk_assessment"] = "high"
            analysis["issues"].append("æœªå®é™…æ‰§è¡Œä»¿çœŸéªŒè¯")
        elif testing_workflow.get("workflow_completeness", 0) < 50 and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            analysis["completeness"] = "incomplete"
            analysis["risk_assessment"] = "high"
            analysis["issues"].append("ä»£ç æµ‹è¯•æµç¨‹ä¸å®Œæ•´")
        elif quality_score >= quality_threshold:
            analysis["completeness"] = "complete"
            analysis["risk_assessment"] = "low"
        elif quality_score >= quality_threshold * 0.7:
            analysis["completeness"] = "partial"
            analysis["risk_assessment"] = "medium"
        else:
            analysis["completeness"] = "incomplete"
            analysis["risk_assessment"] = "high"
        
        # ç”Ÿæˆå…·ä½“å»ºè®®
        analysis["recommendations"] = self._generate_enhanced_recommendations(
            detailed_metrics, quality_score, quality_threshold, 
            file_verification, execution_check, testing_workflow, original_request
        )
        
        return analysis
    
    def _verify_file_generation(self, result_content: str, original_request: str) -> Dict[str, Any]:
        """éªŒè¯æ˜¯å¦å®é™…ç”Ÿæˆäº†æ‰€éœ€æ–‡ä»¶"""
        verification = {
            "verilog_file_mentioned": False,
            "testbench_file_mentioned": False,
            "files_actually_written": False,
            "all_required_files_generated": False,
            "missing_files": []
        }
        
        # æ£€æŸ¥æ˜¯å¦æåˆ°äº†Verilogæ–‡ä»¶
        if ".v" in result_content or "module" in result_content.lower():
            verification["verilog_file_mentioned"] = True
        
        # æ£€æŸ¥æ˜¯å¦æåˆ°äº†æµ‹è¯•å°æ–‡ä»¶
        if ("testbench" in result_content.lower() or "tb_" in result_content.lower() or 
            "_tb.v" in result_content or "test" in result_content.lower()):
            verification["testbench_file_mentioned"] = True
        
        # æ£€æŸ¥æ˜¯å¦å®é™…æ‰§è¡Œäº†æ–‡ä»¶å†™å…¥æ“ä½œ
        # é€šè¿‡æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨çš„è¿¹è±¡
        if ("write_file" in result_content.lower() or 
            "æ–‡ä»¶å·²ç”Ÿæˆ" in result_content or "æ–‡ä»¶åˆ›å»º" in result_content or
            "ä¿å­˜åˆ°" in result_content or "å†™å…¥æ–‡ä»¶" in result_content):
            verification["files_actually_written"] = True
        
        # åˆ†æåŸå§‹éœ€æ±‚ï¼Œç¡®å®šéœ€è¦çš„æ–‡ä»¶
        required_files = []
        if "verilog" in original_request.lower() or "æ¨¡å—" in original_request:
            required_files.append("verilog_module")
        if ("æµ‹è¯•å°" in original_request or "testbench" in original_request.lower() or 
            "éªŒè¯" in original_request or "test" in original_request.lower()):
            required_files.append("testbench")
        
        # æ£€æŸ¥æ‰€æœ‰å¿…éœ€æ–‡ä»¶æ˜¯å¦éƒ½ç”Ÿæˆäº†
        missing_files = []
        if "verilog_module" in required_files and not verification["verilog_file_mentioned"]:
            missing_files.append("Verilogæ¨¡å—æ–‡ä»¶")
        if "testbench" in required_files and not verification["testbench_file_mentioned"]:
            missing_files.append("æµ‹è¯•å°æ–‡ä»¶")
        
        verification["missing_files"] = missing_files
        verification["all_required_files_generated"] = (len(missing_files) == 0 and 
                                                       verification["files_actually_written"])
        
        return verification
    
    def _check_actual_execution(self, result_content: str, original_request: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ˜¯å¦å®é™…æ‰§è¡Œäº†æ‰€éœ€æ“ä½œ"""
        execution_check = {
            "simulation_mentioned": False,
            "simulation_actually_executed": False,
            "tools_called": False,
            "concrete_results_provided": False,
            "missing_executions": []
        }
        
        # æ£€æŸ¥æ˜¯å¦æåˆ°äº†ä»¿çœŸ
        if ("ä»¿çœŸ" in result_content or "simulation" in result_content.lower() or 
            "run_simulation" in result_content or "æ‰§è¡Œä»¿çœŸ" in result_content):
            execution_check["simulation_mentioned"] = True
        
        # æ£€æŸ¥æ˜¯å¦å®é™…æ‰§è¡Œäº†ä»¿çœŸï¼ˆé€šè¿‡å…·ä½“è¾“å‡ºåˆ¤æ–­ï¼‰
        if (("ä»¿çœŸç»“æœ" in result_content or "simulation result" in result_content.lower() or
             "æ³¢å½¢" in result_content or "waveform" in result_content.lower() or
             "ä»¿çœŸè¾“å‡º" in result_content or "æ—¶åº" in result_content) and 
            execution_check["simulation_mentioned"]):
            execution_check["simulation_actually_executed"] = True
        
        # æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†å·¥å…·
        if ("å·¥å…·è°ƒç”¨" in result_content or "tool_call" in result_content.lower() or
            "æ‰§è¡Œå·¥å…·" in result_content or "function calling" in result_content.lower()):
            execution_check["tools_called"] = True
        
        # æ£€æŸ¥æ˜¯å¦æä¾›äº†å…·ä½“ç»“æœ
        if ("æ‰§è¡ŒæˆåŠŸ" in result_content and "ç»“æœ" in result_content) or "è¾“å‡º:" in result_content:
            execution_check["concrete_results_provided"] = True
        
        # åˆ†æç¼ºå¤±çš„æ‰§è¡Œé¡¹
        missing_executions = []
        if ("æµ‹è¯•å°" in original_request or "éªŒè¯" in original_request) and not execution_check["simulation_actually_executed"]:
            missing_executions.append("ä»¿çœŸéªŒè¯æ‰§è¡Œ")
        if ("æ–‡ä»¶" in original_request or "ç”Ÿæˆ" in original_request) and not execution_check["tools_called"]:
            missing_executions.append("æ–‡ä»¶ç”Ÿæˆæ“ä½œ")
        
        execution_check["missing_executions"] = missing_executions
        
        return execution_check
    
    def _check_code_testing_workflow(self, result_content: str, original_request: str, task_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """ä¸“é—¨æ£€æŸ¥ä»£ç æµ‹è¯•æµç¨‹çš„å®Œæ•´æ€§"""
        testing_workflow = {
            "test_plan_created": False,
            "test_cases_designed": False,
            "testbench_generated": False,
            "simulation_executed": False,
            "test_results_analyzed": False,
            "coverage_analysis_performed": False,
            "test_report_generated": False,
            "workflow_completeness": 0.0,
            "missing_testing_steps": [],
            "testing_quality_score": 0.0,
            "test_coverage_metrics": {},
            "test_execution_details": {}
        }
        
        # 1. æ£€æŸ¥æµ‹è¯•è®¡åˆ’åˆ›å»º
        test_plan_indicators = [
            "æµ‹è¯•è®¡åˆ’", "test plan", "æµ‹è¯•ç­–ç•¥", "test strategy", 
            "æµ‹è¯•ç›®æ ‡", "test objectives", "æµ‹è¯•èŒƒå›´", "test scope"
        ]
        if any(indicator in result_content for indicator in test_plan_indicators):
            testing_workflow["test_plan_created"] = True
        
        # 2. æ£€æŸ¥æµ‹è¯•ç”¨ä¾‹è®¾è®¡
        test_case_indicators = [
            "æµ‹è¯•ç”¨ä¾‹", "test case", "æµ‹è¯•å‘é‡", "test vector",
            "è¾¹ç•Œæµ‹è¯•", "boundary test", "åŠŸèƒ½æµ‹è¯•", "functional test",
            "æ—¶åºæµ‹è¯•", "timing test", "å¼‚å¸¸æµ‹è¯•", "exception test"
        ]
        if any(indicator in result_content for indicator in test_case_indicators):
            testing_workflow["test_cases_designed"] = True
        
        # 3. æ£€æŸ¥æµ‹è¯•å°ç”Ÿæˆ
        testbench_indicators = [
            "testbench", "æµ‹è¯•å°", "tb_", "_tb.v", "initial", "always",
            "test stimulus", "æµ‹è¯•æ¿€åŠ±", "æ—¶é’Ÿç”Ÿæˆ", "clock generation"
        ]
        if any(indicator in result_content for indicator in testbench_indicators):
            testing_workflow["testbench_generated"] = True
        
        # 4. æ£€æŸ¥ä»¿çœŸæ‰§è¡Œ
        simulation_indicators = [
            "ä»¿çœŸæ‰§è¡Œ", "simulation executed", "ä»¿çœŸç»“æœ", "simulation result",
            "æ³¢å½¢è¾“å‡º", "waveform output", "ä»¿çœŸå®Œæˆ", "simulation completed",
            "ä»¿çœŸæ—¶é—´", "simulation time", "ä»¿çœŸå‘¨æœŸ", "simulation cycles"
        ]
        if any(indicator in result_content for indicator in simulation_indicators):
            testing_workflow["simulation_executed"] = True
        
        # 5. æ£€æŸ¥æµ‹è¯•ç»“æœåˆ†æ
        result_analysis_indicators = [
            "æµ‹è¯•ç»“æœåˆ†æ", "test result analysis", "ç»“æœéªŒè¯", "result verification",
            "åŠŸèƒ½æ­£ç¡®æ€§", "functional correctness", "æ—¶åºæ­£ç¡®æ€§", "timing correctness",
            "æµ‹è¯•é€šè¿‡", "test passed", "æµ‹è¯•å¤±è´¥", "test failed"
        ]
        if any(indicator in result_content for indicator in result_analysis_indicators):
            testing_workflow["test_results_analyzed"] = True
        
        # 6. æ£€æŸ¥è¦†ç›–ç‡åˆ†æ
        coverage_indicators = [
            "ä»£ç è¦†ç›–ç‡", "code coverage", "åŠŸèƒ½è¦†ç›–ç‡", "functional coverage",
            "åˆ†æ”¯è¦†ç›–ç‡", "branch coverage", "è¯­å¥è¦†ç›–ç‡", "statement coverage",
            "è¦†ç›–ç‡æŠ¥å‘Š", "coverage report", "è¦†ç›–ç‡ç»Ÿè®¡", "coverage statistics"
        ]
        if any(indicator in result_content for indicator in coverage_indicators):
            testing_workflow["coverage_analysis_performed"] = True
        
        # 7. æ£€æŸ¥æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
        report_indicators = [
            "æµ‹è¯•æŠ¥å‘Š", "test report", "æµ‹è¯•æ€»ç»“", "test summary",
            "æµ‹è¯•ç»“è®º", "test conclusion", "æµ‹è¯•å»ºè®®", "test recommendations"
        ]
        if any(indicator in result_content for indicator in report_indicators):
            testing_workflow["test_report_generated"] = True
        
        # è®¡ç®—å·¥ä½œæµå®Œæ•´æ€§
        completed_steps = sum([
            testing_workflow["test_plan_created"],
            testing_workflow["test_cases_designed"],
            testing_workflow["testbench_generated"],
            testing_workflow["simulation_executed"],
            testing_workflow["test_results_analyzed"],
            testing_workflow["coverage_analysis_performed"],
            testing_workflow["test_report_generated"]
        ])
        testing_workflow["workflow_completeness"] = (completed_steps / 7.0) * 100
        
        # è¯†åˆ«ç¼ºå¤±çš„æµ‹è¯•æ­¥éª¤
        missing_steps = []
        if not testing_workflow["test_plan_created"]:
            missing_steps.append("æµ‹è¯•è®¡åˆ’åˆ›å»º")
        if not testing_workflow["test_cases_designed"]:
            missing_steps.append("æµ‹è¯•ç”¨ä¾‹è®¾è®¡")
        if not testing_workflow["testbench_generated"]:
            missing_steps.append("æµ‹è¯•å°ç”Ÿæˆ")
        if not testing_workflow["simulation_executed"]:
            missing_steps.append("ä»¿çœŸæ‰§è¡Œ")
        if not testing_workflow["test_results_analyzed"]:
            missing_steps.append("æµ‹è¯•ç»“æœåˆ†æ")
        if not testing_workflow["coverage_analysis_performed"]:
            missing_steps.append("è¦†ç›–ç‡åˆ†æ")
        if not testing_workflow["test_report_generated"]:
            missing_steps.append("æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ")
        
        testing_workflow["missing_testing_steps"] = missing_steps
        
        # è®¡ç®—æµ‹è¯•è´¨é‡åˆ†æ•°
        quality_factors = {
            "test_plan_created": 15,
            "test_cases_designed": 20,
            "testbench_generated": 25,
            "simulation_executed": 20,
            "test_results_analyzed": 10,
            "coverage_analysis_performed": 5,
            "test_report_generated": 5
        }
        
        quality_score = sum([
            quality_factors[step] for step, completed in testing_workflow.items() 
            if step in quality_factors and completed
        ])
        testing_workflow["testing_quality_score"] = quality_score
        
        # æå–è¦†ç›–ç‡æŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        coverage_metrics = {}
        if "è¦†ç›–ç‡" in result_content or "coverage" in result_content.lower():
            # å°è¯•æå–å…·ä½“çš„è¦†ç›–ç‡æ•°å€¼
            import re
            coverage_patterns = [
                r"ä»£ç è¦†ç›–ç‡[ï¼š:]\s*(\d+(?:\.\d+)?)%",
                r"åŠŸèƒ½è¦†ç›–ç‡[ï¼š:]\s*(\d+(?:\.\d+)?)%",
                r"åˆ†æ”¯è¦†ç›–ç‡[ï¼š:]\s*(\d+(?:\.\d+)?)%",
                r"è¯­å¥è¦†ç›–ç‡[ï¼š:]\s*(\d+(?:\.\d+)?)%",
                r"code coverage[ï¼š:]\s*(\d+(?:\.\d+)?)%",
                r"functional coverage[ï¼š:]\s*(\d+(?:\.\d+)?)%"
            ]
            
            for pattern in coverage_patterns:
                matches = re.findall(pattern, result_content, re.IGNORECASE)
                if matches:
                    coverage_metrics["coverage_percentage"] = float(matches[0])
                    break
        
        testing_workflow["test_coverage_metrics"] = coverage_metrics
        
        # æå–æµ‹è¯•æ‰§è¡Œè¯¦æƒ…
        execution_details = {}
        if testing_workflow["simulation_executed"]:
            # å°è¯•æå–ä»¿çœŸæ—¶é—´ã€å‘¨æœŸç­‰ä¿¡æ¯
            import re
            time_patterns = [
                r"ä»¿çœŸæ—¶é—´[ï¼š:]\s*(\d+(?:\.\d+)?)",
                r"simulation time[ï¼š:]\s*(\d+(?:\.\d+)?)",
                r"ä»¿çœŸå‘¨æœŸ[ï¼š:]\s*(\d+)",
                r"simulation cycles[ï¼š:]\s*(\d+)"
            ]
            
            for pattern in time_patterns:
                matches = re.findall(pattern, result_content, re.IGNORECASE)
                if matches:
                    if "æ—¶é—´" in pattern or "time" in pattern:
                        execution_details["simulation_time"] = float(matches[0])
                    else:
                        execution_details["simulation_cycles"] = int(matches[0])
                    break
        
        testing_workflow["test_execution_details"] = execution_details
        
        return testing_workflow
    
    def _identify_quality_issues(self, metrics: Dict[str, Any], file_verification: Dict[str, Any], 
                               execution_check: Dict[str, Any], testing_workflow: Dict[str, Any], original_request: str) -> List[str]:
        """è¯†åˆ«è´¨é‡é—®é¢˜"""
        issues = []
        
        # æ–‡ä»¶ç”Ÿæˆé—®é¢˜
        if file_verification.get("missing_files"):
            issues.extend([f"ç¼ºå¤±{file}" for file in file_verification["missing_files"]])
        
        # æ‰§è¡Œé—®é¢˜
        if execution_check.get("missing_executions"):
            issues.extend(execution_check["missing_executions"])
        
        # ä»£ç æµ‹è¯•æµç¨‹é—®é¢˜
        if testing_workflow.get("missing_testing_steps"):
            issues.extend([f"æµ‹è¯•æµç¨‹ç¼ºå¤±: {step}" for step in testing_workflow["missing_testing_steps"]])
        
        if testing_workflow.get("workflow_completeness", 0) < 50 and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            issues.append(f"æµ‹è¯•æµç¨‹å®Œæ•´æ€§ä¸è¶³ ({testing_workflow['workflow_completeness']:.1f}%)")
        
        if testing_workflow.get("testing_quality_score", 0) < 50 and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            issues.append(f"æµ‹è¯•è´¨é‡åˆ†æ•°è¿‡ä½ ({testing_workflow['testing_quality_score']:.1f})")
        
        # è´¨é‡æŒ‡æ ‡é—®é¢˜
        if metrics.get("test_coverage", 0) < 50 and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            issues.append("æµ‹è¯•è¦†ç›–ç‡ä¸è¶³")
        
        if metrics.get("code_quality", 0) < 50:
            issues.append("ä»£ç è´¨é‡éœ€è¦æå‡")
        
        return issues
    
    def _identify_quality_strengths(self, metrics: Dict[str, Any], file_verification: Dict[str, Any], 
                                  execution_check: Dict[str, Any], testing_workflow: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«è´¨é‡ä¼˜åŠ¿"""
        strengths = []
        
        if file_verification.get("all_required_files_generated"):
            strengths.append("æ‰€æœ‰å¿…éœ€æ–‡ä»¶å·²ç”Ÿæˆ")
        
        if execution_check.get("simulation_actually_executed"):
            strengths.append("ä»¿çœŸéªŒè¯å·²æ‰§è¡Œ")
        
        # ä»£ç æµ‹è¯•æµç¨‹ä¼˜åŠ¿
        if testing_workflow.get("workflow_completeness", 0) >= 80:
            strengths.append(f"æµ‹è¯•æµç¨‹å®Œæ•´æ€§ä¼˜ç§€ ({testing_workflow['workflow_completeness']:.1f}%)")
        
        if testing_workflow.get("testing_quality_score", 0) >= 80:
            strengths.append(f"æµ‹è¯•è´¨é‡åˆ†æ•°ä¼˜ç§€ ({testing_workflow['testing_quality_score']:.1f})")
        
        if testing_workflow.get("test_plan_created"):
            strengths.append("æµ‹è¯•è®¡åˆ’å·²åˆ›å»º")
        
        if testing_workflow.get("test_cases_designed"):
            strengths.append("æµ‹è¯•ç”¨ä¾‹å·²è®¾è®¡")
        
        if testing_workflow.get("testbench_generated"):
            strengths.append("æµ‹è¯•å°å·²ç”Ÿæˆ")
        
        if testing_workflow.get("simulation_executed"):
            strengths.append("ä»¿çœŸå·²æ‰§è¡Œ")
        
        if testing_workflow.get("test_results_analyzed"):
            strengths.append("æµ‹è¯•ç»“æœå·²åˆ†æ")
        
        if testing_workflow.get("coverage_analysis_performed"):
            strengths.append("è¦†ç›–ç‡åˆ†æå·²å®Œæˆ")
        
        if testing_workflow.get("test_report_generated"):
            strengths.append("æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ")
        
        if metrics.get("code_quality", 0) >= 80:
            strengths.append("ä»£ç è´¨é‡ä¼˜ç§€")
        
        if metrics.get("documentation_quality", 0) >= 70:
            strengths.append("æ–‡æ¡£è´¨é‡è‰¯å¥½")
        
        return strengths
    
    def _generate_enhanced_recommendations(self, metrics: Dict[str, Any], quality_score: float, 
                                         quality_threshold: float, file_verification: Dict[str, Any],
                                         execution_check: Dict[str, Any], testing_workflow: Dict[str, Any], original_request: str) -> List[str]:
        """ç”Ÿæˆå¢å¼ºçš„æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºæ–‡ä»¶éªŒè¯çš„å»ºè®®
        if not file_verification.get("all_required_files_generated"):
            if file_verification.get("missing_files"):
                recommendations.append(f"éœ€è¦è°ƒç”¨enhanced_real_code_review_agentç”Ÿæˆç¼ºå¤±çš„æ–‡ä»¶: {', '.join(file_verification['missing_files'])}")
            else:
                recommendations.append("éœ€è¦ä½¿ç”¨write_fileå·¥å…·å®é™…ç”Ÿæˆæ–‡ä»¶ï¼Œè€Œéä»…åœ¨æŠ¥å‘Šä¸­æè¿°")
        
        # åŸºäºæ‰§è¡Œæ£€æŸ¥çš„å»ºè®®
        if not execution_check.get("simulation_actually_executed") and ("æµ‹è¯•å°" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦è°ƒç”¨enhanced_real_code_review_agentæ‰§è¡Œå®é™…çš„ä»¿çœŸéªŒè¯")
        
        # åŸºäºä»£ç æµ‹è¯•æµç¨‹çš„å»ºè®®
        if testing_workflow.get("workflow_completeness", 0) < 50 and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            missing_steps = testing_workflow.get("missing_testing_steps", [])
            if missing_steps:
                recommendations.append(f"éœ€è¦å®Œå–„æµ‹è¯•æµç¨‹ï¼Œç¼ºå¤±æ­¥éª¤: {', '.join(missing_steps)}")
            else:
                recommendations.append("éœ€è¦å»ºç«‹å®Œæ•´çš„ä»£ç æµ‹è¯•æµç¨‹")
        
        if not testing_workflow.get("test_plan_created") and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦åˆ›å»ºè¯¦ç»†çš„æµ‹è¯•è®¡åˆ’ï¼ŒåŒ…æ‹¬æµ‹è¯•ç›®æ ‡ã€ç­–ç•¥å’ŒèŒƒå›´")
        
        if not testing_workflow.get("test_cases_designed") and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦è®¾è®¡å…¨é¢çš„æµ‹è¯•ç”¨ä¾‹ï¼ŒåŒ…æ‹¬åŠŸèƒ½æµ‹è¯•ã€è¾¹ç•Œæµ‹è¯•å’Œå¼‚å¸¸æµ‹è¯•")
        
        if not testing_workflow.get("testbench_generated") and ("æµ‹è¯•å°" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•å°æ–‡ä»¶ï¼ŒåŒ…å«æµ‹è¯•æ¿€åŠ±å’Œç»“æœéªŒè¯")
        
        if not testing_workflow.get("simulation_executed") and ("ä»¿çœŸ" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦å®é™…æ‰§è¡Œä»¿çœŸéªŒè¯ï¼Œå¹¶æä¾›ä»¿çœŸç»“æœå’Œæ³¢å½¢åˆ†æ")
        
        if not testing_workflow.get("test_results_analyzed") and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦å¯¹æµ‹è¯•ç»“æœè¿›è¡Œè¯¦ç»†åˆ†æï¼ŒéªŒè¯åŠŸèƒ½æ­£ç¡®æ€§å’Œæ—¶åºæ­£ç¡®æ€§")
        
        if not testing_workflow.get("coverage_analysis_performed") and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦è¿›è¡Œä»£ç è¦†ç›–ç‡åˆ†æï¼Œç¡®ä¿æµ‹è¯•çš„å®Œæ•´æ€§")
        
        if not testing_workflow.get("test_report_generated") and ("æµ‹è¯•" in original_request or "éªŒè¯" in original_request):
            recommendations.append("éœ€è¦ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Šï¼ŒåŒ…å«æµ‹è¯•æ€»ç»“ã€ç»“è®ºå’Œå»ºè®®")
        
        # åŸºäºæµ‹è¯•è´¨é‡åˆ†æ•°çš„å»ºè®®
        if testing_workflow.get("testing_quality_score", 0) < 50:
            recommendations.append("æµ‹è¯•è´¨é‡éœ€è¦æ˜¾è‘—æå‡ï¼Œå»ºè®®é‡æ–°è®¾è®¡æµ‹è¯•ç­–ç•¥")
        elif testing_workflow.get("testing_quality_score", 0) < 80:
            recommendations.append("æµ‹è¯•è´¨é‡è‰¯å¥½ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
        
        # åŸºäºè´¨é‡åˆ†æ•°çš„å»ºè®®
        if quality_score < quality_threshold:
            if metrics.get("test_coverage", 0) < 50:
                recommendations.append("éœ€è¦å¢å¼ºæµ‹è¯•è¦†ç›–ç‡ï¼Œæ·»åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹")
            if metrics.get("code_quality", 0) < 60:
                recommendations.append("éœ€è¦æ”¹è¿›ä»£ç è´¨é‡ï¼Œå¢åŠ æ³¨é‡Šå’Œä¼˜åŒ–ç»“æ„")
        
        # åä½œå»ºè®®
        if len(recommendations) > 0:
            recommendations.append("å»ºè®®å¯åŠ¨å¤šæ™ºèƒ½ä½“åä½œæ¨¡å¼ä»¥ç¡®ä¿ä»»åŠ¡å®Œæ•´æ€§")
        
        return recommendations
    
    def _analyze_detailed_metrics(self, content: str, result: Dict[str, Any], 
                                 file_verification: Dict[str, Any] = None, 
                                 execution_check: Dict[str, Any] = None) -> Dict[str, Any]:
        """åˆ†æè¯¦ç»†è´¨é‡æŒ‡æ ‡"""
        metrics = {
            "code_quality": 0.0,
            "documentation_quality": 0.0,
            "test_coverage": 0.0,
            "error_handling": 0.0,
            "performance": 0.0,
            "compliance": 0.0
        }
        
        content_lower = content.lower()
        
        # ä»£ç è´¨é‡è¯„ä¼°
        if "module" in content_lower and "endmodule" in content_lower:
            metrics["code_quality"] += 30
        if "//" in content or "/*" in content:
            metrics["code_quality"] += 20
        if "parameter" in content_lower:
            metrics["code_quality"] += 10
        if "always" in content_lower or "assign" in content_lower:
            metrics["code_quality"] += 20
        
        # æ–‡æ¡£è´¨é‡è¯„ä¼°
        if "module" in content_lower and "description" in content_lower:
            metrics["documentation_quality"] += 30
        if "//" in content and len(content.split("//")) > 5:
            metrics["documentation_quality"] += 20
        if "åŠŸèƒ½" in content or "function" in content_lower:
            metrics["documentation_quality"] += 20
        
        # æµ‹è¯•è¦†ç›–è¯„ä¼°
        if "testbench" in content_lower or "test" in content_lower:
            metrics["test_coverage"] += 40
        if "simulation" in content_lower or "ä»¿çœŸ" in content:
            metrics["test_coverage"] += 30
        if "verification" in content_lower or "éªŒè¯" in content:
            metrics["test_coverage"] += 30
        
        # é”™è¯¯å¤„ç†è¯„ä¼°
        if "error" in content_lower and "fix" in content_lower:
            metrics["error_handling"] += 40
        if "exception" in content_lower or "å¼‚å¸¸" in content:
            metrics["error_handling"] += 30
        if "check" in content_lower or "æ£€æŸ¥" in content:
            metrics["error_handling"] += 30
        
        # æ€§èƒ½è¯„ä¼°
        execution_time = result.get("execution_time", 0)
        if execution_time > 0:
            if execution_time < 30:
                metrics["performance"] = 100
            elif execution_time < 60:
                metrics["performance"] = 80
            elif execution_time < 120:
                metrics["performance"] = 60
            else:
                metrics["performance"] = 40
        
        # åˆè§„æ€§è¯„ä¼°
        if "verilog" in content_lower or "systemverilog" in content_lower:
            metrics["compliance"] += 50
        if "ieee" in content_lower or "æ ‡å‡†" in content:
            metrics["compliance"] += 30
        if "synthesis" in content_lower or "ç»¼åˆ" in content:
            metrics["compliance"] += 20
        
        return metrics
    
    def _calculate_comprehensive_quality_score(self, metrics: Dict[str, float], 
                                             file_verification: Dict[str, Any] = None,
                                             execution_check: Dict[str, Any] = None,
                                             testing_workflow: Dict[str, Any] = None) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•° - åŒ…å«æ–‡ä»¶éªŒè¯ã€å®é™…æ‰§è¡Œå’Œä»£ç æµ‹è¯•æµç¨‹æƒé‡"""
        base_weights = {
            "code_quality": 0.20,
            "documentation_quality": 0.10,
            "test_coverage": 0.15,
            "error_handling": 0.10,
            "performance": 0.05,
            "compliance": 0.05
        }
        
        # åŸºç¡€åˆ†æ•°è®¡ç®—
        base_score = 0.0
        for metric, weight in base_weights.items():
            base_score += metrics.get(metric, 0.0) * weight
        
        # æ–‡ä»¶éªŒè¯æƒé‡ (15%)
        file_score = 0.0
        if file_verification:
            if file_verification.get("all_required_files_generated", False):
                file_score = 100.0
            elif file_verification.get("verilog_file_mentioned", False):
                file_score = 50.0  # ä»…æåˆ°ä½†æœªå®é™…ç”Ÿæˆ
            elif file_verification.get("files_actually_written", False):
                file_score = 30.0  # æœ‰å†™å…¥æ“ä½œä½†ä¸å®Œæ•´
        file_weighted_score = file_score * 0.15
        
        # å®é™…æ‰§è¡Œæƒé‡ (15%)
        execution_score = 0.0
        if execution_check:
            if execution_check.get("simulation_actually_executed", False):
                execution_score += 60.0
            elif execution_check.get("simulation_mentioned", False):
                execution_score += 20.0  # ä»…æåˆ°ä½†æœªå®é™…æ‰§è¡Œ
            
            if execution_check.get("tools_called", False):
                execution_score += 30.0
            
            if execution_check.get("concrete_results_provided", False):
                execution_score += 10.0
        execution_weighted_score = execution_score * 0.15
        
        # ä»£ç æµ‹è¯•æµç¨‹æƒé‡ (20%)
        testing_workflow_score = 0.0
        if testing_workflow:
            workflow_completeness = testing_workflow.get("workflow_completeness", 0)
            testing_quality_score = testing_workflow.get("testing_quality_score", 0)
            
            # åŸºäºå·¥ä½œæµå®Œæ•´æ€§çš„åˆ†æ•°
            testing_workflow_score += workflow_completeness * 0.6  # 60%æƒé‡ç»™å®Œæ•´æ€§
            
            # åŸºäºæµ‹è¯•è´¨é‡åˆ†æ•°çš„åˆ†æ•°
            testing_workflow_score += testing_quality_score * 0.4  # 40%æƒé‡ç»™è´¨é‡åˆ†æ•°
            
            # é¢å¤–å¥–åŠ±ï¼šå¦‚æœæ‰€æœ‰å…³é”®æµ‹è¯•æ­¥éª¤éƒ½å®Œæˆ
            if (testing_workflow.get("test_plan_created") and 
                testing_workflow.get("test_cases_designed") and 
                testing_workflow.get("testbench_generated") and 
                testing_workflow.get("simulation_executed")):
                testing_workflow_score += 20.0  # é¢å¤–å¥–åŠ±
        testing_workflow_weighted_score = testing_workflow_score * 0.20
        
        # ç»¼åˆåˆ†æ•°
        total_score = base_score + file_weighted_score + execution_weighted_score + testing_workflow_weighted_score
        
        # ä¸¥æ ¼çš„æƒ©ç½šæœºåˆ¶ï¼šå¦‚æœå…³é”®è¦æ±‚æœªæ»¡è¶³ï¼Œå¤§å¹…é™ä½åˆ†æ•°
        if file_verification and not file_verification.get("all_required_files_generated", False):
            total_score *= 0.6  # é™ä½40%
        
        if execution_check and execution_check.get("missing_executions"):
            total_score *= 0.7  # é™ä½30%
        
        # æµ‹è¯•æµç¨‹æƒ©ç½šæœºåˆ¶
        if testing_workflow and testing_workflow.get("workflow_completeness", 0) < 30:
            total_score *= 0.8  # æµ‹è¯•æµç¨‹ä¸¥é‡ä¸å®Œæ•´ï¼Œé™ä½20%
        
        return min(100.0, max(0.0, total_score))
    
    def _generate_specific_recommendations(self, metrics: Dict[str, float], 
                                         quality_score: float, 
                                         threshold: float) -> List[str]:
        """ç”Ÿæˆå…·ä½“æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if quality_score < threshold:
            if metrics["code_quality"] < 50:
                recommendations.append("æé«˜ä»£ç è´¨é‡ï¼šæ·»åŠ æ›´å¤šæ³¨é‡Šï¼Œæ”¹è¿›ä»£ç ç»“æ„")
            if metrics["documentation_quality"] < 40:
                recommendations.append("æ”¹è¿›æ–‡æ¡£ï¼šæ·»åŠ è¯¦ç»†çš„åŠŸèƒ½è¯´æ˜å’Œä½¿ç”¨ç¤ºä¾‹")
            if metrics["test_coverage"] < 60:
                recommendations.append("å¢åŠ æµ‹è¯•è¦†ç›–ï¼šç”Ÿæˆæ›´å…¨é¢çš„testbench")
            if metrics["error_handling"] < 30:
                recommendations.append("åŠ å¼ºé”™è¯¯å¤„ç†ï¼šæ·»åŠ è¾¹ç•Œæ¡ä»¶æ£€æŸ¥å’Œå¼‚å¸¸å¤„ç†")
        
        if quality_score >= threshold:
            recommendations.append("è´¨é‡è¾¾æ ‡ï¼Œå¯ä»¥ç»§ç»­ä¸‹ä¸€æ­¥æˆ–å®Œæˆä»»åŠ¡")
        
        return recommendations
    
    def _determine_enhanced_next_action(self, analysis: Dict[str, Any], 
                                      task_context: Dict[str, Any]) -> str:
        """ç¡®å®šå¢å¼ºçš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨ - æ”¯æŒå¤šæ™ºèƒ½ä½“åä½œå†³ç­–"""
        
        completeness = analysis.get("completeness", "unknown")
        quality_score = analysis.get("quality_score", 0)
        risk_assessment = analysis.get("risk_assessment", "low")
        file_verification = analysis.get("file_verification", {})
        execution_check = analysis.get("actual_execution_check", {})
        testing_workflow = analysis.get("code_testing_workflow", {})
        
        # è·å–åŸå§‹éœ€æ±‚
        original_request = task_context.get("original_request", "") if task_context else ""
        
        # å…ˆæ£€æŸ¥ä¼ ç»Ÿçš„å®Œæ•´æ€§æŒ‡æ ‡
        if completeness == "complete" and quality_score >= 80:
            # å³ä½¿æŠ¥å‘Šå®Œæˆï¼Œä¹Ÿè¦éªŒè¯å®é™…æ‰§è¡Œ
            missing_files = file_verification.get("missing_files", [])
            missing_executions = execution_check.get("missing_executions", [])
            missing_testing_steps = testing_workflow.get("missing_testing_steps", [])
            
            if missing_files or missing_executions or missing_testing_steps:
                # è™½ç„¶æ™ºèƒ½ä½“å£°ç§°å®Œæˆï¼Œä½†å®é™…ç¼ºå¤±å…³é”®é¡¹
                return "continue_iteration"  # éœ€è¦ç»§ç»­è¿­ä»£
            else:
                return "complete_task"  # çœŸæ­£å®Œæˆ
        
        elif completeness == "failed" or risk_assessment == "high":
            return "retry_with_different_agent"
        
        elif completeness == "partial" or quality_score < 70:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤šæ™ºèƒ½ä½“åä½œ
            missing_files = file_verification.get("missing_files", [])
            missing_executions = execution_check.get("missing_executions", [])
            missing_testing_steps = testing_workflow.get("missing_testing_steps", [])
            
            if (missing_files or missing_executions or missing_testing_steps or 
                not file_verification.get("all_required_files_generated", False) or
                testing_workflow.get("workflow_completeness", 0) < 50):
                return "continue_iteration"  # éœ€è¦é¢å¤–çš„æ™ºèƒ½ä½“åä½œ
            else:
                return "improve_result"
        
        else:
            return "continue_iteration"
    
    def _generate_improvement_suggestions(self, analysis: Dict[str, Any], agent_id: str) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®® - åŒ…å«å…·ä½“çš„æ™ºèƒ½ä½“åä½œå»ºè®®å’Œå·¥å…·è°ƒç”¨æŒ‡å¯¼"""
        suggestions = []
        
        # è·å–éªŒè¯ç»“æœ
        file_verification = analysis.get("file_verification", {})
        execution_check = analysis.get("actual_execution_check", {})
        testing_workflow = analysis.get("code_testing_workflow", {})
        
        # åŸºäºæ–‡ä»¶å’Œæ‰§è¡ŒéªŒè¯ç”Ÿæˆå…·ä½“å»ºè®®
        missing_files = file_verification.get("missing_files", [])
        missing_executions = execution_check.get("missing_executions", [])
        missing_testing_steps = testing_workflow.get("missing_testing_steps", [])
        
        if missing_files or missing_executions or missing_testing_steps:
            suggestions.append("éœ€è¦è°ƒç”¨é¢å¤–çš„æ™ºèƒ½ä½“æ¥è¡¥å……ç¼ºå¤±çš„åŠŸèƒ½")
            
            if "æµ‹è¯•å°æ–‡ä»¶" in missing_files or "ä»¿çœŸéªŒè¯æ‰§è¡Œ" in missing_executions:
                suggestions.append("å»ºè®®è°ƒç”¨ enhanced_real_code_review_agent ç”Ÿæˆæµ‹è¯•å°å¹¶æ‰§è¡Œä»¿çœŸ")
                suggestions.append("  - å¯ç”¨å·¥å…·: generate_testbench, run_simulation, use_external_testbench")
                suggestions.append("  - å·¥å…·è°ƒç”¨ç¤ºä¾‹: generate_testbench(module_name='xxx', module_code='...')")
                suggestions.append("  - å·¥å…·è°ƒç”¨ç¤ºä¾‹: run_simulation(module_code='...', testbench_code='...')")
            
            if "Verilogæ¨¡å—æ–‡ä»¶" in missing_files:
                suggestions.append("å»ºè®®é‡æ–°è°ƒç”¨ enhanced_real_verilog_agent ç”Ÿæˆå®Œæ•´çš„Verilogæ¨¡å—")
                suggestions.append("  - å¯ç”¨å·¥å…·: analyze_design_requirements, generate_verilog_code, search_existing_modules")
                suggestions.append("  - å·¥å…·è°ƒç”¨ç¤ºä¾‹: analyze_design_requirements(requirements='...')")
                suggestions.append("  - å·¥å…·è°ƒç”¨ç¤ºä¾‹: generate_verilog_code(module_name='xxx', requirements='...')")
            
            # åŸºäºæµ‹è¯•æµç¨‹ç¼ºå¤±æ­¥éª¤çš„å»ºè®®
            if missing_testing_steps:
                if "æµ‹è¯•è®¡åˆ’åˆ›å»º" in missing_testing_steps:
                    suggestions.append("éœ€è¦åˆ›å»ºè¯¦ç»†çš„æµ‹è¯•è®¡åˆ’ï¼ŒåŒ…æ‹¬æµ‹è¯•ç›®æ ‡ã€ç­–ç•¥å’ŒèŒƒå›´")
                if "æµ‹è¯•ç”¨ä¾‹è®¾è®¡" in missing_testing_steps:
                    suggestions.append("éœ€è¦è®¾è®¡å…¨é¢çš„æµ‹è¯•ç”¨ä¾‹ï¼ŒåŒ…æ‹¬åŠŸèƒ½æµ‹è¯•ã€è¾¹ç•Œæµ‹è¯•å’Œå¼‚å¸¸æµ‹è¯•")
                if "æµ‹è¯•å°ç”Ÿæˆ" in missing_testing_steps:
                    suggestions.append("éœ€è¦ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•å°æ–‡ä»¶ï¼ŒåŒ…å«æµ‹è¯•æ¿€åŠ±å’Œç»“æœéªŒè¯")
                    suggestions.append("  - ä½¿ç”¨ enhanced_real_code_review_agent çš„ generate_testbench å·¥å…·")
                if "ä»¿çœŸæ‰§è¡Œ" in missing_testing_steps:
                    suggestions.append("éœ€è¦å®é™…æ‰§è¡Œä»¿çœŸéªŒè¯ï¼Œå¹¶æä¾›ä»¿çœŸç»“æœå’Œæ³¢å½¢åˆ†æ")
                    suggestions.append("  - ä½¿ç”¨ enhanced_real_code_review_agent çš„ run_simulation å·¥å…·")
                if "æµ‹è¯•ç»“æœåˆ†æ" in missing_testing_steps:
                    suggestions.append("éœ€è¦å¯¹æµ‹è¯•ç»“æœè¿›è¡Œè¯¦ç»†åˆ†æï¼ŒéªŒè¯åŠŸèƒ½æ­£ç¡®æ€§å’Œæ—¶åºæ­£ç¡®æ€§")
                    suggestions.append("  - ä½¿ç”¨ enhanced_real_code_review_agent çš„ analyze_test_failures å·¥å…·")
                if "è¦†ç›–ç‡åˆ†æ" in missing_testing_steps:
                    suggestions.append("éœ€è¦è¿›è¡Œä»£ç è¦†ç›–ç‡åˆ†æï¼Œç¡®ä¿æµ‹è¯•çš„å®Œæ•´æ€§")
                if "æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ" in missing_testing_steps:
                    suggestions.append("éœ€è¦ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Šï¼ŒåŒ…å«æµ‹è¯•æ€»ç»“ã€ç»“è®ºå’Œå»ºè®®")
        
        if not file_verification.get("all_required_files_generated", False):
            suggestions.append("éœ€è¦ç¡®ä¿æ‰€æœ‰æ–‡ä»¶éƒ½è¢«å®é™…ç”Ÿæˆè€Œéä»…åœ¨æŠ¥å‘Šä¸­æè¿°")
        
        if not execution_check.get("simulation_actually_executed", False):
            suggestions.append("éœ€è¦æ‰§è¡Œå®é™…çš„ä»¿çœŸéªŒè¯è€Œéä»…ç”Ÿæˆä»¿çœŸä»£ç ")
        
        # åŸºäºæµ‹è¯•æµç¨‹å®Œæ•´æ€§çš„å»ºè®®
        workflow_completeness = testing_workflow.get("workflow_completeness", 0)
        if workflow_completeness < 50:
            suggestions.append(f"æµ‹è¯•æµç¨‹å®Œæ•´æ€§ä¸è¶³ ({workflow_completeness:.1f}%)ï¼Œéœ€è¦å®Œå–„æµ‹è¯•æµç¨‹")
        elif workflow_completeness < 80:
            suggestions.append(f"æµ‹è¯•æµç¨‹åŸºæœ¬å®Œæ•´ ({workflow_completeness:.1f}%)ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
        
        # æ·»åŠ æ™ºèƒ½ä½“å·¥å…·è°ƒç”¨æŒ‡å¯¼
        suggestions.append("\n=== æ™ºèƒ½ä½“å·¥å…·è°ƒç”¨æŒ‡å¯¼ ===")
        suggestions.append("LLMCoordinatorAgent å¯ç”¨å·¥å…·:")
        suggestions.append("  - assign_task_to_agent: åˆ†é…ä»»åŠ¡ç»™æŒ‡å®šæ™ºèƒ½ä½“")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: assign_task_to_agent(agent_id='enhanced_real_verilog_agent', task_description='...')")
        suggestions.append("  - analyze_agent_result: åˆ†ææ™ºèƒ½ä½“æ‰§è¡Œç»“æœ")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: analyze_agent_result(agent_id='xxx', result={...}, quality_threshold=80)")
        suggestions.append("  - check_task_completion: æ£€æŸ¥ä»»åŠ¡å®ŒæˆçŠ¶æ€")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: check_task_completion(task_id='xxx', all_results={...}, original_requirements='...')")
        suggestions.append("  - query_agent_status: æŸ¥è¯¢æ™ºèƒ½ä½“çŠ¶æ€")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: query_agent_status(agent_id='enhanced_real_verilog_agent', include_performance=True)")
        suggestions.append("  - identify_task_type: è¯†åˆ«ä»»åŠ¡ç±»å‹")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: identify_task_type(user_request='...', context={...})")
        suggestions.append("  - recommend_agent: æ¨èåˆé€‚çš„æ™ºèƒ½ä½“")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: recommend_agent(task_type='design', task_description='...', priority='high')")
        suggestions.append("  - provide_final_answer: æä¾›æœ€ç»ˆç­”æ¡ˆ")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: provide_final_answer(final_summary='...', task_status='success')")
        
        suggestions.append("\nenhanced_real_verilog_agent å¯ç”¨å·¥å…·:")
        suggestions.append("  - analyze_design_requirements: åˆ†æè®¾è®¡éœ€æ±‚")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: analyze_design_requirements(requirements='è®¾è®¡ä¸€ä¸ª8ä½åŠ æ³•å™¨', design_type='combinational')")
        suggestions.append("  - generate_verilog_code: ç”ŸæˆVerilogä»£ç ")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: generate_verilog_code(module_name='adder_8bit', requirements='8ä½åŠ æ³•å™¨', input_ports=[...])")
        suggestions.append("  - search_existing_modules: æœç´¢ç°æœ‰æ¨¡å—")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: search_existing_modules(module_type='arithmetic', functionality='åŠ æ³•å™¨')")
        suggestions.append("  - analyze_code_quality: åˆ†æä»£ç è´¨é‡")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: analyze_code_quality(verilog_code='...', module_name='adder_8bit')")
        suggestions.append("  - validate_design_specifications: éªŒè¯è®¾è®¡è§„æ ¼")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: validate_design_specifications(requirements='...', generated_code='...')")
        suggestions.append("  - generate_design_documentation: ç”Ÿæˆè®¾è®¡æ–‡æ¡£")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: generate_design_documentation(module_name='adder_8bit', verilog_code='...', requirements='...')")
        suggestions.append("  - optimize_verilog_code: ä¼˜åŒ–Verilogä»£ç ")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: optimize_verilog_code(verilog_code='...', optimization_target='area')")
        
        suggestions.append("\nenhanced_real_code_review_agent å¯ç”¨å·¥å…·:")
        suggestions.append("  - generate_testbench: ç”Ÿæˆæµ‹è¯•å°")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: generate_testbench(module_name='adder_8bit', module_code='...', test_scenarios=[...])")
        suggestions.append("  - run_simulation: è¿è¡Œä»¿çœŸ")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: run_simulation(module_code='...', testbench_code='...', simulator='iverilog')")
        suggestions.append("  - use_external_testbench: ä½¿ç”¨å¤–éƒ¨æµ‹è¯•å°")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: use_external_testbench(design_code='...', external_testbench_path='testbench.v', design_module_name='adder_8bit')")
        suggestions.append("  - generate_build_script: ç”Ÿæˆæ„å»ºè„šæœ¬")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: generate_build_script(verilog_files=['design.v'], testbench_files=['tb.v'], script_type='makefile')")
        suggestions.append("  - execute_build_script: æ‰§è¡Œæ„å»ºè„šæœ¬")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: execute_build_script(script_name='Makefile', action='all', timeout=300)")
        suggestions.append("  - analyze_test_failures: åˆ†ææµ‹è¯•å¤±è´¥")
        suggestions.append("    è°ƒç”¨ç¤ºä¾‹: analyze_test_failures(design_code='...', compilation_errors='...', testbench_code='...')")
        
        suggestions.append("\n=== å·¥å…·å‚æ•°è¯¦ç»†è¯´æ˜ ===")
        suggestions.append("LLMCoordinatorAgent å·¥å…·å‚æ•°:")
        suggestions.append("  assign_task_to_agent:")
        suggestions.append("    - agent_id: æ™ºèƒ½ä½“ID ('enhanced_real_verilog_agent' æˆ– 'enhanced_real_code_review_agent')")
        suggestions.append("    - task_description: è¯¦ç»†ä»»åŠ¡æè¿°")
        suggestions.append("    - expected_output: æœŸæœ›è¾“å‡ºæ ¼å¼")
        suggestions.append("    - task_type: ä»»åŠ¡ç±»å‹ ('design', 'verification', 'analysis', 'debug', 'composite')")
        suggestions.append("    - priority: ä¼˜å…ˆçº§ ('high', 'medium', 'low')")
        
        suggestions.append("  analyze_agent_result:")
        suggestions.append("    - agent_id: æ™ºèƒ½ä½“ID")
        suggestions.append("    - result: æ™ºèƒ½ä½“æ‰§è¡Œç»“æœ")
        suggestions.append("    - task_context: ä»»åŠ¡ä¸Šä¸‹æ–‡")
        suggestions.append("    - quality_threshold: è´¨é‡é˜ˆå€¼ (0-100)")
        
        suggestions.append("\nenhanced_real_verilog_agent å·¥å…·å‚æ•°:")
        suggestions.append("  generate_verilog_code:")
        suggestions.append("    - module_name: æ¨¡å—åç§° (å­—æ¯å¼€å¤´)")
        suggestions.append("    - requirements: è®¾è®¡éœ€æ±‚æè¿°")
        suggestions.append("    - input_ports: è¾“å…¥ç«¯å£åˆ—è¡¨ [{'name': 'clk', 'width': 1}]")
        suggestions.append("    - output_ports: è¾“å‡ºç«¯å£åˆ—è¡¨ [{'name': 'result', 'width': 8}]")
        suggestions.append("    - coding_style: ç¼–ç é£æ ¼ ('behavioral', 'structural', 'rtl', 'mixed')")
        
        suggestions.append("  analyze_design_requirements:")
        suggestions.append("    - requirements: è®¾è®¡éœ€æ±‚æè¿°")
        suggestions.append("    - design_type: è®¾è®¡ç±»å‹ ('combinational', 'sequential', 'mixed', 'custom')")
        suggestions.append("    - complexity_level: å¤æ‚åº¦ ('simple', 'medium', 'complex', 'advanced')")
        
        suggestions.append("\nenhanced_real_code_review_agent å·¥å…·å‚æ•°:")
        suggestions.append("  generate_testbench:")
        suggestions.append("    - module_name: ç›®æ ‡æ¨¡å—åç§°")
        suggestions.append("    - module_code: å®Œæ•´çš„Verilogæ¨¡å—ä»£ç ")
        suggestions.append("    - test_scenarios: æµ‹è¯•åœºæ™¯åˆ—è¡¨")
        suggestions.append("    - clock_period: æ—¶é’Ÿå‘¨æœŸ (ns)")
        suggestions.append("    - simulation_time: ä»¿çœŸæ—¶é—´ (æ—¶é’Ÿå‘¨æœŸæ•°)")
        
        suggestions.append("  run_simulation:")
        suggestions.append("    - module_code: æ¨¡å—ä»£ç å†…å®¹")
        suggestions.append("    - testbench_code: æµ‹è¯•å°ä»£ç å†…å®¹")
        suggestions.append("    - simulator: ä»¿çœŸå™¨ ('iverilog', 'modelsim', 'vivado', 'auto')")
        suggestions.append("    - simulation_options: ä»¿çœŸé€‰é¡¹")
        
        suggestions.append("  analyze_test_failures:")
        suggestions.append("    - design_code: è®¾è®¡ä»£ç ")
        suggestions.append("    - compilation_errors: ç¼–è¯‘é”™è¯¯ä¿¡æ¯")
        suggestions.append("    - simulation_errors: ä»¿çœŸé”™è¯¯ä¿¡æ¯")
        suggestions.append("    - test_assertions: æµ‹è¯•æ–­è¨€å¤±è´¥ä¿¡æ¯")
        suggestions.append("    - testbench_code: æµ‹è¯•å°ä»£ç ")
        suggestions.append("    - iteration_number: å½“å‰è¿­ä»£æ¬¡æ•°")
        
        suggestions.append("\n=== å·¥å…·è°ƒç”¨æœ€ä½³å®è·µ ===")
        suggestions.append("1. ä»»åŠ¡åˆ†é…æµç¨‹:")
        suggestions.append("   - å…ˆä½¿ç”¨ identify_task_type è¯†åˆ«ä»»åŠ¡ç±»å‹")
        suggestions.append("   - ä½¿ç”¨ recommend_agent æ¨èåˆé€‚çš„æ™ºèƒ½ä½“")
        suggestions.append("   - ä½¿ç”¨ assign_task_to_agent åˆ†é…ä»»åŠ¡")
        suggestions.append("   - ä½¿ç”¨ analyze_agent_result åˆ†æç»“æœ")
        suggestions.append("   - ä½¿ç”¨ check_task_completion æ£€æŸ¥å®ŒæˆçŠ¶æ€")
        
        suggestions.append("2. Verilogè®¾è®¡æµç¨‹:")
        suggestions.append("   - ä½¿ç”¨ analyze_design_requirements åˆ†æéœ€æ±‚")
        suggestions.append("   - ä½¿ç”¨ search_existing_modules æœç´¢ç°æœ‰æ¨¡å—")
        suggestions.append("   - ä½¿ç”¨ generate_verilog_code ç”Ÿæˆä»£ç ")
        suggestions.append("   - ä½¿ç”¨ analyze_code_quality åˆ†æè´¨é‡")
        suggestions.append("   - ä½¿ç”¨ validate_design_specifications éªŒè¯è§„æ ¼")
        
        suggestions.append("3. æµ‹è¯•éªŒè¯æµç¨‹:")
        suggestions.append("   - ä½¿ç”¨ generate_testbench ç”Ÿæˆæµ‹è¯•å°")
        suggestions.append("   - ä½¿ç”¨ run_simulation æ‰§è¡Œä»¿çœŸ")
        suggestions.append("   - ä½¿ç”¨ analyze_test_failures åˆ†æå¤±è´¥")
        suggestions.append("   - ä½¿ç”¨ generate_build_script ç”Ÿæˆæ„å»ºè„šæœ¬")
        suggestions.append("   - ä½¿ç”¨ execute_build_script æ‰§è¡Œæ„å»º")
        
        suggestions.append("4. é”™è¯¯å¤„ç†ç­–ç•¥:")
        suggestions.append("   - ç¼–è¯‘é”™è¯¯: æ£€æŸ¥è¯­æ³•å’Œç«¯å£å®šä¹‰")
        suggestions.append("   - ä»¿çœŸé”™è¯¯: æ£€æŸ¥æ—¶åºå’Œé€»è¾‘")
        suggestions.append("   - æµ‹è¯•å¤±è´¥: åˆ†ææ–­è¨€å’ŒæœŸæœ›å€¼")
        suggestions.append("   - è´¨é‡ä¸è¶³: é‡æ–°è®¾è®¡æˆ–ä¼˜åŒ–ä»£ç ")
        
        # æ·»åŠ LLMCoordinatorAgentçš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼
        coordinator_guide = self._generate_coordinator_tool_guide()
        suggestions.extend(coordinator_guide)
        
        # åŸºäºæµ‹è¯•è´¨é‡åˆ†æ•°çš„å»ºè®®
        testing_quality_score = testing_workflow.get("testing_quality_score", 0)
        if testing_quality_score < 50:
            suggestions.append(f"æµ‹è¯•è´¨é‡åˆ†æ•°è¿‡ä½ ({testing_quality_score:.1f})ï¼Œéœ€è¦é‡æ–°è®¾è®¡æµ‹è¯•ç­–ç•¥")
        elif testing_quality_score < 80:
            suggestions.append(f"æµ‹è¯•è´¨é‡è‰¯å¥½ ({testing_quality_score:.1f})ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
        
        # åŸºäºè´¨é‡åˆ†æ•°ç”Ÿæˆå»ºè®®
        if analysis.get("quality_score", 0) < 70:
            suggestions.append("è´¨é‡åˆ†æ•°åä½ï¼Œéœ€è¦æ”¹è¿›æˆ–ä½¿ç”¨å¤šæ™ºèƒ½ä½“åä½œ")
        
        # åŸºäºæ™ºèƒ½ä½“å†å²è¡¨ç°ç”Ÿæˆå»ºè®®
        if agent_id in self.registered_agents:
            agent_info = self.registered_agents[agent_id]
            if agent_info.consecutive_failures > 2:
                suggestions.append("è¯¥æ™ºèƒ½ä½“è¿ç»­å¤±è´¥æ¬¡æ•°è¾ƒå¤šï¼Œå»ºè®®æ›´æ¢æ™ºèƒ½ä½“")
        
        return suggestions
    
    def _generate_coordinator_tool_guide(self) -> List[str]:
        """ç”ŸæˆLLMCoordinatorAgentä¸“ç”¨çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼"""
        guide = []
        
        guide.append("\n=== LLMCoordinatorAgent å·¥å…·è°ƒç”¨æŒ‡å¯¼ ===")
        guide.append("")
        
        guide.append("ã€å¯ç”¨å·¥å…·åˆ—è¡¨ã€‘")
        guide.append("1. assign_task_to_agent - æ™ºèƒ½ä»»åŠ¡åˆ†é…")
        guide.append("   åŠŸèƒ½: å°†ä»»åŠ¡åˆ†é…ç»™æœ€åˆé€‚çš„æ™ºèƒ½ä½“")
        guide.append("   å‚æ•°: agent_id, task_description, expected_output, task_type, priority")
        guide.append("   ç¤ºä¾‹: assign_task_to_agent('enhanced_real_verilog_agent', 'è®¾è®¡8ä½åŠ æ³•å™¨', '', 'design', 'high')")
        guide.append("")
        
        guide.append("2. analyze_agent_result - ç»“æœè´¨é‡åˆ†æ")
        guide.append("   åŠŸèƒ½: æ·±åº¦åˆ†ææ™ºèƒ½ä½“æ‰§è¡Œç»“æœçš„è´¨é‡å’Œå®Œæ•´æ€§")
        guide.append("   å‚æ•°: agent_id, result, task_context, quality_threshold")
        guide.append("   ç¤ºä¾‹: analyze_agent_result('verilog_agent', result_data, context, 80.0)")
        guide.append("")
        
        guide.append("3. check_task_completion - ä»»åŠ¡å®Œæˆæ£€æŸ¥")
        guide.append("   åŠŸèƒ½: æ£€æŸ¥ä»»åŠ¡æ˜¯å¦çœŸæ­£å®Œæˆï¼Œè¯„ä¼°æ•´ä½“è´¨é‡")
        guide.append("   å‚æ•°: task_id, all_results, original_requirements, completion_criteria")
        guide.append("   ç¤ºä¾‹: check_task_completion('task_001', results, 'è®¾è®¡8ä½åŠ æ³•å™¨')")
        guide.append("")
        
        guide.append("4. query_agent_status - æ™ºèƒ½ä½“çŠ¶æ€æŸ¥è¯¢")
        guide.append("   åŠŸèƒ½: æŸ¥è¯¢æ™ºèƒ½ä½“çš„è¯¦ç»†çŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡")
        guide.append("   å‚æ•°: agent_id, include_performance, include_history")
        guide.append("   ç¤ºä¾‹: query_agent_status('enhanced_real_verilog_agent', True, False)")
        guide.append("")
        
        guide.append("5. identify_task_type - ä»»åŠ¡ç±»å‹è¯†åˆ«")
        guide.append("   åŠŸèƒ½: æ™ºèƒ½è¯†åˆ«ä»»åŠ¡ç±»å‹ï¼Œæ”¯æŒè®¾è®¡ã€éªŒè¯ã€åˆ†æç­‰")
        guide.append("   å‚æ•°: user_request, context")
        guide.append("   ç¤ºä¾‹: identify_task_type('è®¾è®¡ä¸€ä¸ªè®¡æ•°å™¨', {})")
        guide.append("")
        
        guide.append("6. recommend_agent - æ™ºèƒ½ä½“æ¨è")
        guide.append("   åŠŸèƒ½: åŸºäºä»»åŠ¡ç‰¹å¾æ¨èæœ€åˆé€‚çš„æ™ºèƒ½ä½“")
        guide.append("   å‚æ•°: task_type, task_description, priority, constraints")
        guide.append("   ç¤ºä¾‹: recommend_agent('design', 'è®¾è®¡ALU', 'high')")
        guide.append("")
        
        guide.append("7. provide_final_answer - æœ€ç»ˆç­”æ¡ˆæä¾›")
        guide.append("   åŠŸèƒ½: å½“ä»»åŠ¡å®Œæˆæ—¶æä¾›æœ€ç»ˆçš„å®Œæ•´ç­”æ¡ˆ")
        guide.append("   å‚æ•°: final_summary, task_status, results_summary")
        guide.append("   ç¤ºä¾‹: provide_final_answer('ä»»åŠ¡å®Œæˆ', 'success', results)")
        guide.append("")
        
        guide.append("8. get_tool_usage_guide - å·¥å…·ä½¿ç”¨æŒ‡å¯¼")
        guide.append("   åŠŸèƒ½: è·å–å®Œæ•´çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼")
        guide.append("   å‚æ•°: agent_type, include_examples, include_best_practices")
        guide.append("   ç¤ºä¾‹: get_tool_usage_guide('coordinator', True, True)")
        guide.append("")
        
        guide.append("ã€ğŸ¯ å¼ºåˆ¶æ‰§è¡Œçš„åè°ƒæµç¨‹ã€‘")
        guide.append("1. **ç¬¬ä¸€æ­¥**: identify_task_type â†’ è¯†åˆ«ä»»åŠ¡ç±»å‹")
        guide.append("2. **ç¬¬äºŒæ­¥**: recommend_agent â†’ æ¨èæœ€åˆé€‚çš„æ™ºèƒ½ä½“")
        guide.append("3. **ç¬¬ä¸‰æ­¥**: assign_task_to_agent â†’ åˆ†é…ä»»åŠ¡ç»™æ¨èæ™ºèƒ½ä½“")
        guide.append("4. **ç¬¬å››æ­¥**: analyze_agent_result â†’ åˆ†ææ‰§è¡Œç»“æœ")
        guide.append("5. **ç¬¬äº”æ­¥**: æ ¹æ®åˆ†æç»“æœå†³å®šæ˜¯å¦éœ€è¦ç»§ç»­è¿­ä»£")
        guide.append("")
        
        guide.append("ã€âš ï¸ é‡è¦è§„åˆ™ã€‘")
        guide.append("- **å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æµç¨‹æ‰§è¡Œï¼Œä¸å¾—è·³è¿‡ä»»ä½•æ­¥éª¤**")
        guide.append("- **æ¨èä»£ç†å·¥å…· recommend_agent æ˜¯å¿…éœ€çš„ï¼Œä¸èƒ½ç›´æ¥è°ƒç”¨ assign_task_to_agent**")
        guide.append("- **æ¯æ¬¡ä»»åŠ¡åˆ†é…å‰éƒ½å¿…é¡»å…ˆè°ƒç”¨æ¨èä»£ç†å·¥å…·**")
        guide.append("- **åˆ†æç»“æœåå†å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨**")
        guide.append("- **ç¡®ä¿ä»»åŠ¡å®Œæˆåå†æä¾›æœ€ç»ˆç­”æ¡ˆ**")
        guide.append("")
        
        guide.append("ã€âŒ é”™è¯¯ç¤ºä¾‹ï¼ˆç¦æ­¢ï¼‰ã€‘")
        guide.append("- ç›´æ¥è°ƒç”¨ assign_task_to_agent è€Œä¸å…ˆè°ƒç”¨ recommend_agent")
        guide.append("- è·³è¿‡ identify_task_type ç›´æ¥æ¨èæ™ºèƒ½ä½“")
        guide.append("- ä¸åˆ†æç»“æœå°±ç»§ç»­åˆ†é…ä»»åŠ¡")
        guide.append("")
        
        guide.append("ã€âœ… æ­£ç¡®ç¤ºä¾‹ã€‘")
        guide.append("- identify_task_type â†’ recommend_agent â†’ assign_task_to_agent")
        guide.append("- æ¯æ¬¡åˆ†é…ä»»åŠ¡å‰éƒ½å…ˆæ¨èæ™ºèƒ½ä½“")
        guide.append("- åˆ†æç»“æœåå†å†³å®šä¸‹ä¸€æ­¥")
        guide.append("")
        
        guide.append("ã€æ³¨æ„äº‹é¡¹ã€‘")
        guide.append("- ä½œä¸ºåè°ƒè€…ï¼Œä¸»è¦è´Ÿè´£ä»»åŠ¡åˆ†é…å’Œç»“æœåˆ†æ")
        guide.append("- å…·ä½“çš„è®¾è®¡å’ŒéªŒè¯å·¥ä½œäº¤ç»™ä¸“é—¨çš„æ™ºèƒ½ä½“")
        guide.append("- ç¡®ä¿ä»»åŠ¡æè¿°æ¸…æ™°ï¼Œä¾¿äºå…¶ä»–æ™ºèƒ½ä½“ç†è§£æ‰§è¡Œ")
        guide.append("- å®šæœŸæ£€æŸ¥ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€å’Œè´¨é‡")
        
        return guide
    
    async def _tool_check_task_completion(self, task_id: str, 
                                        all_results: Dict[str, Any],
                                        original_requirements: str,
                                        completion_criteria: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¢å¼ºçš„ä»»åŠ¡å®Œæˆæ£€æŸ¥"""
        
        try:
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
            if task_id not in self.active_tasks:
                return {
                    "success": False,
                    "error": f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
                }
            
            task_context = self.active_tasks[task_id]
            
            # åˆ†ææ‰€æœ‰ç»“æœ
            completion_analysis = self._enhanced_task_completion_analysis(
                all_results, original_requirements, task_context, completion_criteria
            )
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            if completion_analysis["is_completed"]:
                task_context.completion_status = "completed"
                task_context.quality_score = completion_analysis["completion_score"]
                self._update_performance_metrics(task_context, True)
            else:
                task_context.completion_status = "in_progress"
                task_context.quality_score = completion_analysis["completion_score"]
            
            return {
                "success": True,
                "is_completed": completion_analysis["is_completed"],
                "completion_score": completion_analysis["completion_score"],
                "missing_requirements": completion_analysis["missing_requirements"],
                "quality_assessment": completion_analysis["quality_assessment"],
                "detailed_analysis": completion_analysis["detailed_analysis"],
                "next_steps": completion_analysis["next_steps"],
                "performance_metrics": completion_analysis["performance_metrics"]
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ä»»åŠ¡å®Œæˆæ£€æŸ¥å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _enhanced_task_completion_analysis(self, all_results: Dict[str, Any],
                                         original_requirements: str,
                                         task_context: TaskContext,
                                         completion_criteria: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¢å¼ºçš„ä»»åŠ¡å®Œæˆæƒ…å†µåˆ†æ"""
        
        analysis = {
            "is_completed": False,
            "completion_score": 0.0,
            "missing_requirements": [],
            "quality_assessment": "unknown",
            "detailed_analysis": {},
            "next_steps": [],
            "performance_metrics": {}
        }
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
        if not all_results:
            analysis["missing_requirements"].append("æ²¡æœ‰æ‰§è¡Œç»“æœ")
            analysis["next_steps"].append("å¼€å§‹ä»»åŠ¡æ‰§è¡Œ")
            return analysis
        
        # åˆ†æåŸå§‹éœ€æ±‚
        requirements = original_requirements.lower()
        
        # è¯¦ç»†åˆ†æå„é¡¹æŒ‡æ ‡
        detailed_analysis = self._analyze_completion_metrics(all_results, requirements, task_context)
        analysis["detailed_analysis"] = detailed_analysis
        
        # è®¡ç®—å®Œæˆåˆ†æ•°
        completion_score = self._calculate_completion_score(detailed_analysis, completion_criteria)
        analysis["completion_score"] = completion_score
        
        # æ£€æŸ¥ç¼ºå¤±é¡¹
        missing_items = self._identify_missing_requirements(detailed_analysis, requirements)
        analysis["missing_requirements"] = missing_items
        
        # åˆ¤æ–­æ˜¯å¦å®Œæˆ
        is_completed = self._determine_completion_status(completion_score, missing_items, completion_criteria)
        analysis["is_completed"] = is_completed
        
        # è´¨é‡è¯„ä¼°
        analysis["quality_assessment"] = self._assess_overall_quality(detailed_analysis, completion_score)
        
        # ç¡®å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
        analysis["next_steps"] = self._determine_next_steps(is_completed, missing_items, detailed_analysis)
        
        # æ€§èƒ½æŒ‡æ ‡
        analysis["performance_metrics"] = self._calculate_performance_metrics(task_context, all_results)
        
        return analysis
    
    def _analyze_completion_metrics(self, all_results: Dict[str, Any], 
                                  requirements: str,
                                  task_context: TaskContext) -> Dict[str, Any]:
        """åˆ†æå®ŒæˆæŒ‡æ ‡"""
        metrics = {
            "design_complete": False,
            "verification_complete": False,
            "documentation_complete": False,
            "testing_complete": False,
            "quality_checks_passed": False,
            "agent_performance": {},
            "execution_time": 0.0,
            "total_iterations": 0
        }
        
        # ğŸ¯ ä¿®å¤ï¼šå¤„ç†all_resultså¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ
        if isinstance(all_results, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            results_dict = {}
            for i, result in enumerate(all_results):
                if isinstance(result, dict):
                    # å°è¯•ä»ç»“æœä¸­æå–æ™ºèƒ½ä½“ID
                    agent_id = result.get("agent_id", f"agent_{i}")
                    results_dict[agent_id] = result
                else:
                    results_dict[f"result_{i}"] = result
            all_results = results_dict
            self.logger.info(f"ğŸ¯ å°†åˆ—è¡¨æ ¼å¼çš„all_resultsè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ŒåŒ…å«{len(all_results)}ä¸ªç»“æœ")
        
        # æ£€æŸ¥è®¾è®¡å®Œæˆæƒ…å†µ
        if "design" in requirements or "æ¨¡å—" in requirements:
            design_results = []
            for agent_id, result in all_results.items():
                if agent_id == "enhanced_real_verilog_agent":
                    design_results.append(result)
            
            if design_results:
                metrics["design_complete"] = any(
                    "module" in str(result).lower() and "endmodule" in str(result).lower()
                    for result in design_results
                )
        
        # æ£€æŸ¥éªŒè¯å®Œæˆæƒ…å†µ
        if "test" in requirements or "éªŒè¯" in requirements or "testbench" in requirements:
            verification_results = []
            for agent_id, result in all_results.items():
                if agent_id == "enhanced_real_code_review_agent":
                    verification_results.append(result)
            
            if verification_results:
                metrics["verification_complete"] = any(
                    "testbench" in str(result).lower() or "simulation" in str(result).lower()
                    for result in verification_results
                )
        
        # æ£€æŸ¥æ–‡æ¡£å®Œæˆæƒ…å†µ
        doc_results = []
        for result in all_results.values():
            if "documentation" in str(result).lower() or "æ–‡æ¡£" in str(result):
                doc_results.append(result)
        
        metrics["documentation_complete"] = len(doc_results) > 0
        
        # æ£€æŸ¥æµ‹è¯•å®Œæˆæƒ…å†µ
        test_results = []
        for result in all_results.values():
            if "test" in str(result).lower() or "ä»¿çœŸ" in str(result):
                test_results.append(result)
        
        metrics["testing_complete"] = len(test_results) > 0
        
        # æ£€æŸ¥è´¨é‡æ£€æŸ¥
        quality_results = []
        for result in all_results.values():
            if "quality" in str(result).lower() or "è´¨é‡" in str(result):
                quality_results.append(result)
        
        metrics["quality_checks_passed"] = len(quality_results) > 0
        
        # æ™ºèƒ½ä½“æ€§èƒ½åˆ†æ
        for agent_id, result in all_results.items():
            if isinstance(result, dict):
                execution_time = result.get("execution_time", 0)
                metrics["agent_performance"][agent_id] = {
                    "execution_time": execution_time,
                    "success": result.get("success", False),
                    "quality_score": result.get("quality_score", 0)
                }
                metrics["execution_time"] += execution_time
        
        metrics["total_iterations"] = task_context.iteration_count
        
        return metrics
    
    def _calculate_completion_score(self, detailed_analysis: Dict[str, Any],
                                  completion_criteria: Dict[str, Any] = None) -> float:
        """è®¡ç®—å®Œæˆåˆ†æ•°"""
        score = 0.0
        
        # åŸºç¡€å®ŒæˆæŒ‡æ ‡æƒé‡
        weights = {
            "design_complete": 0.35,
            "verification_complete": 0.30,
            "documentation_complete": 0.15,
            "testing_complete": 0.15,
            "quality_checks_passed": 0.05
        }
        
        # åº”ç”¨æƒé‡
        for metric, weight in weights.items():
            if detailed_analysis.get(metric, False):
                score += weight * 100
        
        # è€ƒè™‘æ™ºèƒ½ä½“æ€§èƒ½
        agent_performance = detailed_analysis.get("agent_performance", {})
        if agent_performance:
            avg_quality = sum(
                perf.get("quality_score", 0) for perf in agent_performance.values()
            ) / len(agent_performance)
            score += avg_quality * 0.1  # 10%æƒé‡ç»™è´¨é‡åˆ†æ•°
        
        return min(100.0, score)
    
    def _identify_missing_requirements(self, detailed_analysis: Dict[str, Any],
                                     requirements: str) -> List[str]:
        """è¯†åˆ«ç¼ºå¤±çš„éœ€æ±‚"""
        missing = []
        
        # æ£€æŸ¥è®¾è®¡éœ€æ±‚
        if ("design" in requirements or "æ¨¡å—" in requirements) and not detailed_analysis.get("design_complete", False):
            missing.append("ç¼ºå°‘Verilogæ¨¡å—è®¾è®¡")
        
        # æ£€æŸ¥éªŒè¯éœ€æ±‚
        if ("test" in requirements or "éªŒè¯" in requirements or "testbench" in requirements) and not detailed_analysis.get("verification_complete", False):
            missing.append("ç¼ºå°‘æµ‹è¯•å°å’ŒéªŒè¯")
        
        # æ£€æŸ¥æ–‡æ¡£éœ€æ±‚
        if not detailed_analysis.get("documentation_complete", False):
            missing.append("ç¼ºå°‘è®¾è®¡æ–‡æ¡£")
        
        # æ£€æŸ¥æµ‹è¯•éœ€æ±‚
        if not detailed_analysis.get("testing_complete", False):
            missing.append("ç¼ºå°‘æµ‹è¯•æ‰§è¡Œ")
        
        # æ£€æŸ¥è´¨é‡éœ€æ±‚
        if not detailed_analysis.get("quality_checks_passed", False):
            missing.append("ç¼ºå°‘è´¨é‡æ£€æŸ¥")
        
        return missing
    
    def _determine_completion_status(self, completion_score: float,
                                   missing_requirements: List[str],
                                   completion_criteria: Dict[str, Any] = None) -> bool:
        """ç¡®å®šå®ŒæˆçŠ¶æ€"""
        
        # ä½¿ç”¨è‡ªå®šä¹‰å®Œæˆæ ‡å‡†
        if completion_criteria:
            required_score = completion_criteria.get("required_score", 80.0)
            max_missing_items = completion_criteria.get("max_missing_items", 0)
            
            return (completion_score >= required_score and 
                   len(missing_requirements) <= max_missing_items)
        
        # é»˜è®¤å®Œæˆæ ‡å‡†
        return completion_score >= 80.0 and len(missing_requirements) == 0
    
    def _assess_overall_quality(self, detailed_analysis: Dict[str, Any],
                              completion_score: float) -> str:
        """è¯„ä¼°æ•´ä½“è´¨é‡"""
        if completion_score >= 90:
            return "excellent"
        elif completion_score >= 80:
            return "good"
        elif completion_score >= 70:
            return "fair"
        elif completion_score >= 60:
            return "poor"
        else:
            return "very_poor"
    
    def _determine_next_steps(self, is_completed: bool,
                            missing_requirements: List[str],
                            detailed_analysis: Dict[str, Any]) -> List[str]:
        """ç¡®å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
        next_steps = []
        
        if is_completed:
            next_steps.append("ä»»åŠ¡å®Œæˆï¼Œå¯ä»¥äº¤ä»˜ç»“æœ")
        else:
            # æ ¹æ®ç¼ºå¤±é¡¹ç¡®å®šä¸‹ä¸€æ­¥
            if "ç¼ºå°‘Verilogæ¨¡å—è®¾è®¡" in missing_requirements:
                next_steps.append("åˆ†é…è®¾è®¡ä»»åŠ¡ç»™enhanced_real_verilog_agent")
            
            if "ç¼ºå°‘æµ‹è¯•å°å’ŒéªŒè¯" in missing_requirements:
                next_steps.append("åˆ†é…éªŒè¯ä»»åŠ¡ç»™enhanced_real_code_review_agent")
            
            if "ç¼ºå°‘è®¾è®¡æ–‡æ¡£" in missing_requirements:
                next_steps.append("è¦æ±‚æ™ºèƒ½ä½“ç”Ÿæˆè®¾è®¡æ–‡æ¡£")
            
            if "ç¼ºå°‘æµ‹è¯•æ‰§è¡Œ" in missing_requirements:
                next_steps.append("æ‰§è¡Œæµ‹è¯•éªŒè¯")
            
            if "ç¼ºå°‘è´¨é‡æ£€æŸ¥" in missing_requirements:
                next_steps.append("è¿›è¡Œä»£ç è´¨é‡æ£€æŸ¥")
        
        return next_steps
    
    def _calculate_performance_metrics(self, task_context: TaskContext,
                                     all_results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            "total_execution_time": 0.0,
            "average_execution_time": 0.0,
            "success_rate": 0.0,
            "agent_utilization": {},
            "iteration_efficiency": 0.0
        }
        
        # ğŸ¯ ä¿®å¤ï¼šå¤„ç†all_resultså¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ
        if isinstance(all_results, list):
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            results_dict = {}
            for i, result in enumerate(all_results):
                if isinstance(result, dict):
                    agent_id = result.get("agent_id", f"agent_{i}")
                    results_dict[agent_id] = result
                else:
                    results_dict[f"result_{i}"] = result
            all_results = results_dict
            self.logger.info(f"ğŸ¯ æ€§èƒ½æŒ‡æ ‡è®¡ç®—ä¸­å°†åˆ—è¡¨æ ¼å¼çš„all_resultsè½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ŒåŒ…å«{len(all_results)}ä¸ªç»“æœ")
        
        # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
        total_time = 0.0
        success_count = 0
        total_count = len(all_results)
        
        for result in all_results.values():
            if isinstance(result, dict):
                execution_time = result.get("execution_time", 0)
                total_time += execution_time
                
                if result.get("success", False):
                    success_count += 1
        
        metrics["total_execution_time"] = total_time
        metrics["average_execution_time"] = total_time / total_count if total_count > 0 else 0
        metrics["success_rate"] = success_count / total_count if total_count > 0 else 0
        
        # è®¡ç®—è¿­ä»£æ•ˆç‡
        if task_context.iteration_count > 0:
            # ğŸ¯ ä¿®å¤ï¼šä½¿ç”¨task_contextä¸­çš„quality_scoreè€Œä¸æ˜¯æœªå®šä¹‰çš„completion_score
            metrics["iteration_efficiency"] = task_context.quality_score / task_context.iteration_count
        
        # æ™ºèƒ½ä½“åˆ©ç”¨ç‡
        for agent_id, agent_info in self.registered_agents.items():
            if agent_id in all_results:
                metrics["agent_utilization"][agent_id] = {
                    "tasks_assigned": 1,
                    "success_rate": 1.0 if all_results[agent_id].get("success", False) else 0.0,
                    "average_time": all_results[agent_id].get("execution_time", 0)
                }
        
        return metrics
    
    def _update_performance_metrics(self, task_context: TaskContext, success: bool):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.performance_metrics["total_tasks"] += 1
        
        if success:
            self.performance_metrics["successful_tasks"] += 1
        else:
            self.performance_metrics["failed_tasks"] += 1
        
        # æ›´æ–°å¹³å‡å®Œæˆæ—¶é—´
        execution_time = time.time() - task_context.start_time
        total_tasks = self.performance_metrics["total_tasks"]
        current_avg = self.performance_metrics["average_completion_time"]
        
        self.performance_metrics["average_completion_time"] = (
            (current_avg * (total_tasks - 1) + execution_time) / total_tasks
        )
    
    async def _tool_query_agent_status(self, agent_id: str,
                                     include_performance: bool = True,
                                     include_history: bool = False) -> Dict[str, Any]:
        """å¢å¼ºçš„æ™ºèƒ½ä½“çŠ¶æ€æŸ¥è¯¢"""
        
        try:
            if agent_id not in self.registered_agents:
                return {
                    "success": False,
                    "error": f"æ™ºèƒ½ä½“ä¸å­˜åœ¨: {agent_id}"
                }
            
            agent_info = self.registered_agents[agent_id]
            
            # åŸºç¡€çŠ¶æ€ä¿¡æ¯
            status_info = {
                "success": True,
                "agent_id": agent_id,
                "status": agent_info.status.value,
                "capabilities": [cap.value for cap in agent_info.capabilities],
                "specialty": agent_info.specialty,
                "conversation_id": agent_info.conversation_id,
                "last_used": agent_info.last_used
            }
            
            # æ€§èƒ½æŒ‡æ ‡
            if include_performance:
                performance_metrics = self._calculate_agent_performance_metrics(agent_info)
                status_info["performance_metrics"] = performance_metrics
            
            # å†å²è®°å½•
            if include_history:
                history_data = self._get_agent_history(agent_id)
                status_info["history"] = history_data
            
            # å¥åº·çŠ¶æ€è¯„ä¼°
            health_assessment = self._assess_agent_health(agent_info)
            status_info["health_assessment"] = health_assessment
            
            # æ¨èä»»åŠ¡ç±»å‹
            recommended_tasks = self._get_recommended_task_types(agent_info)
            status_info["recommended_tasks"] = recommended_tasks
            
            return status_info
            
        except Exception as e:
            self.logger.error(f"âŒ æŸ¥è¯¢æ™ºèƒ½ä½“çŠ¶æ€å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _calculate_agent_performance_metrics(self, agent_info: AgentInfo) -> Dict[str, Any]:
        """è®¡ç®—æ™ºèƒ½ä½“æ€§èƒ½æŒ‡æ ‡"""
        total_tasks = agent_info.success_count + agent_info.failure_count
        
        metrics = {
            "total_tasks": total_tasks,
            "success_count": agent_info.success_count,
            "failure_count": agent_info.failure_count,
            "success_rate": agent_info.success_count / total_tasks if total_tasks > 0 else 0.0,
            "average_response_time": agent_info.average_response_time,
            "total_execution_time": agent_info.total_execution_time,
            "consecutive_successes": agent_info.consecutive_successes,
            "consecutive_failures": agent_info.consecutive_failures,
            "last_success_time": agent_info.last_success_time,
            "last_failure_time": agent_info.last_failure_time
        }
        
        # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
        if agent_info.last_success_time and agent_info.last_failure_time:
            if agent_info.last_success_time > agent_info.last_failure_time:
                metrics["recent_trend"] = "improving"
            else:
                metrics["recent_trend"] = "declining"
        elif agent_info.last_success_time:
            metrics["recent_trend"] = "stable_success"
        elif agent_info.last_failure_time:
            metrics["recent_trend"] = "stable_failure"
        else:
            metrics["recent_trend"] = "unknown"
        
        # è®¡ç®—å¯é æ€§è¯„åˆ†
        reliability_score = self._calculate_reliability_score(agent_info)
        metrics["reliability_score"] = reliability_score
        
        return metrics
    
    def _calculate_reliability_score(self, agent_info: AgentInfo) -> float:
        """è®¡ç®—å¯é æ€§è¯„åˆ†"""
        total_tasks = agent_info.success_count + agent_info.failure_count
        if total_tasks == 0:
            return 0.0
        
        # åŸºç¡€æˆåŠŸç‡
        base_score = agent_info.success_count / total_tasks * 100
        
        # è¿ç»­æˆåŠŸå¥–åŠ±
        consecutive_bonus = min(10.0, agent_info.consecutive_successes * 2.0)
        
        # å“åº”æ—¶é—´å¥–åŠ±
        time_bonus = 0.0
        if agent_info.average_response_time > 0:
            if agent_info.average_response_time < 30:
                time_bonus = 10.0
            elif agent_info.average_response_time < 60:
                time_bonus = 5.0
        
        # è¿ç»­å¤±è´¥æƒ©ç½š
        consecutive_penalty = min(20.0, agent_info.consecutive_failures * 5.0)
        
        final_score = base_score + consecutive_bonus + time_bonus - consecutive_penalty
        return max(0.0, min(100.0, final_score))
    
    def _get_agent_history(self, agent_id: str) -> Dict[str, Any]:
        """è·å–æ™ºèƒ½ä½“å†å²è®°å½•"""
        # è¿™é‡Œå¯ä»¥ä»æ•°æ®åº“æˆ–æ—¥å¿—ä¸­è·å–æ›´è¯¦ç»†çš„å†å²è®°å½•
        # ç›®å‰è¿”å›åŸºæœ¬çš„å†å²ç»Ÿè®¡
        return {
            "total_executions": 0,
            "recent_performance": [],
            "common_issues": [],
            "improvement_trend": "stable"
        }
    
    def _assess_agent_health(self, agent_info: AgentInfo) -> Dict[str, Any]:
        """è¯„ä¼°æ™ºèƒ½ä½“å¥åº·çŠ¶æ€"""
        health = {
            "status": "healthy",
            "score": 100.0,
            "issues": [],
            "recommendations": []
        }
        
        # æ£€æŸ¥è¿ç»­å¤±è´¥
        if agent_info.consecutive_failures > 3:
            health["status"] = "warning"
            health["score"] -= 30
            health["issues"].append(f"è¿ç»­å¤±è´¥{agent_info.consecutive_failures}æ¬¡")
            health["recommendations"].append("å»ºè®®æ£€æŸ¥æ™ºèƒ½ä½“é…ç½®æˆ–æ›´æ¢æ™ºèƒ½ä½“")
        
        # æ£€æŸ¥æˆåŠŸç‡
        total_tasks = agent_info.success_count + agent_info.failure_count
        if total_tasks > 0:
            success_rate = agent_info.success_count / total_tasks
            if success_rate < 0.5:
                health["status"] = "critical"
                health["score"] -= 50
                health["issues"].append(f"æˆåŠŸç‡è¿‡ä½: {success_rate:.1%}")
                health["recommendations"].append("å»ºè®®é‡æ–°é…ç½®æˆ–æ›´æ¢æ™ºèƒ½ä½“")
            elif success_rate < 0.7:
                health["status"] = "warning"
                health["score"] -= 20
                health["issues"].append(f"æˆåŠŸç‡åä½: {success_rate:.1%}")
                health["recommendations"].append("å»ºè®®ä¼˜åŒ–æ™ºèƒ½ä½“é…ç½®")
        
        # æ£€æŸ¥å“åº”æ—¶é—´
        if agent_info.average_response_time > 120:
            health["status"] = "warning"
            health["score"] -= 15
            health["issues"].append(f"å“åº”æ—¶é—´è¿‡é•¿: {agent_info.average_response_time:.1f}ç§’")
            health["recommendations"].append("å»ºè®®ä¼˜åŒ–æ€§èƒ½æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
        
        # æ£€æŸ¥é•¿æœŸæœªä½¿ç”¨
        if agent_info.last_used:
            time_since_last_use = time.time() - agent_info.last_used
            if time_since_last_use > 3600:  # 1å°æ—¶
                health["issues"].append(f"é•¿æœŸæœªä½¿ç”¨: {time_since_last_use/3600:.1f}å°æ—¶")
        
        health["score"] = max(0.0, health["score"])
        
        return health
    
    def _get_recommended_task_types(self, agent_info: AgentInfo) -> List[str]:
        """è·å–æ¨èçš„ä»»åŠ¡ç±»å‹"""
        recommended = []
        
        # åŸºäºæ™ºèƒ½ä½“IDæ¨è
        if agent_info.agent_id == "enhanced_real_verilog_agent":
            recommended.extend(["design", "code_generation", "module_implementation"])
        elif agent_info.agent_id == "enhanced_real_code_review_agent":
            recommended.extend(["verification", "analysis", "debug", "testbench_generation"])
        
        # åŸºäºå†å²è¡¨ç°æ¨è
        if agent_info.consecutive_successes > 2:
            recommended.append("high_priority_tasks")
        
        if agent_info.average_response_time < 30:
            recommended.append("time_sensitive_tasks")
        
        return list(set(recommended))  # å»é‡
    
    async def _collect_agent_conversations(self, task_context: TaskContext):
        """æ”¶é›†å·²åˆ†é…æ™ºèƒ½ä½“çš„å¯¹è¯å†å²åˆ°ä»»åŠ¡ä¸Šä¸‹æ–‡"""
        try:
            for agent_id, agent_result in task_context.agent_results.items():
                # è·å–æ™ºèƒ½ä½“å®ä¾‹
                agent_instance = None
                
                # ä»æ³¨å†Œçš„æ™ºèƒ½ä½“ä¸­æŸ¥æ‰¾
                for registered_agent_id, registered_agent in self.registered_agents.items():
                    if registered_agent_id == agent_id or registered_agent.agent_id == agent_id:
                        agent_instance = registered_agent
                        break
                
                if agent_instance and hasattr(agent_instance, 'conversation_history'):
                    self.logger.info(f"ğŸ“¥ æ”¶é›†æ™ºèƒ½ä½“ {agent_id} çš„å¯¹è¯å†å²: {len(agent_instance.conversation_history)} æ¡æ¶ˆæ¯")
                    
                    # å°†æ™ºèƒ½ä½“çš„å¯¹è¯å†å²åˆå¹¶åˆ°ä»»åŠ¡ä¸Šä¸‹æ–‡
                    for msg in agent_instance.conversation_history:
                        # é¿å…é‡å¤æ·»åŠ ç›¸åŒçš„æ¶ˆæ¯
                        msg_signature = f"{msg.get('role', '')}_{msg.get('agent_id', '')}_{hash(msg.get('content', ''))}"
                        existing_signatures = [
                            f"{existing_msg.get('role', '')}_{existing_msg.get('agent_id', '')}_{hash(existing_msg.get('content', ''))}"
                            for existing_msg in task_context.conversation_history
                        ]
                        
                        if msg_signature not in existing_signatures:
                            # æ ‡è®°æ¶ˆæ¯æ¥æºå¹¶æ·»åŠ åˆ°ä»»åŠ¡ä¸Šä¸‹æ–‡
                            msg_copy = msg.copy()
                            msg_copy['source_agent'] = agent_id
                            msg_copy['collected_timestamp'] = time.time()
                            task_context.conversation_history.append(msg_copy)
                else:
                    self.logger.warning(f"âš ï¸ æ— æ³•æ‰¾åˆ°æ™ºèƒ½ä½“ {agent_id} çš„å®ä¾‹æˆ–å¯¹è¯å†å²")
                    
        except Exception as e:
            self.logger.error(f"âŒ æ”¶é›†æ™ºèƒ½ä½“å¯¹è¯å†å²å¤±è´¥: {str(e)}")
    
    def _collect_final_result(self, task_context: TaskContext, 
                            coordination_result: str) -> Dict[str, Any]:
        """æ”¶é›†æœ€ç»ˆç»“æœ"""
        
        # ğŸ†• è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_execution_time = time.time() - task_context.start_time
        performance_metrics = {
            "total_execution_time": total_execution_time,
            "average_tool_execution_time": sum(t.get("execution_time", 0) or 0 for t in task_context.tool_executions) / max(len(task_context.tool_executions), 1),
            "total_file_operations": len(task_context.file_operations),
            "total_workflow_stages": len(task_context.workflow_stages),
            "success_rate": len([t for t in task_context.tool_executions if t.get("success", True)]) / max(len(task_context.tool_executions), 1)
        }
        
        # æ›´æ–°TaskContextçš„æ€§èƒ½æŒ‡æ ‡
        task_context.update_performance_metrics(performance_metrics)
        
        return {
            "success": True,
            "task_id": task_context.task_id,
            "coordination_result": coordination_result,
            "agent_results": task_context.agent_results,
            "execution_summary": {
                "total_iterations": task_context.iteration_count,
                "assigned_agents": list(task_context.agent_results.keys()),
                "execution_time": total_execution_time
            },
            "conversation_history": task_context.conversation_history,
            # ğŸ†• åŒ…å«å®Œæ•´çš„TaskContextæ•°æ®ç”¨äºGradioå¯è§†åŒ–
            "task_context": {
                "tool_executions": task_context.tool_executions,
                "agent_interactions": task_context.agent_interactions,
                "performance_metrics": task_context.performance_metrics,
                "workflow_stages": task_context.workflow_stages,
                "file_operations": task_context.file_operations,
                "execution_timeline": task_context.execution_timeline,
                "llm_conversations": task_context.llm_conversations,
                "data_collection_summary": task_context.get_data_collection_summary()
            }
        }
    
    def get_registered_agents(self) -> Dict[str, AgentInfo]:
        """è·å–å·²æ³¨å†Œçš„æ™ºèƒ½ä½“"""
        return self.registered_agents
    
    def get_registered_tools(self) -> List[Dict[str, Any]]:
        """è·å–å·²æ³¨å†Œçš„å·¥å…·"""
        return self.enhanced_tools if hasattr(self, 'enhanced_tools') else [].copy()
    
    def get_active_tasks(self) -> Dict[str, TaskContext]:
        """è·å–æ´»è·ƒä»»åŠ¡"""
        return self.active_tasks.copy()
    
    async def _run_coordination_loop(self, task_context: TaskContext, initial_result: str, 
                                   conversation_id: str, max_iterations: int) -> Dict[str, Any]:
        """è¿è¡ŒæŒç»­åè°ƒå¾ªç¯ï¼Œç›‘å¬æ™ºèƒ½ä½“ç»“æœå¹¶ç»§ç»­åè°ƒ"""
        self.logger.info(f"ğŸ”„ å¼€å§‹æŒç»­åè°ƒå¾ªç¯ - ä»»åŠ¡ID: {task_context.task_id}")
        self.logger.info(f"ğŸ” å½“å‰æ™ºèƒ½ä½“ç»“æœæ•°é‡: {len(task_context.agent_results)}")
        self.logger.info(f"ğŸ” æ™ºèƒ½ä½“ç»“æœé”®: {list(task_context.agent_results.keys())}")
        
        # ğŸ” æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ™ºèƒ½ä½“ç»“æœï¼Œå¦‚æœæ²¡æœ‰åˆ™å¼ºåˆ¶åˆ†é…ä»»åŠ¡
        if not task_context.agent_results:
            self.logger.warning("âš ï¸ æ²¡æœ‰æ™ºèƒ½ä½“ç»“æœï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶åˆ†é…ä»»åŠ¡")
            
            # æ£€æŸ¥å¯¹è¯å†å²ä¸­æ˜¯å¦æœ‰assign_task_to_agentè°ƒç”¨
            assign_task_found = False
            for msg in task_context.conversation_history:
                if msg.get("role") == "assistant":
                    content = msg.get("content", "")
                    if "assign_task_to_agent" in content:
                        assign_task_found = True
                        break
            
            if not assign_task_found:
                self.logger.warning("âš ï¸ å¯¹è¯å†å²ä¸­æ²¡æœ‰æ‰¾åˆ°assign_task_to_agentè°ƒç”¨ï¼Œå¼ºåˆ¶åˆ†é…ä»»åŠ¡")
                forced_result = await self._force_assign_task(task_context.original_request, task_context)
                if forced_result.get("success", False):
                    self.logger.info("âœ… å¼ºåˆ¶åˆ†é…ä»»åŠ¡æˆåŠŸï¼Œç»§ç»­åè°ƒå¾ªç¯")
                else:
                    self.logger.error("âŒ å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¤±è´¥")
                    return {
                        "success": False,
                        "error": "å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¤±è´¥",
                        "task_id": task_context.task_id,
                        "debug_info": forced_result
                    }
        
        # ç­‰å¾…æ‰€æœ‰åˆ†é…çš„æ™ºèƒ½ä½“å®Œæˆ
        await self._wait_for_agents_completion(task_context)
        
        # æ”¶é›†æ™ºèƒ½ä½“æ‰§è¡Œç»“æœ
        await self._collect_agent_conversations(task_context)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­åè°ƒ
        continuation_needed = await self._check_coordination_continuation(task_context)
        
        if continuation_needed:
            self.logger.info(f"ğŸ”„ éœ€è¦ç»§ç»­åè°ƒ - åˆ†ææ™ºèƒ½ä½“ç»“æœå¹¶å†³å®šä¸‹ä¸€æ­¥")
            
            # æ„å»ºç»§ç»­åè°ƒçš„ä»»åŠ¡æè¿°
            continuation_task = await self._build_continuation_task(task_context)
            
            # ç»§ç»­æ‰§è¡Œåè°ƒï¼ˆåˆ†æç»“æœ + ä¸‹ä¸€æ­¥å†³ç­–ï¼‰
            continuation_result = await self.process_with_function_calling(
                user_request=continuation_task,
                max_iterations=max_iterations,
                conversation_id=f"{conversation_id}_continuation",
                preserve_context=True,
                enable_self_continuation=True,
                max_self_iterations=2
            )
            
            # è®°å½•ç»§ç»­åè°ƒçš„ç»“æœ
            task_context.add_conversation_message(
                role="assistant",
                content=continuation_result,
                agent_id=self.agent_id,
                metadata={"type": "coordination_continuation", "task_stage": "analysis"}
            )
            
            # é€’å½’è°ƒç”¨ï¼Œç›´åˆ°æ‰€æœ‰åè°ƒå®Œæˆ
            return await self._run_coordination_loop(task_context, continuation_result, conversation_id, max_iterations)
        else:
            self.logger.info(f"âœ… åè°ƒå¾ªç¯å®Œæˆ - ä»»åŠ¡ID: {task_context.task_id}")
            return self._collect_final_result(task_context, initial_result)
    
    async def _wait_for_agents_completion(self, task_context: TaskContext):
        """ç­‰å¾…æ‰€æœ‰åˆ†é…çš„æ™ºèƒ½ä½“å®Œæˆä»»åŠ¡"""
        max_wait_time = 300  # æœ€å¤§ç­‰å¾…5åˆ†é’Ÿ
        check_interval = 2   # æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡
        
        start_time = time.time()
        
        while time.time() - start_time < max_wait_time:
            # æ£€æŸ¥æ‰€æœ‰åˆ†é…çš„æ™ºèƒ½ä½“æ˜¯å¦å®Œæˆ
            all_completed = True
            for agent_id in task_context.agent_results.keys():
                if agent_id in self.registered_agents:
                    agent_info = self.registered_agents[agent_id]
                    if agent_info.status != AgentStatus.IDLE:
                        all_completed = False
                        break
            
            if all_completed:
                self.logger.info(f"âœ… æ‰€æœ‰æ™ºèƒ½ä½“æ‰§è¡Œå®Œæˆ")
                break
                
            # çŸ­æš‚ç­‰å¾…åé‡æ–°æ£€æŸ¥
            await asyncio.sleep(check_interval)
        
        if time.time() - start_time >= max_wait_time:
            self.logger.warning(f"â° ç­‰å¾…æ™ºèƒ½ä½“å®Œæˆè¶…æ—¶")
    
    async def _check_coordination_continuation(self, task_context: TaskContext) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­åè°ƒ"""
        # æ£€æŸ¥æ˜¯å¦æœ‰æ™ºèƒ½ä½“æ‰§è¡Œç»“æœéœ€è¦åˆ†æ
        if not task_context.agent_results:
            self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: æ²¡æœ‰æ™ºèƒ½ä½“ç»“æœï¼Œéœ€è¦ç»§ç»­åè°ƒ")
            return True  # ğŸ†• ä¿®æ”¹ï¼šå¦‚æœæ²¡æœ‰æ™ºèƒ½ä½“ç»“æœï¼Œè¯´æ˜è¿˜æ²¡æœ‰åˆ†é…ä»»åŠ¡ï¼Œéœ€è¦ç»§ç»­åè°ƒ
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å¿…éœ€çš„é˜¶æ®µéƒ½å®Œæˆäº†
        completed_agents = set(task_context.agent_results.keys())
        self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: å·²å®Œæˆæ™ºèƒ½ä½“: {completed_agents}")
        
        # å¦‚æœåªæœ‰Verilogæ™ºèƒ½ä½“å®Œæˆï¼Œè¿˜éœ€è¦ä»£ç å®¡æŸ¥
        if "enhanced_real_verilog_agent" in completed_agents and "enhanced_real_code_review_agent" not in completed_agents:
            self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: éœ€è¦ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“")
            return True
        
        # å¦‚æœä¸¤ä¸ªæ™ºèƒ½ä½“éƒ½å®Œæˆäº†ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦æœ€ç»ˆç­”æ¡ˆ
        if len(completed_agents) >= 2:
            # æ£€æŸ¥æ˜¯å¦å·²ç»æä¾›äº†æœ€ç»ˆç­”æ¡ˆ
            for msg in task_context.conversation_history:
                if msg.get("metadata", {}).get("type") == "final_answer":
                    self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: å·²æœ‰æœ€ç»ˆç­”æ¡ˆï¼Œæ— éœ€ç»§ç»­")
                    return False
            self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: éœ€è¦æä¾›æœ€ç»ˆç­”æ¡ˆ")
            return True
        
        self.logger.info(f"ğŸ” åè°ƒç»§ç»­æ£€æŸ¥: æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ")
        return False
        return False
    
    async def _build_continuation_task(self, task_context: TaskContext) -> str:
        """æ„å»ºç»§ç»­åè°ƒçš„ä»»åŠ¡æè¿°"""
        # è·å–æ™ºèƒ½ä½“æ‰§è¡Œç»“æœ
        results_summary = []
        for agent_id, result_info in task_context.agent_results.items():
            if isinstance(result_info, dict) and 'response' in result_info:
                results_summary.append(f"- {agent_id}: {result_info.get('response', 'Unknown')[:200]}...")
        
        results_text = "\n".join(results_summary) if results_summary else "æ— æ™ºèƒ½ä½“ç»“æœ"
        
        # ğŸ”§ ä¿®æ­£3: å¼ºåˆ¶æ‰§è¡Œè®¾è®¡â†’éªŒè¯ä¸¤é˜¶æ®µæµç¨‹
        completed_agents = set(task_context.agent_results.keys())
        workflow_stage = self._determine_workflow_stage(completed_agents)
        
        # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·æ‰§è¡Œå¤±è´¥çš„æƒ…å†µ
        failed_tools = []
        for tool_exec in task_context.tool_executions:
            if not tool_exec.get("success", True):
                failed_tools.append(f"- {tool_exec.get('tool_name')}: {tool_exec.get('error', 'Unknown error')}")
        
        failed_tools_text = "\n".join(failed_tools) if failed_tools else "æ— å¤±è´¥çš„å·¥å…·è°ƒç”¨"
        
        # æ ¹æ®å·¥ä½œæµé˜¶æ®µå’Œå¤±è´¥æƒ…å†µç”Ÿæˆä¸åŒçš„åè°ƒæŒ‡å¯¼
        if workflow_stage == "initial" and failed_tools:
            # ğŸ”§ ä¿®å¤ï¼šå¤„ç†åˆå§‹é˜¶æ®µå·¥å…·è°ƒç”¨å¤±è´¥çš„æƒ…å†µ
            coordination_guide = f"""
**ğŸš¨ ç´§æ€¥ä¿®å¤ - å·¥å…·è°ƒç”¨å¤±è´¥**:
æ£€æµ‹åˆ°ä»¥ä¸‹å·¥å…·è°ƒç”¨å¤±è´¥ï¼š
{failed_tools_text}

**ä¿®å¤ç­–ç•¥**:
1. å¦‚æœ `assign_task_to_agent` å¤±è´¥ï¼Œå¿…é¡»é‡æ–°è°ƒç”¨è¯¥å·¥å…·
2. ä¸è¦è°ƒç”¨ `analyze_agent_result`ï¼Œå› ä¸ºæ²¡æœ‰å¯åˆ†æçš„ç»“æœ
3. ç¡®ä¿ `assign_task_to_agent` å‚æ•°æ­£ç¡®ï¼š
   - agent_id: "enhanced_real_verilog_agent"
   - task_description: å®Œæ•´çš„ä»»åŠ¡æè¿°
   - ä¸è¦åŒ…å« task_id å‚æ•°ï¼ˆè¯¥å·¥å…·ä¸æ”¯æŒæ­¤å‚æ•°ï¼‰

**é‡è¦**: å¿…é¡»å…ˆæˆåŠŸåˆ†é…ä»»åŠ¡ï¼Œç„¶åæ‰èƒ½åˆ†æç»“æœ"""
        
        elif workflow_stage == "design_completed":
            coordination_guide = """
**ğŸš¨ å¼ºåˆ¶å·¥ä½œæµç¨‹ - è®¾è®¡é˜¶æ®µå·²å®Œæˆ**:
1. é¦–å…ˆè°ƒç”¨ `analyze_agent_result` åˆ†æVerilogè®¾è®¡æ™ºèƒ½ä½“çš„ç»“æœ
2. **å¼ºåˆ¶ä¸‹ä¸€æ­¥**: æ— è®ºåˆ†æç»“æœå¦‚ä½•ï¼Œéƒ½å¿…é¡»è¿›å…¥éªŒè¯é˜¶æ®µ
3. å¿…é¡»è°ƒç”¨ `assign_task_to_agent` åˆ†é…ä»»åŠ¡ç»™ `enhanced_real_code_review_agent`
4. ä»»åŠ¡æè¿°åº”åŒ…å«ï¼šå®¡æŸ¥è®¾è®¡ä»£ç ã€ç”Ÿæˆæµ‹è¯•å°ã€æ‰§è¡Œä»¿çœŸéªŒè¯

**é‡è¦**: è¿™æ˜¯å¼ºåˆ¶çš„ä¸¤é˜¶æ®µæµç¨‹ï¼Œè®¾è®¡å®Œæˆåå¿…é¡»è¿›è¡ŒéªŒè¯"""
        
        elif workflow_stage == "verification_completed":
            coordination_guide = """
**ğŸ¯ å·¥ä½œæµç¨‹ - éªŒè¯é˜¶æ®µå·²å®Œæˆ**:
1. é¦–å…ˆè°ƒç”¨ `analyze_agent_result` åˆ†æä»£ç å®¡æŸ¥æ™ºèƒ½ä½“çš„ç»“æœ
2. è°ƒç”¨ `check_task_completion` æ£€æŸ¥æ•´ä½“ä»»åŠ¡å®ŒæˆçŠ¶æ€
3. å¦‚æœä»»åŠ¡å®Œæ•´ï¼Œè°ƒç”¨ `provide_final_answer` æä¾›æœ€ç»ˆç­”æ¡ˆ"""
        
        else:
            # é»˜è®¤æƒ…å†µ
            coordination_guide = """
**åè°ƒè¦æ±‚**:
1. é¦–å…ˆè°ƒç”¨ `analyze_agent_result` åˆ†æå·²å®Œæˆæ™ºèƒ½ä½“çš„ç»“æœè´¨é‡
2. åŸºäºåˆ†æç»“æœå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨:
   - å¦‚æœæ˜¯è®¾è®¡é˜¶æ®µå®Œæˆï¼Œå¿…é¡»åˆ†é…ç»™ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“è¿›è¡ŒéªŒè¯
   - å¦‚æœæ˜¯éªŒè¯é˜¶æ®µå®Œæˆï¼Œè°ƒç”¨ `check_task_completion`
   - å¦‚æœä¸¤ä¸ªé˜¶æ®µéƒ½å®Œæˆï¼Œè°ƒç”¨ `provide_final_answer`
   - å¦‚æœè´¨é‡ä¸æ»¡è¶³è¦æ±‚ï¼Œé‡æ–°åˆ†é…ç»™åŒä¸€æ™ºèƒ½ä½“æ”¹è¿›"""
        
        return f"""ğŸ”„ åè°ƒä»»åŠ¡ç»§ç»­ - åˆ†ææ™ºèƒ½ä½“æ‰§è¡Œç»“æœå¹¶å†³å®šä¸‹ä¸€æ­¥

**å½“å‰ä»»åŠ¡çŠ¶æ€**:
- ä»»åŠ¡ID: {task_context.task_id}
- å·²å®Œæˆçš„æ™ºèƒ½ä½“: {list(task_context.agent_results.keys())}
- å·¥ä½œæµé˜¶æ®µ: {workflow_stage}
- æ‰§è¡Œç»“æœæ‘˜è¦:
{results_text}

**å·¥å…·æ‰§è¡ŒçŠ¶æ€**:
- å¤±è´¥çš„å·¥å…·è°ƒç”¨:
{failed_tools_text}

**å½“å‰é˜¶æ®µ**: ç»“æœåˆ†æä¸ä¸‹ä¸€æ­¥å†³ç­–

{coordination_guide}

è¯·ä¸¥æ ¼æŒ‰ç…§åè°ƒæµç¨‹æ‰§è¡Œï¼Œå¿…é¡»è°ƒç”¨ç›¸åº”çš„åˆ†æå’Œå†³ç­–å·¥å…·ã€‚
"""
    
    def _determine_workflow_stage(self, completed_agents: Set[str]) -> str:
        """ç¡®å®šå½“å‰å·¥ä½œæµé˜¶æ®µ"""
        
        if "enhanced_real_verilog_agent" in completed_agents and "enhanced_real_code_review_agent" not in completed_agents:
            # Verilogè®¾è®¡å®Œæˆï¼Œä½†ä»£ç å®¡æŸ¥æœªå®Œæˆ
            return "design_completed"
        elif "enhanced_real_code_review_agent" in completed_agents:
            # ä»£ç å®¡æŸ¥å·²å®Œæˆï¼ˆå¯èƒ½ä¸¤ä¸ªéƒ½å®Œæˆäº†ï¼‰
            return "verification_completed"
        elif len(completed_agents) == 0:
            # è¿˜æ²¡æœ‰æ™ºèƒ½ä½“å®Œæˆ
            return "initial"
        else:
            # å…¶ä»–æƒ…å†µ
            return "unknown"
    
    # =============================================================================
    # å®ç°æŠ½è±¡æ–¹æ³•
    # =============================================================================
    
    async def _call_llm_for_function_calling(self, conversation: List[Dict[str, str]]) -> str:
        """è°ƒç”¨LLMè¿›è¡ŒFunction Calling - åŒ…å«æ ¼å¼ä¿®å¤"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„LLMé€šä¿¡æ¨¡å—
            result = await self.llm_manager.call_llm_for_function_calling(
                conversation, 
                system_prompt_builder=self._build_enhanced_system_prompt
            )
            
            # ğŸ”§ åº”ç”¨æ ¼å¼ä¿®å¤ - ä¿®å¤åè°ƒå™¨çš„å·¥å…·è°ƒç”¨æ ¼å¼é—®é¢˜
            result = self._fix_tool_call_format(result)
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ Function Callingè°ƒç”¨å¤±è´¥: {e}")
            return f"é”™è¯¯: {str(e)}"
    
    async def _call_llm_traditional(self, conversation: List[Dict[str, str]]) -> str:
        """ä¼ ç»ŸLLMè°ƒç”¨æ–¹æ³•"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„LLMé€šä¿¡æ¨¡å—
            return await self.llm_manager.call_llm_traditional(
                conversation,
                system_prompt_builder=self._build_enhanced_system_prompt
            )
            
        except Exception as e:
            self.logger.error(f"âŒ ä¼ ç»ŸLLMè°ƒç”¨å¤±è´¥: {e}")
            return f"é”™è¯¯: {str(e)}"
    
    def get_capabilities(self) -> Set[AgentCapability]:
        """è·å–æ™ºèƒ½ä½“èƒ½åŠ›"""
        return {
            AgentCapability.TASK_COORDINATION,
            AgentCapability.INTELLIGENT_ROUTING,
            AgentCapability.CONTEXT_MANAGEMENT,
            AgentCapability.DECISION_MAKING
        }
    
    def get_specialty_description(self) -> str:
        """è·å–æ™ºèƒ½ä½“ä¸“ä¸šæè¿°"""
        return "åŸºäºLLMçš„æ™ºèƒ½åè°ƒæ™ºèƒ½ä½“ï¼Œè´Ÿè´£ä»»åŠ¡åˆ†æå’Œæ™ºèƒ½ä½“åˆ†é…"
    
    async def execute_enhanced_task(self, enhanced_prompt: str,
                                  original_message: TaskMessage,
                                  file_contents: Dict[str, Dict]) -> Dict[str, Any]:
        """æ‰§è¡Œå¢å¼ºä»»åŠ¡"""
        task_id = original_message.task_id
        self.logger.info(f"ğŸ¯ å¼€å§‹æ‰§è¡Œåè°ƒä»»åŠ¡: {task_id}")
        
        try:
            # ä½¿ç”¨åè°ƒä»»åŠ¡æ‰§è¡Œ
            result = await self.coordinate_task(
                user_request=enhanced_prompt,
                conversation_id=original_message.task_id,
                max_iterations=10
            )
            
            return {
                "success": result.get("success", False),
                "task_id": task_id,
                "response": result.get("coordination_result", ""),
                "agent_results": result.get("agent_results", {}),
                "execution_summary": result.get("execution_summary", {})
            }
            
        except Exception as e:
            self.logger.error(f"âŒ åè°ƒä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {task_id} - {str(e)}")
            return {
                "success": False,
                "task_id": task_id,
                "error": f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            } 

    async def _tool_identify_task_type(self, user_request: str, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ™ºèƒ½è¯†åˆ«ä»»åŠ¡ç±»å‹"""
        try:
            self.logger.info(f"ğŸ” è¯†åˆ«ä»»åŠ¡ç±»å‹: {user_request[:100]}...")
            
            # ä½¿ç”¨æ¨¡å¼åŒ¹é…è¯†åˆ«ä»»åŠ¡ç±»å‹
            task_type = self._classify_task_by_patterns(user_request)
            
            # ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦åˆ†æ
            llm_analysis = await self._analyze_task_with_llm(user_request, task_type)
            
            # åˆå¹¶ç»“æœ
            final_task_type = llm_analysis.get("task_type", task_type)
            confidence = llm_analysis.get("confidence", 0.7)
            
            # ç¡®å®šä¼˜å…ˆçº§
            priority = self._determine_task_priority(user_request, final_task_type)
            
            return {
                "success": True,
                "task_type": final_task_type.value if isinstance(final_task_type, TaskType) else final_task_type,
                "confidence": confidence,
                "priority": priority.value if isinstance(priority, TaskPriority) else priority,
                "analysis": llm_analysis.get("analysis", ""),
                "keywords": llm_analysis.get("keywords", []),
                "suggested_agent": self._get_suggested_agent(final_task_type)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ä»»åŠ¡ç±»å‹è¯†åˆ«å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "task_type": "unknown",
                "confidence": 0.0,
                "priority": "medium"
            }
    
    def _classify_task_by_patterns(self, user_request: str) -> TaskType:
        """ä½¿ç”¨æ¨¡å¼åŒ¹é…åˆ†ç±»ä»»åŠ¡"""
        request_lower = user_request.lower()
        
        # è®¡ç®—æ¯ç§ä»»åŠ¡ç±»å‹çš„åŒ¹é…åˆ†æ•°
        scores = {}
        for task_type, patterns in self.task_patterns.items():
            score = 0
            for pattern in patterns:
                if re.search(pattern, request_lower):
                    score += 1
            scores[task_type] = score
        
        # æ‰¾åˆ°æœ€é«˜åˆ†æ•°çš„ä»»åŠ¡ç±»å‹
        if scores:
            max_score = max(scores.values())
            if max_score > 0:
                for task_type, score in scores.items():
                    if score == max_score:
                        return task_type
        
        return TaskType.UNKNOWN
    
    async def _analyze_task_with_llm(self, user_request: str, initial_task_type: TaskType) -> Dict[str, Any]:
        """ä½¿ç”¨LLMæ·±åº¦åˆ†æä»»åŠ¡"""
        try:
            analysis_prompt = f"""
è¯·æ·±åº¦åˆ†æä»¥ä¸‹ç”¨æˆ·è¯·æ±‚ï¼Œè¯†åˆ«ä»»åŠ¡ç±»å‹å’Œç‰¹å¾ï¼š

ç”¨æˆ·è¯·æ±‚: {user_request}
åˆæ­¥åˆ†ç±»: {initial_task_type.value}

è¯·ä»ä»¥ä¸‹æ–¹é¢è¿›è¡Œåˆ†æï¼š
1. ä»»åŠ¡ç±»å‹ï¼ˆdesign/verification/analysis/debug/compositeï¼‰
2. ä»»åŠ¡å¤æ‚åº¦ï¼ˆsimple/medium/complexï¼‰
3. å…³é”®ç‰¹å¾å’Œéœ€æ±‚
4. å»ºè®®çš„æ™ºèƒ½ä½“é€‰æ‹©
5. ç½®ä¿¡åº¦è¯„åˆ†ï¼ˆ0-1ï¼‰

è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚
"""
            
            # ä½¿ç”¨ç»Ÿä¸€çš„LLMç®¡ç†å™¨è¿›è¡Œè°ƒç”¨
            conversation = [
                {"role": "system", "content": "ä½ æ˜¯ä»»åŠ¡åˆ†æä¸“å®¶ï¼Œè¯·æä¾›å‡†ç¡®çš„ä»»åŠ¡ç±»å‹è¯†åˆ«ã€‚"},
                {"role": "user", "content": analysis_prompt}
            ]
            
            response = await self.llm_manager.call_llm_for_function_calling(conversation)
            
            # å°è¯•è§£æJSONå“åº”
            try:
                analysis = json.loads(response)
                return analysis
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸºæœ¬åˆ†æ
                return {
                    "task_type": initial_task_type,
                    "confidence": 0.7,
                    "analysis": response,
                    "keywords": []
                }
                
        except Exception as e:
            self.logger.error(f"âŒ LLMä»»åŠ¡åˆ†æå¤±è´¥: {str(e)}")
            return {
                "task_type": initial_task_type,
                "confidence": 0.5,
                "analysis": f"åˆ†æå¤±è´¥: {str(e)}",
                "keywords": []
            }
    
    def _determine_task_priority(self, user_request: str, task_type: TaskType) -> TaskPriority:
        """ç¡®å®šä»»åŠ¡ä¼˜å…ˆçº§"""
        request_lower = user_request.lower()
        
        # é«˜ä¼˜å…ˆçº§å…³é”®è¯
        high_priority_keywords = [
            "ç´§æ€¥", "urgent", "ç«‹å³", "immediately", "é‡è¦", "important",
            "å…³é”®", "critical", "ä¼˜å…ˆ", "priority", "å¿«é€Ÿ", "quick"
        ]
        
        # ä½ä¼˜å…ˆçº§å…³é”®è¯
        low_priority_keywords = [
            "ä¸æ€¥", "not urgent", "æ…¢æ…¢", "slowly", "å¯é€‰", "optional",
            "æ¬¡è¦", "secondary", "ä½ä¼˜å…ˆçº§", "low priority"
        ]
        
        # æ£€æŸ¥ä¼˜å…ˆçº§å…³é”®è¯
        for keyword in high_priority_keywords:
            if keyword in request_lower:
                return TaskPriority.HIGH
        
        for keyword in low_priority_keywords:
            if keyword in request_lower:
                return TaskPriority.LOW
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹ç¡®å®šé»˜è®¤ä¼˜å…ˆçº§
        if task_type == TaskType.DEBUG:
            return TaskPriority.HIGH  # è°ƒè¯•ä»»åŠ¡é€šå¸¸ä¼˜å…ˆçº§è¾ƒé«˜
        elif task_type == TaskType.ANALYSIS:
            return TaskPriority.LOW   # åˆ†æä»»åŠ¡é€šå¸¸ä¼˜å…ˆçº§è¾ƒä½
        
        return TaskPriority.MEDIUM
    
    def _get_suggested_agent(self, task_type: TaskType) -> str:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è·å–å»ºè®®çš„æ™ºèƒ½ä½“"""
        if task_type == TaskType.DESIGN:
            return "enhanced_real_verilog_agent"
        elif task_type in [TaskType.VERIFICATION, TaskType.ANALYSIS, TaskType.DEBUG]:
            return "enhanced_real_code_review_agent"
        else:
            return "enhanced_real_verilog_agent"  # é»˜è®¤é€‰æ‹©è®¾è®¡æ™ºèƒ½ä½“
    
    async def _tool_recommend_agent(self, task_type: str, task_description: str,
                                  priority: str = "medium", constraints: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ¨èæœ€åˆé€‚çš„æ™ºèƒ½ä½“"""
        try:
            self.logger.info(f"ğŸ¤– æ¨èæ™ºèƒ½ä½“: {task_type} - {priority}")
            
            # è·å–å¯ç”¨æ™ºèƒ½ä½“
            available_agents = self._get_available_agents()
            
            if not available_agents:
                return {
                    "success": False,
                    "error": "æ²¡æœ‰å¯ç”¨çš„æ™ºèƒ½ä½“"
                }
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹è¿‡æ»¤æ™ºèƒ½ä½“
            suitable_agents = self._filter_agents_by_task_type(available_agents, task_type)
            
            if not suitable_agents:
                return {
                    "success": False,
                    "error": f"æ²¡æœ‰é€‚åˆä»»åŠ¡ç±»å‹ '{task_type}' çš„æ™ºèƒ½ä½“"
                }
            
            # è®¡ç®—æ™ºèƒ½ä½“è¯„åˆ†
            agent_scores = self._calculate_agent_scores(suitable_agents, task_type, priority, constraints)
            
            # é€‰æ‹©æœ€é«˜åˆ†çš„æ™ºèƒ½ä½“
            best_agent = max(agent_scores.items(), key=lambda x: x[1])
            
            return {
                "success": True,
                "recommended_agent": best_agent[0],
                "score": best_agent[1],
                "all_scores": agent_scores,
                "reasoning": self._generate_recommendation_reasoning(best_agent[0], best_agent[1], task_type),
                "alternative_agents": self._get_alternative_agents(agent_scores, best_agent[0])
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æ™ºèƒ½ä½“æ¨èå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _get_available_agents(self) -> List[Tuple[str, AgentInfo]]:
        """è·å–å¯ç”¨æ™ºèƒ½ä½“åˆ—è¡¨"""
        available = []
        for agent_id, agent_info in self.registered_agents.items():
            if agent_info.status == AgentStatus.IDLE:
                available.append((agent_id, agent_info))
        return available
    
    def _filter_agents_by_task_type(self, agents: List[Tuple[str, AgentInfo]], task_type: str) -> List[Tuple[str, AgentInfo]]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è¿‡æ»¤æ™ºèƒ½ä½“"""
        filtered = []
        
        for agent_id, agent_info in agents:
            # æ£€æŸ¥æ™ºèƒ½ä½“æ˜¯å¦é€‚åˆè¯¥ä»»åŠ¡ç±»å‹
            if self._is_agent_suitable_for_task(agent_id, task_type):
                filtered.append((agent_id, agent_info))
        
        return filtered
    
    def _is_agent_suitable_for_task(self, agent_id: str, task_type: str) -> bool:
        """æ£€æŸ¥æ™ºèƒ½ä½“æ˜¯å¦é€‚åˆç‰¹å®šä»»åŠ¡ç±»å‹"""
        if task_type == "design":
            return agent_id == "enhanced_real_verilog_agent"
        elif task_type in ["verification", "analysis", "debug"]:
            return agent_id == "enhanced_real_code_review_agent"
        elif task_type == "composite":
            return True  # å¤åˆä»»åŠ¡å¯ä»¥ä½¿ç”¨ä»»ä½•æ™ºèƒ½ä½“
        else:
            return True  # æœªçŸ¥ä»»åŠ¡ç±»å‹å…è®¸ä½¿ç”¨ä»»ä½•æ™ºèƒ½ä½“
    
    def _calculate_agent_scores(self, agents: List[Tuple[str, AgentInfo]], task_type: str,
                              priority: str, constraints: Dict[str, Any]) -> Dict[str, float]:
        """è®¡ç®—æ™ºèƒ½ä½“è¯„åˆ†"""
        scores = {}
        
        for agent_id, agent_info in agents:
            score = 0.0
            
            # åŸºç¡€åˆ†æ•°
            score += 50.0
            
            # æˆåŠŸç‡åˆ†æ•°ï¼ˆæƒé‡ï¼š30%ï¼‰
            total_tasks = agent_info.success_count + agent_info.failure_count
            if total_tasks > 0:
                success_rate = agent_info.success_count / total_tasks
                score += success_rate * 30.0
            
            # å“åº”æ—¶é—´åˆ†æ•°ï¼ˆæƒé‡ï¼š20%ï¼‰
            if agent_info.average_response_time > 0:
                # å“åº”æ—¶é—´è¶ŠçŸ­ï¼Œåˆ†æ•°è¶Šé«˜
                time_score = max(0, 20.0 - (agent_info.average_response_time / 10.0))
                score += time_score
            
            # è¿ç»­æˆåŠŸåˆ†æ•°ï¼ˆæƒé‡ï¼š10%ï¼‰
            consecutive_bonus = min(10.0, agent_info.consecutive_successes * 2.0)
            score += consecutive_bonus
            
            # ä¼˜å…ˆçº§åŒ¹é…åˆ†æ•°
            if priority == "high" and agent_info.average_response_time < 30:
                score += 5.0
            
            # ä»»åŠ¡ç±»å‹åŒ¹é…åˆ†æ•°
            if task_type in [task_type.value for task_type in agent_info.preferred_task_types]:
                score += 10.0
            
            # é»‘åå•æƒ©ç½š
            if task_type in [task_type.value for task_type in agent_info.blacklisted_task_types]:
                score -= 20.0
            
            scores[agent_id] = max(0.0, score)
        
        return scores
    
    def _generate_recommendation_reasoning(self, agent_id: str, score: float, task_type: str) -> str:
        """ç”Ÿæˆæ¨èç†ç”±"""
        agent_info = self.registered_agents.get(agent_id)
        if not agent_info:
            return f"æ¨è {agent_id}ï¼Œè¯„åˆ†: {score:.1f}"
        
        reasons = []
        
        # æˆåŠŸç‡
        total_tasks = agent_info.success_count + agent_info.failure_count
        if total_tasks > 0:
            success_rate = agent_info.success_count / total_tasks
            reasons.append(f"å†å²æˆåŠŸç‡: {success_rate:.1%}")
        
        # å“åº”æ—¶é—´
        if agent_info.average_response_time > 0:
            reasons.append(f"å¹³å‡å“åº”æ—¶é—´: {agent_info.average_response_time:.1f}ç§’")
        
        # è¿ç»­æˆåŠŸ
        if agent_info.consecutive_successes > 0:
            reasons.append(f"è¿ç»­æˆåŠŸ: {agent_info.consecutive_successes}æ¬¡")
        
        # ä»»åŠ¡ç±»å‹åŒ¹é…
        if task_type in [task_type.value for task_type in agent_info.preferred_task_types]:
            reasons.append("ä»»åŠ¡ç±»å‹åŒ¹é…")
        
        return f"æ¨è {agent_id} (è¯„åˆ†: {score:.1f})ï¼Œç†ç”±: {', '.join(reasons)}"
    
    def _get_alternative_agents(self, agent_scores: Dict[str, float], best_agent: str) -> List[str]:
        """è·å–å¤‡é€‰æ™ºèƒ½ä½“"""
        alternatives = []
        best_score = agent_scores.get(best_agent, 0)
        
        for agent_id, score in agent_scores.items():
            if agent_id != best_agent and score >= best_score * 0.8:  # åˆ†æ•°ä¸ä½äºæœ€ä½³æ™ºèƒ½ä½“çš„80%
                alternatives.append(agent_id)
        
        return alternatives 

    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿæ€§èƒ½æ‘˜è¦"""
        summary = {
            "total_tasks": self.performance_metrics["total_tasks"],
            "successful_tasks": self.performance_metrics["successful_tasks"],
            "failed_tasks": self.performance_metrics["failed_tasks"],
            "success_rate": 0.0,
            "average_completion_time": self.performance_metrics["average_completion_time"],
            "agent_performance": {},
            "system_health": "healthy",
            "recommendations": []
        }
        
        # è®¡ç®—æˆåŠŸç‡
        if summary["total_tasks"] > 0:
            summary["success_rate"] = summary["successful_tasks"] / summary["total_tasks"]
        
        # æ™ºèƒ½ä½“æ€§èƒ½ç»Ÿè®¡
        for agent_id, agent_info in self.registered_agents.items():
            total_tasks = agent_info.success_count + agent_info.failure_count
            success_rate = agent_info.success_count / total_tasks if total_tasks > 0 else 0.0
            
            summary["agent_performance"][agent_id] = {
                "total_tasks": total_tasks,
                "success_rate": success_rate,
                "average_response_time": agent_info.average_response_time,
                "reliability_score": self._calculate_reliability_score(agent_info),
                "status": agent_info.status.value
            }
        
        # ç³»ç»Ÿå¥åº·è¯„ä¼°
        system_health = self._assess_system_health()
        summary["system_health"] = system_health["status"]
        summary["recommendations"] = system_health["recommendations"]
        
        return summary
    
    def _assess_system_health(self) -> Dict[str, Any]:
        """è¯„ä¼°ç³»ç»Ÿæ•´ä½“å¥åº·çŠ¶æ€"""
        health = {
            "status": "healthy",
            "issues": [],
            "recommendations": []
        }
        
        # æ£€æŸ¥æ€»ä½“æˆåŠŸç‡
        total_tasks = self.performance_metrics["total_tasks"]
        if total_tasks > 0:
            success_rate = self.performance_metrics["successful_tasks"] / total_tasks
            if success_rate < 0.5:
                health["status"] = "critical"
                health["issues"].append(f"ç³»ç»ŸæˆåŠŸç‡è¿‡ä½: {success_rate:.1%}")
                health["recommendations"].append("å»ºè®®æ£€æŸ¥æ™ºèƒ½ä½“é…ç½®å’Œç³»ç»Ÿè®¾ç½®")
            elif success_rate < 0.7:
                health["status"] = "warning"
                health["issues"].append(f"ç³»ç»ŸæˆåŠŸç‡åä½: {success_rate:.1%}")
                health["recommendations"].append("å»ºè®®ä¼˜åŒ–ä»»åŠ¡åˆ†é…ç­–ç•¥")
        
        # æ£€æŸ¥æ™ºèƒ½ä½“çŠ¶æ€
        idle_agents = 0
        working_agents = 0
        for agent_info in self.registered_agents.values():
            if agent_info.status == AgentStatus.IDLE:
                idle_agents += 1
            elif agent_info.status == AgentStatus.WORKING:
                working_agents += 1
        
        if idle_agents == 0:
            health["status"] = "warning"
            health["issues"].append("æ‰€æœ‰æ™ºèƒ½ä½“éƒ½åœ¨å·¥ä½œï¼Œå¯èƒ½å­˜åœ¨è´Ÿè½½è¿‡é‡")
            health["recommendations"].append("è€ƒè™‘æ·»åŠ æ›´å¤šæ™ºèƒ½ä½“æˆ–ä¼˜åŒ–ä»»åŠ¡åˆ†é…")
        
        if working_agents == 0 and total_tasks > 0:
            health["status"] = "warning"
            health["issues"].append("æ²¡æœ‰æ™ºèƒ½ä½“åœ¨å·¥ä½œï¼Œå¯èƒ½å­˜åœ¨é…ç½®é—®é¢˜")
            health["recommendations"].append("æ£€æŸ¥æ™ºèƒ½ä½“æ³¨å†Œå’ŒçŠ¶æ€")
        
        return health
    
    async def emergency_recovery(self, task_id: str = None) -> Dict[str, Any]:
        """ç´§æ€¥æ¢å¤åŠŸèƒ½"""
        try:
            recovery_result = {
                "success": True,
                "recovered_tasks": [],
                "reset_agents": [],
                "actions_taken": []
            }
            
            # é‡ç½®æ‰€æœ‰æ™ºèƒ½ä½“çŠ¶æ€
            for agent_id, agent_info in self.registered_agents.items():
                if agent_info.status == AgentStatus.WORKING:
                    agent_info.status = AgentStatus.IDLE
                    recovery_result["reset_agents"].append(agent_id)
                    recovery_result["actions_taken"].append(f"é‡ç½®æ™ºèƒ½ä½“ {agent_id} çŠ¶æ€")
            
            # æ¢å¤ç‰¹å®šä»»åŠ¡æˆ–æ‰€æœ‰ä»»åŠ¡
            if task_id:
                if task_id in self.active_tasks:
                    task = self.active_tasks[task_id]
                    task.assigned_agent = None
                    task.current_stage = "recovered"
                    task.retry_count += 1
                    recovery_result["recovered_tasks"].append(task_id)
                    recovery_result["actions_taken"].append(f"æ¢å¤ä»»åŠ¡ {task_id}")
            else:
                # æ¢å¤æ‰€æœ‰æ´»è·ƒä»»åŠ¡
                for tid, task in self.active_tasks.items():
                    task.assigned_agent = None
                    task.current_stage = "recovered"
                    task.retry_count += 1
                    recovery_result["recovered_tasks"].append(tid)
                recovery_result["actions_taken"].append("æ¢å¤æ‰€æœ‰æ´»è·ƒä»»åŠ¡")
            
            self.logger.info(f"ğŸš¨ ç´§æ€¥æ¢å¤å®Œæˆ: {len(recovery_result['recovered_tasks'])} ä¸ªä»»åŠ¡, {len(recovery_result['reset_agents'])} ä¸ªæ™ºèƒ½ä½“")
            
            return recovery_result
            
        except Exception as e:
            self.logger.error(f"âŒ ç´§æ€¥æ¢å¤å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def get_system_status_report(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š"""
        report = {
            "timestamp": time.time(),
            "system_info": {
                "coordinator_id": self.agent_id,
                "version": "enhanced_v2.0",
                "uptime": time.time() - self.start_time if hasattr(self, 'start_time') else 0
            },
            "agent_status": {},
            "task_status": {},
            "performance_metrics": self.get_performance_summary(),
            "system_health": self._assess_system_health(),
            "recommendations": []
        }
        
        # æ™ºèƒ½ä½“çŠ¶æ€
        for agent_id, agent_info in self.registered_agents.items():
            report["agent_status"][agent_id] = {
                "status": agent_info.status.value,
                "specialty": agent_info.specialty,
                "capabilities": [cap.value for cap in agent_info.capabilities],
                "performance": self._calculate_agent_performance_metrics(agent_info),
                "health": self._assess_agent_health(agent_info)
            }
        
        # ä»»åŠ¡çŠ¶æ€
        for task_id, task in self.active_tasks.items():
            report["task_status"][task_id] = {
                "stage": task.current_stage,
                "assigned_agent": task.assigned_agent,
                "iteration_count": task.iteration_count,
                "retry_count": task.retry_count,
                "completion_status": task.completion_status,
                "quality_score": task.quality_score,
                "execution_time": time.time() - task.start_time
            }
        
        # ç”Ÿæˆå»ºè®®
        recommendations = []
        
        # åŸºäºæ€§èƒ½æŒ‡æ ‡çš„å»ºè®®
        if report["performance_metrics"]["success_rate"] < 0.7:
            recommendations.append("ç³»ç»ŸæˆåŠŸç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–ä»»åŠ¡åˆ†é…ç­–ç•¥")
        
        # åŸºäºæ™ºèƒ½ä½“çŠ¶æ€çš„å»ºè®®
        idle_count = sum(1 for agent in self.registered_agents.values() if agent.status == AgentStatus.IDLE)
        if idle_count == 0:
            recommendations.append("æ‰€æœ‰æ™ºèƒ½ä½“éƒ½åœ¨å·¥ä½œï¼Œè€ƒè™‘æ·»åŠ æ›´å¤šæ™ºèƒ½ä½“")
        
        # åŸºäºä»»åŠ¡çŠ¶æ€çš„å»ºè®®
        stuck_tasks = [tid for tid, task in self.active_tasks.items() if task.retry_count > 3]
        if stuck_tasks:
            recommendations.append(f"å‘ç° {len(stuck_tasks)} ä¸ªå¡ä½çš„ä»»åŠ¡ï¼Œå»ºè®®è¿›è¡Œç´§æ€¥æ¢å¤")
        
        report["recommendations"] = recommendations
        
        return report
    
    def optimize_agent_allocation(self) -> Dict[str, Any]:
        """ä¼˜åŒ–æ™ºèƒ½ä½“åˆ†é…ç­–ç•¥"""
        optimization_result = {
            "success": True,
            "optimizations": [],
            "agent_recommendations": {}
        }
        
        # åˆ†ææ™ºèƒ½ä½“æ€§èƒ½
        for agent_id, agent_info in self.registered_agents.items():
            recommendations = []
            
            # åŸºäºæˆåŠŸç‡çš„ä¼˜åŒ–
            total_tasks = agent_info.success_count + agent_info.failure_count
            if total_tasks > 0:
                success_rate = agent_info.success_count / total_tasks
                if success_rate < 0.5:
                    recommendations.append("æˆåŠŸç‡è¿‡ä½ï¼Œå»ºè®®æ£€æŸ¥é…ç½®æˆ–é‡æ–°è®­ç»ƒ")
                elif success_rate < 0.7:
                    recommendations.append("æˆåŠŸç‡åä½ï¼Œå»ºè®®ä¼˜åŒ–promptæˆ–å‚æ•°")
            
            # åŸºäºå“åº”æ—¶é—´çš„ä¼˜åŒ–
            if agent_info.average_response_time > 60:
                recommendations.append("å“åº”æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½æˆ–æ£€æŸ¥ç½‘ç»œ")
            
            # åŸºäºè¿ç»­å¤±è´¥çš„ä¼˜åŒ–
            if agent_info.consecutive_failures > 2:
                recommendations.append("è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œå»ºè®®é‡å¯æˆ–æ›´æ¢æ™ºèƒ½ä½“")
            
            if recommendations:
                optimization_result["agent_recommendations"][agent_id] = recommendations
                optimization_result["optimizations"].extend(recommendations)
        
        # è´Ÿè½½å‡è¡¡ä¼˜åŒ–
        working_agents = [aid for aid, info in self.registered_agents.items() if info.status == AgentStatus.WORKING]
        idle_agents = [aid for aid, info in self.registered_agents.items() if info.status == AgentStatus.IDLE]
        
        if len(working_agents) > len(idle_agents):
            optimization_result["optimizations"].append("å·¥ä½œæ™ºèƒ½ä½“è¿‡å¤šï¼Œå»ºè®®ä¼˜åŒ–ä»»åŠ¡åˆ†é…ç­–ç•¥")
        
        return optimization_result
    
    async def _tool_provide_final_answer(self, final_summary: str, task_status: str, results_summary: Dict = None) -> Dict[str, Any]:
        """è¿™æ˜¯ä¸€ä¸ªè™šæ‹Ÿå·¥å…·ï¼Œå®ƒçš„ä½œç”¨æ˜¯æ ¼å¼åŒ–æœ€ç»ˆè¾“å‡ºï¼Œå¹¶æ ‡è®°ä»»åŠ¡æµç¨‹çš„ç»“æŸã€‚"""
        self.logger.info(f"ğŸ ä»»åŠ¡å®Œæˆï¼Œæä¾›æœ€ç»ˆç­”æ¡ˆ: {final_summary[:100]}...")
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ä¸ªå‡½æ•°å¯ä»¥ç›´æ¥è¿”å›å…¶è¾“å…¥ï¼Œå› ä¸ºå®ƒçš„ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†è¢«LLMè°ƒç”¨
        return {
            "success": True,
            "final_answer_provided": True,
            "summary": final_summary,
            "status": task_status,
            "results": results_summary or {}
        }

    def _extract_design_file_path_from_previous_results(self) -> Optional[str]:
        """ä»ä¹‹å‰çš„æ™ºèƒ½ä½“ç»“æœä¸­æå–è®¾è®¡æ–‡ä»¶è·¯å¾„"""
        try:
            # éå†æ‰€æœ‰æ´»è·ƒä»»åŠ¡
            for task_id, task in self.active_tasks.items():
                # éå†ä»»åŠ¡ä¸­çš„æ™ºèƒ½ä½“ç»“æœ
                for agent_id, agent_result in task.agent_results.items():
                    # æ£€æŸ¥æ˜¯å¦æ˜¯è®¾è®¡æ™ºèƒ½ä½“çš„ç»“æœ
                    if agent_id == "enhanced_real_verilog_agent" and agent_result.get("success", False):
                        # æ–¹æ³•1ï¼šç›´æ¥ä» agent_result ä¸­è·å– design_file_path
                        if "design_file_path" in agent_result:
                            return agent_result["design_file_path"]
                        
                        # æ–¹æ³•2ï¼šä» response ä¸­è§£æ file_path
                        response = agent_result.get("response", "")
                        if isinstance(response, dict) and "file_path" in response:
                            return response["file_path"]
                        
                        # æ–¹æ³•3ï¼šä» response å­—ç¬¦ä¸²ä¸­æå– file_path
                        if isinstance(response, str):
                            # å°è¯•ä» JSON æ ¼å¼çš„å“åº”ä¸­æå–
                            import re
                            import json
                            
                            # æŸ¥æ‰¾ JSON æ ¼å¼çš„ file_path
                            json_pattern = r'"file_path"\s*:\s*"([^"]+)"'
                            match = re.search(json_pattern, response)
                            if match:
                                return match.group(1)
                            
                            # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„è·¯å¾„æ ¼å¼
                            path_patterns = [
                                r'file_path[:\s]+([^\s\n,}]+)',
                                r'path[:\s]+([^\s\n,}]+\.v)',
                                r'saved.*?([^\s\n,}]+\.v)',
                                r'ç”Ÿæˆ.*?([^\s\n,}]+\.v)'
                            ]
                            
                            for pattern in path_patterns:
                                match = re.search(pattern, response, re.IGNORECASE)
                                if match:
                                    path = match.group(1).strip('"\'')
                                    if path.endswith('.v'):
                                        return path
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»æœ€è¿‘çš„å¯¹è¯å†å²ä¸­æŸ¥æ‰¾
            for task_id, task in self.active_tasks.items():
                for assignment in task.agent_assignments:
                    if assignment.get("design_file_path"):
                        return assignment["design_file_path"]
            
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ä¹‹å‰çš„è®¾è®¡æ–‡ä»¶è·¯å¾„")
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ æå–è®¾è®¡æ–‡ä»¶è·¯å¾„æ—¶å‡ºé”™: {str(e)}")
            return None
    
    async def _tool_get_tool_usage_guide(self, include_examples: bool = True,
                                       include_best_practices: bool = True) -> Dict[str, Any]:
        """è·å–LLMCoordinatorAgentä¸“ç”¨çš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼"""
        try:
            guide = self._generate_coordinator_tool_guide()
            
            return {
                "success": True,
                "guide": guide,
                "agent_type": "LLMCoordinatorAgent",
                "include_examples": include_examples,
                "include_best_practices": include_best_practices,
                "total_tools": 8,  # LLMCoordinatorAgentæœ‰8ä¸ªå·¥å…·
                "message": "æˆåŠŸç”ŸæˆLLMCoordinatorAgentçš„å·¥å…·ä½¿ç”¨æŒ‡å¯¼"
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆå·¥å…·ä½¿ç”¨æŒ‡å¯¼å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "message": "ç”Ÿæˆå·¥å…·ä½¿ç”¨æŒ‡å¯¼æ—¶å‘ç”Ÿé”™è¯¯"
            }
    
    def _check_assign_task_called(self, result: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†assign_task_to_agentå·¥å…·"""
        if not isinstance(result, str) or not result.strip():
            return False
        
        # æå–JSONå†…å®¹
        json_content = self._extract_json_from_response(result.strip())
        if not json_content:
            return False
            
        try:
            data = json.loads(json_content)
            
            if "tool_calls" in data and isinstance(data["tool_calls"], list):
                for call in data["tool_calls"]:
                    if isinstance(call, dict) and call.get("tool_name") == "assign_task_to_agent":
                        self.logger.info(f"âœ… æ£€æµ‹åˆ°assign_task_to_agentè°ƒç”¨")
                        return True
            
            return False
        except json.JSONDecodeError:
            return False
    
    async def _force_assign_task(self, user_request: str, task_context: TaskContext) -> Dict[str, Any]:
        """å¼ºåˆ¶åˆ†é…ä»»åŠ¡ç»™æ™ºèƒ½ä½“"""
        try:
            self.logger.info(f"ğŸš¨ å¼ºåˆ¶åˆ†é…ä»»åŠ¡: {user_request[:100]}...")
            
            # è®°å½•å¼ºåˆ¶åˆ†é…äº‹ä»¶
            task_context.add_conversation_message(
                role="system",
                content="æ£€æµ‹åˆ°LLMæœªæ­£ç¡®è°ƒç”¨æ¨èä»£ç†å·¥å…·ï¼Œå¯åŠ¨å¼ºåˆ¶åˆ†é…æœºåˆ¶",
                agent_id=self.agent_id,
                metadata={"type": "force_assignment", "reason": "missing_recommend_agent"}
            )
            
            # åˆ†æä»»åŠ¡ç±»å‹
            task_analysis = await self._tool_identify_task_type(user_request)
            if not task_analysis.get("success", False):
                return {"success": False, "error": "ä»»åŠ¡ç±»å‹è¯†åˆ«å¤±è´¥"}
            
            task_type = task_analysis.get("task_type", "design")
            
            # è®°å½•ä»»åŠ¡ç±»å‹è¯†åˆ«ç»“æœ
            task_context.add_conversation_message(
                role="system",
                content=f"å¼ºåˆ¶è¯†åˆ«ä»»åŠ¡ç±»å‹: {task_type}",
                agent_id=self.agent_id,
                metadata={"type": "task_type_identification", "task_type": task_type}
            )
            
            # æ¨èæ™ºèƒ½ä½“
            agent_recommendation = await self._tool_recommend_agent(
                task_type=task_type,
                task_description=user_request,
                priority="medium"
            )
            
            if not agent_recommendation.get("success", False):
                return {"success": False, "error": "æ™ºèƒ½ä½“æ¨èå¤±è´¥"}
            
            recommended_agent = agent_recommendation.get("recommended_agent", "enhanced_real_verilog_agent")
            recommendation_score = agent_recommendation.get("score", 0)
            
            # è®°å½•æ™ºèƒ½ä½“æ¨èç»“æœ
            task_context.add_conversation_message(
                role="system",
                content=f"å¼ºåˆ¶æ¨èæ™ºèƒ½ä½“: {recommended_agent} (è¯„åˆ†: {recommendation_score:.1f})",
                agent_id=self.agent_id,
                metadata={"type": "agent_recommendation", "agent_id": recommended_agent, "score": recommendation_score}
            )
            
            # å¼ºåˆ¶åˆ†é…ä»»åŠ¡
            assign_result = await self._tool_assign_task_to_agent(
                agent_id=recommended_agent,
                task_description=user_request,
                expected_output="ç”Ÿæˆå®Œæ•´çš„Verilogä»£ç æ–‡ä»¶",
                task_type=task_type,
                priority="medium"
            )
            
            # è®°å½•ä»»åŠ¡åˆ†é…ç»“æœ
            if assign_result.get("success", False):
                self.logger.info(f"âœ… å¼ºåˆ¶åˆ†é…ä»»åŠ¡æˆåŠŸ: {recommended_agent}")
                task_context.add_conversation_message(
                    role="system",
                    content=f"å¼ºåˆ¶åˆ†é…ä»»åŠ¡æˆåŠŸ: {recommended_agent}",
                    agent_id=self.agent_id,
                    metadata={"type": "task_assignment", "agent_id": recommended_agent, "success": True}
                )
                return {"success": True, "agent_id": recommended_agent, "result": assign_result}
            else:
                self.logger.error(f"âŒ å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¤±è´¥: {assign_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                task_context.add_conversation_message(
                    role="system",
                    content=f"å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¤±è´¥: {assign_result.get('error')}",
                    agent_id=self.agent_id,
                    metadata={"type": "task_assignment", "success": False, "error": assign_result.get('error')}
                )
                return {"success": False, "error": assign_result.get("error", "åˆ†é…ä»»åŠ¡å¤±è´¥")}
                
        except Exception as e:
            self.logger.error(f"âŒ å¼ºåˆ¶åˆ†é…ä»»åŠ¡å¼‚å¸¸: {str(e)}")
            return {"success": False, "error": str(e)}