#!/usr/bin/env python3
"""
ç³»ç»Ÿæç¤ºè¯ä¼ é€’éªŒè¯æµ‹è¯•
éªŒè¯LLMåè°ƒæ™ºèƒ½ä½“çš„ç³»ç»Ÿæç¤ºè¯æ˜¯å¦æ­£ç¡®ä¼ é€’ç»™LLM
"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config.config import FrameworkConfig
from core.llm_coordinator_agent import LLMCoordinatorAgent
from core.schema_system.enhanced_base_agent import EnhancedBaseAgent
from agents.enhanced_real_verilog_agent import EnhancedRealVerilogAgent
from agents.enhanced_real_code_reviewer import EnhancedRealCodeReviewAgent
from core.enhanced_logging_config import setup_enhanced_logging

class SystemPromptVerificationTest:
    """ç³»ç»Ÿæç¤ºè¯éªŒè¯æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.config = FrameworkConfig.from_env()
        self.logger = logging.getLogger(__name__)
        self.test_results = []
        
    async def setup_agents(self):
        """è®¾ç½®æ™ºèƒ½ä½“"""
        self.logger.info("ğŸ”§ è®¾ç½®æ™ºèƒ½ä½“...")
        
        # åˆ›å»ºåè°ƒæ™ºèƒ½ä½“
        self.coordinator = LLMCoordinatorAgent(self.config)
        
        # åˆ›å»ºå…¶ä»–æ™ºèƒ½ä½“
        self.verilog_agent = EnhancedRealVerilogAgent(self.config)
        self.code_review_agent = EnhancedRealCodeReviewAgent(self.config)
        
        # æ³¨å†Œæ™ºèƒ½ä½“
        await self.coordinator.register_agent(self.verilog_agent)
        await self.coordinator.register_agent(self.code_review_agent)
        
        self.logger.info("âœ… æ™ºèƒ½ä½“è®¾ç½®å®Œæˆ")
    
    def get_system_prompt(self):
        """è·å–å½“å‰çš„ç³»ç»Ÿæç¤ºè¯"""
        return self.coordinator._build_enhanced_system_prompt()
    
    async def test_system_prompt_injection(self):
        """æµ‹è¯•ç³»ç»Ÿæç¤ºè¯æ³¨å…¥"""
        self.logger.info("ğŸ§ª æµ‹è¯•ç³»ç»Ÿæç¤ºè¯æ³¨å…¥...")
        
        # è·å–ç³»ç»Ÿæç¤ºè¯
        system_prompt = self.get_system_prompt()
        
        # æ£€æŸ¥ç³»ç»Ÿæç¤ºè¯çš„å…³é”®å†…å®¹
        checks = {
            "åŒ…å«å¼ºåˆ¶è§„åˆ™": "ç¦æ­¢ç›´æ¥å›ç­”" in system_prompt,
            "åŒ…å«å·¥å…·è°ƒç”¨æ ¼å¼": "tool_calls" in system_prompt,
            "åŒ…å«å·¥å…·åˆ—è¡¨": "identify_task_type" in system_prompt,
            "åŒ…å«æ™ºèƒ½ä½“è°ƒç”¨ç¤ºä¾‹": "assign_task_to_agent" in system_prompt,
            "åŒ…å«ç¦æ­¢æè¿°æ€§æ–‡æœ¬": "ç¦æ­¢ç”Ÿæˆæè¿°æ€§æ–‡æœ¬" in system_prompt,
            "åŒ…å«é‡è¦æé†’": "é‡è¦æé†’" in system_prompt
        }
        
        self.logger.info("ğŸ“‹ ç³»ç»Ÿæç¤ºè¯æ£€æŸ¥ç»“æœ:")
        for check_name, result in checks.items():
            status = "âœ…" if result else "âŒ"
            self.logger.info(f"  {status} {check_name}: {result}")
        
        # è®°å½•ç³»ç»Ÿæç¤ºè¯é•¿åº¦
        prompt_length = len(system_prompt)
        self.logger.info(f"ğŸ“ ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {prompt_length} å­—ç¬¦")
        
        # ä¿å­˜ç³»ç»Ÿæç¤ºè¯åˆ°æ–‡ä»¶
        prompt_file = f"system_prompt_verification_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(prompt_file, 'w', encoding='utf-8') as f:
            f.write("=== ç³»ç»Ÿæç¤ºè¯å†…å®¹ ===\n")
            f.write(system_prompt)
            f.write("\n\n=== æ£€æŸ¥ç»“æœ ===\n")
            for check_name, result in checks.items():
                f.write(f"{check_name}: {result}\n")
        
        self.logger.info(f"ğŸ’¾ ç³»ç»Ÿæç¤ºè¯å·²ä¿å­˜åˆ°: {prompt_file}")
        
        return {
            "system_prompt": system_prompt,
            "checks": checks,
            "prompt_length": prompt_length,
            "prompt_file": prompt_file
        }
    
    async def test_llm_response_with_system_prompt(self):
        """æµ‹è¯•LLMåœ¨ç³»ç»Ÿæç¤ºè¯ä¸‹çš„å“åº”"""
        self.logger.info("ğŸ§ª æµ‹è¯•LLMå“åº”ï¼ˆå¸¦ç³»ç»Ÿæç¤ºè¯ï¼‰...")
        
        # æ„å»ºæµ‹è¯•å¯¹è¯
        conversation = [
            {"role": "system", "content": self.get_system_prompt()},
            {"role": "user", "content": "è¯·è®¾è®¡ä¸€ä¸ª4ä½è®¡æ•°å™¨æ¨¡å—"}
        ]
        
        try:
            # è°ƒç”¨LLM
            response = await self.coordinator._call_llm_for_function_calling(conversation)
            
            # åˆ†æå“åº”
            analysis = self.analyze_llm_response(response)
            
            self.logger.info("ğŸ“Š LLMå“åº”åˆ†æç»“æœ:")
            for key, value in analysis.items():
                self.logger.info(f"  {key}: {value}")
            
            return {
                "response": response,
                "analysis": analysis
            }
            
        except Exception as e:
            self.logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            return {
                "error": str(e),
                "analysis": {}
            }
    
    def analyze_llm_response(self, response: str) -> dict:
        """åˆ†æLLMå“åº”"""
        analysis = {
            "response_length": len(response),
            "is_json_format": False,
            "contains_tool_calls": False,
            "contains_descriptive_text": False,
            "starts_with_json": False,
            "has_valid_tool_structure": False
        }
        
        # æ£€æŸ¥æ˜¯å¦ä»¥JSONæ ¼å¼å¼€å§‹
        response_trimmed = response.strip()
        analysis["starts_with_json"] = response_trimmed.startswith('{')
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æè¿°æ€§æ–‡æœ¬
        descriptive_indicators = [
            "###", "---", "####", "**", "ç”¨æˆ·éœ€æ±‚", "ä»»åŠ¡åˆ†æ", 
            "æ‰§è¡Œç­–ç•¥", "å…³é”®ç›®æ ‡", "æ­£åœ¨è°ƒç”¨", "ç­‰å¾…æ™ºèƒ½ä½“"
        ]
        analysis["contains_descriptive_text"] = any(
            indicator in response for indicator in descriptive_indicators
        )
        
        # å°è¯•è§£æJSON
        try:
            if response_trimmed.startswith('{'):
                data = json.loads(response_trimmed)
                analysis["is_json_format"] = True
                analysis["contains_tool_calls"] = "tool_calls" in data
                
                if "tool_calls" in data and isinstance(data["tool_calls"], list):
                    tool_calls = data["tool_calls"]
                    if tool_calls and "tool_name" in tool_calls[0]:
                        analysis["has_valid_tool_structure"] = True
        except json.JSONDecodeError:
            pass
        
        return analysis
    
    async def test_coordination_without_self_continuation(self):
        """æµ‹è¯•åè°ƒä»»åŠ¡ï¼ˆç¦ç”¨è‡ªæˆ‘ç»§ç»­ï¼‰"""
        self.logger.info("ğŸ§ª æµ‹è¯•åè°ƒä»»åŠ¡ï¼ˆç¦ç”¨è‡ªæˆ‘ç»§ç»­ï¼‰...")
        
        try:
            result = await self.coordinator.coordinate_task(
                user_request="è¯·è®¾è®¡ä¸€ä¸ª4ä½è®¡æ•°å™¨æ¨¡å—",
                conversation_id="test_system_prompt_verification",
                max_iterations=1,  # åªæ‰§è¡Œä¸€æ¬¡
                external_testbench_path=None
            )
            
            # æ£€æŸ¥ç»“æœ
            coordination_result = result.get("coordination_result", "")
            analysis = self.analyze_llm_response(coordination_result)
            
            self.logger.info("ğŸ“Š åè°ƒä»»åŠ¡ç»“æœåˆ†æ:")
            for key, value in analysis.items():
                self.logger.info(f"  {key}: {value}")
            
            return {
                "result": result,
                "analysis": analysis
            }
            
        except Exception as e:
            self.logger.error(f"âŒ åè°ƒä»»åŠ¡å¤±è´¥: {str(e)}")
            return {
                "error": str(e),
                "analysis": {}
            }
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.logger.info("ğŸš€ å¼€å§‹ç³»ç»Ÿæç¤ºè¯éªŒè¯æµ‹è¯•")
        
        try:
            # è®¾ç½®æ™ºèƒ½ä½“
            await self.setup_agents()
            
            # æµ‹è¯•1: ç³»ç»Ÿæç¤ºè¯æ³¨å…¥
            test1_result = await self.test_system_prompt_injection()
            self.test_results.append(("ç³»ç»Ÿæç¤ºè¯æ³¨å…¥", test1_result))
            
            # æµ‹è¯•2: LLMå“åº”æµ‹è¯•
            test2_result = await self.test_llm_response_with_system_prompt()
            self.test_results.append(("LLMå“åº”æµ‹è¯•", test2_result))
            
            # æµ‹è¯•3: åè°ƒä»»åŠ¡æµ‹è¯•
            test3_result = await self.test_coordination_without_self_continuation()
            self.test_results.append(("åè°ƒä»»åŠ¡æµ‹è¯•", test3_result))
            
            # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            self.generate_test_report()
            
        except Exception as e:
            self.logger.error(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        self.logger.info("ğŸ“‹ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        
        report_file = f"system_prompt_verification_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ç³»ç»Ÿæç¤ºè¯éªŒè¯æµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"**æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            for test_name, result in self.test_results:
                f.write(f"## {test_name}\n\n")
                
                if "error" in result:
                    f.write(f"âŒ **é”™è¯¯**: {result['error']}\n\n")
                else:
                    if "checks" in result:
                        f.write("### ç³»ç»Ÿæç¤ºè¯æ£€æŸ¥\n\n")
                        for check_name, check_result in result["checks"].items():
                            status = "âœ…" if check_result else "âŒ"
                            f.write(f"- {status} {check_name}\n")
                        f.write(f"\n**æç¤ºè¯é•¿åº¦**: {result.get('prompt_length', 0)} å­—ç¬¦\n\n")
                    
                    if "analysis" in result:
                        f.write("### LLMå“åº”åˆ†æ\n\n")
                        for key, value in result["analysis"].items():
                            f.write(f"- **{key}**: {value}\n")
                        f.write("\n")
                    
                    if "response" in result:
                        f.write("### LLMå“åº”å†…å®¹\n\n")
                        f.write("```\n")
                        f.write(result["response"][:1000])  # åªæ˜¾ç¤ºå‰1000å­—ç¬¦
                        if len(result["response"]) > 1000:
                            f.write("\n... (å†…å®¹å·²æˆªæ–­)")
                        f.write("\n```\n\n")
        
        self.logger.info(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        
        # è¾“å‡ºå…³é”®å‘ç°
        self.logger.info("\nğŸ” å…³é”®å‘ç°:")
        for test_name, result in self.test_results:
            if "error" not in result:
                if "checks" in result:
                    all_checks_passed = all(result["checks"].values())
                    self.logger.info(f"  {test_name}: {'âœ… é€šè¿‡' if all_checks_passed else 'âŒ å¤±è´¥'}")
                
                if "analysis" in result:
                    analysis = result["analysis"]
                    if analysis.get("has_valid_tool_structure"):
                        self.logger.info(f"  {test_name}: âœ… ç”Ÿæˆäº†æœ‰æ•ˆçš„å·¥å…·è°ƒç”¨")
                    elif analysis.get("contains_descriptive_text"):
                        self.logger.info(f"  {test_name}: âŒ ç”Ÿæˆäº†æè¿°æ€§æ–‡æœ¬è€Œéå·¥å…·è°ƒç”¨")

async def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    setup_enhanced_logging()
    logger = logging.getLogger(__name__)
    
    logger.info("ğŸ§  ç³»ç»Ÿæç¤ºè¯ä¼ é€’éªŒè¯æµ‹è¯•")
    logger.info("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    test = SystemPromptVerificationTest()
    
    try:
        # è¿è¡Œæµ‹è¯•
        await test.run_all_tests()
        logger.info("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 