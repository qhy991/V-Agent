#!/usr/bin/env python3
"""
é›†æˆSchemaç³»ç»Ÿçš„å¢å¼ºä»£ç å®¡æŸ¥æ™ºèƒ½ä½“

Enhanced Code Review Agent with Schema Integration
"""

import json
import asyncio
import subprocess
import tempfile
import os
import re
from typing import Dict, Any, Set, List, Tuple
from pathlib import Path

from core.schema_system.enhanced_base_agent import EnhancedBaseAgent
from core.enums import AgentCapability
from core.base_agent import TaskMessage
from core.response_format import ResponseFormat, TaskStatus, ResponseType, QualityMetrics
from llm_integration.enhanced_llm_client import EnhancedLLMClient
from config.config import FrameworkConfig
from core.enhanced_logging_config import get_agent_logger, get_artifacts_dir
from tools.script_tools import ScriptManager


class EnhancedRealCodeReviewAgent(EnhancedBaseAgent):
    """é›†æˆSchemaç³»ç»Ÿçš„å¢å¼ºä»£ç å®¡æŸ¥æ™ºèƒ½ä½“"""
    
    def __init__(self, config: FrameworkConfig = None):
        super().__init__(
            agent_id="enhanced_real_code_review_agent",
            role="code_reviewer",
            capabilities={
                AgentCapability.CODE_REVIEW,
                AgentCapability.QUALITY_ANALYSIS,
                AgentCapability.SPECIFICATION_ANALYSIS,
                AgentCapability.TEST_GENERATION,
                AgentCapability.VERIFICATION
            },
            config=config
        )
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.config = config or FrameworkConfig.from_env()
        self.llm_client = EnhancedLLMClient(self.config.llm)
        
        # è®¾ç½®ä¸“ç”¨æ—¥å¿—å™¨
        self.agent_logger = get_agent_logger('EnhancedRealCodeReviewAgent')
        self.artifacts_dir = get_artifacts_dir()
        
        # åˆå§‹åŒ–è„šæœ¬ç®¡ç†å™¨
        self.script_manager = ScriptManager(work_dir=self.artifacts_dir)
        
        # æ³¨å†Œå¢å¼ºå·¥å…·
        self._register_enhanced_code_review_tools()
        
        self.logger.info(f"ğŸ” å¢å¼ºä»£ç å®¡æŸ¥æ™ºèƒ½ä½“(Schemaæ”¯æŒ)åˆå§‹åŒ–å®Œæˆ")
        self.agent_logger.info("EnhancedRealCodeReviewAgentåˆå§‹åŒ–å®Œæˆ")
    
    def _register_enhanced_code_review_tools(self):
        """æ³¨å†Œå¸¦SchemaéªŒè¯çš„ä»£ç å®¡æŸ¥å·¥å…·"""
        
        # 1. æµ‹è¯•å°ç”Ÿæˆå·¥å…·
        self.register_enhanced_tool(
            name="generate_testbench",
            func=self._tool_generate_testbench,
            description="ä¸ºVerilogæ¨¡å—ç”Ÿæˆå…¨é¢çš„æµ‹è¯•å°(testbench)",
            security_level="normal",
            category="verification",
            schema={
                "type": "object",
                "properties": {
                    "module_name": {
                        "type": "string",
                        "pattern": r"^[a-zA-Z][a-zA-Z0-9_]*$",
                        "minLength": 1,
                        "maxLength": 100,
                        "description": "ç›®æ ‡æ¨¡å—åç§°ï¼Œå¿…é¡»ä»¥å­—æ¯å¼€å¤´"
                    },
                    "module_code": {
                        "type": "string",
                        "minLength": 20,
                        "maxLength": 100000,
                        "description": "å®Œæ•´çš„Verilogæ¨¡å—ä»£ç "
                    },
                    "test_scenarios": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "name": {
                                    "type": "string",
                                    "minLength": 1,
                                    "maxLength": 100,
                                    "description": "æµ‹è¯•åœºæ™¯åç§°"
                                },
                                "description": {
                                    "type": "string",
                                    "minLength": 5,
                                    "maxLength": 1000,
                                    "description": "æµ‹è¯•åœºæ™¯æè¿°"
                                },
                                "inputs": {
                                    "type": "object",
                                    "description": "è¾“å…¥ä¿¡å·å€¼"
                                },
                                "expected_outputs": {
                                    "type": "object",
                                    "description": "æœŸæœ›çš„è¾“å‡ºå€¼"
                                }
                            },
                            "required": ["name", "description"],
                            "additionalProperties": False
                        },
                        "maxItems": 50,
                        "description": "æµ‹è¯•åœºæ™¯å®šä¹‰åˆ—è¡¨"
                    },
                    "clock_period": {
                        "type": "number",
                        "minimum": 0.1,
                        "maximum": 1000.0,
                        "default": 10.0,
                        "description": "æ—¶é’Ÿå‘¨æœŸ(ns)"
                    },
                    "simulation_time": {
                        "type": "integer",
                        "minimum": 100,
                        "maximum": 1000000,
                        "default": 10000,
                        "description": "ä»¿çœŸæ—¶é—´(æ—¶é’Ÿå‘¨æœŸæ•°)"
                    },
                    "coverage_options": {
                        "type": "object",
                        "properties": {
                            "enable_coverage": {
                                "type": "boolean",
                                "default": False,
                                "description": "æ˜¯å¦å¯ç”¨è¦†ç›–ç‡æ”¶é›†"
                            },
                            "coverage_type": {
                                "type": "array",
                                "items": {
                                    "type": "string",
                                    "enum": ["line", "toggle", "branch", "condition"]
                                },
                                "default": ["line", "toggle"],
                                "description": "è¦†ç›–ç‡ç±»å‹"
                            }
                        },
                        "additionalProperties": False,
                        "description": "è¦†ç›–ç‡é€‰é¡¹é…ç½®"
                    }
                },
                "required": ["module_name", "module_code"],
                "additionalProperties": False
            }
        )
        
        # 2. ä»¿çœŸæ‰§è¡Œå·¥å…·
        self.register_enhanced_tool(
            name="run_simulation",
            func=self._tool_run_simulation,
            description="ä½¿ç”¨ä¸“ä¸šå·¥å…·è¿è¡ŒVerilogä»¿çœŸå’ŒéªŒè¯",
            security_level="high",
            category="verification",
            schema={
                "type": "object",
                "properties": {
                    "module_file": {
                        "type": "string",
                        "pattern": r"^[a-zA-Z0-9_./\-]+\.v$",
                        "maxLength": 500,
                        "description": "æ¨¡å—æ–‡ä»¶è·¯å¾„ï¼Œå¿…é¡»ä»¥.vç»“å°¾"
                    },
                    "testbench_file": {
                        "type": "string",
                        "pattern": r"^[a-zA-Z0-9_./\-]+\.v$",
                        "maxLength": 500,
                        "description": "æµ‹è¯•å°æ–‡ä»¶è·¯å¾„ï¼Œå¿…é¡»ä»¥.vç»“å°¾"
                    },
                    "module_code": {
                        "type": "string",
                        "minLength": 20,
                        "maxLength": 100000,
                        "description": "æ¨¡å—ä»£ç å†…å®¹(å¦‚æœä¸æä¾›æ–‡ä»¶è·¯å¾„)"
                    },
                    "testbench_code": {
                        "type": "string",
                        "minLength": 20,
                        "maxLength": 100000,
                        "description": "æµ‹è¯•å°ä»£ç å†…å®¹(å¦‚æœä¸æä¾›æ–‡ä»¶è·¯å¾„)"
                    },
                    "simulator": {
                        "type": "string",
                        "enum": ["iverilog", "modelsim", "vivado", "auto"],
                        "default": "iverilog",
                        "description": "ä»¿çœŸå™¨é€‰æ‹©"
                    },
                    "simulation_options": {
                        "type": "object",
                        "properties": {
                            "timescale": {
                                "type": "string",
                                "pattern": r"^\d+[a-z]+/\d+[a-z]+$",
                                "default": "1ns/1ps",
                                "description": "æ—¶é—´ç²¾åº¦è®¾ç½®"
                            },
                            "dump_waves": {
                                "type": "boolean",
                                "default": True,
                                "description": "æ˜¯å¦ç”Ÿæˆæ³¢å½¢æ–‡ä»¶"
                            },
                            "max_sim_time": {
                                "type": "integer",
                                "minimum": 100,
                                "maximum": 10000000,
                                "default": 100000,
                                "description": "æœ€å¤§ä»¿çœŸæ—¶é—´(æ—¶é—´å•ä½)"
                            }
                        },
                        "additionalProperties": False,
                        "description": "ä»¿çœŸé€‰é¡¹é…ç½®"
                    }
                },
                "anyOf": [
                    {"required": ["module_file", "testbench_file"]},
                    {"required": ["module_code", "testbench_code"]}
                ],
                "additionalProperties": False
            }
        )
        
        # 3. ä»£ç è´¨é‡åˆ†æå·¥å…·
        self.register_enhanced_tool(
            name="analyze_code_quality",
            func=self._tool_analyze_code_quality,
            description="æ·±åº¦åˆ†æVerilogä»£ç è´¨é‡ã€è§„èŒƒæ€§å’Œå¯ç»´æŠ¤æ€§",
            security_level="normal",
            category="quality_assurance",
            schema={
                "type": "object",
                "properties": {
                    "code": {
                        "type": "string",
                        "minLength": 10,
                        "maxLength": 500000,
                        "description": "å¾…åˆ†æçš„Verilogä»£ç "
                    },
                    "analysis_scope": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "enum": [
                                "syntax", "style", "naming", "structure", 
                                "timing", "synthesis", "testability", "documentation",
                                "complexity", "maintainability", "performance"
                            ]
                        },
                        "minItems": 1,
                        "maxItems": 11,
                        "default": ["syntax", "style", "structure"],
                        "description": "åˆ†æèŒƒå›´é€‰æ‹©"
                    },
                    "coding_standard": {
                        "type": "string",
                        "enum": ["ieee1800", "systemverilog", "verilog2001", "custom"],
                        "default": "ieee1800",
                        "description": "ç¼–ç æ ‡å‡†è§„èŒƒ"
                    },
                    "severity_filter": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "enum": ["error", "warning", "info", "suggestion"]
                        },
                        "default": ["error", "warning"],
                        "description": "ä¸¥é‡åº¦è¿‡æ»¤çº§åˆ«"
                    },
                    "generate_report": {
                        "type": "boolean",
                        "default": True,
                        "description": "æ˜¯å¦ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"
                    }
                },
                "required": ["code"],
                "additionalProperties": False
            }
        )
        
        # 4. æ„å»ºè„šæœ¬ç”Ÿæˆå·¥å…·
        self.register_enhanced_tool(
            name="generate_build_script",
            func=self._tool_generate_build_script,
            description="ç”Ÿæˆä¸“ä¸šçš„æ„å»ºè„šæœ¬(Makefileæˆ–shellè„šæœ¬)",
            security_level="high",
            category="build_automation",
            schema={
                "type": "object",
                "properties": {
                    "verilog_files": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "pattern": r"^[a-zA-Z0-9_./\-]+\.v$",
                            "maxLength": 500
                        },
                        "minItems": 1,
                        "maxItems": 100,
                        "description": "Verilogæºæ–‡ä»¶åˆ—è¡¨"
                    },
                    "testbench_files": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "pattern": r"^[a-zA-Z0-9_./\-]+\.v$",
                            "maxLength": 500
                        },
                        "minItems": 1,
                        "maxItems": 100,
                        "description": "æµ‹è¯•å°æ–‡ä»¶åˆ—è¡¨"
                    },
                    "script_type": {
                        "type": "string",
                        "enum": ["makefile", "bash", "tcl", "python"],
                        "default": "makefile",
                        "description": "è„šæœ¬ç±»å‹é€‰æ‹©"
                    },
                    "target_name": {
                        "type": "string",
                        "pattern": r"^[a-zA-Z][a-zA-Z0-9_\-]*$",
                        "minLength": 1,
                        "maxLength": 50,
                        "default": "simulation",
                        "description": "æ„å»ºç›®æ ‡åç§°"
                    },
                    "build_options": {
                        "type": "object",
                        "properties": {
                            "simulator": {
                                "type": "string",
                                "enum": ["iverilog", "modelsim", "vivado", "verilator"],
                                "default": "iverilog",
                                "description": "ç›®æ ‡ä»¿çœŸå™¨"
                            },
                            "optimization_level": {
                                "type": "string",
                                "enum": ["none", "basic", "aggressive"],
                                "default": "basic",
                                "description": "ä¼˜åŒ–çº§åˆ«"
                            },
                            "include_dirs": {
                                "type": "array",
                                "items": {
                                    "type": "string",
                                    "pattern": r"^[a-zA-Z0-9_./\-]+$",
                                    "maxLength": 500
                                },
                                "maxItems": 20,
                                "description": "åŒ…å«ç›®å½•åˆ—è¡¨"
                            },
                            "defines": {
                                "type": "object",
                                "patternProperties": {
                                    "^[A-Z][A-Z0-9_]*$": {
                                        "type": "string",
                                        "maxLength": 100
                                    }
                                },
                                "maxProperties": 50,
                                "description": "é¢„å®šä¹‰å®"
                            }
                        },
                        "additionalProperties": False,
                        "description": "æ„å»ºé€‰é¡¹é…ç½®"
                    }
                },
                "required": ["verilog_files", "testbench_files"],
                "additionalProperties": False
            }
        )
        
        # 5. è„šæœ¬æ‰§è¡Œå·¥å…·
        self.register_enhanced_tool(
            name="execute_build_script",
            func=self._tool_execute_build_script,
            description="å®‰å…¨æ‰§è¡Œæ„å»ºè„šæœ¬è¿›è¡Œç¼–è¯‘å’Œä»¿çœŸ",
            security_level="high",
            category="build_automation",
            schema={
                "type": "object",
                "properties": {
                    "script_name": {
                        "type": "string",
                        "pattern": r"^[a-zA-Z0-9_.\-]+$",
                        "minLength": 1,
                        "maxLength": 100,
                        "description": "è„šæœ¬æ–‡ä»¶åï¼Œåªå…è®¸å®‰å…¨å­—ç¬¦"
                    },
                    "action": {
                        "type": "string",
                        "enum": ["all", "compile", "simulate", "clean", "test", "lint"],
                        "default": "all",
                        "description": "æ‰§è¡Œçš„åŠ¨ä½œç±»å‹"
                    },
                    "arguments": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "pattern": r"^[a-zA-Z0-9_=.\-/]+$",
                            "maxLength": 200
                        },
                        "maxItems": 20,
                        "description": "å®‰å…¨çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œè¿‡æ»¤å±é™©å­—ç¬¦"
                    },
                    "timeout": {
                        "type": "integer",
                        "minimum": 10,
                        "maximum": 3600,
                        "default": 300,
                        "description": "æ‰§è¡Œè¶…æ—¶æ—¶é—´(ç§’)"
                    },
                    "working_directory": {
                        "type": "string",
                        "pattern": r"^[a-zA-Z0-9_./\-]+$",
                        "maxLength": 500,
                        "description": "å·¥ä½œç›®å½•è·¯å¾„"
                    }
                },
                "required": ["script_name"],
                "additionalProperties": False
            }
        )
        
        # 6. è¦†ç›–ç‡åˆ†æå·¥å…·
        self.register_enhanced_tool(
            name="analyze_coverage",
            func=self._tool_analyze_coverage,
            description="åˆ†æä»£ç è¦†ç›–ç‡å’Œæµ‹è¯•å®Œæ•´æ€§",
            security_level="normal",
            category="verification",
            schema={
                "type": "object",
                "properties": {
                    "coverage_data_file": {
                        "type": "string",
                        "pattern": r"^[a-zA-Z0-9_./\-]+\.(dat|xml|json|vcd|txt|log)$",
                        "maxLength": 500,
                        "description": "è¦†ç›–ç‡æ•°æ®æ–‡ä»¶è·¯å¾„"
                    },
                    "coverage_types": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "enum": ["line", "toggle", "branch", "condition", "fsm", "expression"]
                        },
                        "minItems": 1,
                        "default": ["line", "toggle", "branch"],
                        "description": "è¦†ç›–ç‡ç±»å‹åˆ†æ"
                    },
                    "threshold": {
                        "type": "object",
                        "properties": {
                            "line_coverage": {
                                "type": "number",
                                "minimum": 0,
                                "maximum": 100,
                                "default": 80,
                                "description": "è¡Œè¦†ç›–ç‡é˜ˆå€¼(%)"
                            },
                            "branch_coverage": {
                                "type": "number",
                                "minimum": 0,
                                "maximum": 100,
                                "default": 70,
                                "description": "åˆ†æ”¯è¦†ç›–ç‡é˜ˆå€¼(%)"
                            },
                            "toggle_coverage": {
                                "type": "number",
                                "minimum": 0,
                                "maximum": 100,
                                "default": 60,
                                "description": "ç¿»è½¬è¦†ç›–ç‡é˜ˆå€¼(%)"
                            }
                        },
                        "additionalProperties": False,
                        "description": "è¦†ç›–ç‡é˜ˆå€¼é…ç½®"
                    },
                    "report_format": {
                        "type": "string",
                        "enum": ["text", "html", "xml", "json"],
                        "default": "html",
                        "description": "æŠ¥å‘Šæ ¼å¼"
                    }
                },
                "required": ["coverage_data_file"],
                "additionalProperties": False
            }
        )
        
        # 7. æµ‹è¯•å¤±è´¥åˆ†æå·¥å…· - ä¸“é—¨ç”¨äºåˆ†æTDDå¾ªç¯ä¸­çš„å¤±è´¥æ¨¡å¼
        self.register_enhanced_tool(
            name="analyze_test_failures",
            func=self._tool_analyze_test_failures,
            description="åˆ†ææµ‹è¯•å¤±è´¥åŸå› å¹¶æä¾›å…·ä½“ä¿®å¤å»ºè®®",
            security_level="normal",
            category="debugging",
            schema={
                "type": "object",
                "properties": {
                    "compilation_errors": {
                        "type": "string",
                        "maxLength": 5000,
                        "description": "ç¼–è¯‘é”™è¯¯è¾“å‡ºä¿¡æ¯"
                    },
                    "simulation_errors": {
                        "type": "string",
                        "maxLength": 5000,
                        "description": "ä»¿çœŸé”™è¯¯è¾“å‡ºä¿¡æ¯"
                    },
                    "test_assertions": {
                        "type": "string",
                        "maxLength": 5000,
                        "description": "æµ‹è¯•æ–­è¨€å¤±è´¥ä¿¡æ¯"
                    },
                    "design_code": {
                        "type": "string",
                        "maxLength": 10000,
                        "description": "éœ€è¦åˆ†æçš„è®¾è®¡ä»£ç "
                    },
                    "testbench_code": {
                        "type": "string",
                        "maxLength": 10000,
                        "description": "æµ‹è¯•å°ä»£ç "
                    },
                    "iteration_number": {
                        "type": "integer",
                        "minimum": 1,
                        "maximum": 50,
                        "description": "å½“å‰TDDè¿­ä»£æ¬¡æ•°"
                    },
                    "previous_fixes": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "maxLength": 500
                        },
                        "maxItems": 10,
                        "description": "ä¹‹å‰å°è¯•çš„ä¿®å¤æ–¹æ³•åˆ—è¡¨"
                    }
                },
                "required": ["design_code"],
                "additionalProperties": False
            }
        )
    
    async def _call_llm_for_function_calling(self, conversation: List[Dict[str, str]]) -> str:
        """å®ç°LLMè°ƒç”¨ - æ™ºèƒ½å¤„ç†SchemaéªŒè¯é”™è¯¯"""
        # æ„å»ºå®Œæ•´çš„prompt
        full_prompt = ""
        system_prompt = self._build_enhanced_system_prompt()
        
        for msg in conversation:
            if msg["role"] == "system":
                system_prompt = msg["content"]  # è¦†ç›–é»˜è®¤system prompt
            elif msg["role"] == "user":
                full_prompt += f"User: {msg['content']}\n\n"
            elif msg["role"] == "assistant":
                full_prompt += f"Assistant: {msg['content']}\n\n"
        
        try:
            response = await self.llm_client.send_prompt(
                prompt=full_prompt.strip(),
                system_prompt=system_prompt,
                temperature=0.2,  # ä»£ç å®¡æŸ¥éœ€è¦æ›´é«˜çš„ä¸€è‡´æ€§
                max_tokens=4000
            )
            return response
        except Exception as e:
            self.logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def _build_enhanced_system_prompt(self) -> str:
        """æ„å»ºå¢å¼ºçš„System Promptï¼ˆæ”¯æŒæ™ºèƒ½Schemaé€‚é…ï¼‰"""
        base_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ç¡¬ä»¶éªŒè¯å’Œä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œå…·å¤‡ä»¥ä¸‹ä¸“ä¸šèƒ½åŠ›ï¼š

ğŸ” **æ ¸å¿ƒä¸“é•¿**:
- Verilog/SystemVerilogä»£ç å®¡æŸ¥å’Œè´¨é‡åˆ†æ
- æµ‹è¯•å°(Testbench)è®¾è®¡å’ŒéªŒè¯æ–¹æ³•å­¦
- ä»£ç è¦†ç›–ç‡åˆ†æå’Œæµ‹è¯•å®Œæ•´æ€§è¯„ä¼°
- æ„å»ºè‡ªåŠ¨åŒ–å’ŒCI/CDæµç¨‹
- é™æ€åˆ†æå’Œä»£ç è§„èŒƒæ£€æŸ¥
- æ—¶åºåˆ†æå’Œå¯ç»¼åˆæ€§éªŒè¯

ğŸ“‹ **å®¡æŸ¥æ ‡å‡†**:
1. IEEE 1800æ ‡å‡†åˆè§„æ€§æ£€æŸ¥
2. ä»£ç å¯è¯»æ€§å’Œç»´æŠ¤æ€§è¯„ä¼°
3. ç»¼åˆæ€§å’Œæ—¶åºæ”¶æ•›åˆ†æ
4. æµ‹è¯•è¦†ç›–ç‡å’ŒéªŒè¯å®Œæ•´æ€§
5. æœ€ä½³å®è·µå’Œè®¾è®¡æ¨¡å¼åº”ç”¨
6. å®‰å…¨æ€§å’Œå¯é æ€§è€ƒé‡

ğŸ› ï¸ **å·¥å…·è°ƒç”¨è§„åˆ™**:
ä½ å¿…é¡»ä½¿ç”¨JSONæ ¼å¼è°ƒç”¨å·¥å…·ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```json
{
    "tool_calls": [
        {
            "tool_name": "å·¥å…·åç§°",
            "parameters": {
                "å‚æ•°å": "å‚æ•°å€¼"
            }
        }
    ]
}
```

âœ¨ **æ™ºèƒ½Schemaé€‚é…ç³»ç»Ÿ**:
ç³»ç»Ÿç°åœ¨å…·å¤‡æ™ºèƒ½å‚æ•°é€‚é…èƒ½åŠ›ï¼Œæ”¯æŒä»¥ä¸‹çµæ´»æ ¼å¼ï¼š

ğŸ“Œ **å­—æ®µåæ™ºèƒ½æ˜ å°„**:
- `code` â†” `verilog_code` (è‡ªåŠ¨åŒå‘æ˜ å°„)
- `test_cases` â†’ `test_scenarios`
- `files` â†’ `verilog_files`
- `script` â†’ `script_name`
- `coverage_file` â†’ `coverage_data_file` (æ”¯æŒ .vcd, .dat, .xml, .json, .txt, .log æ ¼å¼)
- ğŸ’¡ ä½¿ç”¨ä»»ä¸€æ ¼å¼éƒ½ä¼šè¢«æ™ºèƒ½è¯†åˆ«

ğŸ“Œ **æµ‹è¯•åœºæ™¯çµæ´»æ ¼å¼**:
- âœ… å­—ç¬¦ä¸²æ•°ç»„: `["åŸºæœ¬åŠŸèƒ½æµ‹è¯•", "è¾¹ç•Œæ¡ä»¶æµ‹è¯•"]`
- âœ… å¯¹è±¡æ•°ç»„: `[{"name": "basic_test", "description": "åŸºæœ¬åŠŸèƒ½æµ‹è¯•"}]`
- ğŸ’¡ ç³»ç»Ÿä¼šè‡ªåŠ¨è½¬æ¢æ ¼å¼

ğŸ“Œ **ç¼ºå¤±å­—æ®µæ™ºèƒ½æ¨æ–­**:
- ç¼ºå°‘ `module_name` æ—¶ä¼šä»ä»£ç ä¸­è‡ªåŠ¨æå–
- ç¼ºå°‘å¿…éœ€å­—æ®µæ—¶ä¼šæä¾›åˆç†é»˜è®¤å€¼
- ğŸ’¡ æ— éœ€æ‹…å¿ƒé—æ¼å‚æ•°

ğŸ¯ **æ¨èçš„å·¥å…·è°ƒç”¨æ–¹å¼**:

### æ–¹å¼1: ä½¿ç”¨è‡ªç„¶å­—ç¬¦ä¸²æ ¼å¼ï¼ˆæ¨èï¼‰
```json
{
    "tool_calls": [
        {
            "tool_name": "analyze_code_quality",
            "parameters": {
                "code": "module test(); endmodule",
                "analysis_scope": ["syntax", "style"],
                "coding_standard": "ieee1800"
            }
        }
    ]
}
```

### æ–¹å¼2: ä½¿ç”¨æ ‡å‡†å¯¹è±¡æ ¼å¼
```json
{
    "tool_calls": [
        {
            "tool_name": "generate_testbench",
            "parameters": {
                "module_name": "simple_adder",
                "verilog_code": "module simple_adder(...); endmodule",
                "test_scenarios": [
                    {"name": "basic_test", "description": "åŸºæœ¬åŠŸèƒ½éªŒè¯"},
                    {"name": "corner_test", "description": "è¾¹ç•Œæ¡ä»¶æµ‹è¯•"}
                ]
            }
        }
    ]
}
```

ğŸ¯ **å¯ç”¨å·¥å…·åŠå…¶å‚æ•°**:

### 1. generate_testbench
**å¿…éœ€å‚æ•°**:
- `module_name` (string): ç›®æ ‡æ¨¡å—åç§°
- `verilog_code` (string): ç›®æ ‡æ¨¡å—ä»£ç ï¼ˆä¹Ÿå¯ä½¿ç”¨ `code`, `module_code`ï¼‰
**å¯é€‰å‚æ•°**:
- `test_scenarios` (array): æµ‹è¯•åœºæ™¯åˆ—è¡¨ï¼ˆä¹Ÿå¯ä½¿ç”¨ `test_cases`ï¼‰
- `clock_period` (number): æ—¶é’Ÿå‘¨æœŸ(ns)ï¼Œ0.1-1000.0
- `simulation_time` (integer): ä»¿çœŸæ—¶é—´ï¼Œ100-1000000

### 2. run_simulation
**å¿…éœ€å‚æ•°**:
- `module_file` æˆ– `module_code`: æ¨¡å—æ–‡ä»¶è·¯å¾„æˆ–ä»£ç å†…å®¹
- `testbench_file` æˆ– `testbench_code`: æµ‹è¯•å°æ–‡ä»¶è·¯å¾„æˆ–ä»£ç å†…å®¹
**å¯é€‰å‚æ•°**:
- `simulator` (string): "iverilog", "modelsim", "vivado", "auto"
- `simulation_options` (object): ä»¿çœŸé€‰é¡¹é…ç½®

### 3. analyze_code_quality
**å¿…éœ€å‚æ•°**:
- `code` (string): å¾…åˆ†æä»£ç ï¼ˆä¹Ÿå¯ä½¿ç”¨ `verilog_code`ï¼‰
**å¯é€‰å‚æ•°**:
- `analysis_scope` (array): åˆ†æèŒƒå›´é€‰æ‹©
- `coding_standard` (string): "ieee1800", "custom", "industry"
- `severity_filter` (array): ä¸¥é‡åº¦è¿‡æ»¤

### 4. generate_build_script
**å¿…éœ€å‚æ•°**:
- `verilog_files` (array): Verilogæ–‡ä»¶åˆ—è¡¨ï¼ˆä¹Ÿå¯ä½¿ç”¨ `design_files`ï¼‰
- `testbench_files` (array): æµ‹è¯•å°æ–‡ä»¶åˆ—è¡¨
**å¯é€‰å‚æ•°**:
- `script_type` (string): "makefile", "bash", "tcl", "python"
- `build_options` (object): æ„å»ºé€‰é¡¹é…ç½®

### 5. execute_build_script
**å¿…éœ€å‚æ•°**:
- `script_name` (string): è„šæœ¬æ–‡ä»¶å
**å¯é€‰å‚æ•°**:
- `action` (string): "all", "compile", "simulate", "clean"
- `timeout` (integer): è¶…æ—¶æ—¶é—´(ç§’)

### 6. analyze_coverage
**å¿…éœ€å‚æ•°**:
- `coverage_data_file` (string): è¦†ç›–ç‡æ•°æ®æ–‡ä»¶è·¯å¾„ (æ”¯æŒ .vcd, .dat, .xml, .json, .txt, .log)
**å¯é€‰å‚æ•°**:
- `coverage_types` (array): è¦†ç›–ç‡ç±»å‹
- `threshold` (object): é˜ˆå€¼é…ç½®

### 7. analyze_test_failures â­ **TDDä¸“ç”¨**
**å¿…éœ€å‚æ•°**:
- `design_code` (string): éœ€è¦åˆ†æçš„è®¾è®¡ä»£ç 
**å¯é€‰å‚æ•°**:
- `compilation_errors` (string): ç¼–è¯‘é”™è¯¯è¾“å‡º
- `simulation_errors` (string): ä»¿çœŸé”™è¯¯è¾“å‡º
- `test_assertions` (string): æµ‹è¯•æ–­è¨€å¤±è´¥ä¿¡æ¯
- `testbench_code` (string): æµ‹è¯•å°ä»£ç 
- `iteration_number` (integer): å½“å‰TDDè¿­ä»£æ¬¡æ•°
- `previous_fixes` (array): ä¹‹å‰å°è¯•çš„ä¿®å¤æ–¹æ³•

ğŸ¯ **ä½¿ç”¨å»ºè®®**:
1. ä¼˜å…ˆä½¿ç”¨ç®€æ´ç›´è§‚çš„å­—æ®µåï¼Œå¦‚ `code` è€Œä¸æ˜¯ `verilog_code`
2. å­—æ®µåç§°å¯ä»¥ä½¿ç”¨ä½ ä¹ æƒ¯çš„æ–¹å¼ï¼Œç³»ç»Ÿä¼šæ™ºèƒ½é€‚é…
3. ä¸å¿…æ‹…å¿ƒå‚æ•°æ ¼å¼é”™è¯¯ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä¿®æ­£
4. ä¸“æ³¨äºå®¡æŸ¥é€»è¾‘ï¼Œè®©ç³»ç»Ÿå¤„ç†æ ¼å¼ç»†èŠ‚

ğŸ“Š **æ¨èå·¥ä½œæµç¨‹**:
æ”¶åˆ°ä»£ç å®¡æŸ¥ä»»åŠ¡æ—¶ï¼Œå»ºè®®æµç¨‹ï¼š
1. é¦–å…ˆåˆ†æä»£ç è´¨é‡å’Œè§„èŒƒæ€§ (analyze_code_quality)
2. ç”Ÿæˆå…¨é¢çš„æµ‹è¯•å°è¿›è¡ŒéªŒè¯ (generate_testbench)
3. æ‰§è¡Œä»¿çœŸå¹¶åˆ†æç»“æœ (run_simulation)
4. ç”Ÿæˆæ„å»ºè„šæœ¬ç¡®ä¿å¯é‡ç°æ€§ (generate_build_script)
5. åˆ†ææµ‹è¯•è¦†ç›–ç‡å¹¶æå‡ºæ”¹è¿›å»ºè®® (analyze_coverage)
6. æä¾›è¯¦ç»†çš„å®¡æŸ¥æŠ¥å‘Šå’Œå»ºè®®

ğŸ’¡ **å…³é”®ä¼˜åŠ¿**: ç°åœ¨ä½ å¯ä»¥ä½¿ç”¨è‡ªç„¶ç›´è§‚çš„å‚æ•°æ ¼å¼ï¼Œç³»ç»Ÿçš„æ™ºèƒ½é€‚é…å±‚ä¼šç¡®ä¿ä¸åº•å±‚å·¥å…·çš„å®Œç¾å…¼å®¹ï¼
"""
        return base_prompt
    
    def get_capabilities(self) -> Set[AgentCapability]:
        return {
            AgentCapability.CODE_REVIEW,
            AgentCapability.QUALITY_ANALYSIS,
            AgentCapability.SPECIFICATION_ANALYSIS,
            AgentCapability.TEST_GENERATION,
            AgentCapability.VERIFICATION,
            AgentCapability.COVERAGE_ANALYSIS
        }
        
    def get_specialty_description(self) -> str:
        return "é›†æˆSchemaéªŒè¯çš„å¢å¼ºä»£ç å®¡æŸ¥æ™ºèƒ½ä½“ï¼Œæä¾›ä¸¥æ ¼å‚æ•°éªŒè¯å’Œæ™ºèƒ½é”™è¯¯ä¿®å¤çš„ä¸“ä¸šç¡¬ä»¶éªŒè¯æœåŠ¡"
    
    async def execute_enhanced_task(self, enhanced_prompt: str,
                                  original_message: TaskMessage,
                                  file_contents: Dict[str, Dict]) -> Dict[str, Any]:
        """æ‰§è¡Œå¢å¼ºçš„ä»£ç å®¡æŸ¥ä»»åŠ¡"""
        task_id = original_message.task_id
        self.logger.info(f"ğŸ¯ å¼€å§‹æ‰§è¡Œå¢å¼ºä»£ç å®¡æŸ¥ä»»åŠ¡: {task_id}")
        
        try:
            # ä½¿ç”¨å¢å¼ºéªŒè¯å¤„ç†æµç¨‹
            result = await self.process_with_enhanced_validation(
                user_request=enhanced_prompt,
                max_iterations=5
            )
            
            if result["success"]:
                self.logger.info(f"âœ… ä»£ç å®¡æŸ¥ä»»åŠ¡å®Œæˆ: {task_id}")
                return {
                    "success": True,
                    "task_id": task_id,
                    "response": result.get("response", ""),
                    "tool_results": result.get("tool_results", []),
                    "iterations": result.get("iterations", 1),
                    "quality_metrics": {
                        "schema_validation_passed": True,
                        "parameter_errors_fixed": result.get("iterations", 1) > 1,
                        "security_checks_passed": True
                    }
                }
            else:
                self.logger.error(f"âŒ ä»£ç å®¡æŸ¥ä»»åŠ¡å¤±è´¥: {task_id} - {result.get('error')}")
                return {
                    "success": False,
                    "task_id": task_id,
                    "error": result.get("error", "Unknown error"),
                    "iterations": result.get("iterations", 1)
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ä»£ç å®¡æŸ¥ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {task_id} - {str(e)}")
            return {
                "success": False,
                "task_id": task_id,
                "error": f"æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            }
    
    # =============================================================================
    # å·¥å…·å®ç°æ–¹æ³•
    # =============================================================================
    
    async def _tool_generate_testbench(self, module_name: str, module_code: str,
                                     test_scenarios: List[Dict] = None,
                                     clock_period: float = 10.0,
                                     simulation_time: int = 10000,
                                     coverage_options: Dict = None) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•å°å·¥å…·å®ç°"""
        try:
            self.logger.info(f"ğŸ§ª ç”Ÿæˆæµ‹è¯•å°: {module_name}")
            
            test_scenarios = test_scenarios or [
                {"name": "basic_test", "description": "åŸºç¡€åŠŸèƒ½æµ‹è¯•"}
            ]
            coverage_options = coverage_options or {"enable_coverage": False}
            
            # æ„å»ºæµ‹è¯•å°ç”Ÿæˆæç¤º
            scenarios_desc = "\n".join([
                f"- {s['name']}: {s['description']}" 
                for s in test_scenarios
            ])
            
            testbench_prompt = f"""
è¯·ä¸ºä»¥ä¸‹Verilogæ¨¡å—ç”Ÿæˆä¸€ä¸ªå®Œæ•´ã€ä¸“ä¸šçš„æµ‹è¯•å°ï¼š

ç›®æ ‡æ¨¡å—: {module_name}
```verilog
{module_code}
```

æµ‹è¯•è¦æ±‚:
- æ—¶é’Ÿå‘¨æœŸ: {clock_period}ns
- ä»¿çœŸæ—¶é—´: {simulation_time} ä¸ªæ—¶é’Ÿå‘¨æœŸ
- è¦†ç›–ç‡æ”¶é›†: {'å¯ç”¨' if coverage_options.get('enable_coverage') else 'ç¦ç”¨'}

æµ‹è¯•åœºæ™¯:
{scenarios_desc}

è¯·ç”ŸæˆåŒ…å«ä»¥ä¸‹å†…å®¹çš„ä¸“ä¸šæµ‹è¯•å°ï¼š
1. å®Œæ•´çš„testbenchæ¨¡å—å£°æ˜
2. æ‰€æœ‰å¿…è¦çš„ä¿¡å·å£°æ˜
3. æ—¶é’Ÿå’Œå¤ä½ç”Ÿæˆé€»è¾‘
4. è¢«æµ‹æ¨¡å—çš„æ­£ç¡®å®ä¾‹åŒ–
5. ç³»ç»ŸåŒ–çš„æµ‹è¯•æ¿€åŠ±ç”Ÿæˆ
6. ç»“æœæ£€æŸ¥å’Œæ–­è¨€
7. é€‚å½“çš„$displayã€$monitorå’Œ$finishè¯­å¥
8. æ³¢å½¢è½¬å‚¨è®¾ç½®ï¼ˆVCDæ–‡ä»¶ï¼‰
9. æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

ç¡®ä¿æµ‹è¯•å°èƒ½å¤Ÿå……åˆ†éªŒè¯æ¨¡å—çš„æ‰€æœ‰åŠŸèƒ½ã€‚
"""
            
            response = await self.llm_client.send_prompt(
                prompt=testbench_prompt,
                system_prompt="ä½ æ˜¯ä¸“ä¸šçš„éªŒè¯å·¥ç¨‹å¸ˆï¼Œè¯·ç”Ÿæˆé«˜è´¨é‡çš„Verilogæµ‹è¯•å°ã€‚",
                temperature=0.1
            )
            
            # ä½¿ç”¨Function Calling write_fileå·¥å…·ä¿å­˜æµ‹è¯•å°
            tb_filename = f"{module_name}_tb.v"
            write_result = await self._tool_write_file(
                filename=tb_filename,
                content=response,
                description=f"ç”Ÿæˆçš„{module_name}æ¨¡å—æµ‹è¯•å°"
            )
            
            if not write_result.get("success", False):
                self.logger.error(f"âŒ æµ‹è¯•å°æ–‡ä»¶ä¿å­˜å¤±è´¥: {write_result.get('error', 'Unknown error')}")
                return {
                    "success": False,
                    "error": f"æµ‹è¯•å°æ–‡ä»¶ä¿å­˜å¤±è´¥: {write_result.get('error', 'Unknown error')}"
                }
            
            return {
                "success": True,
                "module_name": module_name,
                "testbench_code": response,
                "file_path": write_result.get("file_path"),
                "file_id": write_result.get("file_id"),
                "test_scenarios": test_scenarios,
                "simulation_config": {
                    "clock_period": clock_period,
                    "simulation_time": simulation_time,
                    "coverage_enabled": coverage_options.get('enable_coverage', False)
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æµ‹è¯•å°ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _tool_run_simulation(self, module_file: str = None, testbench_file: str = None,
                                 module_code: str = None, testbench_code: str = None,
                                 simulator: str = "iverilog",
                                 simulation_options: Dict = None) -> Dict[str, Any]:
        """è¿è¡Œä»¿çœŸå·¥å…·å®ç° - é›†æˆæ™ºèƒ½ä¾èµ–åˆ†æ"""
        try:
            self.logger.info(f"ğŸ”¬ è¿è¡Œä»¿çœŸ: {simulator}")
            simulation_options = simulation_options or {}
            
            # ç¡®å®šä½¿ç”¨æ–‡ä»¶è¿˜æ˜¯ä»£ç å†…å®¹
            files_to_compile = []
            
            if module_file and testbench_file:
                mod_file = Path(module_file)
                tb_file = Path(testbench_file)
                
                # éªŒè¯æ–‡ä»¶å­˜åœ¨
                if not mod_file.exists():
                    return {
                        "success": False,
                        "error": f"æ¨¡å—æ–‡ä»¶ä¸å­˜åœ¨: {module_file}",
                        "stage": "file_validation"
                    }
                if not tb_file.exists():
                    return {
                        "success": False,
                        "error": f"æµ‹è¯•å°æ–‡ä»¶ä¸å­˜åœ¨: {testbench_file}",
                        "stage": "file_validation"
                    }
                
                # ğŸ” åº”ç”¨æ™ºèƒ½ä¾èµ–åˆ†æ
                try:
                    from extensions.verilog_dependency_analyzer import VerilogDependencyAnalyzer
                    dep_analyzer = VerilogDependencyAnalyzer()
                    
                    # åˆ†æä¾èµ–å…³ç³»
                    dep_analyzer.analyze_file(str(mod_file))
                    dep_analyzer.analyze_file(str(tb_file))
                    
                    # ä»æ–‡ä»¶ç®¡ç†å™¨æŸ¥æ‰¾ç¼ºå¤±ä¾èµ–
                    missing_deps = []
                    for module in dep_analyzer.modules.values():
                        for dep in module.dependencies:
                            if dep not in dep_analyzer.modules:
                                missing_deps.append(dep)
                    
                    if missing_deps:
                        self.logger.info(f"ğŸ” å‘ç°ç¼ºå¤±ä¾èµ–: {missing_deps}")
                        additional_files = await self._find_missing_dependencies(missing_deps)
                        
                        for add_file in additional_files:
                            dep_analyzer.analyze_file(add_file)
                            files_to_compile.append(Path(add_file))
                            self.logger.info(f"âœ… æ·»åŠ ä¾èµ–æ–‡ä»¶: {Path(add_file).name}")
                    
                    # ç”Ÿæˆç¼–è¯‘é¡ºåº
                    target_modules = [m.name for m in dep_analyzer.modules.values() if m.is_testbench]
                    if target_modules:
                        ordered_files, missing = dep_analyzer.resolve_dependencies(target_modules)
                        if ordered_files:
                            files_to_compile = [Path(f) for f in ordered_files]
                        else:
                            files_to_compile = [mod_file, tb_file]
                    else:
                        files_to_compile = [mod_file, tb_file]
                        
                except Exception as dep_e:
                    self.logger.warning(f"âš ï¸ ä¾èµ–åˆ†æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ–‡ä»¶åˆ—è¡¨: {str(dep_e)}")
                    files_to_compile = [mod_file, tb_file]
                    
            else:
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                mod_file = self.artifacts_dir / "temp_module.v" 
                tb_file = self.artifacts_dir / "temp_testbench.v"
                
                with open(mod_file, 'w') as f:
                    f.write(module_code or "")
                with open(tb_file, 'w') as f:
                    f.write(testbench_code or "")
                
                files_to_compile = [mod_file, tb_file]
            
            # ä½¿ç”¨æ™ºèƒ½ä»¿çœŸæ‰§è¡Œ
            sim_result = await self._run_iverilog_simulation_with_deps(files_to_compile, simulation_options)
            
            # âœ… ä¿®å¤é”™è¯¯å¤„ç† - å‡†ç¡®åæ˜ ä»¿çœŸçŠ¶æ€
            actual_success = sim_result.get("success", False)
            
            if actual_success:
                self.logger.info(f"âœ… ä»¿çœŸæ‰§è¡ŒæˆåŠŸ")
            else:
                error_info = sim_result.get("error", "æœªçŸ¥é”™è¯¯")
                stage = sim_result.get("stage", "unknown")
                self.logger.error(f"âŒ ä»¿çœŸæ‰§è¡Œå¤±è´¥ ({stage}): {error_info}")
            
            return {
                "success": actual_success,  # ğŸ“Œ å‡†ç¡®åæ˜ å®é™…çŠ¶æ€
                "simulator": simulator,
                "simulation_output": sim_result.get("output", ""),
                "compilation_output": sim_result.get("compilation_output", ""),
                "waveform_file": sim_result.get("waveform_file"),
                "simulation_time": sim_result.get("simulation_time", 0),
                "errors": sim_result.get("errors", []),
                "warnings": sim_result.get("warnings", []),
                "stage": sim_result.get("stage", "complete"),
                "files_compiled": [str(f) for f in files_to_compile],
                "dependency_analysis": sim_result.get("dependency_analysis", {})
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ä»¿çœŸæ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return {
                "success": False,
                "error": f"ä»¿çœŸæ‰§è¡Œå¼‚å¸¸: {str(e)}",
                "stage": "exception"
            }
    
    async def _find_missing_dependencies(self, missing_modules: List[str]) -> List[str]:
        """ä»æ–‡ä»¶ç®¡ç†å™¨ä¸­æŸ¥æ‰¾ç¼ºå¤±çš„ä¾èµ–æ¨¡å—"""
        additional_files = []
        
        try:
            from core.file_manager import get_file_manager
            file_manager = get_file_manager()
            
            # è·å–æ‰€æœ‰Verilogæ–‡ä»¶
            all_verilog_files = file_manager.get_files_by_type("verilog")
            
            for missing_module in missing_modules:
                self.logger.info(f"ğŸ” æœç´¢ç¼ºå¤±æ¨¡å—: {missing_module}")
                
                # åœ¨æ–‡ä»¶åæˆ–å†…å®¹ä¸­æœç´¢æ¨¡å—
                for file_ref in all_verilog_files:
                    file_path = file_ref.file_path
                    filename = Path(file_path).stem.lower()
                    
                    # ç®€å•çš„æ–‡ä»¶ååŒ¹é…
                    if missing_module.lower() in filename:
                        if file_path not in additional_files:
                            additional_files.append(file_path)
                            self.logger.info(f"âœ… æ‰¾åˆ°ä¾èµ–æ–‡ä»¶: {Path(file_path).name}")
                        break
                    
                    # å†…å®¹åŒ¹é…ï¼ˆæ›´ç²¾ç¡®ä½†è¾ƒæ…¢ï¼‰
                    try:
                        if Path(file_path).exists():
                            content = Path(file_path).read_text(encoding='utf-8')
                            if f"module {missing_module}" in content:
                                if file_path not in additional_files:
                                    additional_files.append(file_path)
                                    self.logger.info(f"âœ… åœ¨æ–‡ä»¶å†…å®¹ä¸­æ‰¾åˆ°æ¨¡å— {missing_module}: {Path(file_path).name}")
                                break
                    except:
                        continue
        except Exception as e:
            self.logger.warning(f"æŸ¥æ‰¾ç¼ºå¤±ä¾èµ–æ—¶å‡ºé”™: {str(e)}")
        
        return additional_files
    
    async def _run_iverilog_simulation_with_deps(self, files_to_compile: List[Path], 
                                               options: Dict) -> Dict[str, Any]:
        """æ‰§è¡Œiverilogä»¿çœŸï¼ˆæ”¯æŒå¤šæ–‡ä»¶ä¾èµ–ï¼‰"""
        try:
            self.logger.info(f"ğŸ”¨ å¼€å§‹ç¼–è¯‘ {len(files_to_compile)} ä¸ªæ–‡ä»¶")
            
            # éªŒè¯æ‰€æœ‰æ–‡ä»¶å­˜åœ¨
            missing_files = []
            for file_path in files_to_compile:
                if not file_path.exists():
                    missing_files.append(str(file_path))
            
            if missing_files:
                return {
                    "success": False,
                    "error": f"ç¼–è¯‘æ–‡ä»¶ç¼ºå¤±: {missing_files}",
                    "stage": "file_validation"
                }
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            self.artifacts_dir.mkdir(parents=True, exist_ok=True)
            
            # æ„å»ºä»¿çœŸå‘½ä»¤
            output_file = self.artifacts_dir / "simulation"
            vcd_file = self.artifacts_dir / "simulation.vcd"
            
            compile_cmd = ["iverilog", "-o", str(output_file)]
            compile_cmd.extend([str(f) for f in files_to_compile])
            
            self.logger.info(f"ğŸ”¨ ç¼–è¯‘å‘½ä»¤: {' '.join(compile_cmd)}")
            
            # ç¼–è¯‘
            compile_result = await asyncio.create_subprocess_exec(
                *compile_cmd, 
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=self.artifacts_dir
            )
            
            compile_stdout, compile_stderr = await compile_result.communicate()
            
            compile_stdout_str = compile_stdout.decode('utf-8', errors='ignore') if compile_stdout else ""
            compile_stderr_str = compile_stderr.decode('utf-8', errors='ignore') if compile_stderr else ""
            
            if compile_result.returncode != 0:
                self.logger.error(f"âŒ ç¼–è¯‘å¤±è´¥ï¼Œè¿”å›ç : {compile_result.returncode}")
                self.logger.error(f"ç¼–è¯‘é”™è¯¯: {compile_stderr_str}")
                return {
                    "success": False,
                    "error": f"ç¼–è¯‘å¤±è´¥: {compile_stderr_str}",
                    "stage": "compilation",
                    "compilation_output": compile_stderr_str,
                    "command": " ".join(compile_cmd)
                }
            
            # éªŒè¯è¾“å‡ºæ–‡ä»¶å·²åˆ›å»º
            if not output_file.exists():
                return {
                    "success": False,
                    "error": f"ç¼–è¯‘æˆåŠŸä½†è¾“å‡ºæ–‡ä»¶æœªåˆ›å»º: {output_file}",
                    "stage": "compilation",
                    "compilation_output": compile_stdout_str
                }
            
            # è¿è¡Œä»¿çœŸ - ä½¿ç”¨vvpæ‰§è¡Œç¼–è¯‘åçš„ä»¿çœŸæ–‡ä»¶
            run_cmd = ["vvp", str(output_file)]
            run_result = await asyncio.create_subprocess_exec(
                *run_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=self.artifacts_dir
            )
            
            run_stdout, run_stderr = await run_result.communicate()
            
            # å®‰å…¨è§£ç è¾“å‡ºï¼Œå¤„ç†Noneæƒ…å†µ
            stdout_text = run_stdout.decode('utf-8', errors='ignore') if run_stdout else ""
            stderr_text = run_stderr.decode('utf-8', errors='ignore') if run_stderr else ""
            
            simulation_success = run_result.returncode == 0
            
            # è®°å½•ä»¿çœŸç»“æœ
            if simulation_success:
                self.logger.info(f"âœ… ä»¿çœŸæ‰§è¡ŒæˆåŠŸ")
                if stdout_text:
                    self.logger.debug(f"ä»¿çœŸè¾“å‡º: {stdout_text[:200]}...")
            else:
                self.logger.error(f"âŒ ä»¿çœŸæ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {run_result.returncode}")
                if stderr_text:
                    self.logger.error(f"ä»¿çœŸé”™è¯¯: {stderr_text}")
            
            return {
                "success": simulation_success,
                "output": stdout_text,
                "compilation_output": compile_stdout_str,
                "waveform_file": str(vcd_file) if vcd_file.exists() else None,
                "errors": [stderr_text] if stderr_text else [],
                "warnings": [],
                "return_code": run_result.returncode,
                "command": " ".join(run_cmd),
                "stage": "simulation" if simulation_success else "simulation_failed"
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ä»¿çœŸæ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "stage": "exception"
            }

    async def _run_iverilog_simulation(self, module_file: Path, testbench_file: Path,
                                     options: Dict) -> Dict[str, Any]:
        """æ‰§è¡Œiverilogä»¿çœŸ"""
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            self.artifacts_dir.mkdir(parents=True, exist_ok=True)
            
            # æ„å»ºä»¿çœŸå‘½ä»¤
            output_file = self.artifacts_dir / "simulation"
            vcd_file = self.artifacts_dir / "simulation.vcd"
            
            compile_cmd = [
                "iverilog",
                "-o", str(output_file),
                str(module_file),
                str(testbench_file)
            ]
            
            # ç¼–è¯‘
            compile_result = await asyncio.create_subprocess_exec(
                *compile_cmd, 
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=self.artifacts_dir
            )
            
            compile_stdout, compile_stderr = await compile_result.communicate()
            
            if compile_result.returncode != 0:
                return {
                    "success": False,
                    "output": compile_stderr.decode(),
                    "errors": [compile_stderr.decode()]
                }
            
            # è¿è¡Œä»¿çœŸ - ä½¿ç”¨vvpæ‰§è¡Œç¼–è¯‘åçš„ä»¿çœŸæ–‡ä»¶
            run_cmd = ["vvp", str(output_file)]
            run_result = await asyncio.create_subprocess_exec(
                *run_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=self.artifacts_dir
            )
            
            run_stdout, run_stderr = await run_result.communicate()
            
            # å®‰å…¨è§£ç è¾“å‡ºï¼Œå¤„ç†Noneæƒ…å†µ
            stdout_text = run_stdout.decode('utf-8', errors='ignore') if run_stdout else ""
            stderr_text = run_stderr.decode('utf-8', errors='ignore') if run_stderr else ""
            
            simulation_success = run_result.returncode == 0
            
            # è®°å½•ä»¿çœŸç»“æœ
            if simulation_success:
                self.logger.info(f"âœ… ä»¿çœŸæ‰§è¡ŒæˆåŠŸ")
                if stdout_text:
                    self.logger.debug(f"ä»¿çœŸè¾“å‡º: {stdout_text[:200]}...")
            else:
                self.logger.error(f"âŒ ä»¿çœŸæ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {run_result.returncode}")
                if stderr_text:
                    self.logger.error(f"ä»¿çœŸé”™è¯¯: {stderr_text}")
            
            return {
                "success": simulation_success,
                "output": stdout_text,
                "waveform_file": str(vcd_file) if vcd_file.exists() else None,
                "errors": [stderr_text] if stderr_text else [],
                "warnings": [],
                "return_code": run_result.returncode,
                "command": " ".join(run_cmd)
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _tool_analyze_code_quality(self, code: str, analysis_scope: List[str] = None,
                                       coding_standard: str = "ieee1800",
                                       severity_filter: List[str] = None,
                                       generate_report: bool = True) -> Dict[str, Any]:
        """ä»£ç è´¨é‡åˆ†æå·¥å…·å®ç°"""
        try:
            self.logger.info(f"ğŸ” åˆ†æä»£ç è´¨é‡: {coding_standard} æ ‡å‡†")
            
            analysis_scope = analysis_scope or ["syntax", "style", "structure"]
            severity_filter = severity_filter or ["error", "warning"]
            
            issues = []
            metrics = {
                "lines_of_code": len(code.splitlines()),
                "modules_count": code.count("module "),
                "complexity_score": 0,
                "maintainability_index": 0
            }
            
            # å¤šç»´åº¦ä»£ç åˆ†æ
            if "syntax" in analysis_scope:
                syntax_issues = self._analyze_syntax(code)
                issues.extend(syntax_issues)
            
            if "style" in analysis_scope:
                style_issues = self._analyze_style(code)
                issues.extend(style_issues)
            
            if "structure" in analysis_scope:
                structure_issues = self._analyze_structure(code)
                issues.extend(structure_issues)
                
            if "naming" in analysis_scope:
                naming_issues = self._analyze_naming_conventions(code)
                issues.extend(naming_issues)
            
            # è¿‡æ»¤ä¸¥é‡åº¦
            filtered_issues = [
                issue for issue in issues 
                if issue["severity"] in severity_filter
            ]
            
            # è®¡ç®—è´¨é‡è¯„åˆ†
            metrics["complexity_score"] = max(0, 100 - len(filtered_issues) * 5)
            metrics["maintainability_index"] = self._calculate_maintainability_index(code, filtered_issues)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = None
            if generate_report:
                report = self._generate_quality_report(code, filtered_issues, metrics, analysis_scope)
                report_file = self.artifacts_dir / "code_quality_report.html"
                with open(report_file, 'w', encoding='utf-8') as f:
                    f.write(report)
            
            return {
                "success": True,
                "quality_score": metrics["complexity_score"],
                "maintainability_index": metrics["maintainability_index"],
                "metrics": metrics,
                "issues": filtered_issues,
                "analysis_scope": analysis_scope,
                "coding_standard": coding_standard,
                "report_file": str(report_file) if generate_report else None,
                "recommendations": self._generate_recommendations(filtered_issues)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ä»£ç è´¨é‡åˆ†æå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _analyze_syntax(self, code: str) -> List[Dict]:
        """è¯­æ³•åˆ†æ"""
        issues = []
        
        if "module " not in code:
            issues.append({
                "type": "syntax",
                "severity": "error",
                "line": 0,
                "message": "No module declaration found"
            })
        
        if "endmodule" not in code:
            issues.append({
                "type": "syntax",
                "severity": "error", 
                "line": 0,
                "message": "Missing endmodule statement"
            })
        
        # æ£€æŸ¥æ‹¬å·åŒ¹é…
        paren_count = code.count('(') - code.count(')')
        if paren_count != 0:
            issues.append({
                "type": "syntax",
                "severity": "error",
                "line": 0,
                "message": f"Unmatched parentheses: {abs(paren_count)} {'opening' if paren_count > 0 else 'closing'}"
            })
        
        return issues
    
    def _analyze_style(self, code: str) -> List[Dict]:
        """ç¼–ç é£æ ¼åˆ†æ"""
        issues = []
        lines = code.splitlines()
        
        for i, line in enumerate(lines, 1):
            # è¡Œé•¿åº¦æ£€æŸ¥
            if len(line) > 120:
                issues.append({
                    "type": "style",
                    "severity": "warning",
                    "line": i,
                    "message": f"Line exceeds 120 characters ({len(line)} chars)"
                })
            
            # ç¼©è¿›æ£€æŸ¥
            if line.strip() and not line.startswith(' ') and not line.startswith('\t'):
                if any(keyword in line for keyword in ['always', 'if', 'case', 'for']):
                    issues.append({
                        "type": "style",
                        "severity": "info",
                        "line": i,
                        "message": "Consider proper indentation for control structures"
                    })
        
        return issues
    
    def _analyze_structure(self, code: str) -> List[Dict]:
        """ç»“æ„åˆ†æ"""
        issues = []
        
        # æ£€æŸ¥alwayså—çš„æ•æ„Ÿåˆ—è¡¨
        always_blocks = re.findall(r'always\s*@\s*\([^)]+\)', code)
        for block in always_blocks:
            if 'posedge' not in block and 'negedge' not in block and '*' not in block:
                issues.append({
                    "type": "structure",
                    "severity": "warning",
                    "line": 0,
                    "message": "Always block without edge sensitivity may cause synthesis issues"
                })
        
        return issues
    
    def _analyze_naming_conventions(self, code: str) -> List[Dict]:
        """å‘½åè§„èŒƒåˆ†æ"""
        issues = []
        
        # æ£€æŸ¥æ¨¡å—å
        module_matches = re.findall(r'module\s+(\w+)', code)
        for module_name in module_matches:
            if not re.match(r'^[a-z][a-z0-9_]*$', module_name):
                issues.append({
                    "type": "naming",
                    "severity": "suggestion",
                    "line": 0,
                    "message": f"Module name '{module_name}' should follow snake_case convention"
                })
        
        return issues
    
    def _calculate_maintainability_index(self, code: str, issues: List[Dict]) -> float:
        """è®¡ç®—ç»´æŠ¤æ€§æŒ‡æ•°"""
        lines = len(code.splitlines())
        error_count = sum(1 for issue in issues if issue["severity"] == "error")
        warning_count = sum(1 for issue in issues if issue["severity"] == "warning")
        
        # ç®€åŒ–çš„ç»´æŠ¤æ€§æŒ‡æ•°è®¡ç®—
        base_score = 100
        error_penalty = error_count * 10
        warning_penalty = warning_count * 5
        complexity_penalty = max(0, lines - 100) * 0.1
        
        return max(0, base_score - error_penalty - warning_penalty - complexity_penalty)
    
    def _generate_quality_report(self, code: str, issues: List[Dict], 
                               metrics: Dict, analysis_scope: List[str]) -> str:
        """ç”ŸæˆHTMLè´¨é‡æŠ¥å‘Š"""
        html_template = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Code Quality Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background-color: #f0f0f0; padding: 10px; }}
        .metrics {{ display: flex; gap: 20px; margin: 20px 0; }}
        .metric {{ background-color: #e8f4f8; padding: 10px; border-radius: 5px; }}
        .issues {{ margin: 20px 0; }}
        .issue {{ margin: 10px 0; padding: 10px; border-left: 4px solid #ccc; }}
        .error {{ border-left-color: red; }}
        .warning {{ border-left-color: orange; }}
        .info {{ border-left-color: blue; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Code Quality Analysis Report</h1>
        <p>Analysis completed: {analysis_scope}</p>
    </div>
    
    <div class="metrics">
        <div class="metric">
            <h3>Quality Score</h3>
            <p>{metrics['complexity_score']}/100</p>
        </div>
        <div class="metric">
            <h3>Lines of Code</h3>
            <p>{metrics['lines_of_code']}</p>
        </div>
        <div class="metric">
            <h3>Maintainability</h3>
            <p>{metrics['maintainability_index']:.1f}/100</p>
        </div>
    </div>
    
    <div class="issues">
        <h2>Issues Found ({len(issues)})</h2>
        {''.join([f'''
        <div class="issue {issue['severity']}">
            <strong>{issue['severity'].upper()}</strong>
            {f" (Line {issue['line']})" if issue.get('line') else ""}
            : {issue['message']}
        </div>
        ''' for issue in issues])}
    </div>
</body>
</html>
"""
        return html_template
    
    def _generate_recommendations(self, issues: List[Dict]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        error_count = sum(1 for issue in issues if issue["severity"] == "error")
        warning_count = sum(1 for issue in issues if issue["severity"] == "warning")
        
        if error_count > 0:
            recommendations.append(f"ä¿®å¤ {error_count} ä¸ªè¯­æ³•é”™è¯¯ä»¥ç¡®ä¿ä»£ç å¯ç¼–è¯‘")
        
        if warning_count > 5:
            recommendations.append("è€ƒè™‘è§£å†³ä¸»è¦çš„ä»£ç é£æ ¼è­¦å‘Šä»¥æé«˜å¯è¯»æ€§")
        
        if any("line exceeds" in issue["message"] for issue in issues):
            recommendations.append("å°†é•¿è¡Œåˆ†è§£ä¸ºå¤šè¡Œä»¥æé«˜ä»£ç å¯è¯»æ€§")
            
        if len(recommendations) == 0:
            recommendations.append("ä»£ç è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒï¼")
        
        return recommendations
    
    async def _tool_generate_build_script(self, verilog_files: List[str], testbench_files: List[str],
                                        script_type: str = "makefile", target_name: str = "simulation",
                                        build_options: Dict = None) -> Dict[str, Any]:
        """ç”Ÿæˆæ„å»ºè„šæœ¬å·¥å…·å®ç°"""
        try:
            self.logger.info(f"ğŸ“œ ç”Ÿæˆæ„å»ºè„šæœ¬: {script_type}")
            build_options = build_options or {}
            
            script_content = await self._generate_script_content(
                verilog_files, testbench_files, script_type, target_name, build_options
            )
            
            # ç¡®å®šè„šæœ¬æ–‡ä»¶å
            if script_type == "makefile":
                script_filename = "Makefile"
            elif script_type == "bash":
                script_filename = f"build_{target_name}.sh"
            elif script_type == "tcl":
                script_filename = f"build_{target_name}.tcl"
            else:
                script_filename = f"build_{target_name}.py"
            
            # ä¿å­˜è„šæœ¬
            script_path = self.artifacts_dir / script_filename
            with open(script_path, 'w', encoding='utf-8') as f:
                f.write(script_content)
            
            # ä¸ºshellè„šæœ¬æ·»åŠ æ‰§è¡Œæƒé™
            if script_type == "bash":
                os.chmod(script_path, 0o755)
            
            return {
                "success": True,
                "script_type": script_type,
                "script_filename": script_filename,
                "script_path": str(script_path),
                "target_name": target_name,
                "verilog_files": verilog_files,
                "testbench_files": testbench_files,
                "build_options": build_options
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æ„å»ºè„šæœ¬ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _generate_script_content(self, verilog_files: List[str], testbench_files: List[str],
                                     script_type: str, target_name: str, build_options: Dict) -> str:
        """ç”Ÿæˆè„šæœ¬å†…å®¹"""
        simulator = build_options.get("simulator", "iverilog")
        
        if script_type == "makefile":
            return self._generate_makefile(verilog_files, testbench_files, target_name, simulator)
        elif script_type == "bash":
            return self._generate_bash_script(verilog_files, testbench_files, target_name, simulator)
        else:
            return f"# {script_type} script generation not implemented yet"
    
    def _generate_makefile(self, verilog_files: List[str], testbench_files: List[str],
                          target_name: str, simulator: str) -> str:
        """ç”ŸæˆMakefile"""
        all_files = " ".join(verilog_files + testbench_files)
        
        makefile_content = f"""# Generated Makefile for {target_name}
# Simulator: {simulator}

VERILOG_FILES = {' '.join(verilog_files)}
TESTBENCH_FILES = {' '.join(testbench_files)}
ALL_FILES = $(VERILOG_FILES) $(TESTBENCH_FILES)

TARGET = {target_name}
VCD_FILE = $(TARGET).vcd

# Default target
all: compile simulate

# Compile the design
compile: $(TARGET)

$(TARGET): $(ALL_FILES)
\t{simulator} -o $(TARGET) $(ALL_FILES)

# Run simulation
simulate: $(TARGET)
\t./$(TARGET)

# View waveforms (if GTKWave is available)
waves: $(VCD_FILE)
\tgtkwave $(VCD_FILE) &

# Clean generated files
clean:
\trm -f $(TARGET) $(VCD_FILE) *.vvp

# Lint check
lint:
\tverilator --lint-only $(VERILOG_FILES)

.PHONY: all compile simulate waves clean lint
"""
        return makefile_content
    
    def _generate_bash_script(self, verilog_files: List[str], testbench_files: List[str],
                            target_name: str, simulator: str) -> str:
        """ç”ŸæˆBashè„šæœ¬"""
        all_files = " ".join(verilog_files + testbench_files)
        
        bash_content = f"""#!/bin/bash
# Generated build script for {target_name}
# Simulator: {simulator}

set -e  # Exit on any error

VERILOG_FILES="{' '.join(verilog_files)}"
TESTBENCH_FILES="{' '.join(testbench_files)}"
TARGET="{target_name}"
VCD_FILE="${target_name}.vcd"

# Function to compile
compile() {{
    echo "Compiling design..."
    {simulator} -o $TARGET $VERILOG_FILES $TESTBENCH_FILES
    echo "Compilation completed successfully"
}}

# Function to simulate
simulate() {{
    echo "Running simulation..."
    ./$TARGET
    echo "Simulation completed"
}}

# Function to clean
clean() {{
    echo "Cleaning generated files..."
    rm -f $TARGET $VCD_FILE *.vvp
    echo "Clean completed"
}}

# Main execution
case "$1" in
    compile)
        compile
        ;;
    simulate)
        simulate
        ;;
    all)
        compile
        simulate
        ;;
    clean)
        clean
        ;;
    *)
        echo "Usage: $0 {{compile|simulate|all|clean}}"
        echo "  compile  - Compile the design"
        echo "  simulate - Run simulation"
        echo "  all      - Compile and simulate"
        echo "  clean    - Clean generated files"
        exit 1
        ;;
esac
"""
        return bash_content
    
    async def _tool_execute_build_script(self, script_name: str, action: str = "all",
                                       arguments: List[str] = None, timeout: int = 300,
                                       working_directory: str = None) -> Dict[str, Any]:
        """æ‰§è¡Œæ„å»ºè„šæœ¬å·¥å…·å®ç°"""
        try:
            self.logger.info(f"âš™ï¸ æ‰§è¡Œæ„å»ºè„šæœ¬: {script_name} - {action}")
            
            arguments = arguments or []
            work_dir = Path(working_directory) if working_directory else self.artifacts_dir
            script_path = work_dir / script_name
            
            if not script_path.exists():
                return {
                    "success": False,
                    "error": f"Script file not found: {script_name}"
                }
            
            # æ„å»ºå‘½ä»¤
            if script_name.lower() == "makefile" or script_name.endswith(".mk"):
                cmd = ["make", "-f", script_name, action] + arguments
            elif script_name.endswith(".sh"):
                cmd = ["bash", script_name, action] + arguments
            else:
                cmd = [str(script_path), action] + arguments
            
            # æ‰§è¡Œè„šæœ¬
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                cwd=work_dir
            )
            
            try:
                stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=timeout)
            except asyncio.TimeoutError:
                process.kill()
                return {
                    "success": False,
                    "error": f"Script execution timed out after {timeout} seconds"
                }
            
            success = process.returncode == 0
            
            return {
                "success": success,
                "return_code": process.returncode,
                "stdout": stdout.decode() if stdout else "",
                "stderr": stderr.decode() if stderr else "",
                "script_name": script_name,
                "action": action,
                "execution_time": timeout,  # This would be actual time in real implementation
                "working_directory": str(work_dir)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æ„å»ºè„šæœ¬æ‰§è¡Œå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _tool_analyze_coverage(self, coverage_data_file: str, coverage_types: List[str] = None,
                                   threshold: Dict = None, report_format: str = "html") -> Dict[str, Any]:
        """è¦†ç›–ç‡åˆ†æå·¥å…·å®ç°"""
        try:
            self.logger.info(f"ğŸ“Š åˆ†æè¦†ç›–ç‡: {coverage_data_file}")
            
            coverage_types = coverage_types or ["line", "toggle", "branch"]
            threshold = threshold or {
                "line_coverage": 80,
                "branch_coverage": 70,
                "toggle_coverage": 60
            }
            
            # æ¨¡æ‹Ÿè¦†ç›–ç‡æ•°æ®åˆ†æï¼ˆå®é™…å®ç°ä¸­ä¼šè§£æçœŸå®çš„è¦†ç›–ç‡æ–‡ä»¶ï¼‰
            coverage_results = {
                "line_coverage": 85.5,
                "branch_coverage": 78.2,
                "toggle_coverage": 65.8,
                "condition_coverage": 72.1
            }
            
            # æ£€æŸ¥é˜ˆå€¼
            threshold_check = {}
            for cov_type, value in coverage_results.items():
                threshold_key = f"{cov_type}"
                if threshold_key in threshold:
                    threshold_check[cov_type] = {
                        "value": value,
                        "threshold": threshold[threshold_key],
                        "passed": value >= threshold[threshold_key]
                    }
            
            # ç”ŸæˆæŠ¥å‘Š
            report_file = None
            if report_format == "html":
                report_content = self._generate_coverage_html_report(coverage_results, threshold_check)
                report_file = self.artifacts_dir / "coverage_report.html"
                with open(report_file, 'w', encoding='utf-8') as f:
                    f.write(report_content)
            
            return {
                "success": True,
                "coverage_results": coverage_results,
                "threshold_check": threshold_check,
                "overall_passed": all(check["passed"] for check in threshold_check.values()),
                "report_file": str(report_file) if report_file else None,
                "coverage_types": coverage_types,
                "recommendations": self._generate_coverage_recommendations(coverage_results, threshold)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ è¦†ç›–ç‡åˆ†æå¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _generate_coverage_html_report(self, results: Dict, threshold_check: Dict) -> str:
        """ç”ŸæˆHTMLè¦†ç›–ç‡æŠ¥å‘Š"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Coverage Analysis Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background-color: #f0f0f0; padding: 15px; }}
        .coverage-item {{ margin: 10px 0; padding: 10px; border: 1px solid #ddd; }}
        .passed {{ background-color: #d4edda; }}
        .failed {{ background-color: #f8d7da; }}
        .progress-bar {{ width: 100%; height: 20px; background-color: #f0f0f0; }}
        .progress-fill {{ height: 100%; background-color: #28a745; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Coverage Analysis Report</h1>
    </div>
    
    <div class="coverage-results">
        <h2>Coverage Results</h2>
        {''.join([f'''
        <div class="coverage-item {'passed' if check['passed'] else 'failed'}">
            <h3>{cov_type.replace('_', ' ').title()}</h3>
            <p>Coverage: {check['value']:.1f}% (Threshold: {check['threshold']}%)</p>
            <div class="progress-bar">
                <div class="progress-fill" style="width: {check['value']}%;"></div>
            </div>
            <p>Status: {'PASSED' if check['passed'] else 'FAILED'}</p>
        </div>
        ''' for cov_type, check in threshold_check.items()])}
    </div>
</body>
</html>
"""
        return html_content
    
    def _generate_coverage_recommendations(self, results: Dict, threshold: Dict) -> List[str]:
        """ç”Ÿæˆè¦†ç›–ç‡æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        for cov_type, value in results.items():
            threshold_key = f"{cov_type}"
            if threshold_key in threshold:
                if value < threshold[threshold_key]:
                    recommendations.append(
                        f"æé«˜{cov_type.replace('_', ' ')}è¦†ç›–ç‡: å½“å‰{value:.1f}%, ç›®æ ‡{threshold[threshold_key]}%"
                    )
        
        if not recommendations:
            recommendations.append("æ‰€æœ‰è¦†ç›–ç‡ç›®æ ‡å‡å·²è¾¾æˆï¼Œæµ‹è¯•è´¨é‡è‰¯å¥½ï¼")
        
        return recommendations
    
    async def _tool_analyze_test_failures(self, design_code: str, 
                                         compilation_errors: str = "", 
                                         simulation_errors: str = "",
                                         test_assertions: str = "",
                                         testbench_code: str = "",
                                         iteration_number: int = 1,
                                         previous_fixes: List[str] = None) -> Dict[str, Any]:
        """æµ‹è¯•å¤±è´¥åˆ†æå·¥å…·å®ç° - ä¸“é—¨ç”¨äºTDDå¾ªç¯ä¸­çš„é”™è¯¯åˆ†æä¸ä¿®å¤å»ºè®®"""
        try:
            self.logger.info(f"ğŸ” åˆ†æç¬¬{iteration_number}æ¬¡è¿­ä»£çš„æµ‹è¯•å¤±è´¥")
            
            previous_fixes = previous_fixes or []
            analysis_results = {
                "failure_types": [],
                "root_causes": [],
                "fix_suggestions": [],
                "priority_level": "medium",
                "confidence": 0.8
            }
            
            # åˆ†æç¼–è¯‘é”™è¯¯
            if compilation_errors:
                compile_analysis = self._analyze_compilation_errors(compilation_errors, design_code)
                analysis_results["failure_types"].extend(compile_analysis["failure_types"])
                analysis_results["root_causes"].extend(compile_analysis["root_causes"])
                analysis_results["fix_suggestions"].extend(compile_analysis["fix_suggestions"])
            
            # åˆ†æä»¿çœŸé”™è¯¯
            if simulation_errors:
                sim_analysis = self._analyze_simulation_errors(simulation_errors, design_code)
                analysis_results["failure_types"].extend(sim_analysis["failure_types"])
                analysis_results["root_causes"].extend(sim_analysis["root_causes"])
                analysis_results["fix_suggestions"].extend(sim_analysis["fix_suggestions"])
            
            # åˆ†ææµ‹è¯•æ–­è¨€å¤±è´¥
            if test_assertions:
                assertion_analysis = self._analyze_assertion_failures(test_assertions, design_code, testbench_code)
                analysis_results["failure_types"].extend(assertion_analysis["failure_types"])
                analysis_results["root_causes"].extend(assertion_analysis["root_causes"])
                analysis_results["fix_suggestions"].extend(assertion_analysis["fix_suggestions"])
            
            # å»é‡å¹¶ä¼˜å…ˆæ’åº
            analysis_results["failure_types"] = list(set(analysis_results["failure_types"]))
            analysis_results["root_causes"] = list(set(analysis_results["root_causes"]))
            
            # è¿‡æ»¤æ‰ä¹‹å‰å·²å°è¯•çš„ä¿®å¤æ–¹æ³•
            filtered_suggestions = []
            for suggestion in analysis_results["fix_suggestions"]:
                is_duplicate = any(prev_fix.lower() in suggestion.lower() or 
                                 suggestion.lower() in prev_fix.lower() 
                                 for prev_fix in previous_fixes)
                if not is_duplicate:
                    filtered_suggestions.append(suggestion)
            
            analysis_results["fix_suggestions"] = filtered_suggestions[:5]  # æœ€å¤š5ä¸ªå»ºè®®
            
            # æ ¹æ®è¿­ä»£æ¬¡æ•°è°ƒæ•´ä¼˜å…ˆçº§
            if iteration_number >= 3:
                analysis_results["priority_level"] = "high"
            elif len(analysis_results["failure_types"]) > 2:
                analysis_results["priority_level"] = "high"
            
            # ç”Ÿæˆè¯¦ç»†çš„ä¿®å¤æŒ‡å¯¼
            detailed_guidance = self._generate_detailed_fix_guidance(
                analysis_results, design_code, iteration_number
            )
            
            self.logger.info(f"âœ… æµ‹è¯•å¤±è´¥åˆ†æå®Œæˆï¼Œå‘ç° {len(analysis_results['failure_types'])} ç§å¤±è´¥ç±»å‹")
            
            return {
                "success": True,
                "analysis": analysis_results,
                "detailed_guidance": detailed_guidance,
                "iteration_context": {
                    "iteration_number": iteration_number,
                    "previous_attempts": len(previous_fixes),
                    "failure_complexity": len(analysis_results["failure_types"])
                },
                "next_steps": self._recommend_next_steps(analysis_results, iteration_number)
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æµ‹è¯•å¤±è´¥åˆ†æå¼‚å¸¸: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "fallback_suggestions": [
                    "æ£€æŸ¥ä»£ç è¯­æ³•å’Œæ¨¡å—å®ä¾‹åŒ–",
                    "éªŒè¯ä¿¡å·å®½åº¦å’Œç±»å‹åŒ¹é…",
                    "ç¡®è®¤æµ‹è¯•å°æ¿€åŠ±çš„æ­£ç¡®æ€§"
                ]
            }
    
    def _analyze_compilation_errors(self, errors: str, design_code: str) -> Dict[str, List[str]]:
        """åˆ†æç¼–è¯‘é”™è¯¯"""
        analysis = {
            "failure_types": [],
            "root_causes": [],
            "fix_suggestions": []
        }
        
        error_lines = errors.lower()
        
        # å¸¸è§ç¼–è¯‘é”™è¯¯æ¨¡å¼è¯†åˆ«
        if "undefined" in error_lines and "macro" in error_lines:
            analysis["failure_types"].append("æœªå®šä¹‰å®é”™è¯¯")
            analysis["root_causes"].append("ä»£ç ä¸­åŒ…å«æœªå®šä¹‰çš„å®æˆ–é¢„å¤„ç†æŒ‡ä»¤")
            analysis["fix_suggestions"].append("ç§»é™¤æˆ–æ­£ç¡®å®šä¹‰æ‰€æœ‰å®ï¼ˆå¦‚ `defineï¼‰æŒ‡ä»¤")
            analysis["fix_suggestions"].append("æ£€æŸ¥æ˜¯å¦é”™è¯¯ä½¿ç”¨äº†C/C++é£æ ¼çš„å®å®šä¹‰")
        
        if "unknown module" in error_lines or "not found" in error_lines:
            analysis["failure_types"].append("æ¨¡å—å¼•ç”¨é”™è¯¯")
            analysis["root_causes"].append("å¼•ç”¨äº†ä¸å­˜åœ¨çš„æ¨¡å—æˆ–å­æ¨¡å—")
            analysis["fix_suggestions"].append("æ£€æŸ¥æ‰€æœ‰æ¨¡å—å®ä¾‹åŒ–ï¼Œç¡®ä¿è¢«è°ƒç”¨çš„æ¨¡å—å·²å®šä¹‰")
            analysis["fix_suggestions"].append("éªŒè¯æ¨¡å—åç§°æ‹¼å†™å’Œå¤§å°å†™")
        
        if "syntax error" in error_lines or "parse error" in error_lines:
            analysis["failure_types"].append("è¯­æ³•é”™è¯¯")
            analysis["root_causes"].append("Verilogè¯­æ³•ä¸ç¬¦åˆè§„èŒƒ")
            analysis["fix_suggestions"].append("æ£€æŸ¥è¯­æ³•ï¼šæ‹¬å·åŒ¹é…ã€åˆ†å·ã€begin/endé…å¯¹")
            analysis["fix_suggestions"].append("éªŒè¯ç«¯å£å£°æ˜å’Œä¿¡å·å®šä¹‰æ ¼å¼")
        
        if "width mismatch" in error_lines or "size mismatch" in error_lines:
            analysis["failure_types"].append("ä½å®½ä¸åŒ¹é…")
            analysis["root_causes"].append("ä¿¡å·ä½å®½ä¸åŒ¹é…æˆ–èµ‹å€¼é”™è¯¯")
            analysis["fix_suggestions"].append("æ£€æŸ¥æ‰€æœ‰ä¿¡å·çš„ä½å®½å®šä¹‰å’Œè¿æ¥")
            analysis["fix_suggestions"].append("ç¡®ä¿assignè¯­å¥ä¸¤ä¾§ä½å®½åŒ¹é…")
        
        return analysis
    
    def _analyze_simulation_errors(self, errors: str, design_code: str) -> Dict[str, List[str]]:
        """åˆ†æä»¿çœŸé”™è¯¯"""
        analysis = {
            "failure_types": [],
            "root_causes": [],
            "fix_suggestions": []
        }
        
        error_lines = errors.lower()
        
        if "x" in error_lines and "propagation" in error_lines:
            analysis["failure_types"].append("æœªå®šä¹‰çŠ¶æ€ä¼ æ’­")
            analysis["root_causes"].append("ä¿¡å·åˆå§‹åŒ–ä¸å½“å¯¼è‡´XçŠ¶æ€ä¼ æ’­")
            analysis["fix_suggestions"].append("ç¡®ä¿æ‰€æœ‰ä¿¡å·éƒ½æœ‰æ˜ç¡®çš„åˆå§‹å€¼")
            analysis["fix_suggestions"].append("æ£€æŸ¥å¤ä½é€»è¾‘å’Œæ—¶é’ŸåŸŸ")
        
        if "time violation" in error_lines or "setup" in error_lines:
            analysis["failure_types"].append("æ—¶åºè¿ä¾‹")
            analysis["root_causes"].append("æ—¶åºçº¦æŸä¸æ»¡è¶³æˆ–æ—¶é’Ÿåæ–œ")
            analysis["fix_suggestions"].append("è°ƒæ•´æ—¶é’Ÿå‘¨æœŸæˆ–ä¼˜åŒ–å…³é”®è·¯å¾„")
            analysis["fix_suggestions"].append("æ£€æŸ¥setupå’Œholdæ—¶é—´è¦æ±‚")
        
        return analysis
    
    def _analyze_assertion_failures(self, assertions: str, design_code: str, testbench_code: str) -> Dict[str, List[str]]:
        """åˆ†ææµ‹è¯•æ–­è¨€å¤±è´¥"""
        analysis = {
            "failure_types": [],
            "root_causes": [],
            "fix_suggestions": []
        }
        
        assertion_lines = assertions.lower()
        
        if "assert" in assertion_lines and "failed" in assertion_lines:
            analysis["failure_types"].append("åŠŸèƒ½æµ‹è¯•å¤±è´¥")
            analysis["root_causes"].append("è®¾è®¡é€»è¾‘ä¸é¢„æœŸè¡Œä¸ºä¸ç¬¦")
            analysis["fix_suggestions"].append("é€æ­¥è°ƒè¯•ï¼šæ·»åŠ $displayè¯­å¥è·Ÿè¸ªä¿¡å·å˜åŒ–")
            analysis["fix_suggestions"].append("éªŒè¯ç®—æ³•é€»è¾‘ï¼Œç‰¹åˆ«æ˜¯è¿›ä½å’Œè¾¹ç•Œæ¡ä»¶")
            analysis["fix_suggestions"].append("æ£€æŸ¥æµ‹è¯•å‘é‡æ˜¯å¦æ­£ç¡®è®¾ç½®")
        
        if "timeout" in assertion_lines or "infinite" in assertion_lines:
            analysis["failure_types"].append("ä»¿çœŸè¶…æ—¶")
            analysis["root_causes"].append("å¯èƒ½å­˜åœ¨æ— é™å¾ªç¯æˆ–æ­»é”")
            analysis["fix_suggestions"].append("æ£€æŸ¥alwayså—çš„æ•æ„Ÿåˆ—è¡¨")
            analysis["fix_suggestions"].append("ç¡®ä¿æ‰€æœ‰è·¯å¾„éƒ½æœ‰$finishè¯­å¥")
        
        return analysis
    
    def _generate_detailed_fix_guidance(self, analysis: Dict, design_code: str, iteration: int) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†çš„ä¿®å¤æŒ‡å¯¼"""
        guidance = {
            "immediate_actions": [],
            "code_modifications": [],
            "verification_steps": [],
            "debugging_tips": []
        }
        
        # æ ¹æ®å¤±è´¥ç±»å‹ç”Ÿæˆå…·ä½“æŒ‡å¯¼
        for failure_type in analysis["failure_types"]:
            if "æœªå®šä¹‰å®" in failure_type:
                guidance["immediate_actions"].append("æ‰«æä»£ç ä¸­çš„æ‰€æœ‰ ` å­—ç¬¦ï¼Œç§»é™¤ä¸å¿…è¦çš„å®å®šä¹‰")
                guidance["code_modifications"].append("å°†ç±»ä¼¼ `simple_8bit_adderã€`verilog ç­‰æ›¿æ¢ä¸ºæ­£ç¡®çš„æ¨¡å—å")
                guidance["verification_steps"].append("ä½¿ç”¨è¯­æ³•æ£€æŸ¥å™¨éªŒè¯æ¸…ç†åçš„ä»£ç ")
            
            elif "æ¨¡å—å¼•ç”¨é”™è¯¯" in failure_type:
                guidance["immediate_actions"].append("åˆ—å‡ºæ‰€æœ‰æ¨¡å—å®ä¾‹åŒ–ï¼ŒéªŒè¯è¢«è°ƒç”¨æ¨¡å—å­˜åœ¨")
                guidance["code_modifications"].append("ç¡®ä¿å­æ¨¡å—åœ¨åŒä¸€æ–‡ä»¶ä¸­å®šä¹‰æˆ–æ­£ç¡®åŒ…å«")
                guidance["verification_steps"].append("ç¼–è¯‘å•ä¸ªæ¨¡å—æ–‡ä»¶éªŒè¯æ¨¡å—å®šä¹‰æ­£ç¡®")
            
            elif "åŠŸèƒ½æµ‹è¯•å¤±è´¥" in failure_type:
                guidance["debugging_tips"].append("åœ¨å…³é”®ä¿¡å·å¤„æ·»åŠ  $display(\"signal=%b\", signal) è¯­å¥")
                guidance["debugging_tips"].append("æ‰‹åŠ¨è®¡ç®—å‡ ä¸ªæµ‹è¯•æ¡ˆä¾‹çš„é¢„æœŸè¾“å‡º")
                guidance["verification_steps"].append("ä½¿ç”¨æ³¢å½¢æŸ¥çœ‹å™¨åˆ†æä¿¡å·æ—¶åº")
        
        # æ ¹æ®è¿­ä»£æ¬¡æ•°è°ƒæ•´æŒ‡å¯¼ç­–ç•¥
        if iteration >= 3:
            guidance["immediate_actions"].insert(0, "è€ƒè™‘é‡æ–°è®¾è®¡æ¶æ„ï¼Œå½“å‰æ–¹æ³•å¯èƒ½å­˜åœ¨æ ¹æœ¬æ€§é—®é¢˜")
            guidance["debugging_tips"].append("å¯»æ±‚ä»£ç å®¡æŸ¥æˆ–ä½¿ç”¨ä¸åŒçš„å®ç°æ–¹æ³•")
        
        return guidance
    
    def _recommend_next_steps(self, analysis: Dict, iteration: int) -> List[str]:
        """æ¨èä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
        steps = []
        
        if "æœªå®šä¹‰å®é”™è¯¯" in analysis["failure_types"]:
            steps.append("1. æ¸…ç†ä»£ç ä¸­çš„æ‰€æœ‰å®å®šä¹‰é—®é¢˜")
            steps.append("2. é‡æ–°ç¼–è¯‘éªŒè¯è¯­æ³•æ­£ç¡®æ€§")
        
        if "æ¨¡å—å¼•ç”¨é”™è¯¯" in analysis["failure_types"]:
            steps.append("3. éªŒè¯æ‰€æœ‰æ¨¡å—ä¾èµ–å…³ç³»")
            steps.append("4. ç¡®ä¿ç¼–è¯‘é¡ºåºæ­£ç¡®")
        
        if "åŠŸèƒ½æµ‹è¯•å¤±è´¥" in analysis["failure_types"]:
            steps.append("5. æ·»åŠ è°ƒè¯•è¾“å‡ºè¯­å¥")
            steps.append("6. é€æ­¥éªŒè¯ç®—æ³•é€»è¾‘")
        
        if iteration >= 2:
            steps.append("7. è€ƒè™‘ç®€åŒ–è®¾è®¡æˆ–åˆ†è§£å¤æ‚é€»è¾‘")
        
        steps.append("8. è¿è¡Œä¿®å¤åçš„å®Œæ•´æµ‹è¯•éªŒè¯")
        
        return steps