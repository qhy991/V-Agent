#!/usr/bin/env python3
"""
çœŸå®LLMé©±åŠ¨çš„ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“

Real LLM-powered Code Review Agent
"""

import json
import asyncio
from typing import Dict, Any, Set
from pathlib import Path

from core.base_agent import BaseAgent, TaskMessage
from core.enums import AgentCapability
from core.response_format import ResponseFormat, TaskStatus, ResponseType, QualityMetrics
from llm_integration.enhanced_llm_client import EnhancedLLMClient
from config.config import FrameworkConfig


class RealCodeReviewAgent(BaseAgent):
    """çœŸå®LLMé©±åŠ¨çš„ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“"""
    
    def __init__(self, config: FrameworkConfig = None):
        super().__init__(
            agent_id="real_code_review_agent",
            role="code_reviewer",
            capabilities={
                AgentCapability.CODE_REVIEW,
                AgentCapability.QUALITY_ANALYSIS,
                AgentCapability.SPECIFICATION_ANALYSIS
            }
        )
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.config = config or FrameworkConfig.from_env()
        self.llm_client = EnhancedLLMClient(self.config.llm)
        
        self.logger.info(f"ğŸ” çœŸå®ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def get_capabilities(self) -> Set[AgentCapability]:
        return {
            AgentCapability.CODE_REVIEW,
            AgentCapability.QUALITY_ANALYSIS,
            AgentCapability.SPECIFICATION_ANALYSIS
        }
    
    def get_specialty_description(self) -> str:
        return "çœŸå®LLMé©±åŠ¨çš„ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“ï¼Œæä¾›ä¸“ä¸šçš„Verilog/SystemVerilogä»£ç è´¨é‡åˆ†æå’Œæ”¹è¿›å»ºè®®"
    
    async def execute_enhanced_task(self, enhanced_prompt: str,
                                  original_message: TaskMessage,
                                  file_contents: Dict[str, Dict]) -> Dict[str, Any]:
        """æ‰§è¡Œä»£ç å®¡æŸ¥ä»»åŠ¡"""
        task_id = original_message.task_id
        self.logger.info(f"ğŸ” å¼€å§‹æ‰§è¡Œä»£ç å®¡æŸ¥ä»»åŠ¡: {task_id}")
        
        try:
            # 1. æå–è¦å®¡æŸ¥çš„ä»£ç 
            code_to_review = await self._extract_code_from_files(file_contents)
            
            if not code_to_review:
                error_response = self.create_error_response_formatted(
                    task_id=task_id,
                    error_message="æœªæ‰¾åˆ°å¯å®¡æŸ¥çš„ä»£ç æ–‡ä»¶",
                    error_details="è¯·ç¡®ä¿æä¾›äº†åŒ…å«Verilogä»£ç çš„æ–‡ä»¶",
                    format_type=ResponseFormat.JSON
                )
                return {"formatted_response": error_response}
            
            # 2. æ‰§è¡Œè¯¦ç»†çš„ä»£ç å®¡æŸ¥
            review_results = []
            for file_path, code_content in code_to_review.items():
                self.logger.info(f"ğŸ“ å®¡æŸ¥æ–‡ä»¶: {file_path}")
                review_result = await self._perform_detailed_review(
                    file_path, code_content, enhanced_prompt
                )
                review_results.append(review_result)
            
            # 3. ç”Ÿæˆç»¼åˆå®¡æŸ¥æŠ¥å‘Š
            comprehensive_report = await self._generate_comprehensive_report(review_results)
            
            # 4. è®¡ç®—æ•´ä½“è´¨é‡æŒ‡æ ‡
            overall_quality = self._calculate_overall_quality(review_results)
            
            # 5. ä¿å­˜å®¡æŸ¥æŠ¥å‘Š
            output_files = await self._save_review_files(
                comprehensive_report, review_results, task_id
            )
            
            # 6. åˆ›å»ºæ ‡å‡†åŒ–å“åº”
            response = await self._create_review_response(
                task_id, review_results, overall_quality, output_files, comprehensive_report
            )
            
            return {"formatted_response": response}
            
        except Exception as e:
            self.logger.error(f"âŒ ä»£ç å®¡æŸ¥ä»»åŠ¡å¤±è´¥: {str(e)}")
            error_response = self.create_error_response_formatted(
                task_id=task_id,
                error_message=f"ä»£ç å®¡æŸ¥å¤±è´¥: {str(e)}",
                error_details="è¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶å’ŒLLMè¿æ¥çŠ¶æ€",
                format_type=ResponseFormat.JSON
            )
            return {"formatted_response": error_response}
    
    async def _extract_code_from_files(self, file_contents: Dict[str, Dict]) -> Dict[str, str]:
        """ä»æ–‡ä»¶ä¸­æå–ä»£ç å†…å®¹"""
        code_files = {}
        
        for file_path, content_info in file_contents.items():
            file_type = content_info.get('type', '').lower()
            content = content_info.get('content', '')
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä»£ç æ–‡ä»¶
            if (file_type in ['verilog', 'systemverilog', 'vhdl'] or 
                file_path.endswith(('.v', '.sv', '.vhd')) or
                'module' in content):
                
                if content.strip():
                    code_files[file_path] = content
                    self.logger.info(f"ğŸ“„ å‘ç°ä»£ç æ–‡ä»¶: {file_path} ({len(content)} å­—ç¬¦)")
        
        return code_files
    
    async def _perform_detailed_review(self, file_path: str, code_content: str, 
                                     task_context: str) -> Dict[str, Any]:
        """æ‰§è¡Œè¯¦ç»†çš„ä»£ç å®¡æŸ¥"""
        
        review_prompt = f"""
ä½ æ˜¯ä¸€ä½æ‹¥æœ‰15å¹´ç»éªŒçš„èµ„æ·±Verilog/FPGAè®¾è®¡ä¸“å®¶å’Œä»£ç å®¡æŸ¥å‘˜ã€‚è¯·å¯¹ä»¥ä¸‹ä»£ç è¿›è¡Œå…¨é¢ã€æ·±å…¥çš„å®¡æŸ¥ã€‚

æ–‡ä»¶è·¯å¾„: {file_path}
ä»»åŠ¡ä¸Šä¸‹æ–‡: {task_context}

ä»£ç å†…å®¹:
```verilog
{code_content}
```

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œä¸“ä¸šå®¡æŸ¥ï¼š

## 1. è¯­æ³•å’Œè¯­ä¹‰åˆ†æ
- Verilogè¯­æ³•æ­£ç¡®æ€§
- ä¿¡å·å£°æ˜å’Œä½¿ç”¨
- ç«¯å£è¿æ¥æ­£ç¡®æ€§
- æ•°æ®ç±»å‹ä½¿ç”¨è§„èŒƒ

## 2. è®¾è®¡è´¨é‡è¯„ä¼°
- æ¨¡å—åŒ–è®¾è®¡åˆç†æ€§
- æ¥å£è®¾è®¡æ¸…æ™°åº¦
- å‚æ•°åŒ–ç¨‹åº¦
- ä»£ç å¤ç”¨æ€§

## 3. æ—¶åºè®¾è®¡å®¡æŸ¥
- æ—¶é’ŸåŸŸå¤„ç†
- å¤ä½é€»è¾‘è®¾è®¡
- ç»„åˆé€»è¾‘å’Œæ—¶åºé€»è¾‘åˆ†ç¦»
- æ½œåœ¨çš„ç«äº‰å†’é™©

## 4. æ€§èƒ½å’Œæ•ˆç‡
- å…³é”®è·¯å¾„åˆ†æ
- èµ„æºä½¿ç”¨æ•ˆç‡
- æµæ°´çº¿è®¾è®¡ï¼ˆå¦‚é€‚ç”¨ï¼‰
- é¢ç§¯åŠŸè€—è€ƒè™‘

## 5. å¯ç»´æŠ¤æ€§å’Œå¯è¯»æ€§
- å‘½åè§„èŒƒä¸€è‡´æ€§
- æ³¨é‡Šå®Œæ•´æ€§å’Œå‡†ç¡®æ€§
- ä»£ç ç»“æ„æ¸…æ™°åº¦
- è°ƒè¯•å‹å¥½æ€§

## 6. é”™è¯¯æ£€æŸ¥å’Œè¾¹ç•Œå¤„ç†
- è¾¹ç•Œæ¡ä»¶å¤„ç†
- é”™è¯¯çŠ¶æ€å¤„ç†
- å¼‚å¸¸æƒ…å†µè€ƒè™‘
- æ–­è¨€ä½¿ç”¨

## 7. è¡Œä¸šæœ€ä½³å®è·µ
- ç¼–ç è§„èŒƒéµå¾ª
- è®¾è®¡æ¨¡å¼åº”ç”¨
- å¯ç»¼åˆæ€§è€ƒè™‘
- éªŒè¯å‹å¥½æ€§

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯¦ç»†çš„å®¡æŸ¥ç»“æœï¼š

{{
    "file_path": "{file_path}",
    "review_summary": "æ•´ä½“å®¡æŸ¥æ€»ç»“",
    "quality_scores": {{
        "syntax_correctness": 0.95,
        "design_quality": 0.88,
        "timing_design": 0.85,
        "performance": 0.80,
        "maintainability": 0.90,
        "error_handling": 0.75,
        "best_practices": 0.82
    }},
    "critical_issues": [
        {{
            "severity": "high",
            "category": "timing",
            "line_number": 45,
            "description": "å‘ç°çš„å…³é”®é—®é¢˜æè¿°",
            "impact": "å¯¹è®¾è®¡çš„å½±å“",
            "recommendation": "å…·ä½“ä¿®å¤å»ºè®®"
        }}
    ],
    "warnings": [
        {{
            "severity": "medium",
            "category": "style",
            "line_number": 23,
            "description": "è­¦å‘Šé—®é¢˜æè¿°",
            "recommendation": "æ”¹è¿›å»ºè®®"
        }}
    ],
    "suggestions": [
        {{
            "category": "optimization",
            "description": "ä¼˜åŒ–å»ºè®®æè¿°",
            "benefit": "ä¼˜åŒ–å¸¦æ¥çš„å¥½å¤„"
        }}
    ],
    "positive_aspects": [
        "ä»£ç çš„ä¼˜ç‚¹1",
        "ä»£ç çš„ä¼˜ç‚¹2"
    ],
    "overall_assessment": "æ•´ä½“è¯„ä»·å’Œå»ºè®®",
    "next_actions": [
        "å»ºè®®çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨1",
        "å»ºè®®çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨2"
    ]
}}

è¯·ç¡®ä¿å®¡æŸ¥ç»“æœä¸“ä¸šã€è¯¦ç»†ã€å¯æ“ä½œï¼š
"""
        
        try:
            response = await self.llm_client.send_prompt(
                prompt=review_prompt,
                temperature=0.3,
                max_tokens=3000,
                json_mode=True
            )
            
            review_result = json.loads(response)
            self.logger.info(f"âœ… æ–‡ä»¶å®¡æŸ¥å®Œæˆ: {file_path}")
            return review_result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ LLMå®¡æŸ¥å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€å®¡æŸ¥: {str(e)}")
            return self._basic_code_review(file_path, code_content)
    
    def _basic_code_review(self, file_path: str, code_content: str) -> Dict[str, Any]:
        """åŸºç¡€ä»£ç å®¡æŸ¥ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        issues = []
        warnings = []
        
        # åŸºæœ¬è¯­æ³•æ£€æŸ¥
        if "module" not in code_content:
            issues.append({
                "severity": "high",
                "category": "syntax",
                "line_number": 1,
                "description": "ç¼ºå°‘moduleå®šä¹‰",
                "impact": "ä»£ç æ— æ³•ç¼–è¯‘",
                "recommendation": "æ·»åŠ proper moduleå®šä¹‰"
            })
        
        if "endmodule" not in code_content:
            issues.append({
                "severity": "high", 
                "category": "syntax",
                "line_number": len(code_content.split('\n')),
                "description": "ç¼ºå°‘endmodule",
                "impact": "ä»£ç æ— æ³•ç¼–è¯‘",
                "recommendation": "æ·»åŠ endmoduleè¯­å¥"
            })
        
        # åŸºæœ¬é£æ ¼æ£€æŸ¥
        if "//" not in code_content and "/*" not in code_content:
            warnings.append({
                "severity": "medium",
                "category": "style",
                "line_number": 1,
                "description": "ç¼ºå°‘æ³¨é‡Š",
                "recommendation": "ä¸ºä»£ç æ·»åŠ æ³¨é‡Šä»¥æé«˜å¯è¯»æ€§"
            })
        
        return {
            "file_path": file_path,
            "review_summary": "åŸºç¡€ä»£ç å®¡æŸ¥å®Œæˆï¼ˆLLMä¸å¯ç”¨ï¼‰",
            "quality_scores": {
                "syntax_correctness": 0.7,
                "design_quality": 0.6,
                "timing_design": 0.6,
                "performance": 0.6,
                "maintainability": 0.5,
                "error_handling": 0.5,
                "best_practices": 0.6
            },
            "critical_issues": issues,
            "warnings": warnings,
            "suggestions": [
                {"category": "general", "description": "å»ºè®®ä½¿ç”¨å®Œæ•´çš„LLMå®¡æŸ¥è·å¾—æ›´è¯¦ç»†çš„åˆ†æ", "benefit": "æ›´å‡†ç¡®çš„ä»£ç è´¨é‡è¯„ä¼°"}
            ],
            "positive_aspects": ["ä»£ç åŸºæœ¬ç»“æ„å­˜åœ¨"],
            "overall_assessment": "éœ€è¦LLMè¿›è¡Œæ›´è¯¦ç»†çš„å®¡æŸ¥",
            "next_actions": ["ä¿®å¤å‘ç°çš„åŸºæœ¬é—®é¢˜", "ä½¿ç”¨LLMè¿›è¡Œå®Œæ•´å®¡æŸ¥"]
        }
    
    async def _generate_comprehensive_report(self, review_results: list) -> str:
        """ç”Ÿæˆç»¼åˆå®¡æŸ¥æŠ¥å‘Š"""
        
        if not review_results:
            return "æ— å¯ç”¨çš„å®¡æŸ¥ç»“æœ"
        
        # ä½¿ç”¨LLMç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report_prompt = f"""
ä½œä¸ºä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹å„ä¸ªæ–‡ä»¶çš„è¯¦ç»†å®¡æŸ¥ç»“æœï¼Œç”Ÿæˆä¸€ä»½ç»¼åˆçš„ä»£ç å®¡æŸ¥æŠ¥å‘Šã€‚

å®¡æŸ¥ç»“æœ:
{json.dumps(review_results, indent=2, ensure_ascii=False)}

è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„ç»¼åˆå®¡æŸ¥æŠ¥å‘Šï¼ŒåŒ…å«ï¼š

1. **æ‰§è¡Œæ€»ç»“** - æ•´ä½“ä»£ç è´¨é‡æ¦‚è¿°
2. **å…³é”®å‘ç°** - æœ€é‡è¦çš„é—®é¢˜å’Œäº®ç‚¹
3. **è´¨é‡æŒ‡æ ‡æ±‡æ€»** - å„ç»´åº¦è´¨é‡åˆ†æ•°ç»Ÿè®¡
4. **ä¼˜å…ˆçº§è¡ŒåŠ¨è®¡åˆ’** - æŒ‰ä¼˜å…ˆçº§æ’åºçš„æ”¹è¿›å»ºè®®
5. **é£é™©è¯„ä¼°** - æ½œåœ¨é£é™©å’Œå½±å“åˆ†æ
6. **æœ€ä½³å®è·µå»ºè®®** - é•¿æœŸæ”¹è¿›å»ºè®®

æŠ¥å‘Šåº”è¯¥ï¼š
- ç»“æ„æ¸…æ™°ï¼Œä¾¿äºé˜…è¯»
- é‡ç‚¹çªå‡ºï¼Œactionable
- å¹³è¡¡é—®é¢˜å’Œä¼˜ç‚¹
- æä¾›å…·ä½“çš„æ”¹è¿›è·¯å¾„

è¯·ä»¥Markdownæ ¼å¼è¿”å›æŠ¥å‘Šï¼š
"""
        
        try:
            comprehensive_report = await self.llm_client.send_prompt(
                prompt=report_prompt,
                temperature=0.4,
                max_tokens=2500
            )
            
            self.logger.info("ğŸ“Š ç»¼åˆå®¡æŸ¥æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            return comprehensive_report.strip()
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ LLMæŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ¿: {str(e)}")
            return self._generate_basic_report(review_results)
    
    def _generate_basic_report(self, review_results: list) -> str:
        """ç”ŸæˆåŸºç¡€æŠ¥å‘Šï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        total_files = len(review_results)
        total_issues = sum(len(result.get('critical_issues', [])) for result in review_results)
        total_warnings = sum(len(result.get('warnings', [])) for result in review_results)
        
        report = f"""# ä»£ç å®¡æŸ¥ç»¼åˆæŠ¥å‘Š

## å®¡æŸ¥æ¦‚è§ˆ
- å®¡æŸ¥æ–‡ä»¶æ•°: {total_files}
- å‘ç°å…³é”®é—®é¢˜: {total_issues}
- å‘ç°è­¦å‘Š: {total_warnings}

## æ–‡ä»¶å®¡æŸ¥ç»“æœ
"""
        
        for result in review_results:
            file_path = result.get('file_path', 'Unknown')
            summary = result.get('review_summary', 'No summary')
            report += f"\n### {file_path}\n{summary}\n"
        
        report += "\n## å»ºè®®è¡ŒåŠ¨\n1. ä¿®å¤æ‰€æœ‰å…³é”®é—®é¢˜\n2. å¤„ç†è­¦å‘Šé—®é¢˜\n3. è€ƒè™‘ä¼˜åŒ–å»ºè®®"
        
        return report
    
    def _calculate_overall_quality(self, review_results: list) -> QualityMetrics:
        """è®¡ç®—æ•´ä½“è´¨é‡æŒ‡æ ‡"""
        if not review_results:
            return QualityMetrics(0.5, 0.5, 0.5, 0.0, 0.5, 0.5)
        
        # æ”¶é›†æ‰€æœ‰è´¨é‡åˆ†æ•°
        all_scores = []
        syntax_scores = []
        design_scores = []
        maintainability_scores = []
        performance_scores = []
        
        for result in review_results:
            quality_scores = result.get('quality_scores', {})
            
            syntax_score = quality_scores.get('syntax_correctness', 0.5)
            design_score = quality_scores.get('design_quality', 0.5)
            timing_score = quality_scores.get('timing_design', 0.5)
            perf_score = quality_scores.get('performance', 0.5)
            maint_score = quality_scores.get('maintainability', 0.5)
            error_score = quality_scores.get('error_handling', 0.5)
            practice_score = quality_scores.get('best_practices', 0.5)
            
            # è®¡ç®—è¯¥æ–‡ä»¶çš„æ•´ä½“åˆ†æ•°
            file_overall = (syntax_score + design_score + timing_score + 
                          perf_score + maint_score + error_score + practice_score) / 7
            
            all_scores.append(file_overall)
            syntax_scores.append(syntax_score)
            design_scores.append(design_score)
            maintainability_scores.append(maint_score)
            performance_scores.append(perf_score)
        
        # è®¡ç®—å¹³å‡å€¼
        overall_score = sum(all_scores) / len(all_scores)
        avg_syntax = sum(syntax_scores) / len(syntax_scores)
        avg_design = sum(design_scores) / len(design_scores)
        avg_maintainability = sum(maintainability_scores) / len(maintainability_scores)
        avg_performance = sum(performance_scores) / len(performance_scores)
        
        return QualityMetrics(
            overall_score=overall_score,
            syntax_score=avg_syntax,
            functionality_score=avg_design,
            test_coverage=0.0,  # å®¡æŸ¥é˜¶æ®µæ— æµ‹è¯•è¦†ç›–ç‡
            documentation_quality=avg_maintainability,
            performance_score=avg_performance
        )
    
    async def _save_review_files(self, comprehensive_report: str, review_results: list,
                               task_id: str) -> list:
        """ä¿å­˜å®¡æŸ¥æŠ¥å‘Šæ–‡ä»¶"""
        output_files = []
        
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = Path("./output")
            output_dir.mkdir(exist_ok=True)
            
            # ä¿å­˜ç»¼åˆæŠ¥å‘Š
            report_path = output_dir / f"code_review_report_{task_id}.md"
            report_ref = await self.save_result_to_file(
                content=comprehensive_report,
                file_path=str(report_path),
                file_type="documentation"
            )
            output_files.append(report_ref)
            
            # ä¿å­˜è¯¦ç»†å®¡æŸ¥ç»“æœ
            detailed_results = {
                "task_id": task_id,
                "review_timestamp": task_id,
                "total_files_reviewed": len(review_results),
                "detailed_results": review_results
            }
            
            details_path = output_dir / f"review_details_{task_id}.json"
            details_ref = await self.save_result_to_file(
                content=json.dumps(detailed_results, indent=2, ensure_ascii=False),
                file_path=str(details_path),
                file_type="json"
            )
            output_files.append(details_ref)
            
            self.logger.info(f"ğŸ’¾ å®¡æŸ¥æŠ¥å‘Šä¿å­˜å®Œæˆ: {len(output_files)} ä¸ªæ–‡ä»¶")
            return output_files
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜å®¡æŸ¥æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return []
    
    async def _create_review_response(self, task_id: str, review_results: list,
                                    overall_quality: QualityMetrics, output_files: list,
                                    comprehensive_report: str) -> str:
        """åˆ›å»ºæ ‡å‡†åŒ–å®¡æŸ¥å“åº”"""
        
        builder = self.create_response_builder(task_id)
        
        # æ·»åŠ ç”Ÿæˆçš„æ–‡ä»¶
        for file_ref in output_files:
            builder.add_generated_file(
                file_ref.file_path,
                file_ref.file_type,
                file_ref.description
            )
        
        # ç»Ÿè®¡é—®é¢˜æ•°é‡
        total_critical = sum(len(result.get('critical_issues', [])) for result in review_results)
        total_warnings = sum(len(result.get('warnings', [])) for result in review_results)
        
        # æ ¹æ®å®¡æŸ¥ç»“æœæ·»åŠ é—®é¢˜
        if total_critical > 0:
            builder.add_issue(
                "error", "high",
                f"å‘ç° {total_critical} ä¸ªå…³é”®é—®é¢˜éœ€è¦ç«‹å³ä¿®å¤",
                solution="è¯·æŸ¥çœ‹è¯¦ç»†å®¡æŸ¥æŠ¥å‘Šå¹¶é€ä¸€ä¿®å¤å…³é”®é—®é¢˜"
            )
        
        if total_warnings > 0:
            builder.add_issue(
                "warning", "medium",
                f"å‘ç° {total_warnings} ä¸ªè­¦å‘Šé—®é¢˜å»ºè®®å¤„ç†",
                solution="å»ºè®®åœ¨ä¸‹æ¬¡è¿­ä»£ä¸­å¤„ç†è¿™äº›è­¦å‘Šé—®é¢˜"
            )
        
        if overall_quality.overall_score < 0.6:
            builder.add_issue(
                "warning", "high",
                f"æ•´ä½“ä»£ç è´¨é‡è¾ƒä½ (åˆ†æ•°: {overall_quality.overall_score:.2f})",
                solution="å»ºè®®è¿›è¡Œå…¨é¢çš„ä»£ç é‡æ„"
            )
        
        # æ·»åŠ ä¸‹ä¸€æ­¥å»ºè®®
        builder.add_next_step("ä»”ç»†é˜…è¯»ç»¼åˆå®¡æŸ¥æŠ¥å‘Š")
        builder.add_next_step("ä¼˜å…ˆä¿®å¤æ‰€æœ‰å…³é”®é—®é¢˜")
        
        if total_warnings > 0:
            builder.add_next_step("å¤„ç†è­¦å‘Šé—®é¢˜ä»¥æé«˜ä»£ç è´¨é‡")
        
        if overall_quality.performance_score < 0.7:
            builder.add_next_step("è€ƒè™‘æ€§èƒ½ä¼˜åŒ–")
        
        builder.add_next_step("åœ¨ä¿®å¤åé‡æ–°è¿›è¡Œä»£ç å®¡æŸ¥")
        
        # æ·»åŠ å…ƒæ•°æ®
        builder.add_metadata("files_reviewed", len(review_results))
        builder.add_metadata("critical_issues", total_critical)
        builder.add_metadata("warnings", total_warnings)
        builder.add_metadata("overall_quality_score", overall_quality.overall_score)
        builder.add_metadata("review_type", "comprehensive")
        builder.add_metadata("llm_powered", True)
        
        # æ„å»ºå“åº” - ä»£ç å®¡æŸ¥ä»»åŠ¡æ€»æ˜¯100%å®Œæˆ
        status = TaskStatus.SUCCESS
        completion = 100.0
        
        # æ„å»ºæ¶ˆæ¯
        message_parts = [
            f"ä»£ç å®¡æŸ¥å®Œæˆï¼Œå…±å®¡æŸ¥ {len(review_results)} ä¸ªæ–‡ä»¶"
        ]
        
        if total_critical > 0:
            message_parts.append(f"å‘ç° {total_critical} ä¸ªå…³é”®é—®é¢˜")
        if total_warnings > 0:
            message_parts.append(f"{total_warnings} ä¸ªè­¦å‘Š")
            
        message_parts.append(f"æ•´ä½“è´¨é‡åˆ†æ•°: {overall_quality.overall_score:.2f}")
        
        response = builder.build(
            response_type=ResponseType.QUALITY_REPORT,
            status=status,
            message=", ".join(message_parts),
            completion_percentage=completion,
            quality_metrics=overall_quality
        )
        
        return response.format_response(ResponseFormat.JSON)