#!/usr/bin/env python3
"""
å¢å¼ºçš„LLMåè°ƒæ™ºèƒ½ä½“æµ‹è¯•å…¥å£
==================================================

è¿™ä¸ªè„šæœ¬æä¾›äº†ä¸€ä¸ªå®Œæ•´ã€æ˜“ç”¨çš„LLMåè°ƒæµ‹è¯•å…¥å£ï¼Œæ”¯æŒï¼š
- æ¯æ¬¡å®éªŒä½¿ç”¨ç‹¬ç«‹æ–‡ä»¶å¤¹å­˜å‚¨ç»“æœ
- é…ç½®åŒ–çš„å®éªŒå‚æ•°
- è¯¦ç»†çš„è¿›åº¦è·Ÿè¸ªå’Œç»“æœåˆ†æ
- LLMé©±åŠ¨çš„æ™ºèƒ½åè°ƒ
- å®éªŒæŠ¥å‘Šå’Œæ–‡ä»¶ç®¡ç†

ä½¿ç”¨æ–¹æ³•:
    python test_llm_coordinator_enhanced.py --design 4bit_adder
    python test_llm_coordinator_enhanced.py --design counter --iterations 3
    python test_llm_coordinator_enhanced.py --design custom --requirements "è®¾è®¡éœ€æ±‚æ–‡æœ¬"
    python test_llm_coordinator_enhanced.py --output-dir "custom_exp_dir"
    python test_llm_coordinator_enhanced.py --design custom --external-testbench "path/to/testbench.v"
"""

import asyncio
import sys
import argparse
import json
import time
import os
import codecs
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime

# è®¾ç½®ç¼–ç ç¯å¢ƒå˜é‡
os.environ['PYTHONIOENCODING'] = 'utf-8'

def setup_encoding():
    """è®¾ç½®é€‚å½“çš„ç¼–ç ä»¥å¤„ç†ä¸åŒæ“ä½œç³»ç»Ÿçš„è¾“å‡º"""
    if os.name == 'nt':  # Windows
        try:
            os.system('chcp 65001 > nul 2>&1')
        except:
            pass
        os.environ['PYTHONIOENCODING'] = 'utf-8'
        if hasattr(sys.stdout, 'reconfigure'):
            try:
                sys.stdout.reconfigure(encoding='utf-8', errors='replace')
                sys.stderr.reconfigure(encoding='utf-8', errors='replace')
            except:
                pass
    else:
        os.environ['PYTHONIOENCODING'] = 'utf-8'
        if hasattr(sys.stdout, 'reconfigure'):
            sys.stdout.reconfigure(encoding='utf-8')
            sys.stderr.reconfigure(encoding='utf-8')

# åº”ç”¨ç¼–ç è®¾ç½®
setup_encoding()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config.config import FrameworkConfig
from core.llm_coordinator_agent import LLMCoordinatorAgent
from agents.enhanced_real_verilog_agent import EnhancedRealVerilogAgent
from agents.enhanced_real_code_reviewer import EnhancedRealCodeReviewAgent
from core.enhanced_logging_config import get_test_logger


class EnhancedLLMCoordinatorTest:
    """å¢å¼ºçš„LLMåè°ƒæ™ºèƒ½ä½“æµ‹è¯•å…¥å£"""
    
    # é¢„å®šä¹‰çš„è®¾è®¡æ¨¡æ¿
    DESIGN_TEMPLATES = {
        "4bit_adder": {
            "description": """
è®¾è®¡ä¸€ä¸ª4ä½åŠ æ³•å™¨æ¨¡å—ï¼ŒåŒ…å«ï¼š

**åŸºæœ¬è¦æ±‚**ï¼š
1. ä¸¤ä¸ª4ä½è¾“å…¥ç«¯å£ï¼ša[3:0], b[3:0]
2. ä¸€ä¸ªè¿›ä½è¾“å…¥ï¼šcin
3. ä¸€ä¸ª4ä½è¾“å‡ºç«¯å£ï¼šsum[3:0]
4. ä¸€ä¸ªè¿›ä½è¾“å‡ºï¼šcout
5. ä½¿ç”¨ç»„åˆé€»è¾‘å®ç°ï¼ˆä¸åŒ…å«æ—¶é’Ÿå’Œå¤ä½ä¿¡å·ï¼‰

**åŠŸèƒ½è¦æ±‚**ï¼š
- å®ç°åŸºæœ¬çš„äºŒè¿›åˆ¶åŠ æ³•è¿ç®—
- æ­£ç¡®å¤„ç†è¿›ä½ä¼ æ’­
- è¾“å‡ºç»“æœ = a + b + cin

**è´¨é‡è¦æ±‚**ï¼š
- ä»£ç å¿…é¡»ç¬¦åˆVerilog-2001æ ‡å‡†
- åŒ…å«è¯¦ç»†çš„ç«¯å£æ³¨é‡Š
- ç”Ÿæˆå¯¹åº”çš„æµ‹è¯•å°éªŒè¯åŠŸèƒ½
""",
            "expected_files": ["four_bit_adder.v", "four_bit_adder_tb.v"]
        },
        
        "8bit_counter": {
            "description": """
è®¾è®¡ä¸€ä¸ª8ä½äºŒè¿›åˆ¶è®¡æ•°å™¨æ¨¡å—ï¼ŒåŒ…å«ï¼š

**åŸºæœ¬è¦æ±‚**ï¼š
1. æ—¶é’Ÿè¾“å…¥ï¼šclk
2. å¤ä½è¾“å…¥ï¼šrst (é«˜ç”µå¹³æœ‰æ•ˆ)
3. ä½¿èƒ½è¾“å…¥ï¼šen
4. 8ä½è¾“å‡ºï¼šcount[7:0]

**åŠŸèƒ½è¦æ±‚**ï¼š
- åœ¨æ—¶é’Ÿä¸Šå‡æ²¿è®¡æ•°
- å¤ä½æ—¶è¾“å‡ºä¸º0
- ä½¿èƒ½æœ‰æ•ˆæ—¶é€’å¢è®¡æ•°
- è¾¾åˆ°æœ€å¤§å€¼åå›ç»•åˆ°0

**è´¨é‡è¦æ±‚**ï¼š
- ä½¿ç”¨åŒæ­¥å¤ä½
- ç¬¦åˆæ—¶åºè®¾è®¡è§„èŒƒ
- åŒ…å«å®Œæ•´çš„æµ‹è¯•å°
""",
            "expected_files": ["counter_8bit.v", "counter_8bit_tb.v"]
        },
        
        "alu_simple": {
            "description": """
è®¾è®¡ä¸€ä¸ªç®€å•çš„ç®—æœ¯é€»è¾‘å•å…ƒ(ALU)ï¼Œæ”¯æŒåŸºæœ¬æ“ä½œï¼š

**åŸºæœ¬è¦æ±‚**ï¼š
1. ä¸¤ä¸ª8ä½æ•°æ®è¾“å…¥ï¼ša[7:0], b[7:0]
2. 3ä½æ“ä½œç è¾“å…¥ï¼šop[2:0]
3. 8ä½ç»“æœè¾“å‡ºï¼šresult[7:0]
4. æ ‡å¿—ä½è¾“å‡ºï¼šzero_flag

**æ“ä½œç å®šä¹‰**ï¼š
- 3'b000: åŠ æ³• (a + b)
- 3'b001: å‡æ³• (a - b)
- 3'b010: é€»è¾‘ä¸ (a & b)
- 3'b011: é€»è¾‘æˆ– (a | b)
- 3'b100: å¼‚æˆ– (a ^ b)
- å…¶ä»–: è¾“å‡º0

**è´¨é‡è¦æ±‚**ï¼š
- ä½¿ç”¨ç»„åˆé€»è¾‘å®ç°
- åŒ…å«é›¶æ ‡å¿—æ£€æµ‹
- å®Œæ•´çš„åŠŸèƒ½æµ‹è¯•
""",
            "expected_files": ["alu_simple.v", "alu_simple_tb.v"]
        }
    }
    
    # å®éªŒé…ç½®æ¨¡æ¿
    EXPERIMENT_CONFIGS = {
        "fast": {"max_iterations": 1, "timeout_per_iteration": 180},
        "standard": {"max_iterations": 3, "timeout_per_iteration": 300},
        "thorough": {"max_iterations": 5, "timeout_per_iteration": 600}
    }
    
    def __init__(self, 
                design_type: str = "4bit_adder", 
                config_profile: str = "standard",
                custom_requirements: str = None,
                output_dir: str = None,
                max_iterations: int = None,
                external_testbench_path: str = None):
        """åˆå§‹åŒ–å¢å¼ºLLMåè°ƒæµ‹è¯•"""
        
        self.design_type = design_type
        self.config_profile = config_profile
        self.custom_requirements = custom_requirements
        self.external_testbench_path = external_testbench_path
        
        # å®éªŒé…ç½®
        base_config = self.EXPERIMENT_CONFIGS.get(config_profile, self.EXPERIMENT_CONFIGS["standard"])
        self.max_iterations = max_iterations or base_config["max_iterations"]
        self.timeout_per_iteration = base_config["timeout_per_iteration"]
        
        # ç”Ÿæˆå®éªŒIDå’Œè¾“å‡ºç›®å½•
        timestamp = int(time.time())
        self.experiment_id = f"llm_coordinator_{design_type}_{timestamp}"
        
        # åˆ›å»ºä¸“ç”¨è¾“å‡ºç›®å½•
        if output_dir:
            self.output_dir = Path(output_dir)
        else:
            self.output_dir = project_root / "llm_experiments" / self.experiment_id
        
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–å®éªŒçŠ¶æ€
        self.experiment_state = {
            "start_time": timestamp,
            "generated_files": [],
            "agent_results": {},
            "coordination_history": [],
            "total_iterations": 0
        }
        
        print(f"ğŸ§  å¢å¼ºLLMåè°ƒæ™ºèƒ½ä½“æµ‹è¯•")
        print("=" * 60)
        print(f"   è®¾è®¡ç±»å‹: {design_type}")
        print(f"   é…ç½®æ¡£æ¡ˆ: {config_profile}")
        print(f"   æœ€å¤§è¿­ä»£: {self.max_iterations}")
        print(f"   å®éªŒID: {self.experiment_id}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        if self.external_testbench_path:
            print(f"   å¤–éƒ¨Testbench: {self.external_testbench_path}")
        print("=" * 60)
    
    def get_design_requirements(self) -> str:
        """è·å–è®¾è®¡éœ€æ±‚"""
        if self.custom_requirements:
            return self.custom_requirements
        
        template = self.DESIGN_TEMPLATES.get(self.design_type)
        if not template:
            # å¦‚æœæ²¡æœ‰é¢„å®šä¹‰æ¨¡æ¿ï¼Œä½¿ç”¨é€šç”¨æ¨¡æ¿
            return f"""
è¯·è®¾è®¡ä¸€ä¸ªåä¸º {self.design_type} çš„Verilogæ¨¡å—ã€‚

**åŸºæœ¬è¦æ±‚**ï¼š
1. ç”Ÿæˆå®Œæ•´ã€å¯ç¼–è¯‘çš„Verilogä»£ç 
2. åŒ…å«é€‚å½“çš„ç«¯å£å®šä¹‰å’ŒåŠŸèƒ½å®ç°
3. ç¬¦åˆVerilogæ ‡å‡†è¯­æ³•
4. ç”Ÿæˆå¯¹åº”çš„æµ‹è¯•å°è¿›è¡ŒéªŒè¯

**è´¨é‡è¦æ±‚**ï¼š
- ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ³¨é‡Šå®Œå–„
- éµå¾ªè‰¯å¥½çš„å‘½åè§„èŒƒ
- ç¡®ä¿åŠŸèƒ½æ­£ç¡®æ€§
"""
        
        return template["description"]
    
    async def setup_experiment_environment(self):
        """è®¾ç½®å®éªŒç¯å¢ƒ"""
        try:
            print("\nğŸ”§ è®¾ç½®å®éªŒç¯å¢ƒ...")
            
            # åˆ›å»ºå®éªŒç›®å½•ç»“æ„
            subdirs = ["designs", "testbenches", "logs", "artifacts", "reports"]
            for subdir in subdirs:
                (self.output_dir / subdir).mkdir(exist_ok=True)
            
            # åˆ›å»ºå®éªŒå…ƒæ•°æ®
            metadata = {
                "experiment_id": self.experiment_id,
                "design_type": self.design_type,
                "config_profile": self.config_profile,
                "max_iterations": self.max_iterations,
                "created_at": datetime.now().isoformat(),
                "status": "running",
                "output_directory": str(self.output_dir),
                "external_testbench_path": self.external_testbench_path
            }
            
            metadata_file = self.output_dir / "experiment_metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            # åˆå§‹åŒ–æ¡†æ¶é…ç½®
            self.config = FrameworkConfig.from_env()
            
            print(f"   âœ… å®éªŒç›®å½•åˆ›å»º: {self.output_dir}")
            print(f"   âœ… å…ƒæ•°æ®ä¿å­˜: {metadata_file.name}")
            
            return True
            
        except Exception as e:
            print(f"   âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥: {str(e)}")
            return False
    
    async def create_llm_coordinator(self):
        """åˆ›å»ºLLMåè°ƒæ™ºèƒ½ä½“"""
        try:
            print("\nğŸ¤– åˆ›å»ºLLMåè°ƒæ™ºèƒ½ä½“...")
            
            # åˆ›å»ºåè°ƒæ™ºèƒ½ä½“
            self.coordinator = LLMCoordinatorAgent(self.config)
            
            # åˆ›å»ºå¹¶æ³¨å†Œå·¥ä½œæ™ºèƒ½ä½“
            self.verilog_agent = EnhancedRealVerilogAgent(self.config)
            self.code_reviewer_agent = EnhancedRealCodeReviewAgent(self.config)
            
            await self.coordinator.register_agent(self.verilog_agent)
            await self.coordinator.register_agent(self.code_reviewer_agent)
            
            print(f"   âœ… åè°ƒæ™ºèƒ½ä½“åˆ›å»ºå®Œæˆ")
            print(f"   âœ… æ³¨å†Œæ™ºèƒ½ä½“: enhanced_real_verilog_agent")
            print(f"   âœ… æ³¨å†Œæ™ºèƒ½ä½“: enhanced_real_code_review_agent")
            
            return True
            
        except Exception as e:
            print(f"   âŒ åè°ƒæ™ºèƒ½ä½“åˆ›å»ºå¤±è´¥: {str(e)}")
            import traceback
            print(f"   ğŸ“‹ è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
            return False
    
    async def run_experiment(self):
        """è¿è¡Œå®éªŒ"""
        experiment_start = time.time()
        
        try:
            # 1. è®¾ç½®å®éªŒç¯å¢ƒ
            if not await self.setup_experiment_environment():
                return {"success": False, "error": "å®éªŒç¯å¢ƒè®¾ç½®å¤±è´¥"}
            
            # 2. åˆ›å»ºLLMåè°ƒæ™ºèƒ½ä½“
            if not await self.create_llm_coordinator():
                return {"success": False, "error": "åè°ƒæ™ºèƒ½ä½“åˆ›å»ºå¤±è´¥"}
            
            # 3. è·å–è®¾è®¡éœ€æ±‚
            requirements = self.get_design_requirements()
            print(f"\nğŸ“‹ è®¾è®¡éœ€æ±‚:")
            print(requirements)
            
            # 4. æ‰§è¡Œåè°ƒä»»åŠ¡
            print(f"\nğŸš€ å¼€å§‹æ‰§è¡Œåè°ƒä»»åŠ¡...")
            print(f"   æœ€å¤§è¿­ä»£æ¬¡æ•°: {self.max_iterations}")
            print(f"   è¶…æ—¶æ—¶é—´: {self.timeout_per_iteration}ç§’")
            
            # ç”Ÿæˆå”¯ä¸€çš„å¯¹è¯ID
            conversation_id = f"exp_{self.experiment_id}"
            
            task_start = time.time()
            result = await self.coordinator.coordinate_task(
                user_request=requirements,
                conversation_id=conversation_id,
                max_iterations=self.max_iterations,
                external_testbench_path=self.external_testbench_path
            )
            task_duration = time.time() - task_start
            
            # 5. åˆ†æç»“æœ
            analysis = await self.analyze_experiment_result(result, task_duration)
            
            # 6. ä¿å­˜å®éªŒæŠ¥å‘Š
            await self.save_experiment_report(analysis)
            
            # 7. å¤åˆ¶ç”Ÿæˆçš„æ–‡ä»¶
            await self.organize_experiment_files(result)
            
            total_duration = time.time() - experiment_start
            
            # 8. æ˜¾ç¤ºæœ€ç»ˆç»“æœ
            self.display_experiment_summary(analysis, total_duration)
            
            return analysis
            
        except Exception as e:
            error_duration = time.time() - experiment_start
            error_result = {
                "success": False,
                "error": str(e),
                "experiment_id": self.experiment_id,
                "duration": error_duration
            }
            
            print(f"\nâŒ å®éªŒæ‰§è¡Œå¼‚å¸¸: {str(e)}")
            await self.save_experiment_report(error_result)
            return error_result
    
    async def analyze_experiment_result(self, result: Dict[str, Any], task_duration: float) -> Dict[str, Any]:
        """åˆ†æå®éªŒç»“æœ"""
        print(f"\nğŸ“Š åˆ†æå®éªŒç»“æœ...")
        
        success = result.get('success', False)
        
        # åŸºç¡€åˆ†æ
        analysis = {
            "experiment_id": self.experiment_id,
            "design_type": self.design_type,
            "config_profile": self.config_profile,
            "success": success,
            "task_duration": task_duration,
            "timestamp": time.time(),
            "detailed_result": result
        }
        
        if success:
            # æˆåŠŸæƒ…å†µçš„è¯¦ç»†åˆ†æ
            execution_summary = result.get('execution_summary', {})
            agent_results = result.get('agent_results', {})
            
            # ğŸ†• è·å–å¢å¼ºçš„æ•°æ®æ”¶é›†å­—æ®µ
            task_context = result.get('task_context', {})
            tool_executions = task_context.get('tool_executions', [])
            agent_interactions = task_context.get('agent_interactions', [])
            performance_metrics = task_context.get('performance_metrics', {})
            workflow_stages = task_context.get('workflow_stages', [])
            file_operations = task_context.get('file_operations', [])
            execution_timeline = task_context.get('execution_timeline', [])
            data_collection_summary = task_context.get('data_collection_summary', {})
            llm_conversations = task_context.get('llm_conversations', [])
            
            # ğŸ†• å¢å¼ºæ•°æ®åˆ†æ
            analysis.update({
                "tool_executions": tool_executions,
                "agent_interactions": agent_interactions,
                "performance_metrics": performance_metrics,
                "workflow_stages": workflow_stages,
                "file_operations": file_operations,
                "execution_timeline": execution_timeline,
                "data_collection_summary": data_collection_summary,
                "llm_conversations": llm_conversations,
                # ğŸ†• ç»Ÿè®¡ä¿¡æ¯
                "tool_execution_count": len(tool_executions),
                "agent_interaction_count": len(agent_interactions),
                "workflow_stage_count": len(workflow_stages),
                "file_operation_count": len(file_operations),
                "execution_timeline_count": len(execution_timeline),
                "llm_conversation_count": len(llm_conversations),
                # ğŸ†• åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
                "total_iterations": execution_summary.get('total_iterations', 0),
                "agent_count": len(agent_results),
                "coordination_result_length": len(result.get('coordination_result', '')),
                "agent_performance": {}
            })
            
            # åˆ†ææ¯ä¸ªæ™ºèƒ½ä½“çš„è¡¨ç°
            for agent_id, agent_result in agent_results.items():
                agent_exec_time = agent_result.get('execution_time', 0)
                agent_success = agent_result.get('success', False)
                result_length = len(str(agent_result.get('result', '')))
                
                # ğŸ†• ä»å·¥å…·æ‰§è¡Œè®°å½•ä¸­è®¡ç®—æ™ºèƒ½ä½“å·¥å…·ä½¿ç”¨ç»Ÿè®¡
                agent_tool_count = len([t for t in tool_executions if t.get('agent_id') == agent_id])
                
                analysis["agent_performance"][agent_id] = {
                    "execution_time": agent_exec_time,
                    "success": agent_success,
                    "result_length": result_length,
                    "efficiency": result_length / max(agent_exec_time, 0.1),  # å­—ç¬¦/ç§’
                    "tool_usage_count": agent_tool_count  # ğŸ†• å·¥å…·ä½¿ç”¨æ¬¡æ•°
                }
            
            print(f"   âœ… å®éªŒæˆåŠŸå®Œæˆ")
            print(f"   ğŸ“ˆ æ€»è¿­ä»£æ¬¡æ•°: {analysis['total_iterations']}")
            print(f"   ğŸ¤– å‚ä¸æ™ºèƒ½ä½“: {len(agent_results)}")
            print(f"   ğŸ”§ å·¥å…·æ‰§è¡Œæ¬¡æ•°: {analysis['tool_execution_count']}")
            print(f"   ğŸ’¬ æ™ºèƒ½ä½“äº¤äº’æ¬¡æ•°: {analysis['agent_interaction_count']}")
            print(f"   ğŸ“ æ–‡ä»¶æ“ä½œæ¬¡æ•°: {analysis['file_operation_count']}")
            print(f"   â±ï¸ ä»»åŠ¡æ‰§è¡Œæ—¶é—´: {task_duration:.1f}ç§’")
        else:
            analysis.update({
                "error_message": result.get('error', 'æœªçŸ¥é”™è¯¯'),
                "failure_stage": "coordination"
            })
            print(f"   âŒ å®éªŒå¤±è´¥")
            print(f"   ğŸ’¥ é”™è¯¯ä¿¡æ¯: {analysis['error_message']}")
        
        return analysis
    
    async def save_experiment_report(self, analysis: Dict[str, Any]):
        """ä¿å­˜å®éªŒæŠ¥å‘Š"""
        try:
            # ä¿å­˜è¯¦ç»†JSONæŠ¥å‘Š
            report_path = self.output_dir / "reports" / "experiment_report.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(analysis, f, ensure_ascii=False, indent=2, default=str)
            
            # ä¿å­˜äººç±»å¯è¯»çš„æ‘˜è¦
            summary_path = self.output_dir / "reports" / "experiment_summary.txt"
            await self.save_text_summary(analysis, summary_path)
            
            print(f"\nğŸ“„ å®éªŒæŠ¥å‘Šå·²ä¿å­˜:")
            print(f"   ğŸ“Š è¯¦ç»†æŠ¥å‘Š: {report_path}")
            print(f"   ğŸ“ æ‘˜è¦æŠ¥å‘Š: {summary_path}")
            
        except Exception as e:
            print(f"   âš ï¸ æŠ¥å‘Šä¿å­˜å¤±è´¥: {str(e)}")
    
    async def save_text_summary(self, analysis: Dict[str, Any], summary_path: Path):
        """ä¿å­˜äººç±»å¯è¯»çš„æ–‡æœ¬æ‘˜è¦"""
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("LLMåè°ƒæ™ºèƒ½ä½“å®éªŒæŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"å®éªŒID: {analysis['experiment_id']}\n")
            f.write(f"è®¾è®¡ç±»å‹: {analysis['design_type']}\n")
            f.write(f"é…ç½®æ¡£æ¡ˆ: {analysis['config_profile']}\n")
            f.write(f"å®éªŒçŠ¶æ€: {'âœ… æˆåŠŸ' if analysis['success'] else 'âŒ å¤±è´¥'}\n")
            f.write(f"ä»»åŠ¡è€—æ—¶: {analysis['task_duration']:.2f} ç§’\n")
            f.write(f"æ—¶é—´æˆ³: {datetime.fromtimestamp(analysis['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            if analysis.get('success'):
                f.write("æ‰§è¡Œç»Ÿè®¡:\n")
                f.write(f"- æ€»è¿­ä»£æ¬¡æ•°: {analysis.get('total_iterations', 0)}\n")
                f.write(f"- å‚ä¸æ™ºèƒ½ä½“: {analysis.get('agent_count', 0)} ä¸ª\n")
                f.write(f"- åè°ƒç»“æœé•¿åº¦: {analysis.get('coordination_result_length', 0)} å­—ç¬¦\n\n")
                
                # ğŸ†• æ•°æ®æ”¶é›†ç»Ÿè®¡
                data_summary = analysis.get('data_collection_summary', {})
                if data_summary:
                    f.write("æ•°æ®æ”¶é›†ç»Ÿè®¡:\n")
                    
                    tool_stats = data_summary.get('tool_executions', {})
                    f.write(f"- å·¥å…·è°ƒç”¨: {tool_stats.get('total', 0)} æ¬¡ (æˆåŠŸ: {tool_stats.get('successful', 0)}, å¤±è´¥: {tool_stats.get('failed', 0)})\n")
                    f.write(f"- ä½¿ç”¨å·¥å…·: {', '.join(tool_stats.get('unique_tools', []))}\n")
                    f.write(f"- å·¥å…·æ‰§è¡Œæ€»æ—¶é—´: {tool_stats.get('total_execution_time', 0):.2f}ç§’\n")
                    
                    file_stats = data_summary.get('file_operations', {})
                    f.write(f"- æ–‡ä»¶æ“ä½œ: {file_stats.get('total', 0)} æ¬¡ (æˆåŠŸ: {file_stats.get('successful', 0)}, å¤±è´¥: {file_stats.get('failed', 0)})\n")
                    f.write(f"- æ“ä½œç±»å‹: {', '.join(file_stats.get('operation_types', []))}\n")
                    f.write(f"- æ€»æ–‡ä»¶å¤§å°: {file_stats.get('total_file_size', 0)} å­—èŠ‚\n")
                    
                    workflow_stats = data_summary.get('workflow_stages', {})
                    f.write(f"- å·¥ä½œæµé˜¶æ®µ: {workflow_stats.get('total', 0)} ä¸ª (æˆåŠŸ: {workflow_stats.get('successful', 0)}, å¤±è´¥: {workflow_stats.get('failed', 0)})\n")
                    f.write(f"- å·¥ä½œæµæ€»æ—¶é—´: {workflow_stats.get('total_duration', 0):.2f}ç§’\n")
                    
                    agent_stats = data_summary.get('agent_interactions', {})
                    f.write(f"- æ™ºèƒ½ä½“äº¤äº’: {agent_stats.get('total', 0)} æ¬¡ (æˆåŠŸ: {agent_stats.get('successful', 0)}, å¤±è´¥: {agent_stats.get('failed', 0)})\n")
                    f.write(f"- å‚ä¸æ™ºèƒ½ä½“: {', '.join(agent_stats.get('unique_agents', []))}\n")
                    
                    timeline_stats = data_summary.get('execution_timeline', {})
                    f.write(f"- æ‰§è¡Œäº‹ä»¶: {timeline_stats.get('total_events', 0)} ä¸ª\n")
                    f.write(f"- äº‹ä»¶ç±»å‹: {', '.join(timeline_stats.get('event_types', []))}\n\n")
                    
                    # ğŸ†• LLMå¯¹è¯ç»Ÿè®¡
                    llm_stats = data_summary.get('llm_conversations', {})
                    f.write(f"- LLMå¯¹è¯: {llm_stats.get('total', 0)} æ¬¡ (æˆåŠŸ: {llm_stats.get('successful', 0)}, å¤±è´¥: {llm_stats.get('failed', 0)})\n")
                    f.write(f"- å‚ä¸æ™ºèƒ½ä½“: {', '.join(llm_stats.get('unique_agents', []))}\n")
                    f.write(f"- ä½¿ç”¨æ¨¡å‹: {', '.join(llm_stats.get('unique_models', []))}\n")
                    f.write(f"- é¦–æ¬¡è°ƒç”¨: {llm_stats.get('first_calls', 0)} æ¬¡\n")
                    f.write(f"- æ€»å¯¹è¯æ—¶é—´: {llm_stats.get('total_duration', 0):.2f}ç§’\n")
                    f.write(f"- æ€»Tokenæ•°: {llm_stats.get('total_tokens', 0)} ä¸ª\n\n")
                
                # æ™ºèƒ½ä½“æ€§èƒ½åˆ†æ
                agent_perf = analysis.get('agent_performance', {})
                if agent_perf:
                    f.write("æ™ºèƒ½ä½“æ€§èƒ½:\n")
                    for agent_id, perf in agent_perf.items():
                        f.write(f"- {agent_id}:\n")
                        f.write(f"  æ‰§è¡Œæ—¶é—´: {perf['execution_time']:.2f}ç§’\n")
                        f.write(f"  æ‰§è¡ŒçŠ¶æ€: {'æˆåŠŸ' if perf['success'] else 'å¤±è´¥'}\n")
                        f.write(f"  ç»“æœé•¿åº¦: {perf['result_length']} å­—ç¬¦\n")
                        f.write(f"  å¤„ç†æ•ˆç‡: {perf['efficiency']:.1f} å­—ç¬¦/ç§’\n")
                        f.write(f"  å·¥å…·ä½¿ç”¨: {perf.get('tool_usage_count', 0)} æ¬¡\n\n")
            else:
                f.write(f"å¤±è´¥åŸå› : {analysis.get('error_message', 'æœªçŸ¥é”™è¯¯')}\n")
                f.write(f"å¤±è´¥é˜¶æ®µ: {analysis.get('failure_stage', 'æœªçŸ¥')}\n")
    
    async def organize_experiment_files(self, result: Dict[str, Any]):
        """æ•´ç†å®éªŒç”Ÿæˆçš„æ–‡ä»¶"""
        try:
            print(f"\nğŸ“ æ•´ç†å®éªŒæ–‡ä»¶...")
            
            # æ£€æŸ¥å½“å‰å·¥ä½œåŒºåŸŸçš„æ–‡ä»¶
            workspace_dirs = [
                project_root / "file_workspace" / "designs",
                project_root / "file_workspace" / "testbenches",
                self.output_dir / "artifacts"
            ]
            
            copied_files = []
            
            for workspace_dir in workspace_dirs:
                if workspace_dir.exists():
                    # æŸ¥æ‰¾æœ€è¿‘ç”Ÿæˆçš„æ–‡ä»¶ï¼ˆåŸºäºä¿®æ”¹æ—¶é—´ï¼‰
                    recent_files = []
                    current_time = time.time()
                    
                    for file_path in workspace_dir.glob("*"):
                        if file_path.is_file():
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ˜¯æœ€è¿‘åˆ›å»ºçš„ï¼ˆ1å°æ—¶å†…ï¼‰
                            file_mtime = file_path.stat().st_mtime
                            if current_time - file_mtime < 3600:  # 1å°æ—¶
                                recent_files.append(file_path)
                    
                    # å¤åˆ¶æœ€è¿‘çš„æ–‡ä»¶åˆ°å®éªŒç›®å½•
                    for file_path in recent_files:
                        if file_path.suffix in ['.v', '.sv', '.vcd', '.txt', '.md']:
                            # æ ¹æ®æ–‡ä»¶ç±»å‹å†³å®šç›®æ ‡ç›®å½•
                            if 'tb' in file_path.name or 'testbench' in file_path.name:
                                target_dir = self.output_dir / "testbenches"
                            else:
                                target_dir = self.output_dir / "designs"
                            
                            target_path = target_dir / file_path.name
                            
                            try:
                                import shutil
                                shutil.copy2(file_path, target_path)
                                copied_files.append(target_path)
                                print(f"   ğŸ“„ å¤åˆ¶æ–‡ä»¶: {file_path.name} -> {target_dir.name}/")
                            except Exception as e:
                                print(f"   âš ï¸ æ–‡ä»¶å¤åˆ¶å¤±è´¥ {file_path.name}: {str(e)}")
            
            if copied_files:
                print(f"   âœ… æˆåŠŸæ•´ç† {len(copied_files)} ä¸ªæ–‡ä»¶")
                
                # æ›´æ–°å®éªŒçŠ¶æ€
                self.experiment_state["generated_files"] = [str(f) for f in copied_files]
            else:
                print(f"   â„¹ï¸ æœªå‘ç°éœ€è¦æ•´ç†çš„æ–‡ä»¶")
                
        except Exception as e:
            print(f"   âš ï¸ æ–‡ä»¶æ•´ç†å¤±è´¥: {str(e)}")
    
    def display_experiment_summary(self, analysis: Dict[str, Any], total_duration: float):
        """æ˜¾ç¤ºå®éªŒæ‘˜è¦"""
        print(f"\n" + "=" * 80)
        print(f"ğŸ¯ LLMåè°ƒæ™ºèƒ½ä½“å®éªŒå®Œæˆ")
        print("=" * 80)
        
        print(f"ğŸ“Š å®éªŒä¿¡æ¯:")
        print(f"   ID: {analysis['experiment_id']}")
        print(f"   ç±»å‹: {analysis['design_type']}")
        print(f"   çŠ¶æ€: {'âœ… æˆåŠŸ' if analysis['success'] else 'âŒ å¤±è´¥'}")
        print(f"   æ€»è€—æ—¶: {total_duration:.1f}ç§’")
        print(f"   ä»»åŠ¡è€—æ—¶: {analysis['task_duration']:.1f}ç§’")
        
        if analysis.get('success'):
            print(f"\nğŸ‰ æ‰§è¡ŒæˆåŠŸ:")
            print(f"   è¿­ä»£æ¬¡æ•°: {analysis.get('total_iterations', 0)}")
            print(f"   å‚ä¸æ™ºèƒ½ä½“: {analysis.get('agent_count', 0)} ä¸ª")
            print(f"   ç”Ÿæˆæ–‡ä»¶: {len(self.experiment_state.get('generated_files', []))} ä¸ª")
            
            # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
            generated_files = self.experiment_state.get('generated_files', [])
            if generated_files:
                print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
                for file_path in generated_files:
                    file_name = Path(file_path).name
                    print(f"   ğŸ“„ {file_name}")
        else:
            print(f"\nğŸ’¥ æ‰§è¡Œå¤±è´¥:")
            print(f"   é”™è¯¯: {analysis.get('error_message', 'æœªçŸ¥é”™è¯¯')}")
        
        print(f"\nğŸ“‚ å®éªŒç›®å½•: {self.output_dir}")
        print("=" * 80)


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºçš„LLMåè°ƒæ™ºèƒ½ä½“æµ‹è¯•å…¥å£')
    
    parser.add_argument('--design', default='4bit_adder', 
                       help='è®¾è®¡ç±»å‹ (4bit_adder, 8bit_counter, alu_simple, custom)')
    parser.add_argument('--config', default='standard', 
                       help='é…ç½®æ¡£æ¡ˆ (fast, standard, thorough)')
    parser.add_argument('--iterations', type=int, 
                       help='æœ€å¤§è¿­ä»£æ¬¡æ•° (è¦†ç›–é…ç½®æ¡£æ¡ˆè®¾ç½®)')
    parser.add_argument('--requirements', 
                       help='è‡ªå®šä¹‰è®¾è®¡éœ€æ±‚ (ç”¨äºcustomè®¾è®¡ç±»å‹)')
    parser.add_argument('--output-dir', 
                       help='è‡ªå®šä¹‰è¾“å‡ºç›®å½•')
    parser.add_argument('--external-testbench', 
                       help='å¤–éƒ¨testbenchæ–‡ä»¶è·¯å¾„(.væ–‡ä»¶)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = EnhancedLLMCoordinatorTest(
        design_type=args.design,
        config_profile=args.config,
        custom_requirements=args.requirements,
        output_dir=args.output_dir,
        max_iterations=args.iterations,
        external_testbench_path=args.external_testbench
    )
    
    try:
        # è¿è¡Œå®éªŒ
        result = await experiment.run_experiment()
        
        # è¿”å›ç»“æœ
        if result["success"]:
            print("\nğŸ‰ å®éªŒæˆåŠŸå®Œæˆï¼")
            return 0
        else:
            print(f"\nğŸ’¥ å®éªŒå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return 1
            
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
        return 2
    except Exception as e:
        print(f"\nğŸ’¥ å®éªŒæ‰§è¡Œå¼‚å¸¸: {str(e)}")
        return 3


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)