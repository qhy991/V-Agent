#!/usr/bin/env python3
"""
æµ‹è¯•Verilogä»£ç æµ‹è¯•åŠŸèƒ½

Test Verilog Code Testing Functionality
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from agents.real_code_reviewer import RealCodeReviewAgent
from config.config import FrameworkConfig
from core.base_agent import TaskMessage

async def test_verilog_testing():
    """æµ‹è¯•Verilogä»£ç æµ‹è¯•åŠŸèƒ½"""
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ALUæ¨¡å—ç”¨äºæµ‹è¯•
    test_verilog_code = '''
module simple_alu (
    input [3:0] a,
    input [3:0] b,
    input [1:0] op,
    output reg [3:0] result,
    output reg zero_flag
);

always @(*) begin
    case (op)
        2'b00: result = a + b;  // åŠ æ³•
        2'b01: result = a - b;  // å‡æ³•
        2'b10: result = a & b;  // ä¸è¿ç®—
        2'b11: result = a | b;  // æˆ–è¿ç®—
        default: result = 4'b0000;
    endcase
    
    zero_flag = (result == 4'b0000);
end

endmodule
'''
    
    print("ğŸ§ª å¼€å§‹æµ‹è¯•Verilogä»£ç æµ‹è¯•åŠŸèƒ½")
    print("=" * 60)
    
    try:
        # 1. åˆ›å»ºå®¡æŸ¥æ™ºèƒ½ä½“
        config = FrameworkConfig.from_env()
        reviewer = RealCodeReviewAgent(config)
        
        print("âœ… å®¡æŸ¥æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
        
        # 2. å‡†å¤‡æµ‹è¯•ä»»åŠ¡
        task_message = TaskMessage(
            task_id="test_verilog_testing",
            sender_id="test_runner",
            receiver_id="real_code_review_agent",
            message_type="task_execution",
            content="è¯·å¯¹è¿™ä¸ªç®€å•çš„ALUæ¨¡å—è¿›è¡Œä»£ç å®¡æŸ¥ï¼Œå¹¶ç”Ÿæˆæµ‹è¯•å°è¿›è¡ŒåŠŸèƒ½éªŒè¯",
            metadata={"test_mode": True}
        )
        
        # 3. å‡†å¤‡æ–‡ä»¶å†…å®¹
        file_contents = {
            "simple_alu.v": {
                "type": "verilog",
                "content": test_verilog_code
            }
        }
        
        print("ğŸ“ å¼€å§‹æ‰§è¡Œæµ‹è¯•ä»»åŠ¡...")
        
        # 4. æ‰§è¡Œå¢å¼ºä»»åŠ¡
        result = await reviewer.execute_enhanced_task(
            enhanced_prompt="è¯·å¯¹ALUæ¨¡å—è¿›è¡Œå…¨é¢çš„ä»£ç å®¡æŸ¥ï¼ŒåŒ…æ‹¬è¯­æ³•æ£€æŸ¥ã€è®¾è®¡è´¨é‡è¯„ä¼°ï¼Œå¹¶ç”Ÿæˆå®Œæ•´çš„æµ‹è¯•å°éªŒè¯å…¶åŠŸèƒ½æ­£ç¡®æ€§",
            original_message=task_message,
            file_contents=file_contents
        )
        
        print("âœ… æµ‹è¯•ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
        print("\nğŸ“Š æµ‹è¯•ç»“æœ:")
        
        if "formatted_response" in result:
            import json
            try:
                response_data = json.loads(result["formatted_response"])
                print(f"- æ™ºèƒ½ä½“: {response_data.get('agent_name', 'Unknown')}")
                print(f"- çŠ¶æ€: {response_data.get('status', 'Unknown')}")
                print(f"- å®Œæˆåº¦: {response_data.get('completion_percentage', 0)}%")
                print(f"- æ¶ˆæ¯: {response_data.get('message', 'No message')}")
                
                # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
                generated_files = response_data.get('generated_files', [])
                print(f"- ç”Ÿæˆæ–‡ä»¶æ•°: {len(generated_files)}")
                for file_info in generated_files:
                    print(f"  * {file_info.get('path', 'Unknown')}: {file_info.get('description', 'No description')}")
                
                # æ£€æŸ¥æµ‹è¯•ç›¸å…³çš„å…ƒæ•°æ®
                metadata = response_data.get('metadata', {})
                if metadata.get('testing_performed'):
                    print(f"- æµ‹è¯•æ‰§è¡Œ: âœ…")
                    print(f"- æµ‹è¯•æ•°é‡: {metadata.get('total_tests', 0)}")
                    print(f"- æˆåŠŸæµ‹è¯•: {metadata.get('successful_tests', 0)}")
                    print(f"- æµ‹è¯•è¦†ç›–ç‡: {metadata.get('test_coverage', 0):.1%}")
                    if 'average_test_pass_rate' in metadata:
                        print(f"- å¹³å‡é€šè¿‡ç‡: {metadata['average_test_pass_rate']:.1%}")
                else:
                    print(f"- æµ‹è¯•æ‰§è¡Œ: âŒ")
                
                # æ£€æŸ¥è´¨é‡æŒ‡æ ‡
                quality_metrics = response_data.get('quality_metrics', {})
                if quality_metrics:
                    print(f"- æ•´ä½“è´¨é‡: {quality_metrics.get('overall_score', 0):.2f}")
                    print(f"- è¯­æ³•æ­£ç¡®æ€§: {quality_metrics.get('syntax_score', 0):.2f}")
                    print(f"- åŠŸèƒ½è´¨é‡: {quality_metrics.get('functionality_score', 0):.2f}")
                
            except json.JSONDecodeError as e:
                print(f"âŒ å“åº”è§£æå¤±è´¥: {e}")
                print("åŸå§‹å“åº”:", result["formatted_response"][:500] + "...")
        else:
            print("âŒ æœªæ‰¾åˆ°æ ¼å¼åŒ–å“åº”")
            print("åŸå§‹ç»“æœ:", result)
        
        print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(test_verilog_testing())
    sys.exit(0 if success else 1)