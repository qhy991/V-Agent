#!/usr/bin/env python3
"""
å®éªŒåˆ†æè„šæœ¬
Experiment Analysis Script
=========================

ä½¿ç”¨å¢å¼ºæ—¥å¿—è®°å½•ç³»ç»Ÿåˆ†æå®éªŒç»“æœçš„ç¤ºä¾‹è„šæœ¬ã€‚
"""

import argparse
import sys
from pathlib import Path
import json
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from core.enhanced_experiment_logger import EnhancedExperimentLogger
from core.conversation_analyzer import ConversationAnalyzer


def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('experiment_analysis.log')
        ]
    )


def analyze_existing_experiment(report_path: str, output_dir: str = None):
    """åˆ†æå·²æœ‰çš„å®éªŒæŠ¥å‘Š"""
    report_path = Path(report_path)
    
    if not report_path.exists():
        print(f"âŒ å®éªŒæŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {report_path}")
        return
        
    if output_dir is None:
        output_dir = report_path.parent
    else:
        output_dir = Path(output_dir)
        
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ” åˆ†æå®éªŒæŠ¥å‘Š: {report_path}")
    
    # åˆ›å»ºå¯¹è¯åˆ†æå™¨
    analyzer = ConversationAnalyzer(str(report_path))
    
    # æ‰§è¡Œåˆ†æ
    print("ğŸ“Š æ‰§è¡Œå¯¹è¯åˆ†æ...")
    analysis_report = analyzer.analyze()
    
    # ä¿å­˜åˆ†æç»“æœ
    analysis_path = analyzer.save_analysis(analysis_report, output_dir / "conversation_analysis.json")
    
    # ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
    summary_text = analyzer.generate_summary_text(analysis_report)
    summary_path = output_dir / "analysis_summary.md"
    
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(summary_text)
        
    print(f"âœ… åˆ†æå®Œæˆ!")
    print(f"   ğŸ“„ è¯¦ç»†åˆ†æ: {analysis_path}")
    print(f"   ğŸ“ æ–‡æœ¬æ‘˜è¦: {summary_path}")
    
    # æ˜¾ç¤ºå…³é”®å‘ç°
    print(f"\nğŸ¯ å…³é”®å‘ç°:")
    print(f"   ğŸ“ æ€»LLMè°ƒç”¨: {analysis_report.conversation_stats.get('total_llm_calls', 0)}æ¬¡")
    print(f"   âœ… æˆåŠŸç‡: {analysis_report.conversation_stats.get('successful_calls', 0) / max(analysis_report.conversation_stats.get('total_llm_calls', 1), 1) * 100:.1f}%")
    print(f"   â±ï¸ å¹³å‡å“åº”: {analysis_report.conversation_stats.get('average_call_duration', 0):.2f}ç§’")
    print(f"   ğŸ¤– å‚ä¸æ™ºèƒ½ä½“: {analysis_report.conversation_stats.get('unique_agents', 0)}ä¸ª")
    print(f"   ğŸ” è¯†åˆ«æ¨¡å¼: {len(analysis_report.identified_patterns)}ä¸ª")
    print(f"   âš ï¸ å‘ç°é—®é¢˜: {len(analysis_report.problem_diagnosis)}ä¸ª")
    print(f"   ğŸ’¡ æ”¹è¿›å»ºè®®: {len(analysis_report.improvement_recommendations)}ä¸ª")
    
    # æ˜¾ç¤ºé«˜ä¼˜å…ˆçº§å»ºè®®
    high_priority_recommendations = [r for r in analysis_report.improvement_recommendations if r.get('priority') in ['critical', 'high']]
    if high_priority_recommendations:
        print(f"\nğŸš¨ é«˜ä¼˜å…ˆçº§å»ºè®®:")
        for i, rec in enumerate(high_priority_recommendations[:3], 1):
            print(f"   {i}. {rec['title']} ({rec['priority']})")
            
    return analysis_report


def create_enhanced_experiment_report(experiment_data: dict, output_dir: str):
    """åˆ›å»ºå¢å¼ºçš„å®éªŒæŠ¥å‘Š"""
    experiment_id = experiment_data.get('experiment_id', 'unknown')
    output_path = Path(output_dir)
    
    print(f"ğŸ“‹ åˆ›å»ºå¢å¼ºå®éªŒæŠ¥å‘Š: {experiment_id}")
    
    # åˆ›å»ºå¢å¼ºå®éªŒè®°å½•å™¨
    logger = EnhancedExperimentLogger(experiment_id, output_path)
    
    # è®¾ç½®ä»»åŠ¡ä¿¡æ¯
    logger.set_task_description(experiment_data.get('task_description', ''))
    logger.mark_success(experiment_data.get('success', False))
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    print("ğŸ“Š ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
    detailed_report = logger.generate_detailed_report()
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = logger.save_report(detailed_report)
    
    print(f"âœ… å¢å¼ºæŠ¥å‘Šå·²åˆ›å»º: {report_path}")
    
    return report_path


def compare_experiments(report_paths: list, output_dir: str = "comparison"):
    """æ¯”è¾ƒå¤šä¸ªå®éªŒ"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ” æ¯”è¾ƒ {len(report_paths)} ä¸ªå®éªŒ...")
    
    analyzers = []
    reports = []
    
    for path in report_paths:
        if not Path(path).exists():
            print(f"âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶: {path}")
            continue
            
        analyzer = ConversationAnalyzer(path)
        analysis_report = analyzer.analyze()
        
        analyzers.append(analyzer)
        reports.append(analysis_report)
        
    if not reports:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„å®éªŒæŠ¥å‘Šè¿›è¡Œæ¯”è¾ƒ")
        return
        
    # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
    comparison = {
        "comparison_timestamp": __import__('time').time(),
        "experiments": [],
        "summary_comparison": {},
        "recommendations": []
    }
    
    for report in reports:
        stats = report.conversation_stats
        comparison["experiments"].append({
            "experiment_id": report.experiment_id,
            "total_llm_calls": stats.get('total_llm_calls', 0),
            "success_rate": stats.get('successful_calls', 0) / max(stats.get('total_llm_calls', 1), 1),
            "average_duration": stats.get('average_call_duration', 0),
            "total_tokens": stats.get('total_tokens', 0),
            "unique_agents": stats.get('unique_agents', 0),
            "problems_count": len(report.problem_diagnosis),
            "patterns_count": len(report.identified_patterns)
        })
        
    # è®¡ç®—èšåˆç»Ÿè®¡
    if comparison["experiments"]:
        exp_data = comparison["experiments"]
        comparison["summary_comparison"] = {
            "best_success_rate": max(exp["success_rate"] for exp in exp_data),
            "worst_success_rate": min(exp["success_rate"] for exp in exp_data),
            "fastest_average_duration": min(exp["average_duration"] for exp in exp_data),
            "slowest_average_duration": max(exp["average_duration"] for exp in exp_data),
            "most_efficient_experiment": min(exp_data, key=lambda x: x["average_duration"])["experiment_id"],
            "most_successful_experiment": max(exp_data, key=lambda x: x["success_rate"])["experiment_id"]
        }
        
    # ä¿å­˜æ¯”è¾ƒç»“æœ
    comparison_path = output_dir / "experiment_comparison.json"
    with open(comparison_path, 'w', encoding='utf-8') as f:
        json.dump(comparison, f, ensure_ascii=False, indent=2, default=str)
        
    print(f"âœ… æ¯”è¾ƒæŠ¥å‘Šå·²ä¿å­˜: {comparison_path}")
    
    # æ˜¾ç¤ºæ¯”è¾ƒæ‘˜è¦
    print(f"\nğŸ“Š æ¯”è¾ƒæ‘˜è¦:")
    summary = comparison["summary_comparison"]
    if summary:
        print(f"   ğŸ† æœ€é«˜æˆåŠŸç‡: {summary['best_success_rate']*100:.1f}% ({summary['most_successful_experiment']})")
        print(f"   âš¡ æœ€å¿«å“åº”: {summary['fastest_average_duration']:.2f}ç§’ ({summary['most_efficient_experiment']})")
        print(f"   ğŸ“ˆ æˆåŠŸç‡èŒƒå›´: {summary['worst_success_rate']*100:.1f}% - {summary['best_success_rate']*100:.1f}%")
        
    return comparison_path


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å®éªŒåˆ†æå·¥å…·")
    parser.add_argument("command", choices=["analyze", "enhance", "compare"], help="æ‰§è¡Œçš„å‘½ä»¤")
    parser.add_argument("--report", "-r", help="å®éªŒæŠ¥å‘Šæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--reports", "-rs", nargs="+", help="å¤šä¸ªå®éªŒæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºæ¯”è¾ƒï¼‰")
    parser.add_argument("--output", "-o", help="è¾“å‡ºç›®å½•", default="analysis_output")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    setup_logging()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        
    try:
        if args.command == "analyze":
            if not args.report:
                print("âŒ è¯·æä¾› --report å‚æ•°")
                return
                
            analyze_existing_experiment(args.report, args.output)
            
        elif args.command == "enhance":
            if not args.report:
                print("âŒ è¯·æä¾› --report å‚æ•°")
                return
                
            # è¯»å–åŸå§‹å®éªŒæ•°æ®
            with open(args.report, 'r', encoding='utf-8') as f:
                experiment_data = json.load(f)
                
            enhanced_path = create_enhanced_experiment_report(experiment_data, args.output)
            
            # è‡ªåŠ¨åˆ†æå¢å¼ºåçš„æŠ¥å‘Š
            print("ğŸ”„ è‡ªåŠ¨åˆ†æå¢å¼ºæŠ¥å‘Š...")
            analyze_existing_experiment(str(enhanced_path), args.output)
            
        elif args.command == "compare":
            if not args.reports or len(args.reports) < 2:
                print("âŒ è¯·æä¾›è‡³å°‘2ä¸ª --reports å‚æ•°")
                return
                
            compare_experiments(args.reports, args.output)
            
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()