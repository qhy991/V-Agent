#!/usr/bin/env python3
"""
ğŸ”§ ä¿®å¤LLM Tokené™åˆ¶é—®é¢˜

è¿™ä¸ªè„šæœ¬ç”¨äºè§£å†³æ¡†æ¶ä¸­LLMè¾“å‡ºè¢«æˆªæ–­çš„é—®é¢˜ï¼š
1. æ£€æŸ¥å½“å‰çš„max_tokensç¡¬ç¼–ç é—®é¢˜
2. ç»Ÿä¸€æå‡æ‰€æœ‰ç»„ä»¶çš„tokené™åˆ¶
3. ç¡®ä¿ç¯å¢ƒå˜é‡é…ç½®ç”Ÿæ•ˆ
4. æä¾›éªŒè¯å’Œå›æ»šåŠŸèƒ½
"""

import os
import re
import sys
from pathlib import Path
from typing import Dict, List, Tuple
import shutil
from datetime import datetime

class LLMTokenLimitFixer:
    """LLM Tokené™åˆ¶ä¿®å¤å™¨"""
    
    def __init__(self, project_root: Path = None):
        self.project_root = project_root or Path(__file__).parent
        self.backup_dir = self.project_root / "backup_token_limits"
        
        # å½“å‰å‘ç°çš„ç¡¬ç¼–ç max_tokenså€¼å’Œå»ºè®®å€¼
        self.token_fixes = {
            # æ–‡ä»¶è·¯å¾„: [(è¡Œå·, å½“å‰å€¼, å»ºè®®å€¼, æè¿°)]
            "core/centralized_coordinator.py": [
                (146, 3000, 8000, "ä»»åŠ¡å¤„ç†é˜¶æ®µ"),
                (520, 100, 2000, "æ™ºèƒ½ä½“é€‰æ‹©é˜¶æ®µ")  # è¿™ä¸ªæœ€å…³é”®
            ],
            "config/config.py": [
                (54, 1000, 4000, "å†³ç­–é˜¶æ®µæœ€å¤§tokens"),
                (58, 1500, 6000, "åˆ†æé˜¶æ®µæœ€å¤§tokens")
            ],
            "core/base_agent.py": [
                (1395, 800, 3000, "åŸºç¡€æ™ºèƒ½ä½“å“åº”")
            ],
            "core/real_centralized_coordinator.py": [
                (264, 1000, 4000, "çœŸå®åè°ƒå™¨")
            ],
            "agents/real_verilog_agent.py": [
                (313, 1500, 4000, "Verilogè®¾è®¡åˆ†æ"),
                (644, 1500, 4000, "ä»£ç è´¨é‡åˆ†æ")
            ],
            "agents/real_code_reviewer.py": [
                (526, 2500, 6000, "æµ‹è¯•å°ç”Ÿæˆ"),
                (1344, 1500, 4000, "ä»£ç åˆ†æ"),
                (1446, 2000, 5000, "Function callingå“åº”")
            ]
        }
        
        # ç¯å¢ƒå˜é‡æ¨èå€¼
        self.env_recommendations = {
            "CAF_LLM_MAX_TOKENS": "16384",  # æå‡åˆ°16K
            "CAF_ANALYSIS_MAX_TOKENS": "6000",
            "CAF_DECISION_MAX_TOKENS": "4000"
        }
    
    def create_backup(self) -> bool:
        """åˆ›å»ºä¿®æ”¹å‰çš„å¤‡ä»½"""
        try:
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = self.backup_dir / f"backup_{timestamp}"
            backup_path.mkdir(parents=True, exist_ok=True)
            
            print(f"ğŸ“¦ åˆ›å»ºå¤‡ä»½: {backup_path}")
            
            # å¤‡ä»½æ‰€æœ‰éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶
            for file_path in self.token_fixes.keys():
                source = self.project_root / file_path
                if source.exists():
                    dest = backup_path / file_path
                    dest.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(source, dest)
                    print(f"   âœ… å¤‡ä»½: {file_path}")
                else:
                    print(f"   âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # å¤‡ä»½.envæ–‡ä»¶
            env_file = self.project_root / ".env"
            if env_file.exists():
                shutil.copy2(env_file, backup_path / ".env")
                print(f"   âœ… å¤‡ä»½: .env")
            
            print(f"âœ… å¤‡ä»½å®Œæˆ: {backup_path}")
            return True
            
        except Exception as e:
            print(f"âŒ å¤‡ä»½å¤±è´¥: {str(e)}")
            return False
    
    def analyze_current_limits(self) -> Dict[str, List[Tuple[int, int, str]]]:
        """åˆ†æå½“å‰çš„tokené™åˆ¶"""
        print("ğŸ” åˆ†æå½“å‰çš„max_tokensé™åˆ¶...")
        
        analysis = {}
        
        for file_path, fixes in self.token_fixes.items():
            full_path = self.project_root / file_path
            if not full_path.exists():
                print(f"   âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                continue
            
            file_analysis = []
            
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                for line_num, current_val, suggested_val, desc in fixes:
                    if line_num <= len(lines):
                        line_content = lines[line_num - 1].strip()
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«max_tokens
                        if "max_tokens" in line_content and str(current_val) in line_content:
                            file_analysis.append((line_num, current_val, suggested_val, desc, line_content))
                            print(f"   ğŸ“ {file_path}:{line_num} - {desc}: {current_val} â†’ {suggested_val}")
                        else:
                            print(f"   âš ï¸ {file_path}:{line_num} - è¡Œå†…å®¹å¯èƒ½å·²å˜åŒ–")
                
                analysis[file_path] = file_analysis
                
            except Exception as e:
                print(f"   âŒ è¯»å–å¤±è´¥ {file_path}: {str(e)}")
        
        return analysis
    
    def fix_token_limits(self, analysis: Dict[str, List[Tuple[int, int, str]]]) -> bool:
        """ä¿®å¤tokené™åˆ¶"""
        print("ğŸ”§ å¼€å§‹ä¿®å¤max_tokensé™åˆ¶...")
        
        success_count = 0
        total_count = 0
        
        for file_path, file_fixes in analysis.items():
            if not file_fixes:
                continue
                
            full_path = self.project_root / file_path
            
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                with open(full_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                # åº”ç”¨ä¿®å¤
                modified = False
                for line_num, current_val, suggested_val, desc, original_line in file_fixes:
                    if line_num <= len(lines):
                        old_line = lines[line_num - 1]
                        # æ›¿æ¢max_tokenså€¼
                        new_line = re.sub(
                            rf'max_tokens\s*=\s*{current_val}',
                            f'max_tokens={suggested_val}',
                            old_line
                        )
                        
                        if new_line != old_line:
                            lines[line_num - 1] = new_line
                            modified = True
                            total_count += 1
                            print(f"   âœ… {file_path}:{line_num} - {desc}: {current_val} â†’ {suggested_val}")
                        else:
                            print(f"   âš ï¸ {file_path}:{line_num} - æ›¿æ¢å¤±è´¥")
                
                # å†™å…¥ä¿®æ”¹åçš„å†…å®¹
                if modified:
                    with open(full_path, 'w', encoding='utf-8') as f:
                        f.writelines(lines)
                    success_count += len([f for f in file_fixes if f])
                    print(f"   ğŸ’¾ ä¿å­˜: {file_path}")
                
            except Exception as e:
                print(f"   âŒ ä¿®å¤å¤±è´¥ {file_path}: {str(e)}")
        
        print(f"âœ… ä¿®å¤å®Œæˆ: {success_count}/{total_count} ä¸ªtokené™åˆ¶å·²æ›´æ–°")
        return success_count > 0
    
    def update_env_config(self) -> bool:
        """æ›´æ–°ç¯å¢ƒå˜é‡é…ç½®"""
        print("ğŸ”§ æ›´æ–°.envæ–‡ä»¶ä¸­çš„LLMé…ç½®...")
        
        env_file = self.project_root / ".env"
        if not env_file.exists():
            print("   âš ï¸ .envæ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        try:
            # è¯»å–å½“å‰.envå†…å®¹
            with open(env_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # æ›´æ–°æ¨èçš„ç¯å¢ƒå˜é‡
            modified = False
            for var_name, new_value in self.env_recommendations.items():
                updated = False
                for i, line in enumerate(lines):
                    if line.startswith(f"{var_name}="):
                        old_value = line.split('=', 1)[1].strip()
                        lines[i] = f"{var_name}={new_value}\n"
                        print(f"   âœ… {var_name}: {old_value} â†’ {new_value}")
                        modified = True
                        updated = True
                        break
                
                if not updated:
                    # æ·»åŠ æ–°çš„ç¯å¢ƒå˜é‡
                    lines.append(f"\n# Added by LLM Token Limit Fixer\n{var_name}={new_value}\n")
                    print(f"   â• æ·»åŠ : {var_name}={new_value}")
                    modified = True
            
            # å†™å…¥æ›´æ–°åçš„å†…å®¹
            if modified:
                with open(env_file, 'w', encoding='utf-8') as f:
                    f.writelines(lines)
                print("   ğŸ’¾ .envæ–‡ä»¶å·²æ›´æ–°")
                return True
            else:
                print("   âœ… .envæ–‡ä»¶æ— éœ€æ›´æ–°")
                return True
                
        except Exception as e:
            print(f"   âŒ æ›´æ–°.envå¤±è´¥: {str(e)}")
            return False
    
    def create_test_script(self) -> bool:
        """åˆ›å»ºæµ‹è¯•è„šæœ¬éªŒè¯ä¿®å¤æ•ˆæœ"""
        test_script_content = '''#!/usr/bin/env python3
"""
ğŸ§ª æµ‹è¯•LLM Tokené™åˆ¶ä¿®å¤æ•ˆæœ
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config.config import FrameworkConfig
from core.centralized_coordinator import CentralizedCoordinator
from agents.real_verilog_agent import RealVerilogDesignAgent
from llm_integration.enhanced_llm_client import EnhancedLLMClient

async def test_token_limits():
    """æµ‹è¯•å„ç»„ä»¶çš„tokené™åˆ¶"""
    print("ğŸ§ª æµ‹è¯•LLM Tokené™åˆ¶ä¿®å¤æ•ˆæœ")
    print("=" * 50)
    
    # 1. æµ‹è¯•é…ç½®è¯»å–
    config = FrameworkConfig.from_env()
    print(f"ğŸ“‹ ç¯å¢ƒå˜é‡é…ç½®:")
    print(f"   CAF_LLM_MAX_TOKENS: {config.llm.max_tokens}")
    
    # 2. æµ‹è¯•LLMå®¢æˆ·ç«¯
    llm_client = EnhancedLLMClient(config.llm)
    print(f"ğŸ¤– LLMå®¢æˆ·ç«¯é…ç½®:")
    print(f"   é»˜è®¤max_tokens: {llm_client.config.max_tokens}")
    
    # 3. æµ‹è¯•åè°ƒå™¨é…ç½®
    coordinator = CentralizedCoordinator(config)
    print(f"ğŸ›ï¸ åè°ƒå™¨é…ç½®:")
    print(f"   åˆ†æmax_tokens: {config.coordinator.analysis_max_tokens}")
    print(f"   å†³ç­–max_tokens: {config.coordinator.decision_max_tokens}")
    
    # 4. æµ‹è¯•Verilogæ™ºèƒ½ä½“
    verilog_agent = RealVerilogDesignAgent(config)
    print(f"ğŸ”§ Verilogæ™ºèƒ½ä½“å·²åˆå§‹åŒ–")
    
    print("âœ… æ‰€æœ‰ç»„ä»¶tokené™åˆ¶æµ‹è¯•å®Œæˆ")
    print("å¦‚æœçœ‹åˆ°æ›´é«˜çš„tokenå€¼(å¦‚4000+)ï¼Œè¯´æ˜ä¿®å¤æˆåŠŸï¼")

if __name__ == "__main__":
    asyncio.run(test_token_limits())
'''
        
        test_file = self.project_root / "test_token_limits_fix.py"
        try:
            with open(test_file, 'w', encoding='utf-8') as f:
                f.write(test_script_content)
            print(f"ğŸ“ åˆ›å»ºæµ‹è¯•è„šæœ¬: {test_file.name}")
            return True
        except Exception as e:
            print(f"âŒ åˆ›å»ºæµ‹è¯•è„šæœ¬å¤±è´¥: {str(e)}")
            return False
    
    def run_fix(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„ä¿®å¤æµç¨‹"""
        print("ğŸš€ å¼€å§‹LLM Tokené™åˆ¶ä¿®å¤æµç¨‹")
        print("=" * 60)
        
        # 1. åˆ›å»ºå¤‡ä»½
        if not self.create_backup():
            print("âŒ å¤‡ä»½å¤±è´¥ï¼Œç»ˆæ­¢ä¿®å¤")
            return False
        
        # 2. åˆ†æå½“å‰é™åˆ¶
        analysis = self.analyze_current_limits()
        if not any(analysis.values()):
            print("âš ï¸ æœªå‘ç°éœ€è¦ä¿®å¤çš„tokené™åˆ¶")
            return True
        
        # 3. ä¿®å¤tokené™åˆ¶
        if not self.fix_token_limits(analysis):
            print("âŒ ä¿®å¤tokené™åˆ¶å¤±è´¥")
            return False
        
        # 4. æ›´æ–°ç¯å¢ƒå˜é‡
        if not self.update_env_config():
            print("âŒ æ›´æ–°ç¯å¢ƒå˜é‡å¤±è´¥")
            return False
        
        # 5. åˆ›å»ºæµ‹è¯•è„šæœ¬
        self.create_test_script()
        
        print("=" * 60)
        print("ğŸ‰ LLM Tokené™åˆ¶ä¿®å¤å®Œæˆï¼")
        print()
        print("ğŸ“‹ ä¿®å¤æ‘˜è¦:")
        print("âœ… æ™ºèƒ½ä½“é€‰æ‹©é˜¶æ®µ: 100 â†’ 2000 tokens (20å€æå‡)")
        print("âœ… ä»»åŠ¡å¤„ç†é˜¶æ®µ: 3000 â†’ 8000 tokens (2.7å€æå‡)")
        print("âœ… åˆ†æé˜¶æ®µ: 1500 â†’ 6000 tokens (4å€æå‡)")
        print("âœ… å†³ç­–é˜¶æ®µ: 1000 â†’ 4000 tokens (4å€æå‡)")
        print("âœ… ç¯å¢ƒå˜é‡: æå‡åˆ°16K tokens")
        print()
        print("ğŸ§ª éªŒè¯ä¿®å¤æ•ˆæœ:")
        print("   python test_token_limits_fix.py")
        print()
        print("ğŸ”„ å¦‚éœ€å›æ»š:")
        print(f"   å¤‡ä»½ä½ç½®: {self.backup_dir}")
        
        return True
    
    def show_status(self):
        """æ˜¾ç¤ºå½“å‰çŠ¶æ€"""
        print("ğŸ“Š å½“å‰LLM Tokené™åˆ¶çŠ¶æ€")
        print("=" * 40)
        
        analysis = self.analyze_current_limits()
        
        total_issues = sum(len(fixes) for fixes in analysis.values())
        if total_issues == 0:
            print("âœ… æœªå‘ç°tokené™åˆ¶é—®é¢˜")
        else:
            print(f"âš ï¸ å‘ç° {total_issues} ä¸ªéœ€è¦ä¿®å¤çš„tokené™åˆ¶")
            print()
            print("ğŸ”§ è¿è¡Œä¿®å¤:")
            print("   python fix_llm_token_limits.py --fix")


def main():
    """ä¸»ç¨‹åº"""
    fixer = LLMTokenLimitFixer()
    
    if len(sys.argv) > 1 and sys.argv[1] == "--fix":
        success = fixer.run_fix()
        sys.exit(0 if success else 1)
    else:
        fixer.show_status()


if __name__ == "__main__":
    main()