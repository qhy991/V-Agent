#!/usr/bin/env python3
"""
ALUæµ‹è¯•é©±åŠ¨å¼€å‘å®éªŒ - å¯é…ç½®çš„å®éªŒå…¥å£

è¿™ä¸ªè„šæœ¬å±•ç¤ºå®Œæ•´çš„æµ‹è¯•é©±åŠ¨å¼€å‘æµç¨‹ï¼š
1. ç”¨æˆ·æä¾›è®¾è®¡éœ€æ±‚å’Œæµ‹è¯•å°è·¯å¾„
2. ç³»ç»Ÿè‡ªåŠ¨è¿­ä»£ä¼˜åŒ–è®¾è®¡ç›´åˆ°é€šè¿‡æµ‹è¯•
3. æä¾›è¯¦ç»†çš„å®éªŒé…ç½®å’Œç»“æœåˆ†æ
"""

import asyncio
import sys
import json
import time
from pathlib import Path
from typing import Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config.config import FrameworkConfig
from core.centralized_coordinator import CentralizedCoordinator
from agents.real_verilog_agent import RealVerilogDesignAgent
from agents.real_code_reviewer import RealCodeReviewAgent

# å¯¼å…¥æµ‹è¯•é©±åŠ¨æ‰©å±•
from extensions import create_test_driven_coordinator, TestDrivenConfig


class ALUTestDrivenExperiment:
    """ALUæµ‹è¯•é©±åŠ¨å¼€å‘å®éªŒç±»"""
    
    def __init__(self, experiment_config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–å®éªŒ
        
        Args:
            experiment_config: å®éªŒé…ç½®ï¼ŒåŒ…å«ä»¥ä¸‹å¯é€‰å‚æ•°ï¼š
                - max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•° (é»˜è®¤: 5)
                - timeout_per_iteration: æ¯æ¬¡è¿­ä»£è¶…æ—¶ç§’æ•° (é»˜è®¤: 300)
                - enable_deep_analysis: å¯ç”¨æ·±åº¦åˆ†æ (é»˜è®¤: True)
                - save_detailed_logs: ä¿å­˜è¯¦ç»†æ—¥å¿— (é»˜è®¤: True)
                - testbench_path: æµ‹è¯•å°è·¯å¾„ (é»˜è®¤: ä½¿ç”¨å†…ç½®æµ‹è¯•å°)
                - design_complexity: è®¾è®¡å¤æ‚åº¦ (simple/standard/advanced)
        """
        self.config = experiment_config or {}
        self.experiment_id = f"alu_tdd_exp_{int(time.time())}"
        
        # å®éªŒé…ç½®
        self.max_iterations = self.config.get('max_iterations', 5)
        self.timeout_per_iteration = self.config.get('timeout_per_iteration', 300)
        self.enable_deep_analysis = self.config.get('enable_deep_analysis', True)
        self.save_detailed_logs = self.config.get('save_detailed_logs', True)
        
        # æµ‹è¯•å°é…ç½®
        self.testbench_path = self.config.get('testbench_path', 
            str(project_root / "test_cases" / "alu_testbench.v"))
        
        # è®¾è®¡å¤æ‚åº¦é…ç½®
        self.design_complexity = self.config.get('design_complexity', 'standard')
        
        print(f"ğŸ§ª åˆå§‹åŒ–ALUæµ‹è¯•é©±åŠ¨å®éªŒ: {self.experiment_id}")
        print(f"ğŸ“‹ å®éªŒé…ç½®:")
        print(f"   æœ€å¤§è¿­ä»£æ¬¡æ•°: {self.max_iterations}")
        print(f"   æ¯æ¬¡è¿­ä»£è¶…æ—¶: {self.timeout_per_iteration}ç§’")
        print(f"   æ·±åº¦åˆ†æ: {self.enable_deep_analysis}")
        print(f"   è®¾è®¡å¤æ‚åº¦: {self.design_complexity}")
        print(f"   æµ‹è¯•å°è·¯å¾„: {self.testbench_path}")
    
    def get_design_requirements(self) -> str:
        """æ ¹æ®å¤æ‚åº¦é…ç½®ç”Ÿæˆè®¾è®¡éœ€æ±‚"""
        
        base_requirements = """
è¯·è®¾è®¡ä¸€ä¸ª32ä½ALUï¼ˆç®—æœ¯é€»è¾‘å•å…ƒï¼‰æ¨¡å—ï¼Œå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è§„æ ¼å®ç°ï¼š

ğŸ”§ æ¨¡å—æ¥å£è§„æ ¼ï¼š
```verilog
module alu_32bit (
    input  [31:0] a,       // ç¬¬ä¸€ä¸ª32ä½æ“ä½œæ•°
    input  [31:0] b,       // ç¬¬äºŒä¸ª32ä½æ“ä½œæ•°  
    input  [3:0]  op,      // 4ä½æ“ä½œç 
    output [31:0] result,  // 32ä½è¿ç®—ç»“æœ
    output        zero     // é›¶æ ‡å¿—ä½ï¼ˆç»“æœä¸º0æ—¶ä¸º1ï¼‰
);
```

ğŸ“‹ æ“ä½œç å®šä¹‰ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
- 4'b0000 (OP_ADD): åŠ æ³•è¿ç®— (result = a + b)
- 4'b0001 (OP_SUB): å‡æ³•è¿ç®— (result = a - b)
- 4'b0010 (OP_AND): é€»è¾‘ä¸ (result = a & b)
- 4'b0011 (OP_OR):  é€»è¾‘æˆ– (result = a | b)
- 4'b0100 (OP_XOR): å¼‚æˆ– (result = a ^ b)
- 4'b0101 (OP_SLL): å·¦ç§» (result = a << b[4:0])
- 4'b0110 (OP_SRL): å³ç§» (result = a >> b[4:0])
- å…¶ä»–æ“ä½œç : è¾“å‡º0

ğŸ¯ åŠŸèƒ½è¦æ±‚ï¼š
1. æ”¯æŒ32ä½ç®—æœ¯è¿ç®—ï¼ˆåŠ æ³•ã€å‡æ³•ï¼‰
2. æ”¯æŒ32ä½é€»è¾‘è¿ç®—ï¼ˆä¸ã€æˆ–ã€å¼‚æˆ–ï¼‰
3. æ”¯æŒç§»ä½è¿ç®—ï¼ˆå·¦ç§»ã€å³ç§»ï¼Œä½¿ç”¨bçš„ä½5ä½ä½œä¸ºç§»ä½ä½æ•°ï¼‰
4. zeroæ ‡å¿—ä½ï¼šå½“resultä¸º32'h00000000æ—¶ï¼Œzero=1ï¼›å¦åˆ™zero=0
5. å¯¹äºæ— æ•ˆæ“ä½œç ï¼Œè¾“å‡ºresult=32'h00000000, zero=1

âš¡ è®¾è®¡çº¦æŸï¼š
- ä½¿ç”¨çº¯ç»„åˆé€»è¾‘å®ç°ï¼ˆä¸éœ€è¦æ—¶é’Ÿï¼‰
- ç¡®ä¿æ‰€æœ‰è·¯å¾„éƒ½æœ‰æ˜ç¡®çš„è¾“å‡º
- ä»£ç å¿…é¡»å¯ç»¼åˆ
- éµå¾ªè‰¯å¥½çš„Verilogç¼–ç è§„èŒƒ
"""
        
        complexity_additions = {
            'simple': """
ğŸ”° ç®€åŒ–è¦æ±‚ï¼š
- å®ç°åŸºæœ¬çš„åŠ æ³•ã€å‡æ³•å’Œé€»è¾‘è¿ç®—å³å¯
- ç§»ä½è¿ç®—å¯ä»¥ç®€åŒ–å®ç°
            """,
            
            'standard': """
ğŸ¯ æ ‡å‡†è¦æ±‚ï¼š
- å®Œæ•´å®ç°æ‰€æœ‰æŒ‡å®šæ“ä½œ
- æ­£ç¡®å¤„ç†è¾¹ç•Œæ¡ä»¶å’Œæº¢å‡º
- ç¡®ä¿ç§»ä½è¿ç®—çš„æ­£ç¡®æ€§
            """,
            
            'advanced': """
ğŸš€ é«˜çº§è¦æ±‚ï¼š
- å®Œæ•´å®ç°æ‰€æœ‰æŒ‡å®šæ“ä½œ
- ä¼˜åŒ–å…³é”®è·¯å¾„å»¶è¿Ÿ
- æ·»åŠ è¯¦ç»†çš„å†…éƒ¨æ³¨é‡Š
- è€ƒè™‘é¢ç§¯å’ŒåŠŸè€—ä¼˜åŒ–
- æ·»åŠ assertionæ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
            """
        }
        
        test_requirements = f"""

ğŸ§ª æµ‹è¯•è¦æ±‚ï¼š
- æµ‹è¯•å°è·¯å¾„: {self.testbench_path}
- å¿…é¡»é€šè¿‡æ‰€æœ‰16ä¸ªæµ‹è¯•ç”¨ä¾‹
- æµ‹è¯•å°åŒ…å«è¯¦ç»†çš„åˆ†ç»„æµ‹è¯•ï¼š
  * åŠ æ³•è¿ç®—æµ‹è¯•ï¼ˆ3ä¸ªç”¨ä¾‹ï¼‰
  * å‡æ³•è¿ç®—æµ‹è¯•ï¼ˆ3ä¸ªç”¨ä¾‹ï¼‰
  * é€»è¾‘è¿ç®—æµ‹è¯•ï¼ˆ5ä¸ªç”¨ä¾‹ï¼‰
  * ç§»ä½è¿ç®—æµ‹è¯•ï¼ˆ4ä¸ªç”¨ä¾‹ï¼‰
  * è¾¹ç•Œæ¡ä»¶æµ‹è¯•ï¼ˆ2ä¸ªç”¨ä¾‹ï¼‰

âš ï¸ å…³é”®è¦æ±‚ï¼š
- å¦‚æœæµ‹è¯•å¤±è´¥ï¼Œè¯·ä»”ç»†åˆ†æå¤±è´¥åŸå› 
- æ ¹æ®æµ‹è¯•å°çš„å…·ä½“é”™è¯¯ä¿¡æ¯è°ƒæ•´è®¾è®¡
- ç¡®ä¿æ“ä½œç å®šä¹‰ä¸æµ‹è¯•å°å®Œå…¨ä¸€è‡´
- ç¡®ä¿zeroæ ‡å¿—ä½é€»è¾‘æ­£ç¡®
- ç»§ç»­è¿­ä»£ç›´åˆ°æ‰€æœ‰æµ‹è¯•é€šè¿‡

ğŸ“Š æˆåŠŸæ ‡å‡†ï¼š
æµ‹è¯•å°æœ€ç»ˆè¾“å‡º "ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ALUè®¾è®¡æ­£ç¡®ï¼"
"""
        
        return (base_requirements + 
                complexity_additions.get(self.design_complexity, complexity_additions['standard']) +
                test_requirements)
    
    async def setup_framework(self):
        """è®¾ç½®æ¡†æ¶ç»„ä»¶"""
        print("\nğŸ—ï¸ åˆå§‹åŒ–æµ‹è¯•é©±åŠ¨æ¡†æ¶...")
        
        # 1. åˆ›å»ºæ ‡å‡†ç»„ä»¶
        framework_config = FrameworkConfig.from_env()
        self.coordinator = CentralizedCoordinator(framework_config)
        
        # 2. æ³¨å†Œæ™ºèƒ½ä½“
        self.verilog_agent = RealVerilogDesignAgent(framework_config)
        self.reviewer_agent = RealCodeReviewAgent(framework_config)
        
        self.coordinator.register_agent(self.verilog_agent)
        self.coordinator.register_agent(self.reviewer_agent)
        
        print("   âœ… æ ‡å‡†åè°ƒå™¨å’Œæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
        
        # 3. åˆ›å»ºæµ‹è¯•é©±åŠ¨é…ç½®
        tdd_config = TestDrivenConfig(
            max_iterations=self.max_iterations,
            enable_deep_analysis=self.enable_deep_analysis,
            timeout_per_iteration=self.timeout_per_iteration,
            save_iteration_logs=self.save_detailed_logs
        )
        
        # 4. å‡çº§ä¸ºæµ‹è¯•é©±åŠ¨åè°ƒå™¨
        self.tdd_coordinator = create_test_driven_coordinator(self.coordinator, tdd_config)
        
        print("   âœ… æµ‹è¯•é©±åŠ¨æ‰©å±•å¯ç”¨æˆåŠŸ")
        
        # 5. éªŒè¯æµ‹è¯•å°æ–‡ä»¶
        testbench_path = Path(self.testbench_path)
        if not testbench_path.exists():
            raise FileNotFoundError(f"æµ‹è¯•å°æ–‡ä»¶ä¸å­˜åœ¨: {self.testbench_path}")
        
        print(f"   âœ… æµ‹è¯•å°æ–‡ä»¶éªŒè¯é€šè¿‡: {testbench_path.name}")
        
        return True
    
    async def run_experiment(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„ALUæµ‹è¯•é©±åŠ¨å®éªŒ"""
        experiment_start_time = time.time()
        
        print(f"\nğŸš€ å¼€å§‹ALUæµ‹è¯•é©±åŠ¨å¼€å‘å®éªŒ")
        print("=" * 80)
        
        try:
            # 1. è®¾ç½®æ¡†æ¶
            await self.setup_framework()
            
            # 2. æ„å»ºè®¾è®¡ä»»åŠ¡
            design_requirements = self.get_design_requirements()
            
            print(f"\nğŸ“‹ è®¾è®¡éœ€æ±‚å·²ç”Ÿæˆ (é•¿åº¦: {len(design_requirements)} å­—ç¬¦)")
            print(f"ğŸ¯ ç›®æ ‡: è®¾è®¡32ä½ALUå¹¶é€šè¿‡ {Path(self.testbench_path).name} çš„æ‰€æœ‰æµ‹è¯•")
            
            # 3. æ‰§è¡Œæµ‹è¯•é©±åŠ¨ä»»åŠ¡
            print(f"\nğŸ”„ å¯åŠ¨æµ‹è¯•é©±åŠ¨å¼€å‘å¾ªç¯...")
            print(f"   æœ€å¤§è¿­ä»£æ¬¡æ•°: {self.max_iterations}")
            print(f"   æ¯æ¬¡è¿­ä»£è¶…æ—¶: {self.timeout_per_iteration}ç§’")
            
            result = await self.tdd_coordinator.execute_test_driven_task(
                task_description=design_requirements,
                testbench_path=self.testbench_path
            )
            
            # 4. åˆ†æç»“æœ
            experiment_duration = time.time() - experiment_start_time
            analysis = await self._analyze_experiment_result(result, experiment_duration)
            
            return analysis
            
        except Exception as e:
            print(f"âŒ å®éªŒæ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "experiment_id": self.experiment_id,
                "duration": time.time() - experiment_start_time
            }
    
    async def _analyze_experiment_result(self, result: Dict[str, Any], 
                                       duration: float) -> Dict[str, Any]:
        """åˆ†æå®éªŒç»“æœ"""
        print(f"\nğŸ“Š å®éªŒç»“æœåˆ†æ")
        print("=" * 80)
        
        analysis = {
            "experiment_id": self.experiment_id,
            "success": result.get("success", False),
            "total_duration": duration,
            "configuration": self.config,
            "result_summary": {}
        }
        
        if result.get("success"):
            print("ğŸ‰ å®éªŒæˆåŠŸå®Œæˆï¼")
            
            iterations = result.get("total_iterations", 0)
            final_design = result.get("final_design", [])
            
            print(f"   ğŸ“ˆ æ€»è¿­ä»£æ¬¡æ•°: {iterations}/{self.max_iterations}")
            print(f"   â±ï¸ æ€»è€—æ—¶: {duration:.2f} ç§’")
            print(f"   ğŸ“ æœ€ç»ˆè®¾è®¡æ–‡ä»¶: {len(final_design)} ä¸ª")
            
            analysis["result_summary"] = {
                "iterations_used": iterations,
                "iterations_available": self.max_iterations,
                "efficiency": f"{(1 - iterations/self.max_iterations)*100:.1f}%",
                "files_generated": len(final_design),
                "completion_reason": result.get("completion_reason", "unknown")
            }
            
            # æ˜¾ç¤ºè®¾è®¡æ–‡ä»¶ä¿¡æ¯
            if final_design:
                print(f"\nğŸ“„ ç”Ÿæˆçš„è®¾è®¡æ–‡ä»¶:")
                for i, file_info in enumerate(final_design, 1):
                    if isinstance(file_info, dict):
                        file_path = file_info.get('file_path', str(file_info))
                        print(f"   {i}. {Path(file_path).name}")
            
        else:
            print("âŒ å®éªŒæœªèƒ½å®Œæˆ")
            
            iterations = result.get("total_iterations", 0)
            error = result.get("error", "æœªçŸ¥é”™è¯¯")
            
            print(f"   ğŸ“ˆ å·²ç”¨è¿­ä»£æ¬¡æ•°: {iterations}/{self.max_iterations}")
            print(f"   â±ï¸ æ€»è€—æ—¶: {duration:.2f} ç§’")
            print(f"   âŒ å¤±è´¥åŸå› : {error}")
            
            analysis["result_summary"] = {
                "iterations_used": iterations,
                "iterations_available": self.max_iterations,
                "completion_reason": result.get("completion_reason", "failed"),
                "error": error
            }
            
            # åˆ†æéƒ¨åˆ†ç»“æœ
            partial_results = result.get("partial_results", [])
            if partial_results:
                print(f"\nğŸ” è¿­ä»£å†å²åˆ†æ:")
                for i, iteration in enumerate(partial_results, 1):
                    iter_result = iteration.get("result", {})
                    success = iter_result.get("all_tests_passed", False)
                    print(f"   ç¬¬{i}æ¬¡è¿­ä»£: {'âœ… é€šè¿‡' if success else 'âŒ å¤±è´¥'}")
        
        # æ˜¾ç¤ºä¼šè¯ä¿¡æ¯
        session_id = result.get("session_id")
        if session_id:
            session_info = self.tdd_coordinator.get_session_info(session_id)
            if session_info:
                print(f"\nğŸ“‹ ä¼šè¯è¯¦æƒ…:")
                print(f"   ä¼šè¯ID: {session_id}")
                print(f"   çŠ¶æ€: {session_info.get('status', 'unknown')}")
                
                iterations_detail = session_info.get('iterations', [])
                if iterations_detail:
                    print(f"   è¯¦ç»†è¿­ä»£: {len(iterations_detail)} æ¬¡")
        
        print("=" * 80)
        return analysis
    
    def save_experiment_report(self, analysis: Dict[str, Any], 
                             output_path: Optional[str] = None):
        """ä¿å­˜å®éªŒæŠ¥å‘Š"""
        if output_path is None:
            output_path = f"alu_tdd_experiment_report_{self.experiment_id}.json"
        
        report = {
            "experiment_metadata": {
                "experiment_id": self.experiment_id,
                "timestamp": time.time(),
                "framework_version": "1.0.0",
                "experiment_type": "alu_test_driven_development"
            },
            "configuration": self.config,
            "results": analysis,
            "testbench_info": {
                "path": self.testbench_path,
                "exists": Path(self.testbench_path).exists(),
                "size": Path(self.testbench_path).stat().st_size if Path(self.testbench_path).exists() else 0
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ å®éªŒæŠ¥å‘Šå·²ä¿å­˜: {output_path}")


# é¢„å®šä¹‰å®éªŒé…ç½®
EXPERIMENT_CONFIGS = {
    "quick_test": {
        "max_iterations": 3,
        "timeout_per_iteration": 120,
        "design_complexity": "simple",
        "enable_deep_analysis": True,
        "save_detailed_logs": True
    },
    
    "standard_test": {
        "max_iterations": 5,
        "timeout_per_iteration": 300,
        "design_complexity": "standard", 
        "enable_deep_analysis": True,
        "save_detailed_logs": True
    },
    
    "thorough_test": {
        "max_iterations": 8,
        "timeout_per_iteration": 600,
        "design_complexity": "advanced",
        "enable_deep_analysis": True,
        "save_detailed_logs": True
    },
    
    "minimal_test": {
        "max_iterations": 2,
        "timeout_per_iteration": 60,
        "design_complexity": "simple",
        "enable_deep_analysis": False,
        "save_detailed_logs": False
    }
}


async def main():
    """ä¸»å®éªŒå…¥å£"""
    print("ğŸ§ª ALUæµ‹è¯•é©±åŠ¨å¼€å‘å®éªŒå¹³å°")
    print("=" * 60)
    
    # æ˜¾ç¤ºå¯ç”¨é…ç½®
    print("ğŸ“‹ å¯ç”¨å®éªŒé…ç½®:")
    for name, config in EXPERIMENT_CONFIGS.items():
        print(f"   {name}: {config['design_complexity']}å¤æ‚åº¦, "
              f"{config['max_iterations']}æ¬¡è¿­ä»£, "
              f"{config['timeout_per_iteration']}ç§’è¶…æ—¶")
    
    # é€‰æ‹©å®éªŒé…ç½®
    print(f"\nğŸ¯ ä½¿ç”¨é»˜è®¤é…ç½®: standard_test")
    experiment_config = EXPERIMENT_CONFIGS["standard_test"]
    
    # åˆ›å»ºå¹¶è¿è¡Œå®éªŒ
    experiment = ALUTestDrivenExperiment(experiment_config)
    
    try:
        result = await experiment.run_experiment()
        
        # ä¿å­˜å®éªŒæŠ¥å‘Š
        experiment.save_experiment_report(result)
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print(f"\nğŸ å®éªŒå®Œæˆ")
        if result["success"]:
            print("âœ… ALUè®¾è®¡æˆåŠŸå®Œæˆå¹¶é€šè¿‡æ‰€æœ‰æµ‹è¯•ï¼")
        else:
            print("âŒ ALUè®¾è®¡æœªèƒ½é€šè¿‡æ‰€æœ‰æµ‹è¯•ï¼Œè¯·æ£€æŸ¥æ—¥å¿—åˆ†æåŸå› ")
        
        return result["success"]
        
    except Exception as e:
        print(f"ğŸ’¥ å®éªŒæ‰§è¡Œå¼‚å¸¸: {str(e)}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)