#!/usr/bin/env python3
"""
çœŸå®LLMé©±åŠ¨çš„ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“

Real LLM-powered Code Review Agent
"""

import json
import asyncio
import subprocess
import tempfile
import os
import re
from typing import Dict, Any, Set, List, Tuple
from pathlib import Path

from core.base_agent import BaseAgent, TaskMessage
from core.enums import AgentCapability
from core.response_format import ResponseFormat, TaskStatus, ResponseType, QualityMetrics
from llm_integration.enhanced_llm_client import EnhancedLLMClient
from config.config import FrameworkConfig
from core.enhanced_logging_config import get_agent_logger, get_artifacts_dir
from tools.script_tools import ScriptManager


class RealCodeReviewAgent(BaseAgent):
    """çœŸå®LLMé©±åŠ¨çš„ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“"""
    
    def __init__(self, config: FrameworkConfig = None):
        super().__init__(
            agent_id="real_code_review_agent",
            role="code_reviewer",
            capabilities={
                AgentCapability.CODE_REVIEW,
                AgentCapability.QUALITY_ANALYSIS,
                AgentCapability.SPECIFICATION_ANALYSIS,
                AgentCapability.TEST_GENERATION,
                AgentCapability.VERIFICATION
            }
        )
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.config = config or FrameworkConfig.from_env()
        self.llm_client = EnhancedLLMClient(self.config.llm)
        
        # è®¾ç½®ä¸“ç”¨æ—¥å¿—å™¨
        self.agent_logger = get_agent_logger('RealCodeReviewAgent')
        self.artifacts_dir = get_artifacts_dir()
        
        # åˆå§‹åŒ–è„šæœ¬ç®¡ç†å™¨
        self.script_manager = ScriptManager(work_dir=self.artifacts_dir)
        
        self.logger.info(f"ğŸ” çœŸå®ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“(Function Callingæ”¯æŒ)åˆå§‹åŒ–å®Œæˆ")
        self.agent_logger.info("RealCodeReviewAgentåˆå§‹åŒ–å®Œæˆ")
    
    def _register_function_calling_tools(self):
        """æ³¨å†Œå¯ç”¨çš„å·¥å…·"""
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•æ³¨å†ŒåŸºç¡€å·¥å…·
        super()._register_function_calling_tools()
        
        # 1. æµ‹è¯•å°ç”Ÿæˆå·¥å…·
        self.register_function_calling_tool(
            name="generate_testbench",
            func=self._tool_generate_testbench,
            description="ä¸ºVerilogæ¨¡å—ç”Ÿæˆæµ‹è¯•å°(testbench)",
            parameters={
                "module_code": {
                    "type": "string",
                    "description": "å®Œæ•´çš„Verilogæ¨¡å—ä»£ç ",
                    "required": True
                },
                "test_cases": {
                    "type": "array",
                    "description": "æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨",
                    "required": False
                }
            }
        )
        
        # 2. ä»¿çœŸæ‰§è¡Œå·¥å…·
        self.register_function_calling_tool(
            name="run_simulation",
            func=self._tool_run_simulation,
            description="ä½¿ç”¨iverilogè¿è¡ŒVerilogä»¿çœŸ",
            parameters={
                "module_file": {
                    "type": "string",
                    "description": "æ¨¡å—æ–‡ä»¶è·¯å¾„",
                    "required": False
                },
                "testbench_file": {
                    "type": "string", 
                    "description": "æµ‹è¯•å°æ–‡ä»¶è·¯å¾„",
                    "required": False
                },
                "module_code": {
                    "type": "string",
                    "description": "æ¨¡å—ä»£ç å†…å®¹",
                    "required": False
                },
                "testbench_code": {
                    "type": "string",
                    "description": "æµ‹è¯•å°ä»£ç å†…å®¹",
                    "required": False
                }
            }
        )
        
        # 3. ä»£ç åˆ†æå·¥å…·
        self.register_function_calling_tool(
            name="analyze_code_quality",
            func=self._tool_analyze_code_quality,
            description="åˆ†æVerilogä»£ç è´¨é‡",
            parameters={
                "code": {
                    "type": "string",
                    "description": "è¦åˆ†æçš„Verilogä»£ç ",
                    "required": True
                }
            }
        )
        
        # 4. è„šæœ¬ç”Ÿæˆå·¥å…·
        self.register_function_calling_tool(
            name="write_build_script",
            func=self._tool_write_build_script,
            description="ç”Ÿæˆæ„å»ºè„šæœ¬(Makefileæˆ–shellè„šæœ¬)",
            parameters={
                "verilog_files": {
                    "type": "array",
                    "description": "Verilogæºæ–‡ä»¶åˆ—è¡¨",
                    "required": True
                },
                "testbench_files": {
                    "type": "array", 
                    "description": "æµ‹è¯•å°æ–‡ä»¶åˆ—è¡¨",
                    "required": True
                },
                "script_type": {
                    "type": "string",
                    "description": "è„šæœ¬ç±»å‹: 'makefile' æˆ– 'bash'",
                    "required": False
                },
                "target_name": {
                    "type": "string",
                    "description": "ç›®æ ‡åç§°",
                    "required": False
                }
            }
        )
        
        # 5. è„šæœ¬æ‰§è¡Œå·¥å…·
        self.register_function_calling_tool(
            name="execute_build_script",
            func=self._tool_execute_build_script,
            description="æ‰§è¡Œæ„å»ºè„šæœ¬è¿›è¡Œç¼–è¯‘å’Œä»¿çœŸ",
            parameters={
                "script_name": {
                    "type": "string",
                    "description": "è„šæœ¬æ–‡ä»¶å",
                    "required": True
                },
                "action": {
                    "type": "string",
                    "description": "æ‰§è¡Œçš„åŠ¨ä½œ: 'all', 'compile', 'simulate', 'clean'",
                    "required": False
                },
                "arguments": {
                    "type": "array",
                    "description": "é¢å¤–çš„å‘½ä»¤è¡Œå‚æ•°",
                    "required": False
                }
            }
        )
        
        # ä½¿ç”¨çˆ¶ç±»çš„write_fileå·¥å…·ï¼Œæ— éœ€é‡å¤æ³¨å†Œ
    
    async def _call_llm_for_function_calling(self, conversation: List[Dict[str, str]]) -> str:
        """è°ƒç”¨LLMè¿›è¡ŒFunction Calling"""
        # æ„å»ºå®Œæ•´çš„prompt
        full_prompt = ""
        system_prompt = None
        
        for msg in conversation:
            if msg["role"] == "system":
                system_prompt = msg["content"]
            elif msg["role"] == "user":
                full_prompt += f"User: {msg['content']}\n\n"
            elif msg["role"] == "assistant":
                full_prompt += f"Assistant: {msg['content']}\n\n"
        
        try:
            response = await self.llm_client.send_prompt(
                prompt=full_prompt.strip(),
                system_prompt=system_prompt,
                temperature=0.3,
                max_tokens=3000
            )
            return response
        except Exception as e:
            self.logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    # åˆ é™¤é‡å¤çš„æ–¹æ³•ï¼Œä½¿ç”¨BaseAgentä¸­çš„é€šç”¨å®ç°
    
    def get_capabilities(self) -> Set[AgentCapability]:
        return {
            AgentCapability.CODE_REVIEW,
            AgentCapability.QUALITY_ANALYSIS,
            AgentCapability.SPECIFICATION_ANALYSIS,
            AgentCapability.TEST_GENERATION,
            AgentCapability.VERIFICATION
        }
    
    def get_specialty_description(self) -> str:
        return "çœŸå®LLMé©±åŠ¨çš„ä»£ç å®¡æŸ¥æ™ºèƒ½ä½“ï¼Œæä¾›ä¸“ä¸šçš„Verilog/SystemVerilogä»£ç è´¨é‡åˆ†æã€æµ‹è¯•å°ç”Ÿæˆå’ŒåŠŸèƒ½éªŒè¯"
    
    async def execute_enhanced_task(self, enhanced_prompt: str,
                                  original_message: TaskMessage,
                                  file_contents: Dict[str, Dict]) -> Dict[str, Any]:
        """æ‰§è¡Œä»£ç å®¡æŸ¥ä»»åŠ¡"""
        task_id = original_message.task_id
        self.logger.info(f"ğŸ” å¼€å§‹æ‰§è¡Œä»£ç å®¡æŸ¥ä»»åŠ¡: {task_id}")
        
        try:
            # 1. æå–è¦å®¡æŸ¥çš„ä»£ç 
            code_to_review = await self._extract_code_from_files(file_contents)
            
            if not code_to_review:
                error_response = self.create_error_response_formatted(
                    task_id=task_id,
                    error_message="æœªæ‰¾åˆ°å¯å®¡æŸ¥çš„ä»£ç æ–‡ä»¶",
                    error_details="è¯·ç¡®ä¿æä¾›äº†åŒ…å«Verilogä»£ç çš„æ–‡ä»¶",
                    format_type=ResponseFormat.JSON
                )
                return {"formatted_response": error_response}
            
            # 2. æ‰§è¡Œè¯¦ç»†çš„ä»£ç å®¡æŸ¥
            review_results = []
            for file_path, code_content in code_to_review.items():
                self.logger.info(f"ğŸ“ å®¡æŸ¥æ–‡ä»¶: {file_path}")
                review_result = await self._perform_detailed_review(
                    file_path, code_content, enhanced_prompt
                )
                review_results.append(review_result)
            
            # 3. ç”Ÿæˆç»¼åˆå®¡æŸ¥æŠ¥å‘Š
            comprehensive_report = await self._generate_comprehensive_report(review_results)
            
            # 4. æ‰§è¡ŒåŠŸèƒ½æµ‹è¯•éªŒè¯
            test_results = []
            if self._should_perform_testing(enhanced_prompt, code_to_review):
                self.logger.info("ğŸ§ª å¼€å§‹æ‰§è¡ŒåŠŸèƒ½æµ‹è¯•éªŒè¯")
                for file_path, code_content in code_to_review.items():
                    if self._is_testable_module(code_content):
                        test_result = await self._perform_functional_testing(
                            file_path, code_content, enhanced_prompt
                        )
                        test_results.append(test_result)
            
            # 5. è®¡ç®—æ•´ä½“è´¨é‡æŒ‡æ ‡ï¼ˆåŒ…å«æµ‹è¯•ç»“æœï¼‰
            overall_quality = self._calculate_overall_quality(review_results, test_results)
            
            # 6. ä¿å­˜å®¡æŸ¥æŠ¥å‘Šå’Œæµ‹è¯•ç»“æœ
            output_files = await self._save_review_files(
                comprehensive_report, review_results, test_results, task_id
            )
            
            # 7. åˆ›å»ºæ ‡å‡†åŒ–å“åº”
            response = await self._create_review_response(
                task_id, review_results, test_results, overall_quality, output_files, comprehensive_report
            )
            
            return {"formatted_response": response}
            
        except Exception as e:
            self.logger.error(f"âŒ ä»£ç å®¡æŸ¥ä»»åŠ¡å¤±è´¥: {str(e)}")
            error_response = self.create_error_response_formatted(
                task_id=task_id,
                error_message=f"ä»£ç å®¡æŸ¥å¤±è´¥: {str(e)}",
                error_details="è¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶å’ŒLLMè¿æ¥çŠ¶æ€",
                format_type=ResponseFormat.JSON
            )
            return {"formatted_response": error_response}
    
    async def _extract_code_from_files(self, file_contents: Dict[str, Dict]) -> Dict[str, str]:
        """ä»æ–‡ä»¶ä¸­æå–ä»£ç å†…å®¹"""
        code_files = {}
        
        for file_path, content_info in file_contents.items():
            file_type = content_info.get('type', '').lower()
            content = content_info.get('content', '')
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä»£ç æ–‡ä»¶
            if (file_type in ['verilog', 'systemverilog', 'vhdl'] or 
                file_path.endswith(('.v', '.sv', '.vhd')) or
                'module' in content):
                
                if content.strip():
                    code_files[file_path] = content
                    self.logger.info(f"ğŸ“„ å‘ç°ä»£ç æ–‡ä»¶: {file_path} ({len(content)} å­—ç¬¦)")
        
        return code_files
    
    async def _perform_detailed_review(self, file_path: str, code_content: str, 
                                     task_context: str) -> Dict[str, Any]:
        """æ‰§è¡Œè¯¦ç»†çš„ä»£ç å®¡æŸ¥"""
        
        review_prompt = f"""
ä½ æ˜¯ä¸€ä½æ‹¥æœ‰15å¹´ç»éªŒçš„èµ„æ·±Verilog/FPGAè®¾è®¡ä¸“å®¶å’Œä»£ç å®¡æŸ¥å‘˜ã€‚è¯·å¯¹ä»¥ä¸‹ä»£ç è¿›è¡Œå…¨é¢ã€æ·±å…¥çš„å®¡æŸ¥ã€‚

æ–‡ä»¶è·¯å¾„: {file_path}
ä»»åŠ¡ä¸Šä¸‹æ–‡: {task_context}

ä»£ç å†…å®¹:
```verilog
{code_content}
```

è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œä¸“ä¸šå®¡æŸ¥ï¼š

## 1. è¯­æ³•å’Œè¯­ä¹‰åˆ†æ
- Verilogè¯­æ³•æ­£ç¡®æ€§
- ä¿¡å·å£°æ˜å’Œä½¿ç”¨
- ç«¯å£è¿æ¥æ­£ç¡®æ€§
- æ•°æ®ç±»å‹ä½¿ç”¨è§„èŒƒ

## 2. è®¾è®¡è´¨é‡è¯„ä¼°
- æ¨¡å—åŒ–è®¾è®¡åˆç†æ€§
- æ¥å£è®¾è®¡æ¸…æ™°åº¦
- å‚æ•°åŒ–ç¨‹åº¦
- ä»£ç å¤ç”¨æ€§

## 3. æ—¶åºè®¾è®¡å®¡æŸ¥
- æ—¶é’ŸåŸŸå¤„ç†
- å¤ä½é€»è¾‘è®¾è®¡
- ç»„åˆé€»è¾‘å’Œæ—¶åºé€»è¾‘åˆ†ç¦»
- æ½œåœ¨çš„ç«äº‰å†’é™©

## 4. æ€§èƒ½å’Œæ•ˆç‡
- å…³é”®è·¯å¾„åˆ†æ
- èµ„æºä½¿ç”¨æ•ˆç‡
- æµæ°´çº¿è®¾è®¡ï¼ˆå¦‚é€‚ç”¨ï¼‰
- é¢ç§¯åŠŸè€—è€ƒè™‘

## 5. å¯ç»´æŠ¤æ€§å’Œå¯è¯»æ€§
- å‘½åè§„èŒƒä¸€è‡´æ€§
- æ³¨é‡Šå®Œæ•´æ€§å’Œå‡†ç¡®æ€§
- ä»£ç ç»“æ„æ¸…æ™°åº¦
- è°ƒè¯•å‹å¥½æ€§

## 6. é”™è¯¯æ£€æŸ¥å’Œè¾¹ç•Œå¤„ç†
- è¾¹ç•Œæ¡ä»¶å¤„ç†
- é”™è¯¯çŠ¶æ€å¤„ç†
- å¼‚å¸¸æƒ…å†µè€ƒè™‘
- æ–­è¨€ä½¿ç”¨

## 7. è¡Œä¸šæœ€ä½³å®è·µ
- ç¼–ç è§„èŒƒéµå¾ª
- è®¾è®¡æ¨¡å¼åº”ç”¨
- å¯ç»¼åˆæ€§è€ƒè™‘
- éªŒè¯å‹å¥½æ€§

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯¦ç»†çš„å®¡æŸ¥ç»“æœï¼š

{{
    "file_path": "{file_path}",
    "review_summary": "æ•´ä½“å®¡æŸ¥æ€»ç»“",
    "quality_scores": {{
        "syntax_correctness": 0.95,
        "design_quality": 0.88,
        "timing_design": 0.85,
        "performance": 0.80,
        "maintainability": 0.90,
        "error_handling": 0.75,
        "best_practices": 0.82
    }},
    "critical_issues": [
        {{
            "severity": "high",
            "category": "timing",
            "line_number": 45,
            "description": "å‘ç°çš„å…³é”®é—®é¢˜æè¿°",
            "impact": "å¯¹è®¾è®¡çš„å½±å“",
            "recommendation": "å…·ä½“ä¿®å¤å»ºè®®"
        }}
    ],
    "warnings": [
        {{
            "severity": "medium",
            "category": "style",
            "line_number": 23,
            "description": "è­¦å‘Šé—®é¢˜æè¿°",
            "recommendation": "æ”¹è¿›å»ºè®®"
        }}
    ],
    "suggestions": [
        {{
            "category": "optimization",
            "description": "ä¼˜åŒ–å»ºè®®æè¿°",
            "benefit": "ä¼˜åŒ–å¸¦æ¥çš„å¥½å¤„"
        }}
    ],
    "positive_aspects": [
        "ä»£ç çš„ä¼˜ç‚¹1",
        "ä»£ç çš„ä¼˜ç‚¹2"
    ],
    "overall_assessment": "æ•´ä½“è¯„ä»·å’Œå»ºè®®",
    "next_actions": [
        "å»ºè®®çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨1",
        "å»ºè®®çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨2"
    ]
}}

è¯·ç¡®ä¿å®¡æŸ¥ç»“æœä¸“ä¸šã€è¯¦ç»†ã€å¯æ“ä½œï¼š
"""
        
        try:
            response = await self.llm_client.send_prompt(
                prompt=review_prompt,
                temperature=0.3,
                max_tokens=3000,
                json_mode=True
            )
            
            review_result = json.loads(response)
            self.logger.info(f"âœ… æ–‡ä»¶å®¡æŸ¥å®Œæˆ: {file_path}")
            return review_result
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ LLMå®¡æŸ¥å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€å®¡æŸ¥: {str(e)}")
            return self._basic_code_review(file_path, code_content)
    
    def _basic_code_review(self, file_path: str, code_content: str) -> Dict[str, Any]:
        """åŸºç¡€ä»£ç å®¡æŸ¥ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        issues = []
        warnings = []
        
        # åŸºæœ¬è¯­æ³•æ£€æŸ¥
        if "module" not in code_content:
            issues.append({
                "severity": "high",
                "category": "syntax",
                "line_number": 1,
                "description": "ç¼ºå°‘moduleå®šä¹‰",
                "impact": "ä»£ç æ— æ³•ç¼–è¯‘",
                "recommendation": "æ·»åŠ proper moduleå®šä¹‰"
            })
        
        if "endmodule" not in code_content:
            issues.append({
                "severity": "high", 
                "category": "syntax",
                "line_number": len(code_content.split('\n')),
                "description": "ç¼ºå°‘endmodule",
                "impact": "ä»£ç æ— æ³•ç¼–è¯‘",
                "recommendation": "æ·»åŠ endmoduleè¯­å¥"
            })
        
        # åŸºæœ¬é£æ ¼æ£€æŸ¥
        if "//" not in code_content and "/*" not in code_content:
            warnings.append({
                "severity": "medium",
                "category": "style",
                "line_number": 1,
                "description": "ç¼ºå°‘æ³¨é‡Š",
                "recommendation": "ä¸ºä»£ç æ·»åŠ æ³¨é‡Šä»¥æé«˜å¯è¯»æ€§"
            })
        
        return {
            "file_path": file_path,
            "review_summary": "åŸºç¡€ä»£ç å®¡æŸ¥å®Œæˆï¼ˆLLMä¸å¯ç”¨ï¼‰",
            "quality_scores": {
                "syntax_correctness": 0.7,
                "design_quality": 0.6,
                "timing_design": 0.6,
                "performance": 0.6,
                "maintainability": 0.5,
                "error_handling": 0.5,
                "best_practices": 0.6
            },
            "critical_issues": issues,
            "warnings": warnings,
            "suggestions": [
                {"category": "general", "description": "å»ºè®®ä½¿ç”¨å®Œæ•´çš„LLMå®¡æŸ¥è·å¾—æ›´è¯¦ç»†çš„åˆ†æ", "benefit": "æ›´å‡†ç¡®çš„ä»£ç è´¨é‡è¯„ä¼°"}
            ],
            "positive_aspects": ["ä»£ç åŸºæœ¬ç»“æ„å­˜åœ¨"],
            "overall_assessment": "éœ€è¦LLMè¿›è¡Œæ›´è¯¦ç»†çš„å®¡æŸ¥",
            "next_actions": ["ä¿®å¤å‘ç°çš„åŸºæœ¬é—®é¢˜", "ä½¿ç”¨LLMè¿›è¡Œå®Œæ•´å®¡æŸ¥"]
        }
    
    async def _generate_comprehensive_report(self, review_results: list) -> str:
        """ç”Ÿæˆç»¼åˆå®¡æŸ¥æŠ¥å‘Š"""
        
        if not review_results:
            return "æ— å¯ç”¨çš„å®¡æŸ¥ç»“æœ"
        
        # ä½¿ç”¨LLMç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report_prompt = f"""
ä½œä¸ºä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹å„ä¸ªæ–‡ä»¶çš„è¯¦ç»†å®¡æŸ¥ç»“æœï¼Œç”Ÿæˆä¸€ä»½ç»¼åˆçš„ä»£ç å®¡æŸ¥æŠ¥å‘Šã€‚

å®¡æŸ¥ç»“æœ:
{json.dumps(review_results, indent=2, ensure_ascii=False)}

è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„ç»¼åˆå®¡æŸ¥æŠ¥å‘Šï¼ŒåŒ…å«ï¼š

1. **æ‰§è¡Œæ€»ç»“** - æ•´ä½“ä»£ç è´¨é‡æ¦‚è¿°
2. **å…³é”®å‘ç°** - æœ€é‡è¦çš„é—®é¢˜å’Œäº®ç‚¹
3. **è´¨é‡æŒ‡æ ‡æ±‡æ€»** - å„ç»´åº¦è´¨é‡åˆ†æ•°ç»Ÿè®¡
4. **ä¼˜å…ˆçº§è¡ŒåŠ¨è®¡åˆ’** - æŒ‰ä¼˜å…ˆçº§æ’åºçš„æ”¹è¿›å»ºè®®
5. **é£é™©è¯„ä¼°** - æ½œåœ¨é£é™©å’Œå½±å“åˆ†æ
6. **æœ€ä½³å®è·µå»ºè®®** - é•¿æœŸæ”¹è¿›å»ºè®®

æŠ¥å‘Šåº”è¯¥ï¼š
- ç»“æ„æ¸…æ™°ï¼Œä¾¿äºé˜…è¯»
- é‡ç‚¹çªå‡ºï¼Œactionable
- å¹³è¡¡é—®é¢˜å’Œä¼˜ç‚¹
- æä¾›å…·ä½“çš„æ”¹è¿›è·¯å¾„

è¯·ä»¥Markdownæ ¼å¼è¿”å›æŠ¥å‘Šï¼š
"""
        
        try:
            comprehensive_report = await self.llm_client.send_prompt(
                prompt=report_prompt,
                temperature=0.4,
                max_tokens=2500
            )
            
            self.logger.info("ğŸ“Š ç»¼åˆå®¡æŸ¥æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            return comprehensive_report.strip()
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ LLMæŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ¿: {str(e)}")
            return self._generate_basic_report(review_results)
    
    def _generate_basic_report(self, review_results: list) -> str:
        """ç”ŸæˆåŸºç¡€æŠ¥å‘Šï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        total_files = len(review_results)
        total_issues = sum(len(result.get('critical_issues', [])) for result in review_results)
        total_warnings = sum(len(result.get('warnings', [])) for result in review_results)
        
        report = f"""# ä»£ç å®¡æŸ¥ç»¼åˆæŠ¥å‘Š

## å®¡æŸ¥æ¦‚è§ˆ
- å®¡æŸ¥æ–‡ä»¶æ•°: {total_files}
- å‘ç°å…³é”®é—®é¢˜: {total_issues}
- å‘ç°è­¦å‘Š: {total_warnings}

## æ–‡ä»¶å®¡æŸ¥ç»“æœ
"""
        
        for result in review_results:
            file_path = result.get('file_path', 'Unknown')
            summary = result.get('review_summary', 'No summary')
            report += f"\n### {file_path}\n{summary}\n"
        
        report += "\n## å»ºè®®è¡ŒåŠ¨\n1. ä¿®å¤æ‰€æœ‰å…³é”®é—®é¢˜\n2. å¤„ç†è­¦å‘Šé—®é¢˜\n3. è€ƒè™‘ä¼˜åŒ–å»ºè®®"
        
        return report
    
    def _calculate_overall_quality(self, review_results: list, test_results: list = None) -> QualityMetrics:
        """è®¡ç®—æ•´ä½“è´¨é‡æŒ‡æ ‡"""
        if not review_results:
            return QualityMetrics(0.5, 0.5, 0.5, 0.0, 0.5, 0.5)
        
        # æ”¶é›†æ‰€æœ‰è´¨é‡åˆ†æ•°
        all_scores = []
        syntax_scores = []
        design_scores = []
        maintainability_scores = []
        performance_scores = []
        
        for result in review_results:
            quality_scores = result.get('quality_scores', {})
            
            syntax_score = quality_scores.get('syntax_correctness', 0.5)
            design_score = quality_scores.get('design_quality', 0.5)
            timing_score = quality_scores.get('timing_design', 0.5)
            perf_score = quality_scores.get('performance', 0.5)
            maint_score = quality_scores.get('maintainability', 0.5)
            error_score = quality_scores.get('error_handling', 0.5)
            practice_score = quality_scores.get('best_practices', 0.5)
            
            # è®¡ç®—è¯¥æ–‡ä»¶çš„æ•´ä½“åˆ†æ•°
            file_overall = (syntax_score + design_score + timing_score + 
                          perf_score + maint_score + error_score + practice_score) / 7
            
            all_scores.append(file_overall)
            syntax_scores.append(syntax_score)
            design_scores.append(design_score)
            maintainability_scores.append(maint_score)
            performance_scores.append(perf_score)
        
        # è®¡ç®—å¹³å‡å€¼
        overall_score = sum(all_scores) / len(all_scores)
        avg_syntax = sum(syntax_scores) / len(syntax_scores)
        avg_design = sum(design_scores) / len(design_scores)
        avg_maintainability = sum(maintainability_scores) / len(maintainability_scores)
        avg_performance = sum(performance_scores) / len(performance_scores)
        
        # è®¡ç®—æµ‹è¯•è¦†ç›–ç‡ï¼ˆå¦‚æœæœ‰æµ‹è¯•ç»“æœï¼‰
        test_coverage = 0.0
        if test_results:
            total_tests = len(test_results)
            successful_tests = sum(1 for test in test_results if test.get('test_success', False))
            if total_tests > 0:
                test_coverage = successful_tests / total_tests
                # æ ¹æ®æµ‹è¯•é€šè¿‡ç‡è°ƒæ•´æ•´ä½“åˆ†æ•°
                avg_pass_rate = sum(test.get('pass_rate', 0.0) for test in test_results) / total_tests
                overall_score = (overall_score + avg_pass_rate) / 2
        
        return QualityMetrics(
            overall_score=overall_score,
            syntax_score=avg_syntax,
            functionality_score=avg_design,
            test_coverage=test_coverage,
            documentation_quality=avg_maintainability,
            performance_score=avg_performance
        )
    
    async def _save_review_files(self, comprehensive_report: str, review_results: list,
                               test_results: list, task_id: str) -> list:
        """ä¿å­˜å®¡æŸ¥æŠ¥å‘Šæ–‡ä»¶"""
        output_files = []
        
        try:
            # ä½¿ç”¨å·¥ä»¶ç›®å½•ç¡®ä¿ç›®å½•å­˜åœ¨
            output_dir = self.artifacts_dir
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ç»¼åˆæŠ¥å‘Š
            report_path = output_dir / f"code_review_report_{task_id}.md"
            report_ref = await self.save_result_to_file(
                content=comprehensive_report,
                file_path=str(report_path),
                file_type="documentation"
            )
            output_files.append(report_ref)
            
            # ä¿å­˜è¯¦ç»†å®¡æŸ¥ç»“æœ
            detailed_results = {
                "task_id": task_id,
                "review_timestamp": task_id,
                "total_files_reviewed": len(review_results),
                "detailed_results": review_results,
                "test_results": test_results if test_results else [],
                "testing_performed": bool(test_results)
            }
            
            details_path = output_dir / f"review_details_{task_id}.json"
            details_ref = await self.save_result_to_file(
                content=json.dumps(detailed_results, indent=2, ensure_ascii=False),
                file_path=str(details_path),
                file_type="json"
            )
            output_files.append(details_ref)
            
            # ä¿å­˜æµ‹è¯•å°æ–‡ä»¶å¼•ç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if test_results:
                for test_result in test_results:
                    testbench_file = test_result.get('testbench_file')
                    if testbench_file and Path(testbench_file).exists():
                        testbench_ref = await self.save_result_to_file(
                            content=Path(testbench_file).read_text(encoding='utf-8'),
                            file_path=testbench_file,
                            file_type="testbench",
                            description=f"Generated testbench for {test_result.get('file_path', 'unknown')}"
                        )
                        output_files.append(testbench_ref)
            
            self.logger.info(f"ğŸ’¾ å®¡æŸ¥å’Œæµ‹è¯•æŠ¥å‘Šä¿å­˜å®Œæˆ: {len(output_files)} ä¸ªæ–‡ä»¶")
            return output_files
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜å®¡æŸ¥æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return []
    
    async def _create_review_response(self, task_id: str, review_results: list,
                                    test_results: list, overall_quality: QualityMetrics, 
                                    output_files: list, comprehensive_report: str) -> str:
        """åˆ›å»ºæ ‡å‡†åŒ–å®¡æŸ¥å“åº”"""
        
        builder = self.create_response_builder(task_id)
        
        # æ·»åŠ ç”Ÿæˆçš„æ–‡ä»¶
        for file_ref in output_files:
            builder.add_generated_file(
                file_ref.file_path,
                file_ref.file_type,
                file_ref.description
            )
        
        # ç»Ÿè®¡é—®é¢˜æ•°é‡
        total_critical = sum(len(result.get('critical_issues', [])) for result in review_results)
        total_warnings = sum(len(result.get('warnings', [])) for result in review_results)
        
        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        total_tests = len(test_results) if test_results else 0
        successful_tests = sum(1 for test in (test_results or []) if test.get('test_success', False))
        failed_tests = total_tests - successful_tests
        
        # ç»Ÿè®¡å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹
        total_failed_cases = 0
        if test_results:
            for test in test_results:
                total_failed_cases += len(test.get('failed_cases', []))
        
        # æ ¹æ®å®¡æŸ¥ç»“æœæ·»åŠ é—®é¢˜
        if total_critical > 0:
            builder.add_issue(
                "error", "high",
                f"å‘ç° {total_critical} ä¸ªå…³é”®é—®é¢˜éœ€è¦ç«‹å³ä¿®å¤",
                solution="è¯·æŸ¥çœ‹è¯¦ç»†å®¡æŸ¥æŠ¥å‘Šå¹¶é€ä¸€ä¿®å¤å…³é”®é—®é¢˜"
            )
        
        if total_warnings > 0:
            builder.add_issue(
                "warning", "medium",
                f"å‘ç° {total_warnings} ä¸ªè­¦å‘Šé—®é¢˜å»ºè®®å¤„ç†",
                solution="å»ºè®®åœ¨ä¸‹æ¬¡è¿­ä»£ä¸­å¤„ç†è¿™äº›è­¦å‘Šé—®é¢˜"
            )
        
        if overall_quality.overall_score < 0.6:
            builder.add_issue(
                "warning", "high",
                f"æ•´ä½“ä»£ç è´¨é‡è¾ƒä½ (åˆ†æ•°: {overall_quality.overall_score:.2f})",
                solution="å»ºè®®è¿›è¡Œå…¨é¢çš„ä»£ç é‡æ„"
            )
        
        # æ·»åŠ æµ‹è¯•ç›¸å…³é—®é¢˜
        if failed_tests > 0:
            builder.add_issue(
                "error", "high",
                f"åŠŸèƒ½æµ‹è¯•å¤±è´¥: {failed_tests}/{total_tests} ä¸ªæ¨¡å—æµ‹è¯•æœªé€šè¿‡",
                solution="æ£€æŸ¥æµ‹è¯•å¤±è´¥çš„æ¨¡å—å¹¶ä¿®å¤åŠŸèƒ½é”™è¯¯"
            )
        
        if total_failed_cases > 0:
            builder.add_issue(
                "warning", "medium",
                f"æµ‹è¯•ç”¨ä¾‹å¤±è´¥: å…± {total_failed_cases} ä¸ªæµ‹è¯•ç”¨ä¾‹æœªé€šè¿‡",
                solution="æŸ¥çœ‹æµ‹è¯•æŠ¥å‘Šè¯¦æƒ…ï¼Œä¿®å¤å¤±è´¥çš„åŠŸèƒ½ç‚¹"
            )
        
        # æ·»åŠ ä¸‹ä¸€æ­¥å»ºè®®
        builder.add_next_step("ä»”ç»†é˜…è¯»ç»¼åˆå®¡æŸ¥æŠ¥å‘Š")
        
        if total_tests > 0:
            builder.add_next_step("æŸ¥çœ‹åŠŸèƒ½æµ‹è¯•ç»“æœå’Œç”Ÿæˆçš„æµ‹è¯•å°")
        
        builder.add_next_step("ä¼˜å…ˆä¿®å¤æ‰€æœ‰å…³é”®é—®é¢˜")
        
        if total_warnings > 0:
            builder.add_next_step("å¤„ç†è­¦å‘Šé—®é¢˜ä»¥æé«˜ä»£ç è´¨é‡")
        
        if overall_quality.performance_score < 0.7:
            builder.add_next_step("è€ƒè™‘æ€§èƒ½ä¼˜åŒ–")
        
        if failed_tests > 0:
            builder.add_next_step("ä¿®å¤æµ‹è¯•å¤±è´¥çš„åŠŸèƒ½åé‡æ–°è¿è¡Œæµ‹è¯•")
        
        builder.add_next_step("åœ¨ä¿®å¤åé‡æ–°è¿›è¡Œä»£ç å®¡æŸ¥")
        
        # æ·»åŠ å…ƒæ•°æ®
        builder.add_metadata("files_reviewed", len(review_results))
        builder.add_metadata("critical_issues", total_critical)
        builder.add_metadata("warnings", total_warnings)
        builder.add_metadata("overall_quality_score", overall_quality.overall_score)
        builder.add_metadata("review_type", "comprehensive")
        builder.add_metadata("llm_powered", True)
        builder.add_metadata("testing_performed", total_tests > 0)
        builder.add_metadata("total_tests", total_tests)
        builder.add_metadata("successful_tests", successful_tests)
        builder.add_metadata("test_coverage", overall_quality.test_coverage)
        if test_results:
            avg_pass_rate = sum(test.get('pass_rate', 0.0) for test in test_results) / len(test_results)
            builder.add_metadata("average_test_pass_rate", avg_pass_rate)
        
        # æ„å»ºå“åº” - ä»£ç å®¡æŸ¥ä»»åŠ¡æ€»æ˜¯100%å®Œæˆ
        status = TaskStatus.SUCCESS
        completion = 100.0
        
        # æ„å»ºæ¶ˆæ¯
        message_parts = [
            f"ä»£ç å®¡æŸ¥å®Œæˆï¼Œå…±å®¡æŸ¥ {len(review_results)} ä¸ªæ–‡ä»¶"
        ]
        
        if total_tests > 0:
            message_parts.append(f"æ‰§è¡Œäº† {total_tests} ä¸ªæ¨¡å—çš„åŠŸèƒ½æµ‹è¯•")
            if failed_tests > 0:
                message_parts.append(f"{failed_tests} ä¸ªæµ‹è¯•å¤±è´¥")
            if total_failed_cases > 0:
                message_parts.append(f"{total_failed_cases} ä¸ªæµ‹è¯•ç”¨ä¾‹å¤±è´¥")
        
        if total_critical > 0:
            message_parts.append(f"å‘ç° {total_critical} ä¸ªå…³é”®é—®é¢˜")
        if total_warnings > 0:
            message_parts.append(f"{total_warnings} ä¸ªè­¦å‘Š")
            
        message_parts.append(f"æ•´ä½“è´¨é‡åˆ†æ•°: {overall_quality.overall_score:.2f}")
        
        if total_tests > 0:
            message_parts.append(f"æµ‹è¯•è¦†ç›–ç‡: {overall_quality.test_coverage:.1%}")
        
        response = builder.build(
            response_type=ResponseType.QUALITY_REPORT,
            status=status,
            message=", ".join(message_parts),
            completion_percentage=completion,
            quality_metrics=overall_quality
        )
        
        return response.format_response(ResponseFormat.JSON)
    
    # ==========================================================================
    # ğŸ§ª åŠŸèƒ½æµ‹è¯•éªŒè¯
    # ==========================================================================
    
    def _should_perform_testing(self, prompt: str, code_files: Dict[str, str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰§è¡ŒåŠŸèƒ½æµ‹è¯•"""
        # æ£€æŸ¥æ˜¯å¦æ˜ç¡®è¦æ±‚æµ‹è¯•
        test_keywords = ['æµ‹è¯•', 'éªŒè¯', 'test', 'verify', 'simulation', 'testbench']
        if any(keyword in prompt.lower() for keyword in test_keywords):
            self.logger.info(f"ğŸ§ª æ£€æµ‹åˆ°æµ‹è¯•å…³é”®è¯ï¼Œå¯ç”¨åŠŸèƒ½æµ‹è¯•")
            return True
        
        # æ£€æŸ¥ä»£ç è§„æ¨¡å’Œå¤æ‚åº¦ï¼Œå†³å®šæ˜¯å¦éœ€è¦æµ‹è¯•
        total_lines = sum(len(content.split('\n')) for content in code_files.values())
        if total_lines > 50:  # è¶…è¿‡50è¡Œä»£ç å»ºè®®æµ‹è¯•
            self.logger.info(f"ğŸ§ª ä»£ç è§„æ¨¡({total_lines}è¡Œ)è¶…è¿‡é˜ˆå€¼ï¼Œå¯ç”¨åŠŸèƒ½æµ‹è¯•")
            return True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤æ‚é€»è¾‘éœ€è¦æµ‹è¯•
        for file_path, content in code_files.items():
            complex_keywords = ['always', 'case', 'if', 'for', 'while', 'assign']
            keyword_count = sum(1 for keyword in complex_keywords if keyword in content.lower())
            if keyword_count >= 3:  # é™ä½é˜ˆå€¼ï¼ŒåŒ…å«3ä¸ªä»¥ä¸Šå¤æ‚é€»è¾‘å…³é”®è¯å°±æµ‹è¯•
                self.logger.info(f"ğŸ§ª æ£€æµ‹åˆ°å¤æ‚é€»è¾‘({keyword_count}ä¸ªå…³é”®è¯)ï¼Œå¯ç”¨åŠŸèƒ½æµ‹è¯•")
                return True
        
        # é»˜è®¤ä¸ºALUã€è®¡æ•°å™¨ã€å­˜å‚¨å™¨ç­‰å¸¸è§æ¨¡å—å¯ç”¨æµ‹è¯•
        for content in code_files.values():
            common_modules = ['alu', 'counter', 'memory', 'fifo', 'uart', 'spi', 'processor']
            if any(module_type in content.lower() for module_type in common_modules):
                self.logger.info(f"ğŸ§ª æ£€æµ‹åˆ°å¸¸è§è®¾è®¡æ¨¡å—ï¼Œå¯ç”¨åŠŸèƒ½æµ‹è¯•")
                return True
        
        self.logger.info(f"ğŸš« æœªæ»¡è¶³æµ‹è¯•è§¦å‘æ¡ä»¶ï¼Œè·³è¿‡åŠŸèƒ½æµ‹è¯•")
        return False
    
    def _is_testable_module(self, code_content: str) -> bool:
        """åˆ¤æ–­æ¨¡å—æ˜¯å¦å¯æµ‹è¯•"""
        # æ¸…ç†æ–‡ä»¶å†…å®¹ï¼Œç§»é™¤markdownæ ¼å¼æ ‡è®°
        cleaned_content = code_content
        if cleaned_content.startswith('```'):
            lines = cleaned_content.split('\n')
            # ç§»é™¤å¼€å¤´çš„```verilogè¡Œ
            if lines[0].startswith('```'):
                lines = lines[1:]
            # ç§»é™¤ç»“å°¾çš„```è¡Œ
            if lines and lines[-1].strip() == '```':
                lines = lines[:-1]
            cleaned_content = '\n'.join(lines)
        
        self.logger.debug(f"ğŸ” æ£€æŸ¥æ¨¡å—å¯æµ‹è¯•æ€§ï¼Œå†…å®¹é•¿åº¦: {len(cleaned_content)}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„æ¨¡å—
        if 'module' not in cleaned_content or 'endmodule' not in cleaned_content:
            self.logger.debug(f"ğŸš« æœªæ‰¾åˆ°å®Œæ•´çš„moduleå®šä¹‰")
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç«¯å£å®šä¹‰ - ä½¿ç”¨æ›´å®½æ¾çš„æ­£åˆ™è¡¨è¾¾å¼
        module_pattern = r'module\s+\w+\s*(?:\#\([^)]*\))?\s*\([^)]*\)'
        if not re.search(module_pattern, cleaned_content, re.DOTALL):
            self.logger.debug(f"ğŸš« æœªæ‰¾åˆ°æ¨¡å—ç«¯å£å®šä¹‰")
            return False
        
        # æ’é™¤è¿‡äºç®€å•çš„æ¨¡å—
        non_empty_lines = [line.strip() for line in cleaned_content.split('\n') 
                          if line.strip() and not line.strip().startswith('//')]
        if len(non_empty_lines) < 10:
            self.logger.debug(f"ğŸš« æ¨¡å—è¿‡äºç®€å•({len(non_empty_lines)}è¡Œ)")
            return False
        
        self.logger.info(f"âœ… æ¨¡å—å¯æµ‹è¯•ï¼Œæœ‰æ•ˆä»£ç è¡Œæ•°: {len(non_empty_lines)}")
        return True
    
    async def _perform_functional_testing(self, file_path: str, code_content: str, 
                                        task_context: str) -> Dict[str, Any]:
        """æ‰§è¡ŒåŠŸèƒ½æµ‹è¯•ï¼ŒåŒ…å«LLMé©±åŠ¨çš„é”™è¯¯ä¿®å¤"""
        max_retries = 3
        current_code = code_content
        
        for attempt in range(max_retries):
            self.logger.info(f"ğŸ” åŠŸèƒ½æµ‹è¯•å°è¯• {attempt + 1}/{max_retries}")
            
            try:
                # 1. ç”Ÿæˆæµ‹è¯•å°
                testbench_result = await self._generate_testbench(file_path, current_code, task_context)
                
                if not testbench_result['success']:
                    if attempt == max_retries - 1:
                        return {
                            'file_path': file_path,
                            'test_success': False,
                            'error': f"æµ‹è¯•å°ç”Ÿæˆå¤±è´¥: {testbench_result['error']}",
                            'testbench_generated': False,
                            'iterations': attempt + 1
                        }
                    continue
                
                # 2. æ‰§è¡Œiverilogä»¿çœŸ
                simulation_result = await self._run_iverilog_simulation(
                    file_path, current_code, testbench_result['testbench_code']
                )
                
                # 3. å¦‚æœæˆåŠŸï¼Œåˆ†æç»“æœ
                if simulation_result['success']:
                    test_analysis = await self._analyze_test_results(
                        file_path, simulation_result, testbench_result.get('expected_results', [])
                    )
                    
                    return {
                        'file_path': file_path,
                        'test_success': True,
                        'testbench_generated': True,
                        'testbench_file': testbench_result.get('testbench_file'),
                        'simulation_output': simulation_result.get('output', ''),
                        'compilation_success': True,
                        'execution_success': True,
                        'test_cases': test_analysis.get('test_cases', []),
                        'failed_cases': test_analysis.get('failed_cases', []),
                        'pass_rate': test_analysis.get('pass_rate', 0.0),
                        'error_details': '',
                        'recommendations': test_analysis.get('recommendations', []),
                        'iterations': attempt + 1,
                        'code_fixed': attempt > 0
                    }
                
                # 4. å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨LLMä¿®å¤ä»£ç 
                if simulation_result.get('error'):
                    self.logger.info(f"âš ï¸ æ£€æµ‹åˆ°é”™è¯¯ï¼Œä½¿ç”¨LLMä¿®å¤ä»£ç ...")
                    
                    fixed_code = await self._fix_code_with_llm(
                        current_code, simulation_result['error'], task_context
                    )
                    
                    if fixed_code and fixed_code != current_code:
                        current_code = fixed_code
                        self.logger.info(f"âœ… LLMå·²ç”Ÿæˆä¿®å¤åçš„ä»£ç ")
                        continue
                    else:
                        self.logger.warning(f"âš ï¸ LLMæœªèƒ½ä¿®å¤ä»£ç æˆ–ä»£ç æ— å˜åŒ–")
                
                # 5. æœ€åä¸€æ¬¡å°è¯•ä»å¤±è´¥
                if attempt == max_retries - 1:
                    return {
                        'file_path': file_path,
                        'test_success': False,
                        'testbench_generated': True,
                        'error_details': simulation_result.get('error', 'æœªçŸ¥é”™è¯¯'),
                        'iterations': attempt + 1,
                        'final_code': current_code
                    }
                
            except Exception as e:
                self.logger.error(f"âŒ åŠŸèƒ½æµ‹è¯•å¼‚å¸¸ {file_path}: {str(e)}")
                if attempt == max_retries - 1:
                    return {
                        'file_path': file_path,
                        'test_success': False,
                        'error': f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}",
                        'testbench_generated': False,
                        'iterations': attempt + 1
                    }
        
        return {
            'file_path': file_path,
            'test_success': False,
            'error': 'è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°',
            'iterations': max_retries
        }
    
    async def _generate_testbench(self, file_path: str, code_content: str, 
                                task_context: str) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•å°"""
        try:
            # åˆ†ææ¨¡å—ä¿¡æ¯
            module_info = self._parse_module_info(code_content)
            
            testbench_prompt = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„VerilogéªŒè¯å·¥ç¨‹å¸ˆã€‚è¯·ä¸ºä»¥ä¸‹æ¨¡å—ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ã€å…¨é¢çš„æµ‹è¯•å°ã€‚

æ¨¡å—æ–‡ä»¶: {file_path}
ä»»åŠ¡ä¸Šä¸‹æ–‡: {task_context}

æ¨¡å—ä¿¡æ¯:
- æ¨¡å—å: {module_info['module_name']}
- è¾“å…¥ç«¯å£: {module_info['inputs']}
- è¾“å‡ºç«¯å£: {module_info['outputs']}

æ¨¡å—ä»£ç :
```verilog
{code_content}
```

è¯·ç”Ÿæˆä¸€ä¸ªcomprehensive testbenchï¼Œè¦æ±‚ï¼š

1. **æµ‹è¯•è¦†ç›–æ€§**ï¼š
   - è¦†ç›–æ‰€æœ‰è¾“å…¥ç»„åˆï¼ˆåˆç†èŒƒå›´å†…ï¼‰
   - æµ‹è¯•è¾¹ç•Œæ¡ä»¶å’Œæå€¼
   - æµ‹è¯•æ­£å¸¸åŠŸèƒ½å’Œå¼‚å¸¸æƒ…å†µ

2. **æµ‹è¯•å°ç»“æ„**ï¼š
   - åŒ…å«æ—¶é’Ÿç”Ÿæˆï¼ˆå¦‚éœ€è¦ï¼‰
   - å¤ä½åºåˆ—
   - æµ‹è¯•å‘é‡ç”Ÿæˆ
   - ç»“æœæ£€æŸ¥å’Œå¯¹æ¯”

3. **é¢„æœŸç»“æœ**ï¼š
   - ä¸ºæ¯ä¸ªæµ‹è¯•ç”¨ä¾‹å®šä¹‰é¢„æœŸè¾“å‡º
   - å®ç°è‡ªåŠ¨å¯¹æ¯”æ£€æŸ¥
   - è¾“å‡ºè¯¦ç»†çš„æµ‹è¯•ç»“æœ

4. **æµ‹è¯•ç”¨ä¾‹è®¾è®¡**ï¼š
   - åŸºæœ¬åŠŸèƒ½æµ‹è¯•
   - è¾¹ç•Œå€¼æµ‹è¯•
   - éšæœºæµ‹è¯•ï¼ˆé€‚é‡ï¼‰
   - æ€§èƒ½æµ‹è¯•ï¼ˆå¦‚é€‚ç”¨ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "testbench_code": "å®Œæ•´çš„testbench Verilogä»£ç ",
    "expected_results": [
        {{
            "test_case": "æµ‹è¯•ç”¨ä¾‹æè¿°",
            "inputs": {{"input_name": "value"}},
            "expected_outputs": {{"output_name": "expected_value"}},
            "test_type": "basic|boundary|random|performance"
        }}
    ],
    "test_summary": "æµ‹è¯•è®¡åˆ’æ€»ç»“",
    "coverage_description": "æµ‹è¯•è¦†ç›–ç‡è¯´æ˜"
}}

testbenchåº”è¯¥ï¼š
- æ¨¡å—åä¸º `{module_info['module_name']}_tb`
- åŒ…å«å®Œæ•´çš„ä»¿çœŸæ§åˆ¶ï¼ˆ$dumpfile, $dumpvars, $finishï¼‰
- å…·æœ‰æ¸…æ™°çš„æµ‹è¯•æŠ¥å‘Šè¾“å‡º
- èƒ½å¤Ÿåœ¨iverilogä¸­æ­£ç¡®ç¼–è¯‘å’Œè¿è¡Œ
"""
            
            response = await self.llm_client.send_prompt(
                prompt=testbench_prompt,
                temperature=0.4,
                max_tokens=4000,
                json_mode=True
            )
            
            # æ¸…ç†å“åº”ä¸­çš„è½¬ä¹‰å­—ç¬¦
            cleaned_response = response.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]
            cleaned_response = cleaned_response.strip()
            
            testbench_data = json.loads(cleaned_response)
            
            # ä¿å­˜æµ‹è¯•å°åˆ°æ–‡ä»¶
            testbench_file = await self._save_testbench_file(
                module_info['module_name'], testbench_data['testbench_code']
            )
            
            return {
                'success': True,
                'testbench_code': testbench_data['testbench_code'],
                'testbench_file': testbench_file,
                'expected_results': testbench_data.get('expected_results', []),
                'test_summary': testbench_data.get('test_summary', ''),
                'coverage_description': testbench_data.get('coverage_description', '')
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æµ‹è¯•å°ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _parse_module_info(self, code_content: str) -> Dict[str, Any]:
        """è§£ææ¨¡å—ä¿¡æ¯"""
        # æ¸…ç†æ–‡ä»¶å†…å®¹ï¼Œç§»é™¤markdownæ ¼å¼æ ‡è®°
        cleaned_content = code_content
        if cleaned_content.startswith('```'):
            lines = cleaned_content.split('\n')
            if lines[0].startswith('```'):
                lines = lines[1:]
            if lines and lines[-1].strip() == '```':
                lines = lines[:-1]
            cleaned_content = '\n'.join(lines)
        
        module_info = {
            'module_name': 'unknown_module',
            'inputs': [],
            'outputs': [],
            'parameters': []
        }
        
        # æå–æ¨¡å—å
        module_match = re.search(r'module\s+(\w+)', cleaned_content)
        if module_match:
            module_info['module_name'] = module_match.group(1)
        
        # æå–ç«¯å£ä¿¡æ¯ - æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼
        input_matches = re.findall(r'input\s+(?:(?:reg|wire)\s+)?(?:\[[^\]]+\]\s+)?(\w+)', cleaned_content)
        output_matches = re.findall(r'output\s+(?:(?:reg|wire)\s+)?(?:\[[^\]]+\]\s+)?(\w+)', cleaned_content)
        
        module_info['inputs'] = input_matches
        module_info['outputs'] = output_matches
        
        # æå–å‚æ•°ä¿¡æ¯
        param_matches = re.findall(r'parameter\s+(\w+)\s*=\s*([^;,\s]+)', cleaned_content)
        module_info['parameters'] = param_matches
        
        self.logger.info(f"ğŸ“Š è§£ææ¨¡å—ä¿¡æ¯: {module_info['module_name']}, "
                        f"è¾“å…¥: {len(module_info['inputs'])}, "
                        f"è¾“å‡º: {len(module_info['outputs'])}")
        
        return module_info
    
    async def _save_testbench_file(self, module_name: str, testbench_code: str) -> str:
        """ä¿å­˜æµ‹è¯•å°æ–‡ä»¶"""
        output_dir = self.artifacts_dir
        output_dir.mkdir(parents=True, exist_ok=True)
        
        testbench_file = output_dir / f"{module_name}_tb.v"
        
        with open(testbench_file, 'w', encoding='utf-8') as f:
            f.write(testbench_code)
        
        self.logger.info(f"ğŸ’¾ æµ‹è¯•å°å·²ä¿å­˜: {testbench_file}")
        return str(testbench_file)
    
    async def _fix_code_with_llm(self, original_code: str, error_message: str, task_context: str) -> str:
        """ä½¿ç”¨LLMæ ¹æ®é”™è¯¯ä¿¡æ¯ä¿®å¤ä»£ç """
        try:
            self.logger.info("ğŸ”§ ä½¿ç”¨LLMä¿®å¤ä»£ç ...")
            
            fix_prompt = f"""
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„Verilogè®¾è®¡å’Œè°ƒè¯•ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹é”™è¯¯ä¿¡æ¯ä¿®å¤Verilogä»£ç ã€‚

## ä»»åŠ¡èƒŒæ™¯
{task_context}

## å½“å‰ä»£ç 
```verilog
{original_code}
```

## é”™è¯¯ä¿¡æ¯
{error_message}

## ä¿®å¤è¦æ±‚
1. **ç²¾ç¡®å®šä½é”™è¯¯**ï¼šåˆ†æé”™è¯¯ä¿¡æ¯ï¼Œæ‰¾åˆ°ç¡®åˆ‡çš„è¯­æ³•æˆ–é€»è¾‘é—®é¢˜
2. **æœ€å°åŒ–ä¿®æ”¹**ï¼šåªä¿®å¤å¿…è¦çš„é”™è¯¯ï¼Œä¿æŒåŸæœ‰è®¾è®¡æ„å›¾
3. **éªŒè¯ä¿®å¤**ï¼šç¡®ä¿ä¿®å¤åçš„ä»£ç å¯ä»¥æˆåŠŸç¼–è¯‘å’Œä»¿çœŸ
4. **ä¿æŒåŠŸèƒ½**ï¼šä¿®å¤åå¿…é¡»å®ç°åŸæœ‰çš„è®¾è®¡åŠŸèƒ½

## å¸¸è§é”™è¯¯ç±»å‹å’Œä¿®å¤å»ºè®®
- **è¯­æ³•é”™è¯¯**ï¼šæ£€æŸ¥æ‹¬å·åŒ¹é…ã€åˆ†å·ã€æ¨¡å—å£°æ˜
- **ç±»å‹é”™è¯¯**ï¼šreg/wireå£°æ˜ä¸ä½¿ç”¨çš„ä¸€è‡´æ€§
- **ç«¯å£é”™è¯¯**ï¼šæ¨¡å—ç«¯å£ä¸å®ä¾‹åŒ–çš„ä¸€è‡´æ€§
- **æ—¶åºé”™è¯¯**ï¼šæ—¶é’Ÿå’Œå¤ä½ä¿¡å·çš„å¤„ç†

## ä¿®å¤ç­–ç•¥
1. é¦–å…ˆåˆ†æé”™è¯¯ç±»å‹å’Œä½ç½®
2. ç„¶åæä¾›ä¿®å¤åçš„å®Œæ•´ä»£ç 
3. æœ€åç®€è¦è¯´æ˜ä¿®å¤çš„è¦ç‚¹

è¯·åªè¿”å›ä¿®å¤åçš„å®Œæ•´Verilogä»£ç ï¼Œä¸è¦æ·»åŠ è§£é‡Šæ–‡å­—ï¼š
"""
            
            fixed_code = await self.llm_client.send_prompt(
                prompt=fix_prompt,
                temperature=0.3,
                max_tokens=3000,
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Verilogä»£ç ä¿®å¤ä¸“å®¶ï¼Œä¸“æ³¨äºè§£å†³ç¼–è¯‘é”™è¯¯å’Œé€»è¾‘é—®é¢˜ã€‚"
            )
            
            # æ¸…ç†è¿”å›çš„ä»£ç 
            fixed_code = fixed_code.strip()
            if fixed_code.startswith('```verilog'):
                fixed_code = fixed_code[10:]
            if fixed_code.endswith('```'):
                fixed_code = fixed_code[:-3]
            
            self.logger.info(f"âœ… LLMç”Ÿæˆä¿®å¤ä»£ç å®Œæˆï¼Œé•¿åº¦: {len(fixed_code)} å­—ç¬¦")
            return fixed_code.strip()
            
        except Exception as e:
            self.logger.error(f"âŒ LLMä»£ç ä¿®å¤å¤±è´¥: {str(e)}")
            return original_code  # è¿”å›åŸä»£ç å¦‚æœä¿®å¤å¤±è´¥
    
    async def _run_iverilog_simulation(self, module_file: str, module_code: str, 
                                     testbench_code: str) -> Dict[str, Any]:
        """è¿è¡Œiverilogä»¿çœŸ"""
        result = {
            'success': False,
            'compilation_success': False,
            'execution_success': False,
            'output': '',
            'error': ''
        }
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            try:
                temp_path = Path(temp_dir)
                
                # ä¿å­˜æ¨¡å—æ–‡ä»¶
                module_path = temp_path / "module.v"
                with open(module_path, 'w', encoding='utf-8') as f:
                    f.write(module_code)
                
                # ä¿å­˜æµ‹è¯•å°æ–‡ä»¶
                testbench_path = temp_path / "testbench.v"
                with open(testbench_path, 'w', encoding='utf-8') as f:
                    f.write(testbench_code)
                
                # 1. ç¼–è¯‘
                compile_cmd = [
                    'iverilog', 
                    '-o', str(temp_path / 'simulation'),
                    str(module_path),
                    str(testbench_path)
                ]
                
                self.logger.info(f"ğŸ”¨ ç¼–è¯‘å‘½ä»¤: {' '.join(compile_cmd)}")
                
                compile_process = subprocess.run(
                    compile_cmd,
                    capture_output=True,
                    text=True,
                    timeout=30,
                    cwd=temp_dir
                )
                
                if compile_process.returncode != 0:
                    error_message = compile_process.stderr
                    result['error'] = f"ç¼–è¯‘å¤±è´¥: {error_message}"
                    self.logger.error(f"âŒ iverilogç¼–è¯‘å¤±è´¥: {error_message}")
                    
                    # å°è¯•æ™ºèƒ½åˆ†æå’Œä¿®å¤ç¼–è¯‘é”™è¯¯
                    fix_suggestion = await self._analyze_compilation_error(
                        error_message, 
                        module_content, 
                        testbench_content
                    )
                    result['fix_suggestion'] = fix_suggestion
                    result['needs_fix'] = True
                    
                    return result
                
                result['compilation_success'] = True
                self.logger.info("âœ… iverilogç¼–è¯‘æˆåŠŸ")
                
                # 2. è¿è¡Œä»¿çœŸ
                sim_cmd = ['vvp', str(temp_path / 'simulation')]
                
                self.logger.info(f"â–¶ï¸ ä»¿çœŸå‘½ä»¤: {' '.join(sim_cmd)}")
                
                sim_process = subprocess.run(
                    sim_cmd,
                    capture_output=True,
                    text=True,
                    timeout=60,
                    cwd=temp_dir
                )
                
                result['output'] = sim_process.stdout
                result['execution_success'] = sim_process.returncode == 0
                
                if sim_process.returncode != 0:
                    result['error'] += f" | ä»¿çœŸå¤±è´¥: {sim_process.stderr}"
                    self.logger.error(f"âŒ ä»¿çœŸæ‰§è¡Œå¤±è´¥: {sim_process.stderr}")
                else:
                    self.logger.info("âœ… ä»¿çœŸæ‰§è¡ŒæˆåŠŸ")
                
                result['success'] = result['compilation_success'] and result['execution_success']
                
                return result
                
            except subprocess.TimeoutExpired:
                result['error'] = "ä»¿çœŸè¶…æ—¶"
                self.logger.error("âŒ ä»¿çœŸæ‰§è¡Œè¶…æ—¶")
                return result
            except FileNotFoundError:
                result['error'] = "iverilogæœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­"
                self.logger.error("âŒ iverilogæœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿å·²å®‰è£…iverilog")
                return result
            except Exception as e:
                result['error'] = f"ä»¿çœŸæ‰§è¡Œå¼‚å¸¸: {str(e)}"
                self.logger.error(f"âŒ ä»¿çœŸå¼‚å¸¸: {str(e)}")
                return result
    
    async def _analyze_compilation_error(self, error_message: str, module_content: str, testbench_content: str) -> str:
        """æ™ºèƒ½åˆ†æç¼–è¯‘é”™è¯¯å¹¶æä¾›ä¿®å¤å»ºè®®"""
        try:
            self.logger.info("ğŸ” å¼€å§‹æ™ºèƒ½é”™è¯¯åˆ†æ...")
            
            analysis_prompt = f"""
ä½œä¸ºVerilogä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹ç¼–è¯‘é”™è¯¯å¹¶æä¾›å…·ä½“çš„ä¿®å¤å»ºè®®ï¼š

## ç¼–è¯‘é”™è¯¯ä¿¡æ¯ï¼š
```
{error_message}
```

## æ¨¡å—ä»£ç ï¼š
```verilog
{module_content[:2000]}  // æˆªå–å‰2000å­—ç¬¦
```

## æµ‹è¯•å°ä»£ç ï¼š
```verilog  
{testbench_content[:1000]}  // æˆªå–å‰1000å­—ç¬¦
```

è¯·æä¾›ï¼š
1. **é”™è¯¯åŸå› åˆ†æ** - è¯¦ç»†è§£é‡Šä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ä¸ªé”™è¯¯
2. **å…·ä½“ä¿®å¤æ­¥éª¤** - æä¾›å‡†ç¡®çš„ä»£ç ä¿®æ”¹å»ºè®®
3. **ä¿®å¤åçš„ä»£ç ç‰‡æ®µ** - ç»™å‡ºä¿®æ­£åçš„å…³é”®ä»£ç æ®µ
4. **é¢„é˜²æªæ–½** - å¦‚ä½•é¿å…ç±»ä¼¼é”™è¯¯

è¯·ä½¿ç”¨ç»“æ„åŒ–æ ¼å¼å›ç­”ï¼Œé‡ç‚¹å…³æ³¨å®é™…å¯æ‰§è¡Œçš„ä¿®å¤æ–¹æ¡ˆã€‚
"""

            # è°ƒç”¨LLMè¿›è¡Œé”™è¯¯åˆ†æ
            response = await self.llm_client.send_prompt(
                prompt=analysis_prompt,
                system_prompt="ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„Verilog/SystemVerilogè®¾è®¡ä¸“å®¶ï¼Œæ“…é•¿å¿«é€Ÿè¯Šæ–­å’Œä¿®å¤ç¼–è¯‘é”™è¯¯ã€‚",
                temperature=0.1,  # ä½æ¸©åº¦ç¡®ä¿å‡†ç¡®æ€§
                max_tokens=1500
            )
            
            self.logger.info("âœ… é”™è¯¯åˆ†æå®Œæˆ")
            return response
            
        except Exception as e:
            self.logger.error(f"âŒ é”™è¯¯åˆ†æå¤±è´¥: {str(e)}")
            return f"è‡ªåŠ¨é”™è¯¯åˆ†æå¤±è´¥: {str(e)}ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥ç¼–è¯‘é”™è¯¯ä¿¡æ¯ã€‚"
    
    async def _analyze_test_results(self, file_path: str, simulation_result: Dict[str, Any],
                                  expected_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        analysis = {
            'test_cases': [],
            'failed_cases': [],
            'pass_rate': 0.0,
            'recommendations': []
        }
        
        if not simulation_result['success']:
            analysis['recommendations'].append({
                'type': 'critical',
                'description': 'ä»¿çœŸæœªèƒ½æˆåŠŸæ‰§è¡Œï¼Œéœ€è¦ä¿®å¤ç¼–è¯‘æˆ–è¿è¡Œæ—¶é”™è¯¯',
                'action': 'æ£€æŸ¥ä»£ç è¯­æ³•å’Œé€»è¾‘é”™è¯¯'
            })
            return analysis
        
        simulation_output = simulation_result.get('output', '')
        
        # ä½¿ç”¨LLMåˆ†æä»¿çœŸè¾“å‡º
        if self.llm_client and simulation_output.strip():
            try:
                analysis_result = await self._llm_analyze_simulation_output(
                    file_path, simulation_output, expected_results
                )
                analysis.update(analysis_result)
            except Exception as e:
                self.logger.warning(f"âš ï¸ LLMç»“æœåˆ†æå¤±è´¥: {str(e)}")
        
        # åŸºç¡€åˆ†æï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        if not analysis['test_cases']:
            analysis = self._basic_test_analysis(simulation_output, expected_results)
        
        return analysis
    
    async def _llm_analyze_simulation_output(self, file_path: str, simulation_output: str,
                                           expected_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ä½¿ç”¨LLMåˆ†æä»¿çœŸè¾“å‡º"""
        analysis_prompt = f"""
ä½œä¸ºéªŒè¯å·¥ç¨‹å¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹Verilogä»¿çœŸçš„è¾“å‡ºç»“æœã€‚

æ–‡ä»¶: {file_path}
ä»¿çœŸè¾“å‡º:
```
{simulation_output}
```

é¢„æœŸç»“æœï¼ˆå¦‚æœæœ‰ï¼‰:
{json.dumps(expected_results, indent=2, ensure_ascii=False) if expected_results else "æ— é¢„æœŸç»“æœæ•°æ®"}

è¯·åˆ†æï¼š
1. è¯†åˆ«æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹çš„æ‰§è¡Œæƒ…å†µ
2. ç¡®å®šå“ªäº›æµ‹è¯•ç”¨ä¾‹é€šè¿‡/å¤±è´¥
3. åˆ†æå¤±è´¥åŸå› 
4. æä¾›æ”¹è¿›å»ºè®®

è¿”å›JSONæ ¼å¼ï¼š
{{
    "test_cases": [
        {{
            "case_name": "æµ‹è¯•ç”¨ä¾‹åç§°",
            "status": "PASS|FAIL|ERROR",
            "description": "æµ‹è¯•ç”¨ä¾‹æè¿°",
            "expected": "é¢„æœŸç»“æœ",
            "actual": "å®é™…ç»“æœ",
            "failure_reason": "å¤±è´¥åŸå› ï¼ˆå¦‚æœå¤±è´¥ï¼‰"
        }}
    ],
    "failed_cases": [
        {{
            "case_name": "å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹å",
            "failure_type": "åŠŸèƒ½é”™è¯¯|æ—¶åºé”™è¯¯|é€»è¾‘é”™è¯¯",
            "description": "å¤±è´¥è¯¦æƒ…",
            "impact": "å½±å“è¯„ä¼°"
        }}
    ],
    "pass_rate": 0.85,
    "recommendations": [
        {{
            "type": "critical|important|suggestion",
            "description": "å»ºè®®æè¿°",
            "action": "å…·ä½“è¡ŒåŠ¨"
        }}
    ],
    "summary": "æ•´ä½“æµ‹è¯•ç»“æœæ€»ç»“"
}}
"""
        
        response = await self.llm_client.send_prompt(
            prompt=analysis_prompt,
            temperature=0.3,
            max_tokens=2000,
            json_mode=True
        )
        
        return json.loads(response)
    
    def _basic_test_analysis(self, simulation_output: str, 
                           expected_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åŸºç¡€æµ‹è¯•ç»“æœåˆ†æ"""
        lines = simulation_output.split('\n')
        
        # æŸ¥æ‰¾æµ‹è¯•ç›¸å…³çš„è¾“å‡º
        test_lines = [line for line in lines if any(keyword in line.lower() 
                     for keyword in ['test', 'pass', 'fail', 'error', 'case'])]
        
        analysis = {
            'test_cases': [],
            'failed_cases': [],
            'pass_rate': 0.0,
            'recommendations': []
        }
        
        # ç®€å•çš„é€šè¿‡/å¤±è´¥æ£€æµ‹
        pass_count = sum(1 for line in test_lines if 'pass' in line.lower())
        fail_count = sum(1 for line in test_lines if 'fail' in line.lower())
        error_count = sum(1 for line in test_lines if 'error' in line.lower())
        
        total_tests = max(pass_count + fail_count + error_count, 1)
        analysis['pass_rate'] = pass_count / total_tests
        
        # ç”ŸæˆåŸºæœ¬æµ‹è¯•ç”¨ä¾‹è®°å½•
        for i in range(pass_count):
            analysis['test_cases'].append({
                'case_name': f'Test Case {i+1}',
                'status': 'PASS',
                'description': 'åŸºç¡€åŠŸèƒ½æµ‹è¯•',
                'expected': 'Unknown',
                'actual': 'Unknown'
            })
        
        for i in range(fail_count):
            failed_case = {
                'case_name': f'Failed Case {i+1}',
                'failure_type': 'æœªçŸ¥é”™è¯¯',
                'description': 'æ£€æµ‹åˆ°æµ‹è¯•å¤±è´¥',
                'impact': 'éœ€è¦è¿›ä¸€æ­¥åˆ†æ'
            }
            analysis['failed_cases'].append(failed_case)
        
        # ç”Ÿæˆå»ºè®®
        if fail_count > 0:
            analysis['recommendations'].append({
                'type': 'critical',
                'description': f'å‘ç° {fail_count} ä¸ªå¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹',
                'action': 'æ£€æŸ¥ä»£ç é€»è¾‘å’Œæµ‹è¯•å°è®¾è®¡'
            })
        
        if error_count > 0:
            analysis['recommendations'].append({
                'type': 'critical',
                'description': f'å‘ç° {error_count} ä¸ªé”™è¯¯',
                'action': 'ä¿®å¤ä»£ç è¯­æ³•å’Œè¿è¡Œæ—¶é”™è¯¯'
            })
        
        if analysis['pass_rate'] < 0.8:
            analysis['recommendations'].append({
                'type': 'important',
                'description': f'æµ‹è¯•é€šè¿‡ç‡è¾ƒä½ ({analysis["pass_rate"]:.1%})',
                'action': 'éœ€è¦æ”¹è¿›ä»£ç è´¨é‡'
            })
        
        return analysis

    # ==================== å·¥å…·å®ç°æ–¹æ³• ====================
    
    async def _tool_write_code_file(self, filename: str, content: str, directory: str = "./output", **kwargs) -> Dict[str, Any]:
        """å·¥å…·ï¼šå°†ä»£ç å†™å…¥åˆ°æ–‡ä»¶"""
        try:
            self.logger.info(f"ğŸ”§ å·¥å…·è°ƒç”¨: å†™å…¥æ–‡ä»¶ {filename}")
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            output_dir = Path(directory)
            output_dir.mkdir(exist_ok=True)
            
            # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
            file_path = output_dir / filename
            
            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.logger.info(f"âœ… æ–‡ä»¶å†™å…¥æˆåŠŸ: {file_path}")
            
            return {
                "success": True,
                "file_path": str(file_path),
                "filename": filename,
                "directory": str(output_dir),
                "content_length": len(content),
                "message": f"æˆåŠŸå†™å…¥æ–‡ä»¶: {file_path}"
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æ–‡ä»¶å†™å…¥å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": f"æ–‡ä»¶å†™å…¥å¼‚å¸¸: {str(e)}",
                "file_path": None
            }
    
    async def _tool_generate_testbench(self, module_code: str = None, module_name: str = None, code: str = None, test_cases: List[Dict] = None, **kwargs) -> Dict[str, Any]:
        """å·¥å…·ï¼šç”Ÿæˆæµ‹è¯•å°"""
        try:
            self.logger.info("ğŸ”§ å·¥å…·è°ƒç”¨: ç”Ÿæˆæµ‹è¯•å°")
            
            # å¤„ç†å‚æ•°å…¼å®¹æ€§
            actual_module_code = module_code or code
            if actual_module_code is None:
                # ä»æ¨¡å—åç”Ÿæˆç¤ºä¾‹ä»£ç 
                if module_name:
                    actual_module_code = f"""
module {module_name}(
    input wire clk,
    input wire rst_n,
    output reg [7:0] data
);
always @(posedge clk or negedge rst_n) begin
    if (!rst_n) data <= 8'b0;
    else data <= data + 1;
end
endmodule
"""
                else:
                    return {
                        "success": False,
                        "error": "éœ€è¦æä¾›module_codeå‚æ•°",
                        "testbench_code": None
                    }
            
            # è§£ææ¨¡å—ä¿¡æ¯
            module_info = self._parse_module_info(actual_module_code)
            if not module_info:
                return {
                    "success": False,
                    "error": "æ— æ³•è§£ææ¨¡å—ä¿¡æ¯",
                    "testbench_code": None
                }
            
            # ç”Ÿæˆæµ‹è¯•å°ä»£ç 
            testbench_result = await self._generate_testbench(
                file_path="generated_module.v",
                code_content=actual_module_code,
                task_context="å·¥å…·è°ƒç”¨ç”Ÿæˆçš„æµ‹è¯•å°"
            )
            
            if testbench_result.get("success"):
                return {
                    "success": True,
                    "testbench_code": testbench_result.get("testbench_code"),
                    "testbench_file": testbench_result.get("testbench_file"),
                    "module_info": module_info,
                    "message": "æµ‹è¯•å°ç”ŸæˆæˆåŠŸ"
                }
            else:
                return {
                    "success": False,
                    "error": testbench_result.get("error", "æµ‹è¯•å°ç”Ÿæˆå¤±è´¥"),
                    "testbench_code": None
                }
                
        except Exception as e:
            self.logger.error(f"âŒ æµ‹è¯•å°ç”Ÿæˆå·¥å…·å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(e)}",
                "testbench_code": None
            }
    
    async def _tool_run_simulation(self, module_file: str = None, testbench_file: str = None, module_code: str = None, testbench_code: str = None, **kwargs) -> Dict[str, Any]:
        """å·¥å…·ï¼šè¿è¡Œä»¿çœŸ"""
        try:
            self.logger.info("ğŸ”§ å·¥å…·è°ƒç”¨: è¿è¡Œä»¿çœŸ")
            
            # ä¼˜å…ˆä½¿ç”¨æ–‡ä»¶è·¯å¾„
            if module_file and not module_code:
                # ä»æ–‡ä»¶è¯»å–æ¨¡å—ä»£ç 
                try:
                    module_path = Path(module_file)
                    if not module_path.is_absolute():
                        module_path = self.artifacts_dir / module_path
                    
                    with open(module_path, 'r', encoding='utf-8') as f:
                        module_code = f.read()
                    self.logger.info(f"ğŸ“„ ä»æ–‡ä»¶è¯»å–æ¨¡å—: {module_path}")
                    
                except FileNotFoundError:
                    # å°è¯•å…¶ä»–è·¯å¾„
                    alt_path = Path(module_file)
                    if alt_path.exists():
                        with open(alt_path, 'r', encoding='utf-8') as f:
                            module_code = f.read()
                    else:
                        return {
                            "success": False,
                            "error": f"æ¨¡å—æ–‡ä»¶ä¸å­˜åœ¨: {module_file}",
                            "simulation_output": None
                        }
            elif not module_code:
                return {
                    "success": False,
                    "error": "éœ€è¦æä¾›module_codeæˆ–module_fileå‚æ•°",
                    "simulation_output": None
                }
            
            if testbench_file and not testbench_code:
                # ä»æ–‡ä»¶è¯»å–æµ‹è¯•å°ä»£ç 
                try:
                    testbench_path = Path(testbench_file)
                    if not testbench_path.is_absolute():
                        testbench_path = self.artifacts_dir / testbench_path
                    
                    with open(testbench_path, 'r', encoding='utf-8') as f:
                        testbench_code = f.read()
                    self.logger.info(f"ğŸ“„ ä»æ–‡ä»¶è¯»å–æµ‹è¯•å°: {testbench_path}")
                    
                except FileNotFoundError:
                    # å°è¯•å…¶ä»–è·¯å¾„
                    alt_path = Path(testbench_file)
                    if alt_path.exists():
                        with open(alt_path, 'r', encoding='utf-8') as f:
                            testbench_code = f.read()
                    else:
                        return {
                            "success": False,
                            "error": f"æµ‹è¯•å°æ–‡ä»¶ä¸å­˜åœ¨: {testbench_file}",
                            "simulation_output": None
                        }
            elif not testbench_code:
                # å°è¯•è‡ªåŠ¨ç”ŸæˆåŸºç¡€æµ‹è¯•å°
                self.logger.info("ğŸ”§ æ²¡æœ‰æä¾›æµ‹è¯•å°ï¼Œå°è¯•è‡ªåŠ¨ç”ŸæˆåŸºç¡€æµ‹è¯•å°...")
                try:
                    if module_code:
                        # ç®€å•è§£ææ¨¡å—ä¿¡æ¯å¹¶ç”ŸæˆåŸºç¡€æµ‹è¯•å°
                        module_info = self._parse_module_info(module_code)
                        if module_info.get('module_name'):
                            testbench_code = f"""
module tb_{module_info['module_name']};
    // åŸºç¡€æµ‹è¯•å° - è‡ªåŠ¨ç”Ÿæˆ  
    initial begin
        $display("Starting simulation for {module_info['module_name']}");
        #10;
        $display("Simulation completed");
        $finish;
    end
endmodule
"""
                            self.logger.info("âœ… è‡ªåŠ¨ç”ŸæˆåŸºç¡€æµ‹è¯•å°")
                        else:
                            return {
                                "success": False,
                                "error": "æ— æ³•è§£ææ¨¡å—ä¿¡æ¯ï¼Œæ— æ³•è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•å°",
                                "simulation_output": None
                            }
                    else:
                        return {
                            "success": False,
                            "error": "æ— æ³•è¯»å–æ¨¡å—ä»£ç ï¼Œæ— æ³•è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•å°", 
                            "simulation_output": None
                        }
                except Exception as e:
                    return {
                        "success": False,
                        "error": f"è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•å°å¤±è´¥: {str(e)}",
                        "simulation_output": None
                    }
            
            # è¿è¡Œä»¿çœŸ
            simulation_result = await self._run_iverilog_simulation(
                module_file=module_file,
                module_code=module_code,
                testbench_code=testbench_code
            )
            
            if simulation_result.get("success"):
                return {
                    "success": True,
                    "simulation_output": simulation_result.get("output"),
                    "simulation_log": simulation_result.get("log"),
                    "execution_time": simulation_result.get("execution_time"),
                    "message": "ä»¿çœŸæ‰§è¡ŒæˆåŠŸ"
                }
            else:
                return {
                    "success": False,
                    "error": simulation_result.get("error", "ä»¿çœŸæ‰§è¡Œå¤±è´¥"),
                    "simulation_output": simulation_result.get("output"),
                    "simulation_log": simulation_result.get("log")
                }
                
        except Exception as e:
            self.logger.error(f"âŒ ä»¿çœŸæ‰§è¡Œå·¥å…·å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(e)}",
                "simulation_output": None
            }
    
    async def _tool_analyze_code_quality(self, code: str = None, module_code: str = None, **kwargs) -> Dict[str, Any]:
        """å·¥å…·ï¼šåˆ†æä»£ç è´¨é‡"""
        try:
            self.logger.info("ğŸ”§ å·¥å…·è°ƒç”¨: åˆ†æä»£ç è´¨é‡")
            
            # å¤„ç†å‚æ•°å…¼å®¹æ€§
            actual_code = code or module_code
            if actual_code is None:
                return {
                    "success": False,
                    "error": "éœ€è¦æä¾›codeæˆ–module_codeå‚æ•°",
                    "code_quality": None
                }
            
            # æ‰§è¡ŒåŸºç¡€ä»£ç å®¡æŸ¥
            review_result = self._basic_code_review(
                file_path="analyzed_code.v",
                code_content=actual_code
            )
            
            # æå–è´¨é‡æŒ‡æ ‡
            quality_metrics = review_result.get("quality_scores", {})
            
            # ç”Ÿæˆè¯¦ç»†åˆ†æ
            analysis = {
                "success": True,
                "code_quality": {
                    "syntax_score": quality_metrics.get("syntax_correctness", 0.0),
                    "design_score": quality_metrics.get("design_quality", 0.0),
                    "readability_score": quality_metrics.get("maintainability", 0.0), # Assuming readability maps to maintainability
                    "overall_score": quality_metrics.get("overall_score", 0.0)
                },
                "issues": review_result.get("critical_issues", []) + review_result.get("warnings", []), # Combine issues
                "suggestions": review_result.get("suggestions", []),
                "strengths": review_result.get("positive_aspects", []),
                "module_info": review_result.get("module_info", {}),
                "message": "ä»£ç è´¨é‡åˆ†æå®Œæˆ"
            }
            
            return analysis
            
        except Exception as e:
            self.logger.error(f"âŒ ä»£ç è´¨é‡åˆ†æå·¥å…·å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(e)}",
                "code_quality": None
            }
    
    async def _tool_write_build_script(self, verilog_files: List[str], testbench_files: List[str], 
                                     script_type: str = "bash", target_name: str = "simulation", **kwargs) -> Dict[str, Any]:
        """å·¥å…·ï¼šç”Ÿæˆæ„å»ºè„šæœ¬"""
        try:
            self.logger.info(f"ğŸ”§ å·¥å…·è°ƒç”¨: ç”Ÿæˆæ„å»ºè„šæœ¬ ({script_type})")
            
            if script_type.lower() in ["makefile", "make"]:
                # ç”ŸæˆMakefile
                script_content = self.script_manager.generate_makefile(
                    verilog_files=verilog_files,
                    testbench_files=testbench_files,
                    target_name=target_name
                )
                script_name = f"{target_name}.mk"
                result = self.script_manager.write_script(
                    script_name=script_name,
                    script_content=script_content,
                    script_type="make",
                    make_executable=False
                )
            else:
                # ç”ŸæˆBashè„šæœ¬
                script_content = self.script_manager.generate_build_script(
                    verilog_files=verilog_files,
                    testbench_files=testbench_files,
                    target_name=target_name
                )
                script_name = f"build_{target_name}.sh"
                result = self.script_manager.write_script(
                    script_name=script_name,
                    script_content=script_content,
                    script_type="bash",
                    make_executable=True
                )
            
            if result["success"]:
                self.logger.info(f"âœ… æ„å»ºè„šæœ¬ç”ŸæˆæˆåŠŸ: {result['script_path']}")
                return {
                    "success": True,
                    "script_path": result["script_path"],
                    "script_name": result["script_name"],
                    "script_type": script_type,
                    "verilog_files": verilog_files,
                    "testbench_files": testbench_files,
                    "message": f"æ„å»ºè„šæœ¬å·²ç”Ÿæˆ: {result['script_path']}"
                }
            else:
                return {
                    "success": False,
                    "error": result["error"],
                    "script_path": None
                }
                
        except Exception as e:
            self.logger.error(f"âŒ æ„å»ºè„šæœ¬ç”Ÿæˆå·¥å…·å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(e)}",
                "script_path": None
            }
    
    async def _tool_execute_build_script(self, script_name: str, action: str = "all", 
                                       arguments: List[str] = None, **kwargs) -> Dict[str, Any]:
        """å·¥å…·ï¼šæ‰§è¡Œæ„å»ºè„šæœ¬"""
        try:
            self.logger.info(f"ğŸ”§ å·¥å…·è°ƒç”¨: æ‰§è¡Œæ„å»ºè„šæœ¬ ({script_name})")
            
            # å‡†å¤‡å‚æ•°
            script_args = []
            if action and action != "all":
                script_args.append(action)
            
            if arguments:
                script_args.extend(arguments)
            
            # æ‰§è¡Œè„šæœ¬
            result = self.script_manager.execute_script(
                script_path=script_name,
                arguments=script_args,
                working_directory=str(self.artifacts_dir),
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )
            
            if result["success"]:
                self.logger.info(f"âœ… è„šæœ¬æ‰§è¡ŒæˆåŠŸ: {script_name}")
                return {
                    "success": True,
                    "return_code": result["return_code"],
                    "stdout": result["stdout"],
                    "stderr": result["stderr"],
                    "script_path": result["script_path"],
                    "command": result["command"],
                    "action": action,
                    "message": f"è„šæœ¬æ‰§è¡ŒæˆåŠŸ: {script_name}"
                }
            else:
                self.logger.warning(f"âš ï¸ è„šæœ¬æ‰§è¡Œå¤±è´¥: {script_name}")
                error_details = result.get("error_details", {})
                
                # æ„å»ºè¯¦ç»†çš„é”™è¯¯æŠ¥å‘Š
                error_report = f"è„šæœ¬æ‰§è¡Œå¤±è´¥: {script_name}\n"
                error_report += f"å‘½ä»¤: {result.get('command', 'N/A')}\n"
                error_report += f"è¿”å›ç : {result.get('return_code', -1)}\n"
                
                if result.get("stderr"):
                    error_report += f"é”™è¯¯è¾“å‡º: {result['stderr']}\n"
                if result.get("stdout"):
                    error_report += f"æ ‡å‡†è¾“å‡º: {result['stdout']}\n"
                
                if error_details:
                    error_report += f"å·¥ä½œç›®å½•: {error_details.get('working_dir', 'N/A')}\n"
                    error_report += f"è„šæœ¬å­˜åœ¨: {error_details.get('script_exists', False)}\n"
                    if not error_details.get('script_exists', True):
                        error_report += "å»ºè®®: æ£€æŸ¥è„šæœ¬è·¯å¾„æˆ–é‡æ–°ç”Ÿæˆè„šæœ¬\n"
                
                return {
                    "success": False,
                    "return_code": result.get("return_code", -1),
                    "stdout": result.get("stdout", ""),
                    "stderr": result.get("stderr", ""),
                    "error": result.get("error", "è„šæœ¬æ‰§è¡Œå¤±è´¥"),
                    "error_report": error_report,
                    "error_details": error_details,
                    "script_path": result.get("script_path", script_name),
                    "command": result.get("command", ""),
                    "action": action,
                    "message": f"è„šæœ¬æ‰§è¡Œå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                    "suggestion": self._generate_error_fix_suggestion(result, script_name)
                }
                
        except Exception as e:
            self.logger.error(f"âŒ è„šæœ¬æ‰§è¡Œå·¥å…·å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(e)}",
                "return_code": -1,
                "stdout": "",
                "stderr": ""
            }
    
    def _generate_error_fix_suggestion(self, result: Dict[str, Any], script_name: str) -> str:
        """ç”Ÿæˆé”™è¯¯ä¿®å¤å»ºè®®"""
        error_msg = result.get("error", "")
        stderr = result.get("stderr", "")
        return_code = result.get("return_code", -1)
        
        suggestions = []
        
        # åŸºäºè¿”å›ç çš„å»ºè®®
        if return_code == 127:
            suggestions.append("å‘½ä»¤æœªæ‰¾åˆ° - æ£€æŸ¥è„šæœ¬è·¯å¾„å’Œå¯æ‰§è¡Œæƒé™")
        elif return_code == 126:
            suggestions.append("æƒé™æ‹’ç» - æ£€æŸ¥è„šæœ¬æ‰§è¡Œæƒé™")
        elif return_code == 2:
            suggestions.append("æ–‡ä»¶æˆ–ç›®å½•ä¸å­˜åœ¨ - æ£€æŸ¥æ–‡ä»¶è·¯å¾„")
        
        # åŸºäºé”™è¯¯ä¿¡æ¯çš„å»ºè®®
        if "No such file or directory" in stderr or "No such file or directory" in error_msg:
            suggestions.append("æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿æ‰€æœ‰ä¾èµ–æ–‡ä»¶å­˜åœ¨")
        
        if "Permission denied" in stderr:
            suggestions.append("æ£€æŸ¥æ–‡ä»¶æƒé™ï¼Œå¯èƒ½éœ€è¦æ‰§è¡Œ chmod +x script_name")
        
        if "iverilog" in stderr and ("not found" in stderr or "command not found" in stderr):
            suggestions.append("iverilogæœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­ï¼Œè¯·å®‰è£…Icarus Verilog")
        
        if "syntax error" in stderr.lower():
            suggestions.append("Verilogè¯­æ³•é”™è¯¯ - æ£€æŸ¥æºä»£ç è¯­æ³•")
        
        if "undeclared" in stderr.lower():
            suggestions.append("æœªå£°æ˜çš„ä¿¡å·æˆ–å˜é‡ - æ£€æŸ¥ä¿¡å·å£°æ˜")
        
        if not suggestions:
            suggestions.append("æ£€æŸ¥è„šæœ¬å†…å®¹å’Œæ‰§è¡Œç¯å¢ƒï¼ŒæŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        
        return "; ".join(suggestions)