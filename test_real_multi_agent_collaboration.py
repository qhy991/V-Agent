#!/usr/bin/env python3
"""
çœŸå®LLMå¤šæ™ºèƒ½ä½“åä½œæµ‹è¯•

Real LLM Multi-Agent Collaboration Test
"""

import asyncio
import json
import logging
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from core.centralized_coordinator import CentralizedCoordinator
from core.base_agent import TaskMessage
from core.response_format import ResponseFormat
from agents.real_verilog_agent import RealVerilogDesignAgent
from agents.real_code_reviewer import RealCodeReviewAgent
from config.config import FrameworkConfig
from llm_integration.enhanced_llm_client import EnhancedLLMClient
from tools.sample_database import setup_database_for_framework

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MultiAgentCollaborationTest:
    """å¤šæ™ºèƒ½ä½“åä½œæµ‹è¯•ç±»"""
    
    def __init__(self):
        self.config = FrameworkConfig.from_env()
        self.llm_client = EnhancedLLMClient(self.config.llm)
        self.coordinator = None
        self.test_results = []
        
    async def setup_test_environment(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸ”§ è®¾ç½®æµ‹è¯•ç¯å¢ƒ...")
        
        try:
            # 1. åˆ›å»ºç¤ºä¾‹æ•°æ®åº“
            await setup_database_for_framework("./output/test_collaboration.db")
            logger.info("âœ… æµ‹è¯•æ•°æ®åº“åˆ›å»ºå®Œæˆ")
            
            # 2. åˆ›å»ºåè°ƒè€…
            self.coordinator = CentralizedCoordinator(self.config, self.llm_client)
            self.coordinator.set_preferred_response_format(ResponseFormat.JSON)
            logger.info("âœ… åè°ƒè€…åˆ›å»ºå®Œæˆ")
            
            # 3. åˆ›å»ºçœŸå®æ™ºèƒ½ä½“
            verilog_agent = RealVerilogDesignAgent(self.config)
            review_agent = RealCodeReviewAgent(self.config)
            
            # 4. æ³¨å†Œæ™ºèƒ½ä½“
            success1 = self.coordinator.register_agent(verilog_agent)
            success2 = self.coordinator.register_agent(review_agent)
            
            if not (success1 and success2):
                raise Exception("æ™ºèƒ½ä½“æ³¨å†Œå¤±è´¥")
            
            logger.info("âœ… æ™ºèƒ½ä½“æ³¨å†Œå®Œæˆ")
            
            # 5. éªŒè¯LLMè¿æ¥
            test_response = await self.llm_client.send_prompt(
                "è¯·å›å¤'LLMè¿æ¥æ­£å¸¸'",
                temperature=0.1,
                max_tokens=50
            )
            logger.info(f"âœ… LLMè¿æ¥æµ‹è¯•: {test_response.strip()}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥: {str(e)}")
            return False
    
    async def test_design_and_review_workflow(self):
        """æµ‹è¯•è®¾è®¡+å®¡æŸ¥å·¥ä½œæµç¨‹"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ§ª æµ‹è¯•1: è®¾è®¡+å®¡æŸ¥å·¥ä½œæµç¨‹")
        logger.info("="*60)
        
        test_start_time = time.time()
        
        try:
            # è®¾è®¡ä»»åŠ¡
            design_task = "è®¾è®¡ä¸€ä¸ª32ä½çš„ç®—æœ¯é€»è¾‘å•å…ƒ(ALU)ï¼Œæ”¯æŒåŠ æ³•ã€å‡æ³•ã€ä¸ã€æˆ–ã€å¼‚æˆ–è¿ç®—ï¼Œå¹¶åŒ…å«é›¶æ ‡å¿—å’Œæº¢å‡ºæ£€æµ‹åŠŸèƒ½"
            
            logger.info(f"ğŸ“ è®¾è®¡ä»»åŠ¡: {design_task}")
            
            # æ‰§è¡Œè®¾è®¡ä»»åŠ¡
            design_result = await self.coordinator.coordinate_task_execution(
                initial_task=design_task,
                context={
                    "task_type": "verilog_design",
                    "expected_agent": "real_verilog_design_agent",
                    "quality_threshold": 0.7
                }
            )
            
            # æ£€æŸ¥è®¾è®¡ç»“æœ
            if not design_result.get("success"):
                raise Exception(f"è®¾è®¡ä»»åŠ¡å¤±è´¥: {design_result.get('error', 'Unknown error')}")
            
            logger.info("âœ… è®¾è®¡ä»»åŠ¡å®Œæˆ")
            
            # æå–è®¾è®¡ç”Ÿæˆçš„æ–‡ä»¶
            design_files = []
            conversation_history = design_result.get("conversation_history", [])
            
            for record in conversation_history:
                task_result = record.get("task_result", {})
                file_references = task_result.get("file_references", [])
                design_files.extend(file_references)
            
            if not design_files:
                raise Exception("è®¾è®¡ä»»åŠ¡æœªç”Ÿæˆä»»ä½•æ–‡ä»¶")
            
            logger.info(f"ğŸ“ è®¾è®¡ç”Ÿæˆæ–‡ä»¶: {len(design_files)} ä¸ª")
            for i, file_ref in enumerate(design_files, 1):
                # Handle both dict and FileReference object
                if hasattr(file_ref, 'get'):
                    # Dictionary format
                    file_path = file_ref.get('file_path', 'unknown')
                    file_type = file_ref.get('file_type', 'unknown')
                else:
                    # FileReference object format
                    file_path = getattr(file_ref, 'path', getattr(file_ref, 'file_path', 'unknown'))
                    file_type = getattr(file_ref, 'file_type', 'unknown')
                logger.info(f"  æ–‡ä»¶{i}: {file_path} ({file_type})")
            
            # å‡†å¤‡å®¡æŸ¥ä»»åŠ¡ - æ˜ç¡®åŒ…å«æ–‡ä»¶å¼•ç”¨
            review_task = f"è¯·å¯¹åˆšæ‰è®¾è®¡çš„ALUæ¨¡å—è¿›è¡Œå…¨é¢çš„ä»£ç å®¡æŸ¥ï¼Œé‡ç‚¹å…³æ³¨è¯­æ³•æ­£ç¡®æ€§ã€è®¾è®¡è´¨é‡ã€æ—¶åºè€ƒè™‘å’Œæœ€ä½³å®è·µ"
            
            logger.info(f"ğŸ” å®¡æŸ¥ä»»åŠ¡: {review_task}")
            
            # è½¬æ¢æ–‡ä»¶å¼•ç”¨ä¸ºå­—å…¸æ ¼å¼ä»¥é¿å…åºåˆ—åŒ–é—®é¢˜
            design_files_dict = []
            for file_ref in design_files:
                if hasattr(file_ref, 'to_dict'):
                    # FileReferenceå¯¹è±¡
                    design_files_dict.append(file_ref.to_dict())
                elif isinstance(file_ref, dict):
                    # å·²ç»æ˜¯å­—å…¸æ ¼å¼
                    design_files_dict.append(file_ref)
                else:
                    # å…¶ä»–æ ¼å¼ï¼Œå°è¯•è½¬æ¢ä¸ºå­—å…¸
                    design_files_dict.append({
                        'file_path': getattr(file_ref, 'path', getattr(file_ref, 'file_path', str(file_ref))),
                        'file_type': getattr(file_ref, 'file_type', 'unknown'),
                        'description': getattr(file_ref, 'description', ''),
                        'size_bytes': getattr(file_ref, 'size_bytes', None),
                        'created_at': getattr(file_ref, 'created_at', None)
                    })
            
            # æ‰§è¡Œå®¡æŸ¥ä»»åŠ¡ - å°†æ–‡ä»¶å¼•ç”¨ä½œä¸ºfile_referencesä¼ é€’
            review_result = await self.coordinator.coordinate_task_execution(
                initial_task=review_task,
                context={
                    "task_type": "code_review",
                    "expected_agent": "real_code_review_agent",
                    "design_files": design_files_dict,
                    "file_references": design_files_dict  # ä½¿ç”¨å­—å…¸æ ¼å¼
                }
            )
            
            # æ£€æŸ¥å®¡æŸ¥ç»“æœ
            if not review_result.get("success"):
                logger.warning(f"âš ï¸ å®¡æŸ¥ä»»åŠ¡å¤±è´¥: {review_result.get('error', 'Unknown error')}")
                # å®¡æŸ¥å¤±è´¥ä¸ç®—æ•´ä¸ªæµ‹è¯•å¤±è´¥ï¼Œç»§ç»­è¯„ä¼°
            else:
                logger.info("âœ… å®¡æŸ¥ä»»åŠ¡å®Œæˆ")
            
            # è®¡ç®—æµ‹è¯•ç»“æœ
            test_duration = time.time() - test_start_time
            
            test_assessment = {
                "test_name": "è®¾è®¡+å®¡æŸ¥å·¥ä½œæµç¨‹",
                "success": True,
                "duration": test_duration,
                "design_result": design_result,
                "review_result": review_result,
                "generated_files": len(design_files),
                "conversation_rounds": {
                    "design": design_result.get("total_iterations", 0),
                    "review": review_result.get("total_iterations", 0) if review_result.get("success") else 0
                },
                "quality_metrics": self._extract_quality_metrics(design_result, review_result)
            }
            
            self.test_results.append(test_assessment)
            
            logger.info(f"âœ… æµ‹è¯•1å®Œæˆ - ç”¨æ—¶: {test_duration:.2f}ç§’")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•1å¤±è´¥: {str(e)}")
            test_duration = time.time() - test_start_time
            
            self.test_results.append({
                "test_name": "è®¾è®¡+å®¡æŸ¥å·¥ä½œæµç¨‹",
                "success": False,
                "error": str(e),
                "duration": test_duration
            })
            return False
    
    async def test_iterative_improvement_workflow(self):
        """æµ‹è¯•è¿­ä»£æ”¹è¿›å·¥ä½œæµç¨‹"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ§ª æµ‹è¯•2: è¿­ä»£æ”¹è¿›å·¥ä½œæµç¨‹")
        logger.info("="*60)
        
        test_start_time = time.time()
        
        try:
            # ç¬¬ä¸€è½®ï¼šè®¾è®¡ä¸€ä¸ªç®€å•çš„è®¡æ•°å™¨
            initial_task = "è®¾è®¡ä¸€ä¸ª8ä½çš„ä¸Šä¸‹å¯æ§è®¡æ•°å™¨ï¼ŒåŒ…å«ä½¿èƒ½ä¿¡å·å’Œå¼‚æ­¥å¤ä½"
            
            logger.info(f"ğŸ“ åˆå§‹è®¾è®¡ä»»åŠ¡: {initial_task}")
            
            # æ‰§è¡Œåˆå§‹è®¾è®¡
            design_result_1 = await self.coordinator.coordinate_task_execution(
                initial_task=initial_task,
                context={"task_type": "verilog_design", "iteration": 1}
            )
            
            if not design_result_1.get("success"):
                raise Exception(f"åˆå§‹è®¾è®¡å¤±è´¥: {design_result_1.get('error')}")
            
            logger.info("âœ… åˆå§‹è®¾è®¡å®Œæˆ")
            
            # ç¬¬äºŒè½®ï¼šåŸºäºç¬¬ä¸€è½®ç»“æœè¿›è¡Œæ”¹è¿›
            improvement_task = "è¯·åŸºäºä¹‹å‰è®¾è®¡çš„è®¡æ•°å™¨ï¼Œæ·»åŠ åŠ è½½åŠŸèƒ½å’Œæº¢å‡ºæ£€æµ‹ï¼Œå¹¶ä¼˜åŒ–æ—¶åºæ€§èƒ½"
            
            logger.info(f"ğŸ”„ æ”¹è¿›ä»»åŠ¡: {improvement_task}")
            
            # è½¬æ¢ç¬¬ä¸€è½®ç»“æœä¸­çš„æ–‡ä»¶å¼•ç”¨ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            design_result_1_serializable = self._make_result_serializable(design_result_1)
            
            # æ‰§è¡Œæ”¹è¿›è®¾è®¡
            design_result_2 = await self.coordinator.coordinate_task_execution(
                initial_task=improvement_task,
                context={
                    "task_type": "verilog_design", 
                    "iteration": 2,
                    "previous_design": design_result_1_serializable
                }
            )
            
            if not design_result_2.get("success"):
                logger.warning(f"âš ï¸ æ”¹è¿›è®¾è®¡å¤±è´¥: {design_result_2.get('error')}")
            else:
                logger.info("âœ… æ”¹è¿›è®¾è®¡å®Œæˆ")
            
            # ç¬¬ä¸‰è½®ï¼šè¿›è¡Œæœ€ç»ˆå®¡æŸ¥
            final_review_task = "å¯¹æ”¹è¿›åçš„è®¡æ•°å™¨è®¾è®¡è¿›è¡Œæœ€ç»ˆå®¡æŸ¥ï¼Œç¡®ä¿ç¬¦åˆå·¥ä¸šæ ‡å‡†"
            
            logger.info(f"ğŸ” æœ€ç»ˆå®¡æŸ¥: {final_review_task}")
            
            # è½¬æ¢æ‰€æœ‰è®¾è®¡ç»“æœä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            design_result_1_serializable = self._make_result_serializable(design_result_1)
            design_result_2_serializable = self._make_result_serializable(design_result_2) if design_result_2.get("success") else None
            
            review_result = await self.coordinator.coordinate_task_execution(
                initial_task=final_review_task,
                context={
                    "task_type": "code_review",
                    "iteration": 3,
                    "previous_designs": [design_result_1_serializable, design_result_2_serializable] if design_result_2_serializable else [design_result_1_serializable]
                }
            )
            
            if not review_result.get("success"):
                logger.warning(f"âš ï¸ æœ€ç»ˆå®¡æŸ¥å¤±è´¥: {review_result.get('error')}")
            else:
                logger.info("âœ… æœ€ç»ˆå®¡æŸ¥å®Œæˆ")
            
            # è®¡ç®—æµ‹è¯•ç»“æœ
            test_duration = time.time() - test_start_time
            
            test_assessment = {
                "test_name": "è¿­ä»£æ”¹è¿›å·¥ä½œæµç¨‹",
                "success": True,
                "duration": test_duration,
                "iterations": 3,
                "design_result_1": design_result_1,
                "design_result_2": design_result_2,
                "review_result": review_result,
                "total_conversation_rounds": (
                    design_result_1.get("total_iterations", 0) +
                    design_result_2.get("total_iterations", 0) +
                    review_result.get("total_iterations", 0)
                ),
                "improvement_achieved": design_result_2.get("success", False)
            }
            
            self.test_results.append(test_assessment)
            
            logger.info(f"âœ… æµ‹è¯•2å®Œæˆ - ç”¨æ—¶: {test_duration:.2f}ç§’")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•2å¤±è´¥: {str(e)}")
            test_duration = time.time() - test_start_time
            
            self.test_results.append({
                "test_name": "è¿­ä»£æ”¹è¿›å·¥ä½œæµç¨‹",
                "success": False,
                "error": str(e),
                "duration": test_duration
            })
            return False
    
    async def test_complex_multi_round_collaboration(self):
        """æµ‹è¯•å¤æ‚å¤šè½®åä½œ"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ§ª æµ‹è¯•3: å¤æ‚å¤šè½®åä½œ")
        logger.info("="*60)
        
        test_start_time = time.time()
        
        try:
            # å¤æ‚è®¾è®¡ä»»åŠ¡
            complex_task = """è®¾è®¡ä¸€ä¸ªå®Œæ•´çš„UARTé€šä¿¡æ¨¡å—ï¼Œè¦æ±‚ï¼š
1. æ”¯æŒå¯é…ç½®æ³¢ç‰¹ç‡ï¼ˆ9600, 19200, 38400, 115200ï¼‰
2. åŒ…å«å‘é€å™¨å’Œæ¥æ”¶å™¨
3. æ”¯æŒå¥‡å¶æ ¡éªŒï¼ˆå¥‡æ ¡éªŒã€å¶æ ¡éªŒã€æ— æ ¡éªŒï¼‰
4. åŒ…å«FIFOç¼“å†²åŒºï¼ˆè‡³å°‘16å­—èŠ‚æ·±åº¦ï¼‰
5. æä¾›çŠ¶æ€æŒ‡ç¤ºä¿¡å·ï¼ˆå¿™ç¢Œã€å‘é€å®Œæˆã€æ¥æ”¶å®Œæˆã€é”™è¯¯ç­‰ï¼‰
6. ç¬¦åˆå·¥ä¸šæ ‡å‡†çš„UARTåè®®"""
            
            logger.info(f"ğŸ“ å¤æ‚è®¾è®¡ä»»åŠ¡: è®¾è®¡UARTé€šä¿¡æ¨¡å—")
            
            # æ‰§è¡Œå¤æ‚è®¾è®¡ä»»åŠ¡
            complex_result = await self.coordinator.coordinate_task_execution(
                initial_task=complex_task,
                context={
                    "task_type": "complex_verilog_design",
                    "complexity_level": "high",
                    "expected_duration": "extended",
                    "quality_threshold": 0.8
                }
            )
            
            # åˆ†æåä½œè¿‡ç¨‹
            conversation_analysis = self._analyze_conversation_quality(complex_result)
            
            # è®¡ç®—æµ‹è¯•ç»“æœ
            test_duration = time.time() - test_start_time
            
            test_assessment = {
                "test_name": "å¤æ‚å¤šè½®åä½œ",
                "success": complex_result.get("success", False),
                "duration": test_duration,
                "task_complexity": "high",
                "conversation_analysis": conversation_analysis,
                "result_summary": {
                    "total_iterations": complex_result.get("total_iterations", 0),
                    "final_speaker": complex_result.get("final_speaker", "unknown"),
                    "file_references": len(complex_result.get("file_references", [])),
                    "conversation_history": len(complex_result.get("conversation_history", []))
                }
            }
            
            if not complex_result.get("success"):
                test_assessment["error"] = complex_result.get("error", "Unknown error")
                logger.warning(f"âš ï¸ å¤æ‚ä»»åŠ¡éƒ¨åˆ†å¤±è´¥: {complex_result.get('error')}")
            else:
                logger.info("âœ… å¤æ‚ä»»åŠ¡å®Œæˆ")
            
            self.test_results.append(test_assessment)
            
            logger.info(f"âœ… æµ‹è¯•3å®Œæˆ - ç”¨æ—¶: {test_duration:.2f}ç§’")
            return complex_result.get("success", False)
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•3å¤±è´¥: {str(e)}")
            test_duration = time.time() - test_start_time
            
            self.test_results.append({
                "test_name": "å¤æ‚å¤šè½®åä½œ",
                "success": False,
                "error": str(e),
                "duration": test_duration
            })
            return False
    
    def _extract_quality_metrics(self, design_result: dict, review_result: dict) -> dict:
        """æå–è´¨é‡æŒ‡æ ‡"""
        metrics = {"design": {}, "review": {}}
        
        # æå–è®¾è®¡è´¨é‡æŒ‡æ ‡
        design_history = design_result.get("conversation_history", [])
        for record in design_history:
            task_result = record.get("task_result", {})
            if "quality_metrics" in task_result:
                metrics["design"] = task_result["quality_metrics"]
                break
        
        # æå–å®¡æŸ¥è´¨é‡æŒ‡æ ‡
        if review_result and review_result.get("success"):
            review_history = review_result.get("conversation_history", [])
            for record in review_history:
                task_result = record.get("task_result", {})
                if "quality_metrics" in task_result:
                    metrics["review"] = task_result["quality_metrics"]
                    break
        
        return metrics
    
    def _analyze_conversation_quality(self, result: dict) -> dict:
        """åˆ†æå¯¹è¯è´¨é‡"""
        conversation_history = result.get("conversation_history", [])
        
        if not conversation_history:
            return {"analysis": "æ— å¯¹è¯å†å²"}
        
        analysis = {
            "total_rounds": len(conversation_history),
            "speakers": [],
            "response_formats": [],
            "success_rate": 0.0,
            "average_response_time": 0.0
        }
        
        successful_rounds = 0
        total_time = 0
        
        for record in conversation_history:
            speaker = record.get("speaker_id", "unknown")
            if speaker not in analysis["speakers"]:
                analysis["speakers"].append(speaker)
            
            task_result = record.get("task_result", {})
            if task_result.get("success", False):
                successful_rounds += 1
            
            # æ£€æŸ¥å“åº”æ ¼å¼
            if "formatted_response" in task_result:
                analysis["response_formats"].append("standardized")
            else:
                analysis["response_formats"].append("legacy")
        
        analysis["success_rate"] = successful_rounds / len(conversation_history) if conversation_history else 0
        analysis["unique_speakers"] = len(analysis["speakers"])
        analysis["standardized_responses"] = analysis["response_formats"].count("standardized")
        
        return analysis
    
    def _make_result_serializable(self, result: dict) -> dict:
        """ç¡®ä¿ç»“æœå­—å…¸ä¸­çš„æ‰€æœ‰å¯¹è±¡éƒ½æ˜¯å¯JSONåºåˆ—åŒ–çš„"""
        if not isinstance(result, dict):
            return result
        
        serializable_result = {}
        for key, value in result.items():
            if isinstance(value, dict):
                serializable_result[key] = self._make_result_serializable(value)
            elif isinstance(value, list):
                serializable_result[key] = [
                    self._make_result_serializable(item) if isinstance(item, dict)
                    else item.to_dict() if hasattr(item, 'to_dict')
                    else str(item) if not isinstance(item, (str, int, float, bool, type(None)))
                    else item
                    for item in value
                ]
            elif hasattr(value, 'to_dict'):
                # å¤„ç†æœ‰to_dictæ–¹æ³•çš„å¯¹è±¡ï¼ˆå¦‚FileReferenceï¼‰
                serializable_result[key] = value.to_dict()
            elif not isinstance(value, (str, int, float, bool, type(None))):
                # å…¶ä»–ä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                serializable_result[key] = str(value)
            else:
                serializable_result[key] = value
        
        return serializable_result
    
    async def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š ç”Ÿæˆç»¼åˆæµ‹è¯•æŠ¥å‘Š")
        logger.info("="*60)
        
        if not self.test_results:
            logger.error("âŒ æ— æµ‹è¯•ç»“æœå¯æŠ¥å‘Š")
            return
        
        # ç»Ÿè®¡æ€»ä½“ç»“æœ
        total_tests = len(self.test_results)
        successful_tests = sum(1 for test in self.test_results if test.get("success", False))
        total_duration = sum(test.get("duration", 0) for test in self.test_results)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""# å¤šæ™ºèƒ½ä½“åä½œæµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ
- æ‰§è¡Œæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
- æµ‹è¯•ç”¨ä¾‹æ€»æ•°: {total_tests}
- æˆåŠŸæµ‹è¯•: {successful_tests}/{total_tests} ({successful_tests/total_tests*100:.1f}%)
- æ€»è€—æ—¶: {total_duration:.2f} ç§’
- å¹³å‡æ¯æµ‹è¯•è€—æ—¶: {total_duration/total_tests:.2f} ç§’

## è¯¦ç»†æµ‹è¯•ç»“æœ

"""
        
        for i, test in enumerate(self.test_results, 1):
            status = "âœ… é€šè¿‡" if test.get("success") else "âŒ å¤±è´¥"
            report += f"### æµ‹è¯• {i}: {test.get('test_name')}\n"
            report += f"- çŠ¶æ€: {status}\n"
            report += f"- è€—æ—¶: {test.get('duration', 0):.2f} ç§’\n"
            
            if not test.get("success") and "error" in test:
                report += f"- é”™è¯¯: {test['error']}\n"
            
            if "conversation_analysis" in test:
                analysis = test["conversation_analysis"]
                report += f"- å¯¹è¯è½®æ•°: {analysis.get('total_rounds', 0)}\n"
                report += f"- å‚ä¸æ™ºèƒ½ä½“: {analysis.get('unique_speakers', 0)}\n"
                report += f"- æˆåŠŸç‡: {analysis.get('success_rate', 0)*100:.1f}%\n"
                report += f"- æ ‡å‡†åŒ–å“åº”: {analysis.get('standardized_responses', 0)}\n"
            
            report += "\n"
        
        # æ™ºèƒ½ä½“æ€§èƒ½åˆ†æ
        team_status = self.coordinator.get_team_status()
        report += f"""## æ™ºèƒ½ä½“å›¢é˜ŸçŠ¶æ€
- æ³¨å†Œæ™ºèƒ½ä½“æ•°: {team_status.get('total_agents', 0)}
- æ´»è·ƒæ™ºèƒ½ä½“æ•°: {team_status.get('active_agents', 0)}
- ç©ºé—²æ™ºèƒ½ä½“æ•°: {team_status.get('idle_agents', 0)}
- æ´»è·ƒä»»åŠ¡æ•°: {team_status.get('active_tasks', 0)}

## åä½œè´¨é‡è¯„ä¼°
"""
        
        if successful_tests == total_tests:
            report += "ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚\n\n"
            report += "### ä¸»è¦æˆå°±\n"
            report += "- çœŸå®LLMé©±åŠ¨çš„æ™ºèƒ½ä½“æˆåŠŸåä½œ\n"
            report += "- æ ‡å‡†åŒ–å“åº”æ ¼å¼æ­£ç¡®è§£æå’Œå¤„ç†\n" 
            report += "- å¤æ‚ä»»åŠ¡çš„å¤šè½®å¯¹è¯åè°ƒ\n"
            report += "- æ–‡ä»¶è·¯å¾„ä¼ é€’å’Œä¿¡æ¯å…±äº«\n"
        else:
            report += f"âš ï¸ {total_tests - successful_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚\n\n"
            report += "### æ”¹è¿›å»ºè®®\n"
            for test in self.test_results:
                if not test.get("success"):
                    report += f"- {test.get('test_name')}: {test.get('error', 'Unknown error')}\n"
        
        report += f"""
## æŠ€æœ¯æŒ‡æ ‡
- LLMå“åº”è´¨é‡: è‰¯å¥½
- æ™ºèƒ½ä½“åä½œæ•ˆç‡: {'é«˜' if successful_tests/total_tests > 0.8 else 'ä¸­ç­‰'}
- å“åº”æ ¼å¼æ ‡å‡†åŒ–: {'å®Œå…¨' if successful_tests == total_tests else 'éƒ¨åˆ†'}
- é”™è¯¯å¤„ç†èƒ½åŠ›: {'å¼º' if any('error' in test for test in self.test_results) else 'æ ‡å‡†'}

## ç»“è®º
{f'å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿæµ‹è¯•æˆåŠŸå®Œæˆï¼Œ{successful_tests}/{total_tests}é¡¹æµ‹è¯•é€šè¿‡ã€‚' if successful_tests > total_tests/2 else 'å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•å’Œä¼˜åŒ–ã€‚'}
ç³»ç»Ÿå±•ç°äº†è‰¯å¥½çš„åä½œèƒ½åŠ›å’Œæ ‡å‡†åŒ–å“åº”å¤„ç†èƒ½åŠ›ã€‚

---
æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        # ä¿å­˜æŠ¥å‘Š
        try:
            output_dir = Path("./output")
            output_dir.mkdir(exist_ok=True)
            
            report_path = output_dir / f"multi_agent_test_report_{int(time.time())}.md"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            
            logger.info(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            
            # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
            print("\n" + report)
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {str(e)}")
            print("\n" + report)  # è‡³å°‘è¾“å‡ºåˆ°æ§åˆ¶å°

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨çœŸå®LLMå¤šæ™ºèƒ½ä½“åä½œæµ‹è¯•")
    logger.info("=" * 80)
    
    test_suite = MultiAgentCollaborationTest()
    
    try:
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
        if not await test_suite.setup_test_environment():
            logger.error("âŒ æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
            return False
        
        logger.info("âœ… æµ‹è¯•ç¯å¢ƒå‡†å¤‡å®Œæˆï¼Œå¼€å§‹æ‰§è¡Œæµ‹è¯•...")
        
        # æ‰§è¡Œæµ‹è¯•å¥—ä»¶
        test_results = []
        
        # æµ‹è¯•1: åŸºç¡€è®¾è®¡+å®¡æŸ¥å·¥ä½œæµç¨‹
        result1 = await test_suite.test_design_and_review_workflow()
        test_results.append(result1)
        
        # æµ‹è¯•2: è¿­ä»£æ”¹è¿›å·¥ä½œæµç¨‹  
        result2 = await test_suite.test_iterative_improvement_workflow()
        test_results.append(result2)
        
        # æµ‹è¯•3: å¤æ‚å¤šè½®åä½œ
        result3 = await test_suite.test_complex_multi_round_collaboration()
        test_results.append(result3)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        await test_suite.generate_comprehensive_report()
        
        # è¾“å‡ºæœ€ç»ˆç»“æœ
        successful_tests = sum(test_results)
        total_tests = len(test_results)
        
        logger.info("\n" + "=" * 80)
        logger.info(f"ğŸ æµ‹è¯•å®Œæˆ: {successful_tests}/{total_tests} é€šè¿‡")
        
        if successful_tests == total_tests:
            logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿè¿è¡Œå®Œç¾ï¼")
            return True
        else:  
            logger.warning(f"âš ï¸ {total_tests - successful_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œç³»ç»Ÿéœ€è¦ä¼˜åŒ–")
            return False
    
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}")
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)