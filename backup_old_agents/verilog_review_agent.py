#!/usr/bin/env python3
"""
Verilogå®¡æŸ¥æ™ºèƒ½ä½“

Verilog Review Agent for Code Quality Analysis
"""

import asyncio
import json
import logging
import time
import re
from typing import Dict, Any, List, Optional, Set, Tuple
from pathlib import Path
from collections import defaultdict

from core.base_agent import BaseAgent, TaskMessage, FileReference
from core.enums import AgentCapability, AgentStatus
from llm_integration.enhanced_llm_client import EnhancedLLMClient


class VerilogReviewAgent(BaseAgent):
    """
    Verilogå®¡æŸ¥æ™ºèƒ½ä½“
    
    ä¸“é—¨è´Ÿè´£ï¼š
    1. é™æ€ä»£ç åˆ†æå’Œè´¨é‡è¯„ä¼°
    2. è¯­æ³•é”™è¯¯å’Œé£æ ¼æ£€æŸ¥
    3. é€»è¾‘é”™è¯¯å’Œæ½œåœ¨é—®é¢˜è¯†åˆ«
    4. æ€§èƒ½ä¼˜åŒ–å»ºè®®
    5. å¯ç»¼åˆæ€§åˆ†æ
    """
    
    def __init__(self, llm_client: EnhancedLLMClient = None):
        super().__init__(
            agent_id="verilog_review_agent", 
            role="review_engineer",
            capabilities={
                AgentCapability.CODE_REVIEW,
                AgentCapability.QUALITY_ANALYSIS,
                AgentCapability.PERFORMANCE_OPTIMIZATION
            }
        )
        
        self.llm_client = llm_client
        
        # ä»£ç è´¨é‡è¯„ä¼°ç»´åº¦
        self.quality_dimensions = {
            "syntax": "è¯­æ³•æ­£ç¡®æ€§",
            "style": "ä»£ç é£æ ¼",
            "logic": "é€»è¾‘æ­£ç¡®æ€§", 
            "performance": "æ€§èƒ½æ•ˆç‡",
            "maintainability": "å¯ç»´æŠ¤æ€§",
            "synthesizability": "å¯ç»¼åˆæ€§"
        }
        
        # ä¸¥é‡ç¨‹åº¦ç­‰çº§
        self.severity_levels = {
            "critical": {"weight": 1.0, "description": "ä¸¥é‡é”™è¯¯ï¼Œå¿…é¡»ä¿®å¤"},
            "major": {"weight": 0.7, "description": "ä¸»è¦é—®é¢˜ï¼Œå»ºè®®ä¿®å¤"},
            "minor": {"weight": 0.3, "description": "è½»å¾®é—®é¢˜ï¼Œå¯é€‰ä¿®å¤"},
            "info": {"weight": 0.1, "description": "ä¿¡æ¯æç¤ºï¼Œå‚è€ƒå»ºè®®"}
        }
        
        # Verilogå…³é”®å­—å’Œä¿ç•™å­—
        self.verilog_keywords = {
            "always", "and", "assign", "begin", "buf", "bufif0", "bufif1",
            "case", "casex", "casez", "cmos", "deassign", "default", "defparam",
            "disable", "edge", "else", "end", "endcase", "endmodule", "endfunction",
            "endprimitive", "endspecify", "endtable", "endtask", "event", "for",
            "force", "forever", "fork", "function", "highz0", "highz1", "if",
            "ifnone", "initial", "inout", "input", "integer", "join", "large",
            "macromodule", "medium", "module", "nand", "negedge", "nmos", "nor",
            "not", "notif0", "notif1", "or", "output", "parameter", "pmos",
            "posedge", "primitive", "pull0", "pull1", "pulldown", "pullup",
            "rcmos", "real", "realtime", "reg", "release", "repeat", "rnmos",
            "rpmos", "rtran", "rtranif0", "rtranif1", "scalared", "small",
            "specify", "specparam", "strong0", "strong1", "supply0", "supply1",
            "table", "task", "time", "tran", "tranif0", "tranif1", "tri",
            "tri0", "tri1", "triand", "trior", "trireg", "vectored", "wait",
            "wand", "weak0", "weak1", "while", "wire", "wor", "xnor", "xor"
        }
        
        self.logger.info("ğŸ” Verilogå®¡æŸ¥æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def get_capabilities(self) -> Set[AgentCapability]:
        """è·å–å®¡æŸ¥æ™ºèƒ½ä½“èƒ½åŠ›"""
        return {
            AgentCapability.CODE_REVIEW,
            AgentCapability.QUALITY_ANALYSIS,
            AgentCapability.PERFORMANCE_OPTIMIZATION
        }
    
    def get_specialty_description(self) -> str:
        """è·å–ä¸“ä¸šæè¿°"""
        return "ä¸“ä¸šçš„Verilogä»£ç å®¡æŸ¥æ™ºèƒ½ä½“ï¼Œæ“…é•¿é™æ€åˆ†æã€è´¨é‡è¯„ä¼°ã€æ€§èƒ½ä¼˜åŒ–å’Œå¯ç»¼åˆæ€§æ£€æŸ¥"
    
    async def execute_enhanced_task(self, enhanced_prompt: str,
                                  original_message: TaskMessage,
                                  file_contents: Dict[str, Dict]) -> Dict[str, Any]:
        """æ‰§è¡ŒVerilogä»£ç å®¡æŸ¥ä»»åŠ¡"""
        self.logger.info("ğŸ” å¼€å§‹æ‰§è¡ŒVerilogä»£ç å®¡æŸ¥ä»»åŠ¡")
        
        try:
            # 1. æå–å’Œåˆ†æVerilogä»£ç 
            code_analysis = await self._extract_and_analyze_code(enhanced_prompt, file_contents)
            
            # 2. æ‰§è¡Œå¤šç»´åº¦è´¨é‡æ£€æŸ¥
            quality_assessment = await self._perform_quality_assessment(code_analysis)
            
            # 3. æ£€æŸ¥æ½œåœ¨é—®é¢˜å’Œé”™è¯¯
            issue_analysis = await self._analyze_potential_issues(code_analysis)
            
            # 4. ç”Ÿæˆä¼˜åŒ–å»ºè®®
            optimization_suggestions = await self._generate_optimization_suggestions(code_analysis)
            
            # 5. è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
            overall_score = self._calculate_overall_quality_score(
                quality_assessment, issue_analysis
            )
            
            # 6. ä¿å­˜å®¡æŸ¥æŠ¥å‘Š
            output_files = await self._save_review_files(
                code_analysis=code_analysis,
                quality_assessment=quality_assessment,
                issue_analysis=issue_analysis,
                optimization_suggestions=optimization_suggestions,
                overall_score=overall_score,
                task_id=original_message.task_id
            )
            
            # 7. ç”Ÿæˆå®¡æŸ¥æ‘˜è¦
            review_summary = self._generate_review_summary(
                quality_assessment=quality_assessment,
                issue_analysis=issue_analysis,
                overall_score=overall_score,
                output_files=output_files
            )
            
            return {
                "success": True,
                "task_completed": True,
                "agent_id": self.agent_id,
                "overall_quality_score": overall_score,
                "quality_assessment": quality_assessment,
                "issue_analysis": issue_analysis,
                "optimization_suggestions": optimization_suggestions,
                "review_summary": review_summary,
                "file_references": output_files,
                "execution_time": time.time()
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Verilogä»£ç å®¡æŸ¥ä»»åŠ¡å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "task_completed": False,
                "agent_id": self.agent_id,
                "error": str(e),
                "execution_time": time.time()
            }
    
    async def _extract_and_analyze_code(self, task_description: str, 
                                      file_contents: Dict[str, Dict]) -> Dict[str, Any]:
        """æå–å’Œåˆ†æVerilogä»£ç """
        # æŸ¥æ‰¾Verilogæºç æ–‡ä»¶
        verilog_files = []
        for file_path, content_info in file_contents.items():
            if (content_info.get("type") == "verilog" or 
                file_path.endswith((".v", ".sv", ".vh"))):
                verilog_files.append({
                    "path": file_path,
                    "content": content_info.get("content", ""),
                    "type": content_info.get("type", "verilog")
                })
        
        if not verilog_files:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯å®¡æŸ¥çš„Verilogä»£ç æ–‡ä»¶")
        
        analysis_results = []
        
        for verilog_file in verilog_files:
            file_analysis = await self._analyze_single_file(
                verilog_file["path"], 
                verilog_file["content"]
            )
            analysis_results.append(file_analysis)
        
        # åˆå¹¶åˆ†æç»“æœ
        combined_analysis = {
            "total_files": len(verilog_files),
            "files": analysis_results,
            "overall_metrics": self._calculate_combined_metrics(analysis_results)
        }
        
        self.logger.info(f"ğŸ“Š ä»£ç åˆ†æå®Œæˆ: {len(verilog_files)} ä¸ªæ–‡ä»¶")
        return combined_analysis
    
    async def _analyze_single_file(self, file_path: str, 
                                 file_content: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªVerilogæ–‡ä»¶"""
        analysis = {
            "file_path": file_path,
            "file_size": len(file_content),
            "line_count": len(file_content.splitlines()),
            "modules": [],
            "metrics": {},
            "syntax_elements": {},
            "potential_issues": []
        }
        
        try:
            # åŸºæœ¬æŒ‡æ ‡ç»Ÿè®¡
            analysis["metrics"] = self._calculate_code_metrics(file_content)
            
            # æå–æ¨¡å—ä¿¡æ¯
            analysis["modules"] = self._extract_module_information(file_content)
            
            # åˆ†æè¯­æ³•å…ƒç´ 
            analysis["syntax_elements"] = self._analyze_syntax_elements(file_content)
            
            # åŸºæœ¬é—®é¢˜æ£€æµ‹
            analysis["potential_issues"] = self._detect_basic_issues(file_content)
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            analysis["analysis_error"] = str(e)
        
        return analysis
    
    def _calculate_code_metrics(self, code: str) -> Dict[str, Any]:
        """è®¡ç®—ä»£ç æŒ‡æ ‡"""
        lines = code.splitlines()
        
        # åŸºæœ¬è¡Œæ•°ç»Ÿè®¡
        total_lines = len(lines)
        blank_lines = sum(1 for line in lines if not line.strip())
        comment_lines = sum(1 for line in lines if line.strip().startswith("//"))
        code_lines = total_lines - blank_lines - comment_lines
        
        # å¤æ‚åº¦æŒ‡æ ‡
        cyclomatic_complexity = self._calculate_cyclomatic_complexity(code)
        nesting_depth = self._calculate_max_nesting_depth(code)
        
        # å¯è¯»æ€§æŒ‡æ ‡
        avg_line_length = sum(len(line) for line in lines) / max(total_lines, 1)
        long_lines = sum(1 for line in lines if len(line) > 80)
        
        return {
            "total_lines": total_lines,
            "code_lines": code_lines,
            "blank_lines": blank_lines,
            "comment_lines": comment_lines,
            "comment_ratio": comment_lines / max(code_lines, 1),
            "cyclomatic_complexity": cyclomatic_complexity,
            "max_nesting_depth": nesting_depth,
            "average_line_length": avg_line_length,
            "long_lines_count": long_lines,
            "long_lines_ratio": long_lines / max(total_lines, 1)
        }
    
    def _calculate_cyclomatic_complexity(self, code: str) -> int:
        """è®¡ç®—åœˆå¤æ‚åº¦"""
        complexity = 1  # åŸºç¡€å¤æ‚åº¦
        
        # è®¡ç®—å†³ç­–ç‚¹
        decision_keywords = ["if", "else", "case", "for", "while", "repeat"]
        for keyword in decision_keywords:
            complexity += len(re.findall(rf'\b{keyword}\b', code))
        
        # caseè¯­å¥çš„æ¯ä¸ªåˆ†æ”¯å¢åŠ å¤æ‚åº¦
        case_branches = len(re.findall(r'^\s*\w+\s*:', code, re.MULTILINE))
        complexity += max(0, case_branches - 1)  # å‡1æ˜¯å› ä¸ºdefaultåˆ†æ”¯ä¸å¢åŠ å¤æ‚åº¦
        
        return complexity
    
    def _calculate_max_nesting_depth(self, code: str) -> int:
        """è®¡ç®—æœ€å¤§åµŒå¥—æ·±åº¦"""
        max_depth = 0
        current_depth = 0
        
        lines = code.splitlines()
        for line in lines:
            stripped = line.strip()
            
            # å¢åŠ åµŒå¥—æ·±åº¦çš„å…³é”®å­—
            if any(keyword in stripped for keyword in ["begin", "case", "if", "for", "while"]):
                current_depth += 1
                max_depth = max(max_depth, current_depth)
            
            # å‡å°‘åµŒå¥—æ·±åº¦çš„å…³é”®å­—
            if any(keyword in stripped for keyword in ["end", "endcase"]):
                current_depth = max(0, current_depth - 1)
        
        return max_depth
    
    def _extract_module_information(self, code: str) -> List[Dict[str, Any]]:
        """æå–æ¨¡å—ä¿¡æ¯"""
        modules = []
        
        # æŸ¥æ‰¾æ¨¡å—å®šä¹‰
        module_pattern = r'module\s+(\w+)\s*(?:#\s*\([^)]*\))?\s*\(([^;]*)\);'
        matches = re.finditer(module_pattern, code, re.MULTILINE | re.DOTALL)
        
        for match in matches:
            module_name = match.group(1)
            port_list = match.group(2) if match.group(2) else ""
            
            # æå–ç«¯å£ä¿¡æ¯
            ports = self._parse_port_list(port_list)
            
            # æŸ¥æ‰¾æ¨¡å—ç»“æŸä½ç½®
            module_start = match.end()
            endmodule_pattern = rf'endmodule\s*(?://.*)?$'
            endmodule_match = re.search(endmodule_pattern, code[module_start:], re.MULTILINE)
            module_end = module_start + (endmodule_match.end() if endmodule_match else len(code))
            
            module_code = code[match.start():module_end]
            
            modules.append({
                "name": module_name,
                "ports": ports,
                "line_start": code[:match.start()].count('\n') + 1,
                "line_end": code[:module_end].count('\n') + 1,
                "code_length": len(module_code),
                "has_parameters": "#" in match.group(0),
                "port_count": len(ports)
            })
        
        return modules
    
    def _parse_port_list(self, port_list: str) -> List[Dict[str, str]]:
        """è§£æç«¯å£åˆ—è¡¨"""
        ports = []
        if not port_list.strip():
            return ports
        
        # ç®€å•çš„ç«¯å£è§£æï¼ˆå¯ä»¥è¿›ä¸€æ­¥å®Œå–„ï¼‰
        port_items = [p.strip() for p in port_list.split(',')]
        for port_item in port_items:
            if port_item:
                ports.append({
                    "name": port_item,
                    "direction": "unknown",
                    "width": "unknown"
                })
        
        return ports
    
    def _analyze_syntax_elements(self, code: str) -> Dict[str, Any]:
        """åˆ†æè¯­æ³•å…ƒç´ """
        elements = {
            "always_blocks": len(re.findall(r'\balways\b', code)),
            "initial_blocks": len(re.findall(r'\binitial\b', code)),
            "assign_statements": len(re.findall(r'\bassign\b', code)),
            "wire_declarations": len(re.findall(r'\bwire\b', code)),
            "reg_declarations": len(re.findall(r'\breg\b', code)),
            "parameter_declarations": len(re.findall(r'\bparameter\b', code)),
            "function_definitions": len(re.findall(r'\bfunction\b', code)),
            "task_definitions": len(re.findall(r'\btask\b', code)),
            "generate_blocks": len(re.findall(r'\bgenerate\b', code)),
            "case_statements": len(re.findall(r'\bcase\b', code)),
            "if_statements": len(re.findall(r'\bif\b', code))
        }
        
        return elements
    
    def _detect_basic_issues(self, code: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹åŸºæœ¬é—®é¢˜"""
        issues = []
        
        lines = code.splitlines()
        
        for i, line in enumerate(lines, 1):
            stripped = line.strip()
            
            # æ£€æŸ¥è¡Œé•¿åº¦
            if len(line) > 120:
                issues.append({
                    "type": "style",
                    "severity": "minor",
                    "line": i,
                    "message": f"è¡Œé•¿åº¦è¿‡é•¿ ({len(line)} å­—ç¬¦)",
                    "suggestion": "å»ºè®®å°†é•¿è¡Œåˆ†è§£ä¸ºå¤šè¡Œ"
                })
            
            # æ£€æŸ¥tabå’Œç©ºæ ¼æ··ç”¨
            if '\t' in line and '    ' in line:
                issues.append({
                    "type": "style",
                    "severity": "minor",
                    "line": i,
                    "message": "æ··ç”¨tabå’Œç©ºæ ¼ç¼©è¿›",
                    "suggestion": "ç»Ÿä¸€ä½¿ç”¨ç©ºæ ¼æˆ–tabç¼©è¿›"
                })
            
            # æ£€æŸ¥æœªä½¿ç”¨çš„ä¿¡å·ï¼ˆç®€å•æ£€æµ‹ï¼‰
            if stripped.startswith("wire ") or stripped.startswith("reg "):
                signal_match = re.search(r'(wire|reg)\s+(?:\[[^\]]+\])?\s*(\w+)', stripped)
                if signal_match:
                    signal_name = signal_match.group(2)
                    if code.count(signal_name) == 1:  # åªå‡ºç°åœ¨å£°æ˜ä¸­
                        issues.append({
                            "type": "logic",
                            "severity": "minor",
                            "line": i,
                            "message": f"å¯èƒ½æœªä½¿ç”¨çš„ä¿¡å·: {signal_name}",
                            "suggestion": "æ£€æŸ¥ä¿¡å·æ˜¯å¦çœŸçš„éœ€è¦"
                        })
            
            # æ£€æŸ¥blockingå’Œnon-blockingèµ‹å€¼æ··ç”¨
            if "always" in stripped and "=" in stripped:
                if re.search(r'=(?!=)', stripped) and re.search(r'<=', stripped):
                    issues.append({
                        "type": "logic",
                        "severity": "major",
                        "line": i,
                        "message": "åœ¨åŒä¸€alwayså—ä¸­æ··ç”¨blockingå’Œnon-blockingèµ‹å€¼",
                        "suggestion": "æ—¶åºé€»è¾‘ä½¿ç”¨<=ï¼Œç»„åˆé€»è¾‘ä½¿ç”¨="
                    })
        
        return issues
    
    def _calculate_combined_metrics(self, file_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è®¡ç®—ç»¼åˆæŒ‡æ ‡"""
        if not file_analyses:
            return {}
        
        total_lines = sum(analysis.get("metrics", {}).get("total_lines", 0) 
                         for analysis in file_analyses)
        total_code_lines = sum(analysis.get("metrics", {}).get("code_lines", 0) 
                              for analysis in file_analyses)
        total_modules = sum(len(analysis.get("modules", [])) 
                           for analysis in file_analyses)
        
        avg_complexity = sum(analysis.get("metrics", {}).get("cyclomatic_complexity", 0) 
                            for analysis in file_analyses) / len(file_analyses)
        
        max_nesting = max(analysis.get("metrics", {}).get("max_nesting_depth", 0) 
                         for analysis in file_analyses)
        
        return {
            "total_lines": total_lines,
            "total_code_lines": total_code_lines,
            "total_modules": total_modules,
            "average_complexity": avg_complexity,
            "max_nesting_depth": max_nesting,
            "files_with_issues": sum(1 for analysis in file_analyses 
                                   if analysis.get("potential_issues", []))
        }
    
    async def _perform_quality_assessment(self, code_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œè´¨é‡è¯„ä¼°"""
        quality_scores = {}
        
        for dimension in self.quality_dimensions:
            score = await self._assess_quality_dimension(dimension, code_analysis)
            quality_scores[dimension] = {
                "score": score,
                "description": self.quality_dimensions[dimension],
                "details": self._get_dimension_details(dimension, code_analysis, score)
            }
        
        # è®¡ç®—åŠ æƒå¹³å‡åˆ†
        weights = {"syntax": 0.25, "style": 0.15, "logic": 0.25, 
                  "performance": 0.15, "maintainability": 0.15, "synthesizability": 0.05}
        
        overall_score = sum(quality_scores[dim]["score"] * weights.get(dim, 0.1) 
                           for dim in quality_scores)
        
        return {
            "dimension_scores": quality_scores,
            "overall_score": overall_score,
            "grade": self._score_to_grade(overall_score)
        }
    
    async def _assess_quality_dimension(self, dimension: str, 
                                      code_analysis: Dict[str, Any]) -> float:
        """è¯„ä¼°ç‰¹å®šè´¨é‡ç»´åº¦"""
        metrics = code_analysis.get("overall_metrics", {})
        
        if dimension == "syntax":
            # åŸºäºè¯­æ³•é”™è¯¯å’ŒåŸºæœ¬ç»“æ„å®Œæ•´æ€§
            return 0.9  # å‡è®¾è¯­æ³•åŸºæœ¬æ­£ç¡®
        
        elif dimension == "style":
            # åŸºäºä»£ç é£æ ¼æŒ‡æ ‡
            comment_ratio = metrics.get("comment_ratio", 0)
            long_lines_ratio = metrics.get("long_lines_ratio", 0)
            
            style_score = 0.8
            if comment_ratio < 0.1:
                style_score -= 0.2
            if long_lines_ratio > 0.1:
                style_score -= 0.1
            
            return max(0.0, min(1.0, style_score))
        
        elif dimension == "logic":
            # åŸºäºé€»è¾‘é”™è¯¯å’Œæ½œåœ¨é—®é¢˜
            total_issues = sum(len(analysis.get("potential_issues", [])) 
                             for analysis in code_analysis.get("files", []))
            issue_density = total_issues / max(metrics.get("total_code_lines", 1), 1)
            
            logic_score = 1.0 - min(issue_density * 10, 0.5)
            return max(0.0, min(1.0, logic_score))
        
        elif dimension == "performance":
            # åŸºäºæ€§èƒ½ç›¸å…³æŒ‡æ ‡
            avg_complexity = metrics.get("average_complexity", 5)
            performance_score = max(0.5, 1.0 - (avg_complexity - 5) / 20)
            return performance_score
        
        elif dimension == "maintainability":
            # åŸºäºå¯ç»´æŠ¤æ€§æŒ‡æ ‡
            max_nesting = metrics.get("max_nesting_depth", 3)
            maintainability_score = max(0.5, 1.0 - max_nesting / 10)
            return maintainability_score
        
        elif dimension == "synthesizability":
            # åŸºäºå¯ç»¼åˆæ€§æ£€æŸ¥
            return 0.85  # å‡è®¾åŸºæœ¬å¯ç»¼åˆ
        
        return 0.5  # é»˜è®¤åˆ†æ•°
    
    def _get_dimension_details(self, dimension: str, code_analysis: Dict[str, Any], 
                             score: float) -> List[str]:
        """è·å–è´¨é‡ç»´åº¦çš„è¯¦ç»†è¯´æ˜"""
        details = []
        metrics = code_analysis.get("overall_metrics", {})
        
        if dimension == "style":
            comment_ratio = metrics.get("comment_ratio", 0)
            if comment_ratio < 0.1:
                details.append(f"æ³¨é‡Šæ¯”ä¾‹è¾ƒä½ ({comment_ratio:.1%})")
            else:
                details.append(f"æ³¨é‡Šæ¯”ä¾‹è‰¯å¥½ ({comment_ratio:.1%})")
        
        elif dimension == "performance":
            avg_complexity = metrics.get("average_complexity", 5)
            if avg_complexity > 10:
                details.append(f"å¹³å‡å¤æ‚åº¦è¾ƒé«˜ ({avg_complexity:.1f})")
            else:
                details.append(f"å¹³å‡å¤æ‚åº¦åˆç† ({avg_complexity:.1f})")
        
        elif dimension == "maintainability":
            max_nesting = metrics.get("max_nesting_depth", 3)
            if max_nesting > 5:
                details.append(f"åµŒå¥—æ·±åº¦è¿‡æ·± ({max_nesting})")
            else:
                details.append(f"åµŒå¥—æ·±åº¦åˆç† ({max_nesting})")
        
        return details
    
    def _score_to_grade(self, score: float) -> str:
        """å°†åˆ†æ•°è½¬æ¢ä¸ºç­‰çº§"""
        if score >= 0.9:
            return "A"
        elif score >= 0.8:
            return "B"
        elif score >= 0.7:
            return "C"
        elif score >= 0.6:
            return "D"
        else:
            return "F"
    
    async def _analyze_potential_issues(self, code_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ½œåœ¨é—®é¢˜"""
        all_issues = []
        
        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„é—®é¢˜
        for file_analysis in code_analysis.get("files", []):
            file_issues = file_analysis.get("potential_issues", [])
            for issue in file_issues:
                issue["file"] = file_analysis.get("file_path", "unknown")
                all_issues.append(issue)
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»
        issues_by_severity = defaultdict(list)
        for issue in all_issues:
            severity = issue.get("severity", "minor")
            issues_by_severity[severity].append(issue)
        
        # ç»Ÿè®¡ä¿¡æ¯
        issue_stats = {
            "total_issues": len(all_issues),
            "by_severity": {severity: len(issues) for severity, issues in issues_by_severity.items()},
            "by_type": defaultdict(int)
        }
        
        for issue in all_issues:
            issue_stats["by_type"][issue.get("type", "unknown")] += 1
        
        return {
            "all_issues": all_issues,
            "by_severity": dict(issues_by_severity),
            "statistics": issue_stats
        }
    
    async def _generate_optimization_suggestions(self, code_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        metrics = code_analysis.get("overall_metrics", {})
        
        # åŸºäºæŒ‡æ ‡ç”Ÿæˆå»ºè®®
        if metrics.get("average_complexity", 5) > 8:
            suggestions.append({
                "category": "complexity",
                "priority": "high",
                "title": "é™ä½ä»£ç å¤æ‚åº¦",
                "description": "å½“å‰ä»£ç å¤æ‚åº¦è¾ƒé«˜ï¼Œå»ºè®®æ‹†åˆ†å¤æ‚çš„é€»è¾‘å—",
                "implementation": [
                    "å°†å¤æ‚çš„alwayså—æ‹†åˆ†ä¸ºå¤šä¸ªç®€å•çš„å—",
                    "ä½¿ç”¨å‡½æ•°å’Œä»»åŠ¡å°è£…é‡å¤é€»è¾‘",
                    "ç®€åŒ–å¤æ‚çš„caseè¯­å¥"
                ]
            })
        
        if metrics.get("comment_ratio", 0) < 0.1:
            suggestions.append({
                "category": "documentation",
                "priority": "medium",
                "title": "å¢åŠ ä»£ç æ³¨é‡Š",
                "description": "ä»£ç æ³¨é‡Šä¸è¶³ï¼Œå»ºè®®å¢åŠ å¿…è¦çš„è¯´æ˜",
                "implementation": [
                    "ä¸ºæ¯ä¸ªæ¨¡å—æ·»åŠ åŠŸèƒ½è¯´æ˜",
                    "ä¸ºå¤æ‚é€»è¾‘æ·»åŠ å†…è”æ³¨é‡Š",
                    "æ·»åŠ ç«¯å£å’Œå‚æ•°è¯´æ˜"
                ]
            })
        
        if metrics.get("max_nesting_depth", 3) > 6:
            suggestions.append({
                "category": "structure",
                "priority": "high",
                "title": "å‡å°‘åµŒå¥—æ·±åº¦",
                "description": "ä»£ç åµŒå¥—è¿‡æ·±ï¼Œå½±å“å¯è¯»æ€§",
                "implementation": [
                    "ä½¿ç”¨early returnæ¨¡å¼",
                    "æå–åµŒå¥—é€»è¾‘ä¸ºç‹¬ç«‹æ¨¡å—",
                    "ä½¿ç”¨çŠ¶æ€æœºç®€åŒ–å¤æ‚æ§åˆ¶é€»è¾‘"
                ]
            })
        
        # æ€§èƒ½ä¼˜åŒ–å»ºè®®
        suggestions.append({
            "category": "performance",
            "priority": "medium",
            "title": "æ—¶åºä¼˜åŒ–",
            "description": "è€ƒè™‘æ—¶åºä¼˜åŒ–ä»¥æé«˜è®¾è®¡æ€§èƒ½",
            "implementation": [
                "é¿å…è¿‡é•¿çš„ç»„åˆé€»è¾‘è·¯å¾„",
                "é€‚å½“ä½¿ç”¨æµæ°´çº¿æŠ€æœ¯",
                "ä¼˜åŒ–å…³é”®è·¯å¾„æ—¶åº"
            ]
        })
        
        return suggestions
    
    def _calculate_overall_quality_score(self, quality_assessment: Dict[str, Any], 
                                       issue_analysis: Dict[str, Any]) -> float:
        """è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°"""
        base_score = quality_assessment.get("overall_score", 0.5)
        
        # æ ¹æ®é—®é¢˜ä¸¥é‡ç¨‹åº¦è°ƒæ•´åˆ†æ•°
        issues_by_severity = issue_analysis.get("statistics", {}).get("by_severity", {})
        
        penalty = 0.0
        for severity, count in issues_by_severity.items():
            if severity in self.severity_levels:
                penalty += count * self.severity_levels[severity]["weight"] * 0.05
        
        adjusted_score = max(0.0, base_score - penalty)
        return min(1.0, adjusted_score)
    
    async def _save_review_files(self, code_analysis: Dict[str, Any],
                               quality_assessment: Dict[str, Any],
                               issue_analysis: Dict[str, Any],
                               optimization_suggestions: List[Dict[str, Any]],
                               overall_score: float,
                               task_id: str) -> List[FileReference]:
        """ä¿å­˜å®¡æŸ¥æŠ¥å‘Šæ–‡ä»¶"""
        output_files = []
        
        try:
            # 1. ä¿å­˜è¯¦ç»†å®¡æŸ¥æŠ¥å‘Š
            review_report = self._generate_detailed_report(
                code_analysis, quality_assessment, issue_analysis, 
                optimization_suggestions, overall_score
            )
            
            report_path = f"output/{task_id}/code_review_report.md"
            report_ref = await self.save_result_to_file(
                content=review_report,
                file_path=report_path,
                file_type="report"
            )
            output_files.append(report_ref)
            
            # 2. ä¿å­˜é—®é¢˜æ¸…å•JSON
            issues_json = json.dumps(issue_analysis, indent=2, ensure_ascii=False)
            issues_path = f"output/{task_id}/issues_list.json"
            issues_ref = await self.save_result_to_file(
                content=issues_json,
                file_path=issues_path,
                file_type="analysis_data"
            )
            output_files.append(issues_ref)
            
            # 3. ä¿å­˜ä¼˜åŒ–å»ºè®®
            suggestions_content = self._format_optimization_suggestions(optimization_suggestions)
            suggestions_path = f"output/{task_id}/optimization_suggestions.md"
            suggestions_ref = await self.save_result_to_file(
                content=suggestions_content,
                file_path=suggestions_path,
                file_type="recommendations"
            )
            output_files.append(suggestions_ref)
            
            # 4. ä¿å­˜è´¨é‡è¯„ä¼°æ•°æ®
            quality_json = json.dumps(quality_assessment, indent=2, ensure_ascii=False)
            quality_path = f"output/{task_id}/quality_assessment.json"
            quality_ref = await self.save_result_to_file(
                content=quality_json,
                file_path=quality_path,
                file_type="quality_data"
            )
            output_files.append(quality_ref)
            
            self.logger.info(f"ğŸ’¾ å®¡æŸ¥æŠ¥å‘Šä¿å­˜å®Œæˆ: {len(output_files)} ä¸ªæ–‡ä»¶")
            return output_files
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜å®¡æŸ¥æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return output_files
    
    def _generate_detailed_report(self, code_analysis: Dict[str, Any],
                                quality_assessment: Dict[str, Any],
                                issue_analysis: Dict[str, Any],
                                optimization_suggestions: List[Dict[str, Any]],
                                overall_score: float) -> str:
        """ç”Ÿæˆè¯¦ç»†å®¡æŸ¥æŠ¥å‘Š"""
        metrics = code_analysis.get("overall_metrics", {})
        dimension_scores = quality_assessment.get("dimension_scores", {})
        issue_stats = issue_analysis.get("statistics", {})
        
        report = f"""# Verilogä»£ç å®¡æŸ¥æŠ¥å‘Š

## ğŸ“Š å®¡æŸ¥æ¦‚è¦

**æ€»ä½“è¯„åˆ†**: {overall_score:.2f}/1.0 ({quality_assessment.get('grade', 'N/A')})
**å®¡æŸ¥æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}
**å®¡æŸ¥æ™ºèƒ½ä½“**: {self.agent_id}

## ğŸ“‹ ä»£ç æ¦‚å†µ

- **æ–‡ä»¶æ•°é‡**: {code_analysis.get('total_files', 0)}
- **æ€»è¡Œæ•°**: {metrics.get('total_lines', 0)}
- **ä»£ç è¡Œæ•°**: {metrics.get('total_code_lines', 0)}
- **æ³¨é‡Šè¡Œæ•°**: {metrics.get('total_lines', 0) - metrics.get('total_code_lines', 0)}
- **æ¨¡å—æ•°é‡**: {metrics.get('total_modules', 0)}

## ğŸ¯ è´¨é‡è¯„ä¼°

### å„ç»´åº¦è¯„åˆ†
"""
        
        for dimension, info in dimension_scores.items():
            score = info["score"]
            description = info["description"]
            details = info.get("details", [])
            
            report += f"""
**{description}**: {score:.2f}/1.0
{chr(10).join(f"- {detail}" for detail in details)}
"""
        
        report += f"""
## âš ï¸ é—®é¢˜åˆ†æ

**é—®é¢˜æ€»æ•°**: {issue_stats.get('total_issues', 0)}

### æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
"""
        
        for severity, count in issue_stats.get('by_severity', {}).items():
            severity_info = self.severity_levels.get(severity, {})
            description = severity_info.get('description', severity)
            report += f"- **{severity.upper()}**: {count} ä¸ª - {description}\n"
        
        report += "\n### æŒ‰ç±»å‹ç»Ÿè®¡\n"
        for issue_type, count in issue_stats.get('by_type', {}).items():
            report += f"- **{issue_type}**: {count} ä¸ª\n"
        
        # è¯¦ç»†é—®é¢˜æ¸…å•
        report += "\n### è¯¦ç»†é—®é¢˜æ¸…å•\n"
        all_issues = issue_analysis.get("all_issues", [])
        
        for i, issue in enumerate(all_issues[:20], 1):  # æœ€å¤šæ˜¾ç¤º20ä¸ªé—®é¢˜
            report += f"""
**é—®é¢˜ {i}**: {issue.get('message', 'Unknown issue')}
- æ–‡ä»¶: {issue.get('file', 'Unknown')}
- è¡Œå·: {issue.get('line', 'N/A')}
- ç±»å‹: {issue.get('type', 'Unknown')}
- ä¸¥é‡ç¨‹åº¦: {issue.get('severity', 'Unknown')}
- å»ºè®®: {issue.get('suggestion', 'No suggestion')}
"""
        
        if len(all_issues) > 20:
            report += f"\n*è¿˜æœ‰ {len(all_issues) - 20} ä¸ªé—®é¢˜ï¼Œè¯¦è§ issues_list.json*\n"
        
        # ä¼˜åŒ–å»ºè®®
        report += "\n## ğŸš€ ä¼˜åŒ–å»ºè®®\n"
        for i, suggestion in enumerate(optimization_suggestions, 1):
            report += f"""
### {i}. {suggestion.get('title', 'Optimization Suggestion')}
**ä¼˜å…ˆçº§**: {suggestion.get('priority', 'medium')}
**ç±»åˆ«**: {suggestion.get('category', 'general')}

**æè¿°**: {suggestion.get('description', 'No description')}

**å®æ–½æ–¹æ¡ˆ**:
{chr(10).join(f"- {impl}" for impl in suggestion.get('implementation', []))}
"""
        
        report += f"""
## ğŸ“ˆ æŒ‡æ ‡è¯¦æƒ…

### ä»£ç å¤æ‚åº¦
- **å¹³å‡åœˆå¤æ‚åº¦**: {metrics.get('average_complexity', 0):.1f}
- **æœ€å¤§åµŒå¥—æ·±åº¦**: {metrics.get('max_nesting_depth', 0)}

### ä»£ç é£æ ¼
- **æ³¨é‡Šæ¯”ä¾‹**: {metrics.get('comment_ratio', 0):.1%}
- **é•¿è¡Œæ¯”ä¾‹**: {metrics.get('long_lines_ratio', 0):.1%}

### ç»“æ„ä¿¡æ¯
- **æ¨¡å—æ€»æ•°**: {metrics.get('total_modules', 0)}
- **æœ‰é—®é¢˜çš„æ–‡ä»¶æ•°**: {metrics.get('files_with_issues', 0)}

---
*æŠ¥å‘Šç”± Verilogå®¡æŸ¥æ™ºèƒ½ä½“ è‡ªåŠ¨ç”Ÿæˆ*
"""
        
        return report.strip()
    
    def _format_optimization_suggestions(self, suggestions: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–ä¼˜åŒ–å»ºè®®"""
        content = "# Verilogä»£ç ä¼˜åŒ–å»ºè®®\n\n"
        
        # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
        high_priority = [s for s in suggestions if s.get('priority') == 'high']
        medium_priority = [s for s in suggestions if s.get('priority') == 'medium']
        low_priority = [s for s in suggestions if s.get('priority') == 'low']
        
        if high_priority:
            content += "## ğŸ”´ é«˜ä¼˜å…ˆçº§å»ºè®®\n"
            for suggestion in high_priority:
                content += self._format_single_suggestion(suggestion)
        
        if medium_priority:
            content += "\n## ğŸŸ¡ ä¸­ä¼˜å…ˆçº§å»ºè®®\n"
            for suggestion in medium_priority:
                content += self._format_single_suggestion(suggestion)
        
        if low_priority:
            content += "\n## ğŸŸ¢ ä½ä¼˜å…ˆçº§å»ºè®®\n"
            for suggestion in low_priority:
                content += self._format_single_suggestion(suggestion)
        
        return content
    
    def _format_single_suggestion(self, suggestion: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å•ä¸ªå»ºè®®"""
        return f"""
### {suggestion.get('title', 'Optimization')}
**ç±»åˆ«**: {suggestion.get('category', 'general')}

{suggestion.get('description', 'No description')}

**å®æ–½æ­¥éª¤**:
{chr(10).join(f"{i+1}. {impl}" for i, impl in enumerate(suggestion.get('implementation', [])))}

---
"""
    
    def _generate_review_summary(self, quality_assessment: Dict[str, Any],
                                issue_analysis: Dict[str, Any],
                                overall_score: float,
                                output_files: List[FileReference]) -> str:
        """ç”Ÿæˆå®¡æŸ¥æ‘˜è¦"""
        issue_stats = issue_analysis.get("statistics", {})
        grade = quality_assessment.get("grade", "N/A")
        
        summary = f"""
ğŸ” VERILOGä»£ç å®¡æŸ¥æ‘˜è¦
===================

ğŸ“‹ å®¡æŸ¥ç»“æœ:
- æ€»ä½“è¯„åˆ†: {overall_score:.2f}/1.0 ({grade})
- é—®é¢˜æ€»æ•°: {issue_stats.get('total_issues', 0)}
- ä¸¥é‡é—®é¢˜: {issue_stats.get('by_severity', {}).get('critical', 0)}
- ä¸»è¦é—®é¢˜: {issue_stats.get('by_severity', {}).get('major', 0)}

ğŸ¯ è´¨é‡ç»´åº¦:
{chr(10).join(f"- {info['description']}: {info['score']:.2f}" 
             for info in quality_assessment.get('dimension_scores', {}).values())}

ğŸ“ ç”ŸæˆæŠ¥å‘Š: {len(output_files)} ä¸ªæ–‡ä»¶
{chr(10).join(f"- {ref.file_path}" for ref in output_files)}

ğŸ’¡ ä¸»è¦å»ºè®®:
- {'é€šè¿‡åŸºæœ¬è´¨é‡æ£€æŸ¥' if overall_score >= 0.7 else 'éœ€è¦æ”¹è¿›ä»£ç è´¨é‡'}
- {'å»ºè®®è§£å†³ä¸¥é‡å’Œä¸»è¦é—®é¢˜' if issue_stats.get('by_severity', {}).get('critical', 0) > 0 else 'æ•´ä½“ä»£ç ç»“æ„è‰¯å¥½'}
"""
        
        return summary.strip()