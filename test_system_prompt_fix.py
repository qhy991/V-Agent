#!/usr/bin/env python3
"""
æµ‹è¯•System Promptä¿®å¤æ•ˆæžœ
éªŒè¯å¼ºåŒ–çš„å·¥å…·è°ƒç”¨æŒ‡å¯¼æ˜¯å¦æœ‰æ•ˆ
"""

import os
import sys
import re
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.append(str(project_root))

def test_system_prompt_content():
    """æµ‹è¯•System Promptå†…å®¹æ˜¯å¦åŒ…å«æ­£ç¡®çš„æŒ‡å¯¼"""
    print("ðŸ§ª æµ‹è¯•System Promptå†…å®¹...")
    
    # æ¨¡æ‹ŸLLMCoordinatorAgent
    try:
        from core.llm_coordinator_agent import LLMCoordinatorAgent
        from config.config import FrameworkConfig
        
        # åˆ›å»ºåè°ƒå™¨å®žä¾‹
        config = FrameworkConfig.from_env()
        coordinator = LLMCoordinatorAgent(config)
        
        # èŽ·å–System Prompt
        system_prompt = coordinator._build_enhanced_system_prompt()
        
        # éªŒè¯å…³é”®å†…å®¹
        checks = [
            ("åŒ…å«ç¦æ­¢äº‹é¡¹æ ‡é¢˜", "ðŸš¨ðŸš¨ðŸš¨ ç»å¯¹ç¦æ­¢äº‹é¡¹ ðŸš¨ðŸš¨ðŸš¨" in system_prompt),
            ("ç¦æ­¢enhanced_real_verilog_agent", "âŒ enhanced_real_verilog_agent" in system_prompt),
            ("ç¦æ­¢enhanced_real_code_review_agent", "âŒ enhanced_real_code_review_agent" in system_prompt),
            ("è¦æ±‚ä½¿ç”¨assign_task_to_agent", "assign_task_to_agent" in system_prompt),
            ("åŒ…å«æ­£ç¡®ç¤ºä¾‹", "âœ… æ­£ç¡®ç¤ºä¾‹" in system_prompt),
            ("åŒ…å«é”™è¯¯ç¤ºä¾‹", "âŒâŒâŒ é”™è¯¯ç¤ºä¾‹" in system_prompt),
            ("å¼ºè°ƒagent_idå‚æ•°", "agent_id" in system_prompt),
            ("åŒ…å«JSONæ ¼å¼è¦æ±‚", '"tool_calls"' in system_prompt)
        ]
        
        passed_checks = 0
        for description, check in checks:
            if check:
                print(f"  âœ… {description}")
                passed_checks += 1
            else:
                print(f"  âŒ {description}")
        
        success_rate = passed_checks / len(checks)
        print(f"\nðŸ“Š System Promptæ£€æŸ¥ç»“æžœ: {passed_checks}/{len(checks)} ({success_rate:.1%})")
        
        return success_rate >= 0.8
        
    except Exception as e:
        print(f"âŒ System Promptæµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_forced_coordination_task():
    """æµ‹è¯•å¼ºåˆ¶åè°ƒä»»åŠ¡çš„å†…å®¹"""
    print("\nðŸ§ª æµ‹è¯•å¼ºåˆ¶åè°ƒä»»åŠ¡...")
    
    try:
        from core.llm_coordinator_agent import LLMCoordinatorAgent, TaskContext
        from config.config import FrameworkConfig
        
        config = FrameworkConfig.from_env()
        coordinator = LLMCoordinatorAgent(config)
        
        # åˆ›å»ºæ¨¡æ‹Ÿä»»åŠ¡ä¸Šä¸‹æ–‡
        task_context = TaskContext(
            task_id="test_task",
            original_request="è®¾è®¡ä¸€ä¸ªè®¡æ•°å™¨"
        )
        
        # èŽ·å–å¼ºåˆ¶åè°ƒä»»åŠ¡
        forced_task = coordinator._build_forced_coordination_task(
            "è®¾è®¡ä¸€ä¸ªè®¡æ•°å™¨æ¨¡å—", 
            task_context
        )
        
        # éªŒè¯å…³é”®å†…å®¹
        checks = [
            ("åŒ…å«å¼ºåˆ¶æŒ‡ä»¤æ ‡é¢˜", "ðŸš¨ðŸš¨ðŸš¨ å¼ºåˆ¶æŒ‡ä»¤ ðŸš¨ðŸš¨ðŸš¨" in forced_task),
            ("ç¦æ­¢æ™ºèƒ½ä½“åç§°", "âŒ enhanced_real_verilog_agent" in forced_task),
            ("è¦æ±‚identify_task_type", "identify_task_type" in forced_task),
            ("åŒ…å«JSONç¤ºä¾‹", '"tool_calls"' in forced_task),
            ("åŒ…å«ä¸¥æ ¼è¦æ±‚", "ðŸš¨ðŸš¨ðŸš¨ ä¸¥æ ¼è¦æ±‚ ðŸš¨ðŸš¨ðŸš¨" in forced_task),
            ("å¼ºè°ƒç«‹å³æ‰§è¡Œ", "âš¡ ç«‹å³æ‰§è¡Œ" in forced_task)
        ]
        
        passed_checks = 0
        for description, check in checks:
            if check:
                print(f"  âœ… {description}")
                passed_checks += 1
            else:
                print(f"  âŒ {description}")
        
        success_rate = passed_checks / len(checks)
        print(f"\nðŸ“Š å¼ºåˆ¶åè°ƒä»»åŠ¡æ£€æŸ¥ç»“æžœ: {passed_checks}/{len(checks)} ({success_rate:.1%})")
        
        return success_rate >= 0.8
        
    except Exception as e:
        print(f"âŒ å¼ºåˆ¶åè°ƒä»»åŠ¡æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_tool_name_detection():
    """æµ‹è¯•å·¥å…·åç§°æ£€æµ‹é€»è¾‘"""
    print("\nðŸ§ª æµ‹è¯•å·¥å…·åç§°æ£€æµ‹é€»è¾‘...")
    
    # æ¨¡æ‹ŸLLMå“åº”
    test_cases = [
        # é”™è¯¯çš„è°ƒç”¨æ–¹å¼ï¼ˆåº”è¯¥è¢«æ£€æµ‹å¹¶é˜»æ­¢ï¼‰
        {
            "input": '{"tool_calls": [{"tool_name": "enhanced_real_verilog_agent", "parameters": {}}]}',
            "description": "ç›´æŽ¥è°ƒç”¨æ™ºèƒ½ä½“åç§°",
            "should_detect": True,
            "is_correct": False
        },
        
        # æ­£ç¡®çš„è°ƒç”¨æ–¹å¼
        {
            "input": '{"tool_calls": [{"tool_name": "assign_task_to_agent", "parameters": {"agent_id": "enhanced_real_verilog_agent"}}]}',
            "description": "æ­£ç¡®ä½¿ç”¨assign_task_to_agent",
            "should_detect": True,
            "is_correct": True
        },
        
        # å…¶ä»–æ­£ç¡®è°ƒç”¨
        {
            "input": '{"tool_calls": [{"tool_name": "identify_task_type", "parameters": {}}]}',
            "description": "è°ƒç”¨identify_task_type",
            "should_detect": True,
            "is_correct": True
        }
    ]
    
    correct_detections = 0
    total_cases = len(test_cases)
    
    try:
        from core.llm_coordinator_agent import LLMCoordinatorAgent
        from config.config import FrameworkConfig
        
        config = FrameworkConfig.from_env()
        coordinator = LLMCoordinatorAgent(config)
        
        for i, case in enumerate(test_cases):
            detected = coordinator._has_executed_tools(case["input"])
            
            print(f"  æµ‹è¯•ç”¨ä¾‹ {i+1}: {case['description']}")
            print(f"    è¾“å…¥: {case['input'][:50]}...")
            print(f"    æ£€æµ‹ç»“æžœ: {detected}")
            print(f"    é¢„æœŸæ£€æµ‹: {case['should_detect']}")
            
            if detected == case["should_detect"]:
                print(f"    âœ… æ£€æµ‹æ­£ç¡®")
                correct_detections += 1
            else:
                print(f"    âŒ æ£€æµ‹é”™è¯¯")
            print()
        
        success_rate = correct_detections / total_cases
        print(f"ðŸ“Š å·¥å…·æ£€æµ‹ç»“æžœ: {correct_detections}/{total_cases} ({success_rate:.1%})")
        
        return success_rate >= 0.8
        
    except Exception as e:
        print(f"âŒ å·¥å…·æ£€æµ‹æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def analyze_system_prompt_strength():
    """åˆ†æžSystem Promptçš„å¼ºåº¦å’Œæ¸…æ™°åº¦"""
    print("\nðŸ§ª åˆ†æžSystem Promptå¼ºåº¦...")
    
    try:
        from core.llm_coordinator_agent import LLMCoordinatorAgent
        from config.config import FrameworkConfig
        
        config = FrameworkConfig.from_env()
        coordinator = LLMCoordinatorAgent(config)
        system_prompt = coordinator._build_enhanced_system_prompt()
        
        # åˆ†æžæŒ‡æ ‡
        metrics = {
            "æ€»é•¿åº¦": len(system_prompt),
            "ç¦æ­¢è¯­å¥æ•°é‡": len(re.findall(r'[âŒðŸš¨].*?(ç¦æ­¢|ç»å¯¹|ä¸è¦|ä¸èƒ½)', system_prompt)),
            "æ­£ç¡®ç¤ºä¾‹æ•°é‡": len(re.findall(r'âœ….*?ç¤ºä¾‹', system_prompt)),
            "é”™è¯¯ç¤ºä¾‹æ•°é‡": len(re.findall(r'âŒ.*?ç¤ºä¾‹', system_prompt)),
            "å¼ºè°ƒæ ‡è®°æ•°é‡": len(re.findall(r'ðŸš¨', system_prompt)),
            "JSONä»£ç å—æ•°é‡": len(re.findall(r'```json', system_prompt))
        }
        
        print("ðŸ“Š System Promptåˆ†æž:")
        for metric, value in metrics.items():
            print(f"  {metric}: {value}")
        
        # è¯„ä¼°å¼ºåº¦
        strength_score = 0
        if metrics["æ€»é•¿åº¦"] > 2000:
            strength_score += 20
        if metrics["ç¦æ­¢è¯­å¥æ•°é‡"] > 5:
            strength_score += 20
        if metrics["æ­£ç¡®ç¤ºä¾‹æ•°é‡"] >= 3:
            strength_score += 20
        if metrics["é”™è¯¯ç¤ºä¾‹æ•°é‡"] >= 1:
            strength_score += 20
        if metrics["å¼ºè°ƒæ ‡è®°æ•°é‡"] > 10:
            strength_score += 20
        
        print(f"\nðŸ“ˆ System Promptå¼ºåº¦è¯„åˆ†: {strength_score}/100")
        
        if strength_score >= 80:
            print("âœ… System Promptå¼ºåº¦å……è¶³")
            return True
        else:
            print("âš ï¸ System Promptå¼ºåº¦ä¸è¶³ï¼Œå»ºè®®è¿›ä¸€æ­¥å¼ºåŒ–")
            return False
            
    except Exception as e:
        print(f"âŒ System Promptåˆ†æžå¤±è´¥: {str(e)}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ðŸš€ å¼€å§‹æµ‹è¯•System Promptä¿®å¤æ•ˆæžœ...\n")
    
    test_results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("System Promptå†…å®¹æµ‹è¯•", test_system_prompt_content),
        ("å¼ºåˆ¶åè°ƒä»»åŠ¡æµ‹è¯•", test_forced_coordination_task),
        ("å·¥å…·åç§°æ£€æµ‹æµ‹è¯•", test_tool_name_detection),
        ("System Promptå¼ºåº¦åˆ†æž", analyze_system_prompt_strength)
    ]
    
    for test_name, test_func in tests:
        print(f"" + "="*50)
        result = test_func()
        test_results.append((test_name, result))
    
    # æ€»ç»“ç»“æžœ
    print("\n" + "="*50)
    print("ðŸ“Š æµ‹è¯•ç»“æžœæ€»ç»“:")
    
    passed_tests = 0
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {test_name}: {status}")
        if result:
            passed_tests += 1
    
    success_rate = passed_tests / len(test_results)
    print(f"\nðŸŽ¯ æ€»ä½“æˆåŠŸçŽ‡: {passed_tests}/{len(test_results)} ({success_rate:.1%})")
    
    if success_rate >= 0.75:
        print("ðŸŽ‰ ä¿®å¤æ•ˆæžœè‰¯å¥½ï¼ŒSystem Promptå·²æ˜¾è‘—å¼ºåŒ–ï¼")
        print("\nå»ºè®®:")
        print("1. é‡æ–°è¿è¡ŒåŽŸå¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
        print("2. ç›‘æŽ§LLMçš„å·¥å…·è°ƒç”¨è¡Œä¸º")
        print("3. å¦‚æœ‰å¿…è¦ï¼Œè¿›ä¸€æ­¥è°ƒæ•´æŒ‡å¯¼è¯­å¥")
    else:
        print("âš ï¸ ä¿®å¤æ•ˆæžœä¸å¤Ÿç†æƒ³ï¼Œå»ºè®®è¿›ä¸€æ­¥æ”¹è¿›")
        print("\nå»ºè®®:")
        print("1. å¢žåŠ æ›´å¤šç¦æ­¢è¯­å¥")
        print("2. æä¾›æ›´å¤šå…·ä½“ç¤ºä¾‹")
        print("3. ä½¿ç”¨æ›´å¼ºçš„è§†è§‰å¼ºè°ƒ")
    
    return success_rate >= 0.75

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)