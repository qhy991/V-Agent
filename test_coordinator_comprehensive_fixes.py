#!/usr/bin/env python3
"""
V-Agentç³»ç»Ÿåè°ƒå™¨ä¿®å¤ç»¼åˆæµ‹è¯•

è¿™ä¸ªæµ‹è¯•è„šæœ¬éªŒè¯åè°ƒå™¨çš„æ‰€æœ‰ä¿®å¤åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. å¤åˆä»»åŠ¡è¯†åˆ«å’Œåˆ†è§£
2. æ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•ŒéªŒè¯ 
3. ä»»åŠ¡åˆ†é…å†²çªæ£€æµ‹
4. æ™ºèƒ½ä½“å¹»è§‰æ£€æµ‹å’Œæ¢å¤
5. ä¸²è¡Œå·¥ä½œæµç®¡ç† (designâ†’reviewâ†’verification)
6. è´¨é‡é—¨æ§æœºåˆ¶
7. ä»»åŠ¡å®Œæˆå¼ºåˆ¶æ£€æŸ¥

åŸºäºæ—¥å¿—æ–‡ä»¶ counter_test_utf8-28.txt ä¸­å‘ç°çš„é—®é¢˜è¿›è¡Œä¿®å¤éªŒè¯
"""

import asyncio
import sys
import logging
import tempfile
import os
from pathlib import Path
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.absolute()))

from core.llm_coordinator_agent import LLMCoordinatorAgent, TaskContext, TaskType
from core.task_file_context import TaskFileContext, set_task_context
from config.config import FrameworkConfig

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æµ‹è¯•æ•°æ®ï¼šå¤æ‚çš„counterè®¾è®¡ï¼ˆæ¨¡æ‹ŸçœŸå®è®¾è®¡æ™ºèƒ½ä½“è¾“å‡ºï¼‰
COMPLEX_COUNTER_DESIGN = """module counter #(
    parameter WIDTH = 8,
    parameter MAX_COUNT = 255,
    parameter MIN_COUNT = 0
)(
    input wire clk,
    input wire rst_n,
    input wire en,
    input wire up,
    input wire load,
    input wire clear,
    input wire [WIDTH-1:0] preset_value,
    output reg [WIDTH-1:0] count,
    output wire full,
    output wire empty,
    output wire carry_out,
    output wire borrow_out,
    output reg overflow,
    output reg underflow
);

// Internal signals
reg [WIDTH-1:0] next_count;
wire at_max, at_min;
reg prev_full, prev_empty;

// Status flags
assign full = (count == MAX_COUNT);
assign empty = (count == MIN_COUNT);
assign at_max = (count == MAX_COUNT);
assign at_min = (count == MIN_COUNT);
assign carry_out = en & up & at_max;
assign borrow_out = en & (~up) & at_min;

// Next state logic
always @(*) begin
    if (!rst_n || clear) begin
        next_count = MIN_COUNT;
    end else if (load) begin
        next_count = preset_value;
    end else if (en) begin
        if (up) begin
            next_count = at_max ? MIN_COUNT : count + 1'b1;
        end else begin
            next_count = at_min ? MAX_COUNT : count - 1'b1;
        end
    end else begin
        next_count = count;
    end
end

// Sequential logic
always @(posedge clk or negedge rst_n) begin
    if (!rst_n) begin
        count <= MIN_COUNT;
        overflow <= 1'b0;
        underflow <= 1'b0;
        prev_full <= 1'b0;
        prev_empty <= 1'b1;
    end else begin
        count <= next_count;
        
        // Edge detection for overflow/underflow
        prev_full <= full;
        prev_empty <= empty;
        overflow <= (~prev_full) & full & en & up;
        underflow <= (~prev_empty) & empty & en & (~up);
    end
end

endmodule"""

class CoordinatorFixesTest:
    """åè°ƒå™¨ä¿®å¤åŠŸèƒ½ç»¼åˆæµ‹è¯•"""
    
    def __init__(self):
        self.config = FrameworkConfig.from_env()
        self.coordinator = None
        self.test_results = {}
        self.temp_files = []
    
    async def setup_coordinator(self):
        """è®¾ç½®åè°ƒå™¨å®ä¾‹"""
        self.coordinator = LLMCoordinatorAgent(self.config)
        logger.info("ğŸš€ åè°ƒå™¨å®ä¾‹å·²åˆ›å»º")
    
    def cleanup_temp_files(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        for temp_file in self.temp_files:
            try:
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {temp_file}: {e}")
    
    async def test_composite_task_detection(self):
        """æµ‹è¯•1: å¤åˆä»»åŠ¡è¯†åˆ«å’Œåˆ†è§£"""
        logger.info("ğŸ§ª æµ‹è¯•1: å¤åˆä»»åŠ¡è¯†åˆ«å’Œåˆ†è§£")
        
        test_cases = [
            {
                "name": "æ ‡å‡†å¤åˆä»»åŠ¡",
                "request": "è®¾è®¡ä¸€ä¸ª8ä½è®¡æ•°å™¨ï¼ŒåŒ…å«å®Œæ•´çš„ç«¯å£å®šä¹‰ã€åŠŸèƒ½å®ç°å’Œæµ‹è¯•å°",
                "expected_composite": True,
                "expected_subtasks": ["design", "verification"]
            },
            {
                "name": "éšå«å¤åˆä»»åŠ¡", 
                "request": "å®ç°counteræ¨¡å—å¹¶éªŒè¯å…¶åŠŸèƒ½",
                "expected_composite": True,
                "expected_subtasks": ["design", "verification"]
            },
            {
                "name": "å•ä¸€è®¾è®¡ä»»åŠ¡",
                "request": "ä»…ç”Ÿæˆcounteræ¨¡å—çš„Verilogä»£ç ",
                "expected_composite": False,
                "expected_subtasks": ["design"]
            }
        ]
        
        results = []
        for case in test_cases:
            try:
                is_composite, details = self.coordinator._detect_composite_task(case["request"])
                
                # éªŒè¯å¤åˆä»»åŠ¡è¯†åˆ«
                assert is_composite == case["expected_composite"], \
                    f"å¤åˆä»»åŠ¡è¯†åˆ«é”™è¯¯: æœŸæœ›{case['expected_composite']}, å®é™…{is_composite}"
                
                # éªŒè¯å­ä»»åŠ¡åˆ†è§£
                if is_composite:
                    subtasks = details.get("subtasks", [])
                    for expected_subtask in case["expected_subtasks"]:
                        assert expected_subtask in subtasks, \
                            f"ç¼ºå¤±å­ä»»åŠ¡ {expected_subtask}: {subtasks}"
                
                results.append({
                    "case": case["name"],
                    "passed": True,
                    "details": details
                })
                
            except Exception as e:
                results.append({
                    "case": case["name"], 
                    "passed": False,
                    "error": str(e)
                })
        
        self.test_results["composite_task_detection"] = results
        logger.info(f"âœ… å¤åˆä»»åŠ¡è¯†åˆ«æµ‹è¯•å®Œæˆ: {len([r for r in results if r['passed']])}/{len(results)} é€šè¿‡")
        return all(r["passed"] for r in results)
    
    async def test_capability_boundary_verification(self):
        """æµ‹è¯•2: æ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•ŒéªŒè¯"""
        logger.info("ğŸ§ª æµ‹è¯•2: æ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•ŒéªŒè¯")
        
        test_cases = [
            {
                "name": "è®¾è®¡æ™ºèƒ½ä½“åˆ†é…æµ‹è¯•å°ä»»åŠ¡ (åº”è¢«é˜»æ­¢)",
                "agent_id": "enhanced_real_verilog_agent",
                "task_type": "verification",
                "task_description": "ç”Ÿæˆcounteræ¨¡å—çš„testbenchæ–‡ä»¶",
                "expected_valid": False,
                "expected_reason_contains": "æµ‹è¯•å°ç”Ÿæˆ"
            },
            {
                "name": "è®¾è®¡æ™ºèƒ½ä½“åˆ†é…è®¾è®¡ä»»åŠ¡ (åº”å…è®¸)",
                "agent_id": "enhanced_real_verilog_agent", 
                "task_type": "design",
                "task_description": "è®¾è®¡counteræ¨¡å—çš„Verilogä»£ç ",
                "expected_valid": True
            },
            {
                "name": "å®¡æŸ¥æ™ºèƒ½ä½“åˆ†é…æµ‹è¯•å°ä»»åŠ¡ (åº”å…è®¸)",
                "agent_id": "enhanced_real_code_review_agent",
                "task_type": "verification", 
                "task_description": "ç”Ÿæˆcounteræ¨¡å—çš„testbenchå¹¶æ‰§è¡Œä»¿çœŸ",
                "expected_valid": True
            }
        ]
        
        results = []
        for case in test_cases:
            try:
                boundary_check = self.coordinator._verify_task_assignment_boundary(
                    case["agent_id"], case["task_type"], case["task_description"]
                )
                
                is_valid = boundary_check["valid"]
                reason = boundary_check.get("reason", "")
                
                # éªŒè¯è¾¹ç•Œæ£€æŸ¥ç»“æœ
                assert is_valid == case["expected_valid"], \
                    f"è¾¹ç•Œæ£€æŸ¥é”™è¯¯: æœŸæœ›valid={case['expected_valid']}, å®é™…valid={is_valid}, åŸå› : {reason}"
                
                # å¦‚æœæœŸæœ›è¢«é˜»æ­¢ï¼Œæ£€æŸ¥é˜»æ­¢åŸå› æ˜¯å¦æ­£ç¡®
                if not case["expected_valid"] and "expected_reason_contains" in case:
                    assert case["expected_reason_contains"] in reason, \
                        f"é˜»æ­¢åŸå› ä¸æ­£ç¡®: æœŸæœ›åŒ…å«'{case['expected_reason_contains']}', å®é™…: {reason}"
                
                results.append({
                    "case": case["name"],
                    "passed": True,
                    "boundary_check": boundary_check
                })
                
            except Exception as e:
                results.append({
                    "case": case["name"],
                    "passed": False, 
                    "error": str(e)
                })
        
        self.test_results["capability_boundary_verification"] = results
        logger.info(f"âœ… èƒ½åŠ›è¾¹ç•ŒéªŒè¯æµ‹è¯•å®Œæˆ: {len([r for r in results if r['passed']])}/{len(results)} é€šè¿‡")
        return all(r["passed"] for r in results)
    
    async def test_hallucination_detection(self):
        """æµ‹è¯•3: æ™ºèƒ½ä½“å¹»è§‰æ£€æµ‹"""
        logger.info("ğŸ§ª æµ‹è¯•3: æ™ºèƒ½ä½“å¹»è§‰æ£€æµ‹")
        
        test_cases = [
            {
                "name": "è®¾è®¡æ™ºèƒ½ä½“å£°ç§°ç”Ÿæˆtestbench (å¹»è§‰)",
                "agent_id": "enhanced_real_verilog_agent",
                "result": {
                    "success": True,
                    "generated_files": ["counter.v", "counter_tb.v"],
                    "status": "success"
                },
                "expected_hallucination": True,
                "expected_type": "capability_boundary_hallucination"
            },
            {
                "name": "è®¾è®¡æ™ºèƒ½ä½“æ­£å¸¸å®Œæˆè®¾è®¡ (æ— å¹»è§‰)",
                "agent_id": "enhanced_real_verilog_agent", 
                "result": {
                    "success": True,
                    "generated_files": ["counter.v"],
                    "status": "success"
                },
                "expected_hallucination": False
            },
            {
                "name": "æ™ºèƒ½ä½“å£°ç§°ç”Ÿæˆä¸å­˜åœ¨çš„æ–‡ä»¶ (å¹»è§‰)",
                "agent_id": "enhanced_real_code_review_agent",
                "result": {
                    "success": True,
                    "generated_files": ["/nonexistent/path/fake_file.v"],
                    "status": "success"
                },
                "expected_hallucination": True,
                "expected_type": "file_existence_hallucination"
            }
        ]
        
        results = []
        for case in test_cases:
            try:
                hallucination_check = self.coordinator._detect_task_hallucination(
                    case["agent_id"], case["result"]
                )
                
                has_hallucination = hallucination_check.get("has_hallucination", False)
                hallucination_type = hallucination_check.get("hallucination_type", "")
                
                # éªŒè¯å¹»è§‰æ£€æµ‹ç»“æœ
                assert has_hallucination == case["expected_hallucination"], \
                    f"å¹»è§‰æ£€æµ‹é”™è¯¯: æœŸæœ›{case['expected_hallucination']}, å®é™…{has_hallucination}"
                
                # å¦‚æœæœŸæœ›æ£€æµ‹åˆ°å¹»è§‰ï¼ŒéªŒè¯å¹»è§‰ç±»å‹
                if case["expected_hallucination"] and "expected_type" in case:
                    assert hallucination_type == case["expected_type"], \
                        f"å¹»è§‰ç±»å‹é”™è¯¯: æœŸæœ›{case['expected_type']}, å®é™…{hallucination_type}"
                
                results.append({
                    "case": case["name"],
                    "passed": True,
                    "hallucination_check": hallucination_check
                })
                
            except Exception as e:
                results.append({
                    "case": case["name"],
                    "passed": False,
                    "error": str(e)
                })
        
        self.test_results["hallucination_detection"] = results
        logger.info(f"âœ… å¹»è§‰æ£€æµ‹æµ‹è¯•å®Œæˆ: {len([r for r in results if r['passed']])}/{len(results)} é€šè¿‡")
        return all(r["passed"] for r in results)
    
    async def test_serial_workflow_management(self):
        """æµ‹è¯•4: ä¸²è¡Œå·¥ä½œæµç®¡ç†"""
        logger.info("ğŸ§ª æµ‹è¯•4: ä¸²è¡Œå·¥ä½œæµç®¡ç†")
        
        # åˆ›å»ºæ¨¡æ‹Ÿä»»åŠ¡ä¸Šä¸‹æ–‡
        task_context = TaskContext(
            task_id="test_workflow_001", 
            original_request="è®¾è®¡counteræ¨¡å—å¹¶ç”Ÿæˆæµ‹è¯•å°éªŒè¯åŠŸèƒ½"
        )
        
        test_cases = [
            {
                "name": "åˆå§‹é˜¶æ®µ - åº”æ¨èè®¾è®¡ä»»åŠ¡",
                "agent_results": {},
                "expected_stage": "initial",
                "expected_next_agent": "enhanced_real_verilog_agent"
            },
            {
                "name": "è®¾è®¡å®Œæˆ - åº”æ¨èå®¡æŸ¥ä»»åŠ¡", 
                "agent_results": {
                    "enhanced_real_verilog_agent": {
                        "success": True,
                        "generated_files": ["counter.v"],
                        "status": "success"
                    }
                },
                "expected_stage": "design_completed",
                "expected_next_agent": "enhanced_real_code_review_agent"
            },
            {
                "name": "ä¸¤é˜¶æ®µå®Œæˆ - åº”å®Œæˆå·¥ä½œæµ",
                "agent_results": {
                    "enhanced_real_verilog_agent": {
                        "success": True,
                        "generated_files": ["counter.v"],
                        "status": "success"
                    },
                    "enhanced_real_code_review_agent": {
                        "success": True,
                        "generated_files": ["counter_tb.v"],
                        "status": "success"
                    }
                },
                "expected_stage": "verification_completed",
                "expected_workflow_complete": True
            }
        ]
        
        results = []
        for case in test_cases:
            try:
                # è®¾ç½®ä»»åŠ¡ä¸Šä¸‹æ–‡
                task_context.agent_results = case["agent_results"]
                
                # æ¨¡æ‹Ÿå¤åˆä»»åŠ¡åˆ†æ
                task_analysis = {
                    "is_composite": True,
                    "subtasks": ["design", "verification"]
                }
                
                # æµ‹è¯•å·¥ä½œæµç®¡ç†
                workflow_result = self.coordinator._manage_serial_workflow(task_context, task_analysis)
                
                # éªŒè¯å·¥ä½œæµé˜¶æ®µ
                current_stage = workflow_result.get("current_stage")
                assert current_stage == case["expected_stage"], \
                    f"å·¥ä½œæµé˜¶æ®µé”™è¯¯: æœŸæœ›{case['expected_stage']}, å®é™…{current_stage}"
                
                # éªŒè¯ä¸‹ä¸€ä¸ªæ™ºèƒ½ä½“æ¨è
                if "expected_next_agent" in case:
                    next_action = workflow_result.get("next_action", {})
                    next_agent = next_action.get("agent_id")
                    assert next_agent == case["expected_next_agent"], \
                        f"ä¸‹ä¸€ä¸ªæ™ºèƒ½ä½“æ¨èé”™è¯¯: æœŸæœ›{case['expected_next_agent']}, å®é™…{next_agent}"
                
                # éªŒè¯å·¥ä½œæµå®ŒæˆçŠ¶æ€
                if "expected_workflow_complete" in case:
                    workflow_complete = workflow_result.get("workflow_complete", False)
                    assert workflow_complete == case["expected_workflow_complete"], \
                        f"å·¥ä½œæµå®ŒæˆçŠ¶æ€é”™è¯¯: æœŸæœ›{case['expected_workflow_complete']}, å®é™…{workflow_complete}"
                
                results.append({
                    "case": case["name"],
                    "passed": True,
                    "workflow_result": workflow_result
                })
                
            except Exception as e:
                results.append({
                    "case": case["name"],
                    "passed": False,
                    "error": str(e)
                })
        
        self.test_results["serial_workflow_management"] = results
        logger.info(f"âœ… ä¸²è¡Œå·¥ä½œæµç®¡ç†æµ‹è¯•å®Œæˆ: {len([r for r in results if r['passed']])}/{len(results)} é€šè¿‡")
        return all(r["passed"] for r in results)
    
    async def test_quality_gate_mechanism(self):
        """æµ‹è¯•5: è´¨é‡é—¨æ§æœºåˆ¶"""
        logger.info("ğŸ§ª æµ‹è¯•5: è´¨é‡é—¨æ§æœºåˆ¶")
        
        # åˆ›å»ºä¸´æ—¶è®¾è®¡æ–‡ä»¶ç”¨äºæµ‹è¯•
        with tempfile.NamedTemporaryFile(mode='w', suffix='.v', delete=False) as f:
            f.write(COMPLEX_COUNTER_DESIGN)
            temp_design_file = f.name
        self.temp_files.append(temp_design_file)
        
        # åˆ›å»ºæ¨¡æ‹Ÿä»»åŠ¡ä¸Šä¸‹æ–‡
        task_context = TaskContext(
            task_id="test_quality_gate_001",
            original_request="è®¾è®¡counteræ¨¡å—å¹¶ç”Ÿæˆæµ‹è¯•å°éªŒè¯åŠŸèƒ½"
        )
        
        test_cases = [
            {
                "name": "é«˜è´¨é‡è®¾è®¡é˜¶æ®µ - åº”é€šè¿‡è´¨é‡é—¨æ§",
                "agent_results": {
                    "enhanced_real_verilog_agent": {
                        "success": True,
                        "result": {
                            "generated_files": [temp_design_file],
                            "status": "success"
                        },
                        "analysis": {
                            "quality_score": 85.0,
                            "hallucination_detected": False
                        }
                    }
                },
                "expected_design_passed": True
            },
            {
                "name": "ä½è´¨é‡è®¾è®¡é˜¶æ®µ - ä¸åº”é€šè¿‡è´¨é‡é—¨æ§",
                "agent_results": {
                    "enhanced_real_verilog_agent": {
                        "success": True,
                        "result": {
                            "generated_files": [],
                            "status": "failed"
                        },
                        "analysis": {
                            "quality_score": 30.0,
                            "hallucination_detected": True
                        }
                    }
                },
                "expected_design_passed": False
            },
            {
                "name": "å®Œæ•´å·¥ä½œæµ - ä¸¤ä¸ªé˜¶æ®µéƒ½é€šè¿‡",
                "agent_results": {
                    "enhanced_real_verilog_agent": {
                        "success": True,
                        "result": {
                            "generated_files": [temp_design_file],
                            "status": "success"
                        },
                        "analysis": {
                            "quality_score": 85.0,
                            "hallucination_detected": False
                        }
                    },
                    "enhanced_real_code_review_agent": {
                        "success": True,
                        "result": {
                            "generated_files": ["counter_tb.v"],
                            "status": "success",
                            "verification": "completed"
                        },
                        "analysis": {
                            "quality_score": 90.0,
                            "hallucination_detected": False
                        }
                    }
                },
                "expected_design_passed": True,
                "expected_verification_passed": True
            }
        ]
        
        results = []
        for case in test_cases:
            try:
                # è®¾ç½®ä»»åŠ¡ä¸Šä¸‹æ–‡
                task_context.agent_results = case["agent_results"]
                
                # æµ‹è¯•è®¾è®¡è´¨é‡é—¨æ§
                design_quality = self.coordinator._evaluate_design_quality_gate(task_context)
                design_passed = design_quality.get("passed", False)
                
                assert design_passed == case["expected_design_passed"], \
                    f"è®¾è®¡è´¨é‡é—¨æ§é”™è¯¯: æœŸæœ›{case['expected_design_passed']}, å®é™…{design_passed}"
                
                # å¦‚æœæœ‰éªŒè¯é˜¶æ®µï¼Œæµ‹è¯•éªŒè¯è´¨é‡é—¨æ§
                if "expected_verification_passed" in case:
                    verification_quality = self.coordinator._evaluate_verification_quality_gate(task_context)
                    verification_passed = verification_quality.get("passed", False)
                    
                    assert verification_passed == case["expected_verification_passed"], \
                        f"éªŒè¯è´¨é‡é—¨æ§é”™è¯¯: æœŸæœ›{case['expected_verification_passed']}, å®é™…{verification_passed}"
                
                results.append({
                    "case": case["name"],
                    "passed": True,
                    "design_quality": design_quality,
                    "verification_quality": verification_quality if "expected_verification_passed" in case else None
                })
                
            except Exception as e:
                results.append({
                    "case": case["name"],
                    "passed": False,
                    "error": str(e)
                })
        
        self.test_results["quality_gate_mechanism"] = results
        logger.info(f"âœ… è´¨é‡é—¨æ§æœºåˆ¶æµ‹è¯•å®Œæˆ: {len([r for r in results if r['passed']])}/{len(results)} é€šè¿‡")
        return all(r["passed"] for r in results)
    
    async def test_task_completion_enforcement(self):
        """æµ‹è¯•6: ä»»åŠ¡å®Œæˆå¼ºåˆ¶æ£€æŸ¥"""
        logger.info("ğŸ§ª æµ‹è¯•6: ä»»åŠ¡å®Œæˆå¼ºåˆ¶æ£€æŸ¥")
        
        test_cases = [
            {
                "name": "ä»»åŠ¡æœªå®Œæˆ - ä½åˆ†æ•°é˜»æ­¢ç»“æŸ",
                "all_results": {
                    "enhanced_real_verilog_agent": {
                        "success": False,
                        "generated_files": []
                    }
                },
                "expected_completion": False,
                "expected_score_range": (0, 30)
            },
            {
                "name": "éƒ¨åˆ†å®Œæˆ - ä¸­ç­‰åˆ†æ•°éœ€è¦ç»§ç»­",
                "all_results": {
                    "enhanced_real_verilog_agent": {
                        "success": True,
                        "generated_files": ["counter.v"]
                    }
                },
                "expected_completion": False,
                "expected_score_range": (40, 70)
            },
            {
                "name": "å®Œå…¨å®Œæˆ - é«˜åˆ†æ•°å…è®¸ç»“æŸ",
                "all_results": {
                    "enhanced_real_verilog_agent": {
                        "success": True,
                        "generated_files": ["counter.v"]
                    },
                    "enhanced_real_code_review_agent": {
                        "success": True, 
                        "generated_files": ["counter_tb.v"]
                    }
                },
                "expected_completion": True,
                "expected_score_range": (80, 100)
            }
        ]
        
        results = []
        for case in test_cases:
            try:
                # åˆ›å»ºä¸´æ—¶ä»»åŠ¡ä¸Šä¸‹æ–‡
                task_id = f"test_completion_{hash(case['name'])}"
                task_context = TaskContext(
                    task_id=task_id,
                    original_request="è®¾è®¡counteræ¨¡å—å¹¶ç”Ÿæˆæµ‹è¯•å°éªŒè¯åŠŸèƒ½"
                )
                task_context.agent_results = case["all_results"]
                self.coordinator.active_tasks[task_id] = task_context
                
                # è°ƒç”¨ä»»åŠ¡å®Œæˆæ£€æŸ¥
                completion_result = await self.coordinator._tool_check_task_completion(
                    task_id=task_id,
                    all_results=case["all_results"], 
                    original_requirements="è®¾è®¡counteræ¨¡å—å¹¶ç”Ÿæˆæµ‹è¯•å°éªŒè¯åŠŸèƒ½"
                )
                
                assert completion_result.get("success", False), \
                    f"ä»»åŠ¡å®Œæˆæ£€æŸ¥å¤±è´¥: {completion_result.get('error')}"
                
                # éªŒè¯å®ŒæˆçŠ¶æ€
                is_completed = completion_result.get("is_completed", False)
                completion_score = completion_result.get("completion_score", 0)
                
                assert is_completed == case["expected_completion"], \
                    f"å®ŒæˆçŠ¶æ€é”™è¯¯: æœŸæœ›{case['expected_completion']}, å®é™…{is_completed}"
                
                # éªŒè¯åˆ†æ•°èŒƒå›´
                min_score, max_score = case["expected_score_range"]
                assert min_score <= completion_score <= max_score, \
                    f"å®Œæˆåˆ†æ•°è¶…å‡ºèŒƒå›´: æœŸæœ›[{min_score}, {max_score}], å®é™…{completion_score}"
                
                results.append({
                    "case": case["name"],
                    "passed": True,
                    "completion_result": completion_result
                })
                
                # æ¸…ç†ä»»åŠ¡ä¸Šä¸‹æ–‡
                if task_id in self.coordinator.active_tasks:
                    del self.coordinator.active_tasks[task_id]
                
            except Exception as e:
                results.append({
                    "case": case["name"],
                    "passed": False,
                    "error": str(e)
                })
        
        self.test_results["task_completion_enforcement"] = results
        logger.info(f"âœ… ä»»åŠ¡å®Œæˆå¼ºåˆ¶æ£€æŸ¥æµ‹è¯•å®Œæˆ: {len([r for r in results if r['passed']])}/{len(results)} é€šè¿‡")
        return all(r["passed"] for r in results)
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹V-Agentåè°ƒå™¨ä¿®å¤åŠŸèƒ½ç»¼åˆæµ‹è¯•")
        
        try:
            await self.setup_coordinator()
            
            tests = [
                ("å¤åˆä»»åŠ¡è¯†åˆ«å’Œåˆ†è§£", self.test_composite_task_detection),
                ("æ™ºèƒ½ä½“èƒ½åŠ›è¾¹ç•ŒéªŒè¯", self.test_capability_boundary_verification), 
                ("æ™ºèƒ½ä½“å¹»è§‰æ£€æµ‹", self.test_hallucination_detection),
                ("ä¸²è¡Œå·¥ä½œæµç®¡ç†", self.test_serial_workflow_management),
                ("è´¨é‡é—¨æ§æœºåˆ¶", self.test_quality_gate_mechanism),
                ("ä»»åŠ¡å®Œæˆå¼ºåˆ¶æ£€æŸ¥", self.test_task_completion_enforcement)
            ]
            
            test_results = []
            for test_name, test_func in tests:
                logger.info(f"\n{'='*60}")
                logger.info(f"å¼€å§‹æµ‹è¯•: {test_name}")
                logger.info(f"{'='*60}")
                
                try:
                    result = await test_func()
                    test_results.append((test_name, result))
                    
                    if result:
                        logger.info(f"âœ… {test_name} - é€šè¿‡")
                    else:
                        logger.error(f"âŒ {test_name} - å¤±è´¥")
                        
                except Exception as e:
                    logger.error(f"âŒ {test_name} - å¼‚å¸¸: {e}")
                    test_results.append((test_name, False))
            
            # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            return self.generate_test_report(test_results)
            
        finally:
            self.cleanup_temp_files()
    
    def generate_test_report(self, test_results):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info(f"\n{'='*60}")
        logger.info("V-Agentåè°ƒå™¨ä¿®å¤åŠŸèƒ½æµ‹è¯•æŠ¥å‘Š") 
        logger.info(f"{'='*60}")
        
        passed = sum(1 for _, result in test_results if result)
        total = len(test_results)
        
        logger.info(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"æ€»è®¡æµ‹è¯•: {total}")
        logger.info(f"é€šè¿‡æµ‹è¯•: {passed}")
        logger.info(f"å¤±è´¥æµ‹è¯•: {total - passed}")
        logger.info(f"é€šè¿‡ç‡: {passed/total*100:.1f}%")
        
        logger.info("\nè¯¦ç»†æµ‹è¯•ç»“æœ:")
        for test_name, result in test_results:
            status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
            logger.info(f"{status} - {test_name}")
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šæ–‡ä»¶
        report_data = {
            "test_timestamp": datetime.now().isoformat(),
            "total_tests": total,
            "passed_tests": passed,
            "failed_tests": total - passed,
            "pass_rate": passed/total*100,
            "test_results": dict(test_results),
            "detailed_results": self.test_results
        }
        
        report_file = f"coordinator_fixes_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nè¯¦ç»†æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        if passed == total:
            logger.info("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼V-Agentåè°ƒå™¨ä¿®å¤åŠŸèƒ½æ­£å¸¸è¿è¡Œã€‚")
            return True
        else:
            logger.error(f"\nâš ï¸ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
            return False

async def main():
    """ä¸»å‡½æ•°"""
    tester = CoordinatorFixesTest()
    success = await tester.run_all_tests()
    
    if success:
        print("\nğŸ¯ V-Agentåè°ƒå™¨ä¿®å¤åŠŸèƒ½æµ‹è¯•å®Œæˆ - æ‰€æœ‰åŠŸèƒ½æ­£å¸¸")
        sys.exit(0)
    else:
        print("\nâš ï¸ V-Agentåè°ƒå™¨ä¿®å¤åŠŸèƒ½æµ‹è¯•å®Œæˆ - å‘ç°é—®é¢˜")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())