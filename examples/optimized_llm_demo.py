#!/usr/bin/env python3
"""
ä¼˜åŒ–LLMè°ƒç”¨æœºåˆ¶æ¼”ç¤ºè„šæœ¬

æ¼”ç¤ºæ™ºèƒ½System Promptç¼“å­˜å’Œä¸Šä¸‹æ–‡ä¼˜åŒ–åŠŸèƒ½
"""

import asyncio
import time
import logging
from typing import Dict, Any, List
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from llm_integration.enhanced_llm_client import EnhancedLLMClient, OptimizedLLMClient
from config.config import LLMConfig
from core.base_agent import BaseAgent
from core.schema_system.enhanced_base_agent import EnhancedBaseAgent


class DemoAgent(BaseAgent):
    """æ¼”ç¤ºç”¨çš„æ™ºèƒ½ä½“"""
    
    def __init__(self):
        super().__init__("demo_agent", "æ¼”ç¤ºæ™ºèƒ½ä½“", set())
        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¼”ç¤ºåŠ©æ‰‹ï¼Œä¸“é—¨ç”¨äºŽå±•ç¤ºLLMè°ƒç”¨ä¼˜åŒ–åŠŸèƒ½ã€‚

ä½ çš„ä¸»è¦èŒè´£ï¼š
1. å›žç­”ç”¨æˆ·é—®é¢˜
2. å±•ç¤ºä¼˜åŒ–æ•ˆæžœ
3. æä¾›æŠ€æœ¯å»ºè®®

è¯·ä¿æŒä¸“ä¸šã€å‹å¥½çš„æ€åº¦ï¼Œå¹¶å°½å¯èƒ½è¯¦ç»†åœ°å›žç­”é—®é¢˜ã€‚"""


class EnhancedDemoAgent(EnhancedBaseAgent):
    """å¢žå¼ºæ¼”ç¤ºæ™ºèƒ½ä½“"""
    
    def __init__(self):
        super().__init__("enhanced_demo_agent", "å¢žå¼ºæ¼”ç¤ºæ™ºèƒ½ä½“", set())
        self._register_enhanced_tools()
    
    def _register_enhanced_tools(self):
        """æ³¨å†Œå¢žå¼ºå·¥å…·"""
        self.register_enhanced_tool(
            name="get_optimization_stats",
            func=self._tool_get_optimization_stats,
            description="èŽ·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯",
            schema={
                "type": "object",
                "properties": {
                    "include_details": {
                        "type": "boolean",
                        "description": "æ˜¯å¦åŒ…å«è¯¦ç»†ä¿¡æ¯",
                        "default": True
                    }
                },
                "required": []
            }
        )
    
    async def _tool_get_optimization_stats(self, include_details: bool = True) -> Dict[str, Any]:
        """èŽ·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯å·¥å…·"""
        stats = self.get_enhanced_optimization_stats()
        if not include_details:
            # åªè¿”å›žå…³é”®æŒ‡æ ‡
            return {
                "cache_hit_rate": stats.get("cache_hit_rate", 0),
                "total_requests": stats.get("total_requests", 0),
                "average_time": stats.get("average_time", 0),
                "token_savings": stats.get("token_savings", 0)
            }
        return stats
    
    async def _call_llm_for_function_calling(self, conversation: List[Dict[str, str]]) -> str:
        """å®žçŽ°LLMè°ƒç”¨"""
        # ä½¿ç”¨ä¼˜åŒ–çš„LLMè°ƒç”¨
        user_message = conversation[-1]["content"] if conversation else ""
        is_first_call = len(conversation) <= 2  # åªæœ‰systemå’Œuseræ¶ˆæ¯æ—¶æ˜¯ç¬¬ä¸€æ¬¡è°ƒç”¨
        
        return await self._call_llm_optimized_with_history(user_message, conversation[:-1], is_first_call)


async def demo_basic_optimization():
    """æ¼”ç¤ºåŸºç¡€ä¼˜åŒ–åŠŸèƒ½"""
    print("ðŸš€ å¼€å§‹åŸºç¡€ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºLLMé…ç½®
    config = LLMConfig(
        provider="openai",
        model_name="gpt-3.5-turbo",
        api_key="your-api-key",  # è¯·æ›¿æ¢ä¸ºå®žé™…çš„APIå¯†é’¥
        api_base_url="https://api.openai.com/v1",
        temperature=0.3,
        max_tokens=4000,
        timeout=30,
        retry_attempts=3,
        retry_delay=1.0
    )
    
    # åˆ›å»ºä¼˜åŒ–çš„LLMå®¢æˆ·ç«¯
    llm_client = EnhancedLLMClient(config)
    
    # æ¼”ç¤ºå¯¹è¯
    conversation_id = "demo_conversation_001"
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œè¯·ç®€æ´åœ°å›žç­”ç”¨æˆ·é—®é¢˜ã€‚"
    
    questions = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "è¯·è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µã€‚",
        "æ·±åº¦å­¦ä¹ ä¸Žä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "è¯·æŽ¨èä¸€äº›å­¦ä¹ AIçš„èµ„æºã€‚"
    ]
    
    print("ðŸ“ å¼€å§‹å¤šè½®å¯¹è¯æ¼”ç¤º...")
    
    for i, question in enumerate(questions, 1):
        print(f"\nðŸ”„ ç¬¬ {i} è½®å¯¹è¯:")
        print(f"ðŸ‘¤ ç”¨æˆ·: {question}")
        
        start_time = time.time()
        
        # ä½¿ç”¨ä¼˜åŒ–çš„LLMè°ƒç”¨
        response = await llm_client.send_prompt_optimized(
            conversation_id=conversation_id,
            user_message=question,
            system_prompt=system_prompt if i == 1 else None,  # åªåœ¨ç¬¬ä¸€è½®ä¼ é€’system prompt
            temperature=0.3,
            max_tokens=1000
        )
        
        duration = time.time() - start_time
        
        print(f"ðŸ¤– åŠ©æ‰‹: {response[:200]}...")
        print(f"â±ï¸ è€—æ—¶: {duration:.2f}ç§’")
    
    # èŽ·å–ä¼˜åŒ–ç»Ÿè®¡
    stats = llm_client.get_optimization_stats()
    print(f"\nðŸ“Š ä¼˜åŒ–ç»Ÿè®¡:")
    print(f"  ç¼“å­˜å‘½ä¸­çŽ‡: {stats['cache_hit_rate']:.1%}")
    print(f"  æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
    print(f"  å¹³å‡å“åº”æ—¶é—´: {stats['average_time']:.2f}ç§’")
    print(f"  ä¸Šä¸‹æ–‡ä¼˜åŒ–æ¬¡æ•°: {stats['context_optimizations']}")
    
    await llm_client.close()


async def demo_agent_optimization():
    """æ¼”ç¤ºæ™ºèƒ½ä½“ä¼˜åŒ–åŠŸèƒ½"""
    print("\nðŸš€ å¼€å§‹æ™ºèƒ½ä½“ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ¼”ç¤ºæ™ºèƒ½ä½“
    agent = DemoAgent()
    
    # æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯ï¼ˆå®žé™…ä½¿ç”¨æ—¶éœ€è¦çœŸå®žçš„LLMé…ç½®ï¼‰
    print("ðŸ“ æ¼”ç¤ºæ™ºèƒ½ä½“ä¼˜åŒ–åŠŸèƒ½...")
    
    # æ¼”ç¤ºå¤šè½®å¯¹è¯
    conversation_id = "agent_demo_001"
    
    requests = [
        "è¯·ä»‹ç»ä¸€ä¸‹ä½ çš„åŠŸèƒ½ã€‚",
        "ä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆï¼Ÿ",
        "è¯·è§£é‡Šä¸€ä¸‹LLMä¼˜åŒ–çš„åŽŸç†ã€‚",
        "ä¼˜åŒ–åŽçš„æ€§èƒ½æå‡å¦‚ä½•ï¼Ÿ",
        "æœ‰ä»€ä¹ˆä½¿ç”¨å»ºè®®å—ï¼Ÿ"
    ]
    
    for i, request in enumerate(requests, 1):
        print(f"\nðŸ”„ ç¬¬ {i} è½®è¯·æ±‚:")
        print(f"ðŸ‘¤ ç”¨æˆ·: {request}")
        
        start_time = time.time()
        
        # ä½¿ç”¨ä¼˜åŒ–çš„Function Calling
        result = await agent.process_with_optimized_function_calling(
            user_request=request,
            conversation_id=conversation_id,
            max_iterations=3,
            enable_self_continuation=False
        )
        
        duration = time.time() - start_time
        
        print(f"ðŸ¤– æ™ºèƒ½ä½“: {result[:200]}...")
        print(f"â±ï¸ è€—æ—¶: {duration:.2f}ç§’")
    
    # èŽ·å–ä¼˜åŒ–ç»Ÿè®¡
    stats = agent.get_llm_optimization_stats()
    print(f"\nðŸ“Š æ™ºèƒ½ä½“ä¼˜åŒ–ç»Ÿè®¡:")
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.3f}")
        else:
            print(f"  {key}: {value}")


async def demo_enhanced_agent_optimization():
    """æ¼”ç¤ºå¢žå¼ºæ™ºèƒ½ä½“ä¼˜åŒ–åŠŸèƒ½"""
    print("\nðŸš€ å¼€å§‹å¢žå¼ºæ™ºèƒ½ä½“ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºå¢žå¼ºæ¼”ç¤ºæ™ºèƒ½ä½“
    agent = EnhancedDemoAgent()
    
    print("ðŸ“ æ¼”ç¤ºå¢žå¼ºæ™ºèƒ½ä½“ä¼˜åŒ–åŠŸèƒ½...")
    
    # æ¼”ç¤ºå·¥å…·è°ƒç”¨
    conversation_id = "enhanced_agent_demo_001"
    
    requests = [
        "è¯·èŽ·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯ã€‚",
        "è¯·èŽ·å–è¯¦ç»†çš„ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯ã€‚",
        "è¯·åˆ†æžå½“å‰çš„æ€§èƒ½è¡¨çŽ°ã€‚",
        "è¯·æä¾›ä¼˜åŒ–å»ºè®®ã€‚"
    ]
    
    for i, request in enumerate(requests, 1):
        print(f"\nðŸ”„ ç¬¬ {i} è½®è¯·æ±‚:")
        print(f"ðŸ‘¤ ç”¨æˆ·: {request}")
        
        start_time = time.time()
        
        # ä½¿ç”¨å¢žå¼ºéªŒè¯å¤„ç†
        result = await agent.process_with_enhanced_validation(
            user_request=request,
            max_iterations=3
        )
        
        duration = time.time() - start_time
        
        if result["success"]:
            print(f"ðŸ¤– æ™ºèƒ½ä½“: {result['response'][:200]}...")
        else:
            print(f"âŒ é”™è¯¯: {result['error']}")
        
        print(f"â±ï¸ è€—æ—¶: {duration:.2f}ç§’")
        print(f"ðŸ”„ è¿­ä»£æ¬¡æ•°: {result['iterations']}")
    
    # èŽ·å–å¢žå¼ºä¼˜åŒ–ç»Ÿè®¡
    stats = agent.get_enhanced_optimization_stats()
    print(f"\nðŸ“Š å¢žå¼ºæ™ºèƒ½ä½“ä¼˜åŒ–ç»Ÿè®¡:")
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.3f}")
        else:
            print(f"  {key}: {value}")


async def demo_performance_comparison():
    """æ¼”ç¤ºæ€§èƒ½å¯¹æ¯”"""
    print("\nðŸš€ å¼€å§‹æ€§èƒ½å¯¹æ¯”æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = LLMConfig(
        provider="openai",
        model_name="gpt-3.5-turbo",
        api_key="your-api-key",  # è¯·æ›¿æ¢ä¸ºå®žé™…çš„APIå¯†é’¥
        api_base_url="https://api.openai.com/v1",
        temperature=0.3,
        max_tokens=4000,
        timeout=30,
        retry_attempts=3,
        retry_delay=1.0
    )
    
    # åˆ›å»ºä¸¤ä¸ªå®¢æˆ·ç«¯è¿›è¡Œå¯¹æ¯”
    standard_client = EnhancedLLMClient(config)
    optimized_client = EnhancedLLMClient(config)
    
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼Œè¯·ç®€æ´åœ°å›žç­”ç”¨æˆ·é—®é¢˜ã€‚"
    test_questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "è¯·è§£é‡Šæœºå™¨å­¦ä¹ ã€‚",
        "æ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
        "è¯·æŽ¨èå­¦ä¹ èµ„æºã€‚"
    ]
    
    print("ðŸ“Š æ€§èƒ½å¯¹æ¯”æµ‹è¯•...")
    
    # æ ‡å‡†æ–¹å¼æµ‹è¯•
    print("\nðŸ”´ æ ‡å‡†æ–¹å¼æµ‹è¯•:")
    standard_times = []
    standard_tokens = 0
    
    for i, question in enumerate(test_questions, 1):
        print(f"  ç¬¬ {i} è½®...")
        start_time = time.time()
        
        response = await standard_client.send_prompt(
            prompt=question,
            system_prompt=system_prompt,  # æ¯æ¬¡éƒ½ä¼ é€’system prompt
            temperature=0.3,
            max_tokens=1000
        )
        
        duration = time.time() - start_time
        standard_times.append(duration)
        standard_tokens += len(question) + len(response)
        
        print(f"    è€—æ—¶: {duration:.2f}ç§’")
    
    # ä¼˜åŒ–æ–¹å¼æµ‹è¯•
    print("\nðŸŸ¢ ä¼˜åŒ–æ–¹å¼æµ‹è¯•:")
    optimized_times = []
    optimized_tokens = 0
    conversation_id = "performance_test_001"
    
    for i, question in enumerate(test_questions, 1):
        print(f"  ç¬¬ {i} è½®...")
        start_time = time.time()
        
        response = await optimized_client.send_prompt_optimized(
            conversation_id=conversation_id,
            user_message=question,
            system_prompt=system_prompt if i == 1 else None,  # åªåœ¨ç¬¬ä¸€è½®ä¼ é€’
            temperature=0.3,
            max_tokens=1000
        )
        
        duration = time.time() - start_time
        optimized_times.append(duration)
        optimized_tokens += len(question) + len(response)
        
        print(f"    è€—æ—¶: {duration:.2f}ç§’")
    
    # è®¡ç®—æ€§èƒ½æå‡
    avg_standard_time = sum(standard_times) / len(standard_times)
    avg_optimized_time = sum(optimized_times) / len(optimized_times)
    
    time_improvement = ((avg_standard_time - avg_optimized_time) / avg_standard_time) * 100
    token_savings = ((standard_tokens - optimized_tokens) / standard_tokens) * 100
    
    print(f"\nðŸ“ˆ æ€§èƒ½å¯¹æ¯”ç»“æžœ:")
    print(f"  æ ‡å‡†æ–¹å¼å¹³å‡è€—æ—¶: {avg_standard_time:.2f}ç§’")
    print(f"  ä¼˜åŒ–æ–¹å¼å¹³å‡è€—æ—¶: {avg_optimized_time:.2f}ç§’")
    print(f"  æ—¶é—´æå‡: {time_improvement:.1f}%")
    print(f"  TokenèŠ‚çœ: {token_savings:.1f}%")
    
    # èŽ·å–ä¼˜åŒ–ç»Ÿè®¡
    opt_stats = optimized_client.get_optimization_stats()
    print(f"  ç¼“å­˜å‘½ä¸­çŽ‡: {opt_stats['cache_hit_rate']:.1%}")
    print(f"  ä¸Šä¸‹æ–‡ä¼˜åŒ–æ¬¡æ•°: {opt_stats['context_optimizations']}")
    
    await standard_client.close()
    await optimized_client.close()


async def main():
    """ä¸»å‡½æ•°"""
    print("ðŸŽ¯ LLMè°ƒç”¨ä¼˜åŒ–æœºåˆ¶æ¼”ç¤º")
    print("=" * 80)
    
    try:
        # æ¼”ç¤ºåŸºç¡€ä¼˜åŒ–åŠŸèƒ½
        await demo_basic_optimization()
        
        # æ¼”ç¤ºæ™ºèƒ½ä½“ä¼˜åŒ–åŠŸèƒ½
        await demo_agent_optimization()
        
        # æ¼”ç¤ºå¢žå¼ºæ™ºèƒ½ä½“ä¼˜åŒ–åŠŸèƒ½
        await demo_enhanced_agent_optimization()
        
        # æ¼”ç¤ºæ€§èƒ½å¯¹æ¯”
        await demo_performance_comparison()
        
        print("\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nðŸ“‹ ä¼˜åŒ–åŠŸèƒ½æ€»ç»“:")
        print("  ðŸš€ æ™ºèƒ½System Promptç¼“å­˜")
        print("  ðŸ—œï¸ ä¸Šä¸‹æ–‡åŽ‹ç¼©ä¼˜åŒ–")
        print("  ðŸ“Š è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡")
        print("  ðŸ”„ è‡ªåŠ¨ç¼“å­˜ç®¡ç†")
        print("  ðŸ’° Tokenä½¿ç”¨ä¼˜åŒ–")
        print("  âš¡ å“åº”é€Ÿåº¦æå‡")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        logging.exception("æ¼”ç¤ºé”™è¯¯è¯¦æƒ…")


if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main()) 