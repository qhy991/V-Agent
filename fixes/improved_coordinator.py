#!/usr/bin/env python3
"""
æ”¹è¿›çš„LLMåè°ƒæ™ºèƒ½ä½“å®ç°
"""

import json
import time
import logging
import re
from typing import Dict, Any, List, Optional, Set
from enum import Enum

# å¯¼å…¥ä¿®å¤æ¨¡å—
from fixes.improved_tool_detection import ImprovedToolDetection
from fixes.dynamic_system_prompt import DynamicSystemPromptGenerator

class CoordinatorState(Enum):
    """åè°ƒå™¨çŠ¶æ€"""
    INITIALIZING = "initializing"
    ANALYZING_TASK = "analyzing_task"
    ASSIGNING_TASK = "assigning_task"
    MONITORING_EXECUTION = "monitoring_execution"
    ANALYZING_RESULTS = "analyzing_results"
    PROVIDING_ANSWER = "providing_answer"
    COMPLETED = "completed"
    ERROR = "error"

class ImprovedLLMCoordinator:
    """æ”¹è¿›çš„LLMåè°ƒæ™ºèƒ½ä½“"""
    
    def __init__(self, config=None):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.tool_detector = ImprovedToolDetection()
        self.prompt_generator = DynamicSystemPromptGenerator()
        
        # çŠ¶æ€ç®¡ç†
        self.current_state = CoordinatorState.INITIALIZING
        self.task_context = {}
        self.registered_agents = {}
        self.available_tools = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_tasks": 0,
            "successful_tasks": 0,
            "failed_tasks": 0,
            "tool_call_successes": 0,
            "tool_call_failures": 0
        }
        
        self.logger.info("ğŸš€ æ”¹è¿›çš„LLMåè°ƒæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def register_agent(self, agent_id: str, agent_info: Any):
        """æ³¨å†Œæ™ºèƒ½ä½“"""
        self.registered_agents[agent_id] = agent_info
        self.logger.info(f"âœ… æ³¨å†Œæ™ºèƒ½ä½“: {agent_id}")
    
    def register_tool(self, tool_name: str, tool_info: Dict[str, Any]):
        """æ³¨å†Œå·¥å…·"""
        self.available_tools[tool_name] = tool_info
        self.logger.info(f"ğŸ”§ æ³¨å†Œå·¥å…·: {tool_name}")
    
    async def coordinate_task(self, user_request: str, **kwargs) -> Dict[str, Any]:
        """
        åè°ƒä»»åŠ¡æ‰§è¡Œ - æ”¹è¿›ç‰ˆæœ¬
        
        ä¸»è¦æ”¹è¿›ï¼š
        1. æ›´robustçš„å·¥å…·è°ƒç”¨æ£€æµ‹
        2. æ›´å¥½çš„é”™è¯¯æ¢å¤æœºåˆ¶
        3. çŠ¶æ€è·Ÿè¸ªå’Œç›‘æ§
        4. åŠ¨æ€System Promptç”Ÿæˆ
        """
        task_id = f"task_{int(time.time())}"
        self.current_state = CoordinatorState.ANALYZING_TASK
        
        try:
            self.logger.info(f"ğŸ¯ å¼€å§‹åè°ƒä»»åŠ¡: {task_id}")
            self.stats["total_tasks"] += 1
            
            # åˆå§‹åŒ–ä»»åŠ¡ä¸Šä¸‹æ–‡
            self.task_context = {
                "task_id": task_id,
                "user_request": user_request,
                "start_time": time.time(),
                "state_history": [CoordinatorState.ANALYZING_TASK],
                "tool_calls": [],
                "agent_results": {},
                "iteration_count": 0,
                "max_iterations": kwargs.get("max_iterations", 10)
            }
            
            # ç”ŸæˆåŠ¨æ€System Prompt
            system_prompt = self.prompt_generator.generate_coordination_prompt(
                self.available_tools, 
                self.registered_agents
            )
            
            # éªŒè¯System Promptä¸€è‡´æ€§
            validation = self.prompt_generator.validate_prompt_consistency(
                system_prompt, 
                self.available_tools
            )
            
            if not validation["is_consistent"]:
                self.logger.warning(f"âš ï¸ System Promptä¸€è‡´æ€§é—®é¢˜: {validation['issues']}")
            
            # æ‰§è¡Œåè°ƒå¾ªç¯
            result = await self._execute_coordination_loop(
                user_request, 
                system_prompt,
                **kwargs
            )
            
            # æ›´æ–°ç»Ÿè®¡
            if result.get("success", False):
                self.stats["successful_tasks"] += 1
                self.current_state = CoordinatorState.COMPLETED
            else:
                self.stats["failed_tasks"] += 1
                self.current_state = CoordinatorState.ERROR
            
            self.logger.info(f"âœ… ä»»åŠ¡åè°ƒå®Œæˆ: {task_id}, æˆåŠŸ: {result.get('success', False)}")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ åè°ƒä»»åŠ¡å¤±è´¥: {str(e)}")
            self.current_state = CoordinatorState.ERROR
            self.stats["failed_tasks"] += 1
            
            return {
                "success": False,
                "error": str(e),
                "task_id": task_id,
                "debug_info": {
                    "state": self.current_state.value,
                    "task_context": self.task_context
                }
            }
    
    async def _execute_coordination_loop(self, user_request: str, system_prompt: str, **kwargs) -> Dict[str, Any]:
        """æ‰§è¡Œåè°ƒå¾ªç¯"""
        max_iterations = self.task_context["max_iterations"]
        
        for iteration in range(max_iterations):
            self.task_context["iteration_count"] = iteration + 1
            self.logger.info(f"ğŸ”„ åè°ƒå¾ªç¯ç¬¬ {iteration + 1}/{max_iterations} æ¬¡è¿­ä»£")
            
            # æ„å»ºå½“å‰çš„åè°ƒè¯·æ±‚
            coordination_request = self._build_coordination_request(
                user_request, 
                iteration == 0
            )
            
            # è·å–LLMå“åº”
            llm_response = await self._get_llm_response(
                system_prompt,
                coordination_request,
                **kwargs
            )
            
            # æ”¹è¿›çš„å·¥å…·è°ƒç”¨æ£€æµ‹
            if not self.tool_detector.has_executed_tools(llm_response):
                self.logger.warning(f"âš ï¸ ç¬¬{iteration + 1}æ¬¡è¿­ä»£æœªæ£€æµ‹åˆ°æœ‰æ•ˆå·¥å…·è°ƒç”¨")
                
                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡è¿­ä»£ï¼Œå°è¯•å¼ºåˆ¶æ‰§è¡Œ
                if iteration == max_iterations - 1:
                    forced_result = await self._force_tool_execution(user_request)
                    if forced_result:
                        return forced_result
                
                continue
            
            # æå–å’Œæ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_calls = self.tool_detector.extract_tool_calls(llm_response)
            if not tool_calls:
                self.logger.warning(f"âš ï¸ æ— æ³•æå–å·¥å…·è°ƒç”¨: {llm_response[:200]}...")
                continue
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            execution_results = await self._execute_tool_calls(tool_calls)
            
            # åˆ†ææ‰§è¡Œç»“æœ
            analysis_result = self._analyze_execution_results(execution_results)
            
            # æ£€æŸ¥æ˜¯å¦å®Œæˆä»»åŠ¡
            if analysis_result.get("task_completed", False):
                return {
                    "success": True,
                    "task_id": self.task_context["task_id"],
                    "results": execution_results,
                    "analysis": analysis_result,
                    "iterations": iteration + 1
                }
            
            # æ›´æ–°ä»»åŠ¡ä¸Šä¸‹æ–‡
            self.task_context["tool_calls"].extend(tool_calls)
            self.task_context["agent_results"].update(execution_results)
        
        # æ‰€æœ‰è¿­ä»£å®Œæˆä½†ä»»åŠ¡æœªå®Œæˆ
        return {
            "success": False,
            "error": "è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ä½†ä»»åŠ¡æœªå®Œæˆ",
            "task_id": self.task_context["task_id"],
            "partial_results": self.task_context["agent_results"],
            "iterations": max_iterations
        }
    
    def _build_coordination_request(self, user_request: str, is_first_iteration: bool) -> str:
        """æ„å»ºåè°ƒè¯·æ±‚"""
        if is_first_iteration:
            return f"""
ğŸ¯ æ–°ä»»åŠ¡åè°ƒ

**ç”¨æˆ·éœ€æ±‚**: {user_request}

**ä»»åŠ¡ID**: {self.task_context['task_id']}
**å½“å‰çŠ¶æ€**: {self.current_state.value}

è¯·åˆ†æä»»åŠ¡éœ€æ±‚å¹¶å¼€å§‹åè°ƒæ‰§è¡Œã€‚é¦–å…ˆè°ƒç”¨é€‚å½“çš„å·¥å…·æ¥è¯†åˆ«ä»»åŠ¡ç±»å‹ã€‚
"""
        else:
            # åç»­è¿­ä»£ï¼ŒåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡
            previous_results = self._summarize_previous_results()
            return f"""
ğŸ”„ ç»§ç»­ä»»åŠ¡åè°ƒ

**åŸå§‹éœ€æ±‚**: {user_request}
**ä»»åŠ¡ID**: {self.task_context['task_id']}
**å½“å‰è¿­ä»£**: {self.task_context['iteration_count']}
**å½“å‰çŠ¶æ€**: {self.current_state.value}

**å‰æœŸæ‰§è¡Œæƒ…å†µ**:
{previous_results}

è¯·åŸºäºå‰æœŸç»“æœç»§ç»­åè°ƒä»»åŠ¡æ‰§è¡Œã€‚
"""
    
    def _summarize_previous_results(self) -> str:
        """æ€»ç»“å‰æœŸæ‰§è¡Œç»“æœ"""
        if not self.task_context["agent_results"]:
            return "æš‚æ— æ‰§è¡Œç»“æœ"
        
        summary = []
        for agent_id, result in self.task_context["agent_results"].items():
            success = result.get("success", False)
            status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
            summary.append(f"- {agent_id}: {status}")
            
            if not success and "error" in result:
                summary.append(f"  é”™è¯¯: {result['error'][:100]}...")
        
        return "\n".join(summary)
    
    async def _get_llm_response(self, system_prompt: str, user_request: str, **kwargs) -> str:
        """è·å–LLMå“åº”ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„LLMå®¢æˆ·ç«¯
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬è¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿå“åº”
        
        self.logger.info("ğŸ¤– å‘é€è¯·æ±‚åˆ°LLM...")
        
        # æ¨¡æ‹ŸLLMå“åº”æ—¶é—´
        await asyncio.sleep(1)
        
        # è¿”å›æ¨¡æ‹Ÿçš„å·¥å…·è°ƒç”¨å“åº”
        return json.dumps({
            "tool_calls": [
                {
                    "tool_name": "identify_task_type",
                    "parameters": {
                        "user_request": user_request
                    }
                }
            ]
        })
    
    async def _execute_tool_calls(self, tool_calls: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
        results = {}
        
        for tool_call in tool_calls:
            tool_name = tool_call.get("tool_name")
            parameters = tool_call.get("parameters", {})
            
            try:
                self.logger.info(f"ğŸ”§ æ‰§è¡Œå·¥å…·: {tool_name}")
                
                # æ‰§è¡Œå·¥å…·ï¼ˆè¿™é‡Œéœ€è¦å®é™…çš„å·¥å…·æ‰§è¡Œé€»è¾‘ï¼‰
                result = await self._execute_single_tool(tool_name, parameters)
                results[tool_name] = result
                
                if result.get("success", False):
                    self.stats["tool_call_successes"] += 1
                else:
                    self.stats["tool_call_failures"] += 1
                
            except Exception as e:
                self.logger.error(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥ {tool_name}: {str(e)}")
                results[tool_name] = {
                    "success": False,
                    "error": str(e)
                }
                self.stats["tool_call_failures"] += 1
        
        return results
    
    async def _execute_single_tool(self, tool_name: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªå·¥å…·ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„å·¥å…·å®ç°
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬è¿”å›æ¨¡æ‹Ÿç»“æœ
        
        if tool_name == "identify_task_type":
            return {
                "success": True,
                "task_type": "design",
                "confidence": 0.9,
                "suggested_agent": "enhanced_real_verilog_agent"
            }
        elif tool_name == "assign_task_to_agent":
            return {
                "success": True,
                "agent_id": parameters.get("agent_id"),
                "task_assigned": True
            }
        else:
            return {
                "success": False,
                "error": f"æœªçŸ¥å·¥å…·: {tool_name}"
            }
    
    def _analyze_execution_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ‰§è¡Œç»“æœ"""
        analysis = {
            "task_completed": False,
            "success_count": 0,
            "failure_count": 0,
            "next_action": "continue",
            "recommendations": []
        }
        
        for tool_name, result in results.items():
            if result.get("success", False):
                analysis["success_count"] += 1
            else:
                analysis["failure_count"] += 1
        
        # ç®€å•çš„å®Œæˆæ¡ä»¶æ£€æŸ¥
        if analysis["success_count"] > 0 and analysis["failure_count"] == 0:
            # å¦‚æœæ‰€æœ‰å·¥å…·è°ƒç”¨éƒ½æˆåŠŸï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥æ›´å¤šæ¡ä»¶
            if "assign_task_to_agent" in results:
                analysis["task_completed"] = True
                analysis["next_action"] = "provide_final_answer"
        
        return analysis
    
    async def _force_tool_execution(self, user_request: str) -> Optional[Dict[str, Any]]:
        """å¼ºåˆ¶å·¥å…·æ‰§è¡Œï¼ˆæœ€åçš„å°è¯•ï¼‰"""
        self.logger.warning("ğŸš¨ å°è¯•å¼ºåˆ¶å·¥å…·æ‰§è¡Œ")
        
        # æ„å»ºæœ€ç®€å•çš„å·¥å…·è°ƒç”¨
        forced_tool_call = {
            "tool_name": "identify_task_type",
            "parameters": {
                "user_request": user_request
            }
        }
        
        try:
            result = await self._execute_single_tool(
                forced_tool_call["tool_name"],
                forced_tool_call["parameters"]
            )
            
            if result.get("success", False):
                return {
                    "success": True,
                    "task_id": self.task_context["task_id"],
                    "results": {"forced_execution": result},
                    "note": "é€šè¿‡å¼ºåˆ¶æ‰§è¡Œå®Œæˆ"
                }
        except Exception as e:
            self.logger.error(f"âŒ å¼ºåˆ¶æ‰§è¡Œä¹Ÿå¤±è´¥: {str(e)}")
        
        return None
    
    def get_status_report(self) -> Dict[str, Any]:
        """è·å–çŠ¶æ€æŠ¥å‘Š"""
        return {
            "current_state": self.current_state.value,
            "task_context": self.task_context,
            "registered_agents": len(self.registered_agents),
            "available_tools": len(self.available_tools),
            "statistics": self.stats,
            "health_score": self._calculate_health_score()
        }
    
    def _calculate_health_score(self) -> float:
        """è®¡ç®—ç³»ç»Ÿå¥åº·åˆ†æ•°"""
        total_tasks = self.stats["total_tasks"]
        if total_tasks == 0:
            return 1.0
        
        success_rate = self.stats["successful_tasks"] / total_tasks
        tool_call_total = self.stats["tool_call_successes"] + self.stats["tool_call_failures"]
        
        if tool_call_total == 0:
            tool_success_rate = 0.5
        else:
            tool_success_rate = self.stats["tool_call_successes"] / tool_call_total
        
        # ç»¼åˆè¯„åˆ†
        health_score = (success_rate * 0.7) + (tool_success_rate * 0.3)
        return health_score


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async def demonstrate_improved_coordinator():
    """æ¼”ç¤ºæ”¹è¿›çš„åè°ƒå™¨ä½¿ç”¨æ–¹æ³•"""
    import asyncio
    
    # åˆ›å»ºåè°ƒå™¨
    coordinator = ImprovedLLMCoordinator()
    
    # æ³¨å†Œæ¨¡æ‹Ÿæ™ºèƒ½ä½“
    coordinator.register_agent("enhanced_real_verilog_agent", {
        "specialty": "Verilog HDLè®¾è®¡",
        "capabilities": ["code_generation", "module_design"],
        "status": type('Status', (), {'value': 'idle'})()
    })
    
    # æ³¨å†Œæ¨¡æ‹Ÿå·¥å…·
    coordinator.register_tool("identify_task_type", {
        "name": "identify_task_type",
        "description": "è¯†åˆ«ä»»åŠ¡ç±»å‹"
    })
    
    coordinator.register_tool("assign_task_to_agent", {
        "name": "assign_task_to_agent", 
        "description": "åˆ†é…ä»»åŠ¡ç»™æ™ºèƒ½ä½“"
    })
    
    # æ‰§è¡Œä»»åŠ¡åè°ƒ
    result = await coordinator.coordinate_task(
        "è®¾è®¡ä¸€ä¸ª4ä½è®¡æ•°å™¨æ¨¡å—",
        max_iterations=3
    )
    
    print("åè°ƒç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
    
    # è·å–çŠ¶æ€æŠ¥å‘Š
    status = coordinator.get_status_report()
    print("\nçŠ¶æ€æŠ¥å‘Š:")
    print(json.dumps(status, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    import asyncio
    asyncio.run(demonstrate_improved_coordinator())