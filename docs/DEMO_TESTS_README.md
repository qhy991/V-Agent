# 🎭 多智能体架构演示测试套件

## 📋 测试概述

本测试套件专门设计用于展示你的**CentralizedAgentFramework**的核心能力，包括：

- 🧠 **协调器智能选择**: 根据任务类型智能选择最适合的智能体
- 🤝 **多智能体协作**: 智能体间的无缝协作和任务传递
- 🔧 **复杂工具调用**: 多步骤工具调用链的执行和协调
- 🚨 **智能错误处理**: 基于LLM的错误分析、修复和恢复
- 📁 **文件协作**: 智能体间的文件传递和数据共享

## 🚀 快速开始

### 选项1: 快速演示 (推荐)
```bash
python run_demo_tests.py
```
⏱️ **耗时**: 约3-5分钟  
🎯 **展示**: 所有核心功能的快速演示

### 选项2: 分项测试

#### 1. 协调器智能选择测试
```bash
python test_coordinator_agent_selection.py
```
- 🎯 **测试内容**: 协调器根据任务类型智能选择智能体
- ⏱️ **耗时**: 约2-3分钟
- 📊 **评估指标**: 协作能力评分、任务分配准确性

#### 2. 高级工具交互测试
```bash
python test_advanced_tool_interaction.py
```
- 🔧 **测试内容**: 复杂工具调用链、错误注入与恢复
- ⏱️ **耗时**: 约3-5分钟  
- 📊 **评估指标**: 工具调用次数、错误恢复率

#### 3. 完整RISC-V项目测试
```bash
python test_multi_agent_riscv_project.py
```
- 🏗️ **测试内容**: 完整的RISC-V CPU设计与验证流程
- ⏱️ **耗时**: 约5-10分钟
- 📊 **评估指标**: 项目完成度、协作效果

## 📊 测试场景详解

### 🧠 场景1: 协调器智能调度

**测试内容**:
- 设计任务 → 自动选择VerilogAgent
- 审查任务 → 自动选择ReviewAgent  
- 复杂任务 → 智能分解并分配给多个智能体

**关键能力**:
- 任务类型识别
- 智能体能力匹配
- 自动任务分解

### 🔧 场景2: 复杂工具调用链

**测试流程**:
```
读取配置 → 创建文件 → 设计模块 → 生成测试 → 执行仿真 → 构建脚本 → 编译验证
```

**关键能力**:
- 8步工具调用链
- 步骤间数据传递
- 失败时智能回退

### 🚨 场景3: 错误注入与恢复

**错误类型**:
- 文件不存在错误
- Verilog语法错误
- 编译错误
- 路径错误

**修复策略**:
- LLM错误分析
- 针对性修复建议
- 迭代修复验证
- 智能重试机制

### 🏗️ 场景4: RISC-V项目协作

**项目阶段**:
1. **架构设计**: 协调器分析需求，VerilogAgent设计架构
2. **模块开发**: 多智能体并行开发核心模块
3. **交叉审查**: ReviewAgent审查设计，提供优化建议
4. **集成测试**: 智能体协作进行系统级验证
5. **错误修复**: 发现问题时的智能修复流程

## 📈 评估指标

### 🤝 协作能力评分
- **优秀 (80-100%)**: 智能体选择准确，协作流畅
- **良好 (60-79%)**: 基础协作正常，有优化空间
- **需要改进 (<60%)**: 协作机制存在问题

### 🔧 工具交互能力
- **工具调用次数**: 评估工具使用的活跃度
- **调用成功率**: 评估工具执行的可靠性
- **数据传递效果**: 评估步骤间的数据流

### 🚨 错误处理能力
- **错误检测率**: 能否准确识别各类错误
- **修复成功率**: 错误修复的成功比例
- **恢复时间**: 从错误到恢复的平均时间

## 📁 输出文件说明

### 🗂️ 日志目录结构
```
logs/experiment_YYYYMMDD_HHMMSS/
├── artifacts/                 # 生成的工件
│   ├── *.v                   # Verilog模块文件
│   ├── *_tb.v                # 测试台文件
│   ├── *.md                  # 文档文件
│   ├── *.json                # 配置文件
│   └── scripts/              # 构建脚本
├── experiment_summary.log     # 实验总结日志
├── all_errors.log            # 错误汇总日志
└── *.log                     # 各组件详细日志
```

### 📄 关键输出文件
- **Verilog模块**: CPU核心、ALU、寄存器文件等
- **测试台**: 对应的功能验证测试台
- **文档**: 设计规格、测试报告、分析结果
- **脚本**: Makefile、构建脚本等

## 🛠️ 故障排除

### 常见问题

#### 1. LLM连接失败
```bash
# 检查.env文件配置
cat .env
# 确保API密钥正确设置
```

#### 2. 工具依赖缺失
```bash
# 安装Verilog仿真工具 (可选)
sudo apt-get install iverilog
# 或使用Docker环境
```

#### 3. 权限问题
```bash
# 确保有写入权限
chmod +w logs/
chmod +w output/
```

### 调试建议

1. **查看详细日志**: 检查`experiment_summary.log`了解执行流程
2. **检查错误日志**: 查看`all_errors.log`定位问题
3. **验证配置**: 确认`.env`文件中的LLM配置正确
4. **逐步测试**: 从简单的协调器测试开始

## 🎯 预期结果

### 成功标准
- ✅ **协作评分 > 75%**: 智能体协作流畅
- ✅ **工具调用 > 10次**: 工具交互活跃
- ✅ **错误恢复率 > 80%**: 错误处理智能
- ✅ **文件生成 > 5个**: 实际产出丰富

### 典型输出示例
```
📊 综合测试结果
⏱️ 总执行时间: 180.5秒
✅ 成功率: 87.5% (7/8)
🔧 工具调用次数: 23
📁 文件生成总数: 12
🚨 错误恢复率: 85.7%
🔄 迭代优化次数: 4

🌟 综合评级: 优秀
   工具交互能力强，错误处理智能，协作效果好
```

## 🔮 扩展测试

你可以基于这些测试模板创建自己的测试场景：

### 自定义测试场景
```python
async def test_custom_scenario():
    """自定义测试场景"""
    custom_task = """
    你的自定义任务描述...
    """
    
    result = await agent.process_with_function_calling(
        custom_task, 
        max_iterations=10
    )
    
    return analyze_result(result)
```

### 添加新的评估指标
- 代码质量评分
- 性能效率评估  
- 创新性评价
- 用户体验评分

---

## 🏆 总结

这套测试充分展示了你的**CentralizedAgentFramework**的核心优势：

1. **智能化**: 协调器能够智能分析任务并选择最佳智能体
2. **协作性**: 多智能体间无缝协作，任务传递流畅
3. **健壮性**: 强大的错误处理和恢复机制
4. **实用性**: 能够完成复杂的实际工程任务
5. **可扩展性**: 易于添加新智能体和新工具

通过这些测试，你可以向其他人清晰地展示你的多智能体架构的技术实力和实际价值！🚀