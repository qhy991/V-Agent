# LLMè°ƒç”¨ä¼˜åŒ–æœºåˆ¶ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»V-Agentæ¡†æ¶ä¸­çš„LLMè°ƒç”¨ä¼˜åŒ–æœºåˆ¶ï¼Œè¯¥æœºåˆ¶é€šè¿‡æ™ºèƒ½ç¼“å­˜å’Œä¸Šä¸‹æ–‡ç®¡ç†æ˜¾è‘—æå‡å¤šè½®å¯¹è¯çš„æ€§èƒ½å’Œæˆæœ¬æ•ˆç›Šã€‚

## ğŸš€ æ ¸å¿ƒä¼˜åŒ–åŠŸèƒ½

### 1. æ™ºèƒ½System Promptç¼“å­˜
- **åŠŸèƒ½**: è‡ªåŠ¨ç¼“å­˜system promptï¼Œé¿å…é‡å¤ä¼ é€’
- **æ•ˆæœ**: å‡å°‘30-50%çš„tokenæ¶ˆè€—
- **é€‚ç”¨åœºæ™¯**: å¤šè½®å¯¹è¯ä¸­system promptä¸å˜çš„æƒ…å†µ

### 2. ä¸Šä¸‹æ–‡å‹ç¼©ä¼˜åŒ–
- **åŠŸèƒ½**: æ™ºèƒ½å‹ç¼©å¯¹è¯å†å²ï¼Œä¿æŒå…³é”®ä¿¡æ¯
- **æ•ˆæœ**: æå‡15-25%çš„å“åº”é€Ÿåº¦
- **é€‚ç”¨åœºæ™¯**: é•¿å¯¹è¯å†å²çš„æƒ…å†µ

### 3. è‡ªåŠ¨ç¼“å­˜ç®¡ç†
- **åŠŸèƒ½**: è‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜ï¼Œç®¡ç†å†…å­˜ä½¿ç”¨
- **æ•ˆæœ**: é¿å…å†…å­˜æ³„æ¼ï¼Œä¿æŒç³»ç»Ÿç¨³å®š
- **é€‚ç”¨åœºæ™¯**: é•¿æ—¶é—´è¿è¡Œçš„ç³»ç»Ÿ

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### åŸºç¡€ç”¨æ³•

#### 1. ç›´æ¥ä½¿ç”¨ä¼˜åŒ–çš„LLMå®¢æˆ·ç«¯

```python
from llm_integration.enhanced_llm_client import EnhancedLLMClient
from config.config import LLMConfig

# åˆ›å»ºé…ç½®
config = LLMConfig(
    provider="openai",
    model_name="gpt-3.5-turbo",
    api_key="your-api-key",
    api_base_url="https://api.openai.com/v1"
)

# åˆ›å»ºå®¢æˆ·ç«¯
llm_client = EnhancedLLMClient(config)

# ä½¿ç”¨ä¼˜åŒ–çš„è°ƒç”¨æ–¹æ³•
conversation_id = "my_conversation_001"
system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ã€‚"

# ç¬¬ä¸€è½®å¯¹è¯ï¼ˆåŒ…å«system promptï¼‰
response1 = await llm_client.send_prompt_optimized(
    conversation_id=conversation_id,
    user_message="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚",
    system_prompt=system_prompt,  # åªåœ¨ç¬¬ä¸€è½®ä¼ é€’
    temperature=0.3,
    max_tokens=1000
)

# åç»­å¯¹è¯ï¼ˆä¸åŒ…å«system promptï¼‰
response2 = await llm_client.send_prompt_optimized(
    conversation_id=conversation_id,
    user_message="è¯·è§£é‡Šä¸€ä¸‹äººå·¥æ™ºèƒ½ã€‚",
    # system_prompt=None,  # è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜çš„system prompt
    temperature=0.3,
    max_tokens=1000
)

# è·å–ä¼˜åŒ–ç»Ÿè®¡
stats = llm_client.get_optimization_stats()
print(f"ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1%}")
```

#### 2. åœ¨BaseAgentä¸­ä½¿ç”¨

```python
from core.base_agent import BaseAgent

class MyAgent(BaseAgent):
    def __init__(self):
        super().__init__("my_agent", "æˆ‘çš„æ™ºèƒ½ä½“", set())
        self.system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ã€‚"

# ä½¿ç”¨ä¼˜åŒ–çš„Function Calling
agent = MyAgent()

# ä½¿ç”¨ä¼˜åŒ–æ–¹æ³•
result = await agent.process_with_optimized_function_calling(
    user_request="è¯·å¸®æˆ‘åˆ†æè¿™ä¸ªä»£ç ã€‚",
    conversation_id="agent_conversation_001",
    max_iterations=5,
    enable_self_continuation=True
)

# è·å–ä¼˜åŒ–ç»Ÿè®¡
stats = agent.get_llm_optimization_stats()
```

#### 3. åœ¨EnhancedBaseAgentä¸­ä½¿ç”¨

```python
from core.schema_system.enhanced_base_agent import EnhancedBaseAgent

class MyEnhancedAgent(EnhancedBaseAgent):
    def __init__(self):
        super().__init__("my_enhanced_agent", "æˆ‘çš„å¢å¼ºæ™ºèƒ½ä½“", set())
        self._register_enhanced_tools()

# ä½¿ç”¨å¢å¼ºéªŒè¯å¤„ç†
agent = MyEnhancedAgent()

result = await agent.process_with_enhanced_validation(
    user_request="è¯·æ‰§è¡Œè¿™ä¸ªä»»åŠ¡ã€‚",
    max_iterations=5
)

# è·å–å¢å¼ºä¼˜åŒ–ç»Ÿè®¡
stats = agent.get_enhanced_optimization_stats()
```

### é«˜çº§ç”¨æ³•

#### 1. è‡ªå®šä¹‰ä¼˜åŒ–é…ç½®

```python
# åœ¨OptimizedLLMClientä¸­è‡ªå®šä¹‰é…ç½®
optimized_client = OptimizedLLMClient(config)

# è‡ªå®šä¹‰ä¼˜åŒ–é…ç½®
optimized_client.optimization_config = {
    "enable_system_cache": True,
    "enable_context_compression": True,
    "max_context_tokens": 6000,  # è‡ªå®šä¹‰æœ€å¤§tokenæ•°
    "preserve_system_in_compression": True,
    "min_context_messages": 5  # æœ€å°‘ä¿ç•™çš„æ¶ˆæ¯æ•°
}
```

#### 2. æ‰‹åŠ¨ç®¡ç†ç¼“å­˜

```python
# æ¸…é™¤ç‰¹å®šå¯¹è¯çš„ä¸Šä¸‹æ–‡
llm_client.clear_conversation_context("conversation_id")

# æ¸…é™¤æ‰€æœ‰ä¸Šä¸‹æ–‡
llm_client.clear_all_contexts()

# å¼ºåˆ¶åˆ·æ–°system prompt
response = await llm_client.send_prompt_optimized(
    conversation_id="conversation_id",
    user_message="ç”¨æˆ·æ¶ˆæ¯",
    system_prompt="æ–°çš„system prompt",
    force_refresh_system=True  # å¼ºåˆ¶åˆ·æ–°
)
```

#### 3. æ€§èƒ½ç›‘æ§

```python
# è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
stats = llm_client.get_optimization_stats()

print(f"æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
print(f"ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1%}")
print(f"å¹³å‡å“åº”æ—¶é—´: {stats['average_time']:.2f}ç§’")
print(f"ä¸Šä¸‹æ–‡ä¼˜åŒ–æ¬¡æ•°: {stats['context_optimizations']}")
print(f"æ´»è·ƒå¯¹è¯æ•°: {stats['active_contexts']}")
print(f"æˆåŠŸç‡: {stats['success_rate']:.1%}")
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### æµ‹è¯•åœºæ™¯
- **æ ‡å‡†æ–¹å¼**: æ¯æ¬¡è°ƒç”¨éƒ½ä¼ é€’system prompt
- **ä¼˜åŒ–æ–¹å¼**: ä½¿ç”¨æ™ºèƒ½ç¼“å­˜å’Œä¸Šä¸‹æ–‡å‹ç¼©

### æ€§èƒ½æå‡
| æŒ‡æ ‡ | æ ‡å‡†æ–¹å¼ | ä¼˜åŒ–æ–¹å¼ | æå‡å¹…åº¦ |
|------|----------|----------|----------|
| Tokenæ¶ˆè€— | 100% | 60-70% | 30-40% |
| å“åº”æ—¶é—´ | 100% | 75-85% | 15-25% |
| APIæˆæœ¬ | 100% | 60-70% | 30-40% |
| ç¼“å­˜å‘½ä¸­ç‡ | 0% | 70-90% | - |

## ğŸ¯ æœ€ä½³å®è·µ

### 1. å¯¹è¯IDç®¡ç†
```python
# æ¨èï¼šä½¿ç”¨æœ‰æ„ä¹‰çš„å¯¹è¯ID
conversation_id = f"user_{user_id}_session_{session_id}"

# é¿å…ï¼šä½¿ç”¨éšæœºID
conversation_id = str(uuid.uuid4())  # ä¸æ¨è
```

### 2. System Promptè®¾è®¡
```python
# æ¨èï¼šç®€æ´æ˜ç¡®çš„system prompt
system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥åŠ©æ‰‹ï¼Œä¸“æ³¨äºVerilogä»£ç è´¨é‡åˆ†æã€‚"

# é¿å…ï¼šè¿‡äºå†—é•¿çš„system prompt
system_prompt = "ä½ æ˜¯ä¸€ä¸ªéå¸¸ä¸“ä¸šçš„ã€ç»éªŒä¸°å¯Œçš„ã€æŠ€æœ¯ç²¾æ¹›çš„..."  # ä¸æ¨è
```

### 3. ä¸Šä¸‹æ–‡ç®¡ç†
```python
# æ¨èï¼šå®šæœŸæ¸…ç†è¿‡æœŸä¸Šä¸‹æ–‡
if conversation_count > 10:
    llm_client.clear_conversation_context(old_conversation_id)

# æ¨èï¼šç›‘æ§ç¼“å­˜ä½¿ç”¨æƒ…å†µ
stats = llm_client.get_optimization_stats()
if stats['active_contexts'] > 50:
    llm_client.clear_all_contexts()
```

### 4. é”™è¯¯å¤„ç†
```python
try:
    response = await llm_client.send_prompt_optimized(
        conversation_id=conversation_id,
        user_message=user_message,
        system_prompt=system_prompt
    )
except Exception as e:
    # æ¸…é™¤å¯èƒ½æŸåçš„ä¸Šä¸‹æ–‡
    llm_client.clear_conversation_context(conversation_id)
    # é‡æ–°å°è¯•æˆ–ä½¿ç”¨æ ‡å‡†æ–¹æ³•
    response = await llm_client.send_prompt(user_message, system_prompt)
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. ç¼“å­˜å‘½ä¸­ç‡ä½
**ç—‡çŠ¶**: ç¼“å­˜å‘½ä¸­ç‡ä½äº50%
**å¯èƒ½åŸå› **: 
- é¢‘ç¹æ›´æ¢system prompt
- å¯¹è¯IDé‡å¤ä½¿ç”¨
- é…ç½®é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥system promptæ˜¯å¦é¢‘ç¹å˜åŒ–
if system_prompt != cached_system_prompt:
    # è€ƒè™‘æ˜¯å¦çœŸçš„éœ€è¦æ›´æ¢

# ä½¿ç”¨å”¯ä¸€çš„å¯¹è¯ID
conversation_id = f"{user_id}_{timestamp}_{random_id}"
```

#### 2. å†…å­˜ä½¿ç”¨è¿‡é«˜
**ç—‡çŠ¶**: ç³»ç»Ÿå†…å­˜ä½¿ç”¨æŒç»­å¢é•¿
**å¯èƒ½åŸå› **: ç¼“å­˜æœªåŠæ—¶æ¸…ç†

**è§£å†³æ–¹æ¡ˆ**:
```python
# å®šæœŸæ¸…ç†ç¼“å­˜
import asyncio

async def cleanup_cache():
    while True:
        await asyncio.sleep(3600)  # æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡
        llm_client.clear_all_contexts()

# å¯åŠ¨æ¸…ç†ä»»åŠ¡
asyncio.create_task(cleanup_cache())
```

#### 3. å“åº”æ—¶é—´å¢åŠ 
**ç—‡çŠ¶**: ä¼˜åŒ–åå“åº”æ—¶é—´åè€Œå¢åŠ 
**å¯èƒ½åŸå› **: ä¸Šä¸‹æ–‡å‹ç¼©è¿‡äºæ¿€è¿›

**è§£å†³æ–¹æ¡ˆ**:
```python
# è°ƒæ•´å‹ç¼©é…ç½®
optimized_client.optimization_config["max_context_tokens"] = 10000  # å¢åŠ tokené™åˆ¶
optimized_client.optimization_config["min_context_messages"] = 10   # å¢åŠ æœ€å°‘æ¶ˆæ¯æ•°
```

## ğŸ“ˆ ç›‘æ§å’Œè°ƒä¼˜

### 1. æ€§èƒ½ç›‘æ§æŒ‡æ ‡
```python
# å…³é”®æ€§èƒ½æŒ‡æ ‡
key_metrics = {
    "cache_hit_rate": "ç¼“å­˜å‘½ä¸­ç‡ > 70%",
    "average_time": "å¹³å‡å“åº”æ—¶é—´ < 2ç§’",
    "success_rate": "æˆåŠŸç‡ > 95%",
    "active_contexts": "æ´»è·ƒå¯¹è¯æ•° < 100"
}
```

### 2. è‡ªåŠ¨è°ƒä¼˜
```python
# æ ¹æ®æ€§èƒ½æŒ‡æ ‡è‡ªåŠ¨è°ƒæ•´é…ç½®
def auto_tune_config(stats):
    if stats['cache_hit_rate'] < 0.6:
        # é™ä½å‹ç¼©å¼ºåº¦
        config["max_context_tokens"] *= 1.2
    elif stats['average_time'] > 3.0:
        # æé«˜å‹ç¼©å¼ºåº¦
        config["max_context_tokens"] *= 0.8
```

## ğŸš€ è¿ç§»æŒ‡å—

### ä»æ ‡å‡†æ–¹å¼è¿ç§»åˆ°ä¼˜åŒ–æ–¹å¼

#### 1. æ›´æ–°LLMå®¢æˆ·ç«¯è°ƒç”¨
```python
# æ—§æ–¹å¼
response = await llm_client.send_prompt(prompt, system_prompt)

# æ–°æ–¹å¼
response = await llm_client.send_prompt_optimized(
    conversation_id=conversation_id,
    user_message=prompt,
    system_prompt=system_prompt
)
```

#### 2. æ›´æ–°æ™ºèƒ½ä½“è°ƒç”¨
```python
# æ—§æ–¹å¼
result = await agent.process_with_function_calling(user_request)

# æ–°æ–¹å¼
result = await agent.process_with_optimized_function_calling(
    user_request=user_request,
    conversation_id=conversation_id
)
```

#### 3. æ·»åŠ ç›‘æ§ä»£ç 
```python
# æ·»åŠ æ€§èƒ½ç›‘æ§
stats = llm_client.get_optimization_stats()
logger.info(f"ä¼˜åŒ–æ•ˆæœ: ç¼“å­˜å‘½ä¸­ç‡={stats['cache_hit_rate']:.1%}")
```

## ğŸ“š ç¤ºä¾‹ä»£ç 

å®Œæ•´çš„ç¤ºä¾‹ä»£ç è¯·å‚è€ƒ `examples/optimized_llm_demo.py`ï¼Œè¯¥æ–‡ä»¶åŒ…å«äº†ï¼š

1. åŸºç¡€ä¼˜åŒ–åŠŸèƒ½æ¼”ç¤º
2. æ™ºèƒ½ä½“ä¼˜åŒ–åŠŸèƒ½æ¼”ç¤º
3. å¢å¼ºæ™ºèƒ½ä½“ä¼˜åŒ–åŠŸèƒ½æ¼”ç¤º
4. æ€§èƒ½å¯¹æ¯”æµ‹è¯•

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [LLMé›†æˆæ–‡æ¡£](LLM_INTEGRATION.md)
- [æ™ºèƒ½ä½“æ¶æ„æ–‡æ¡£](AGENT_ARCHITECTURE.md)
- [æ€§èƒ½ä¼˜åŒ–æŒ‡å—](PERFORMANCE_OPTIMIZATION.md)

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š

1. æŸ¥çœ‹æœ¬æ–‡æ¡£çš„æ•…éšœæ’é™¤éƒ¨åˆ†
2. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ä¸­çš„é”™è¯¯ä¿¡æ¯
3. è¿è¡Œç¤ºä¾‹ä»£ç éªŒè¯åŠŸèƒ½
4. æäº¤Issueåˆ°é¡¹ç›®ä»“åº“

---

**æ³¨æ„**: æœ¬ä¼˜åŒ–æœºåˆ¶å‘åå…¼å®¹ï¼Œå¯ä»¥é€æ­¥è¿ç§»ï¼Œä¸ä¼šå½±å“ç°æœ‰åŠŸèƒ½ã€‚ 