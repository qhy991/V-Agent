# ğŸš€ LLMé€šä¿¡æŠ½è±¡å±‚ä¼˜åŒ–å®Œæ•´æ–¹æ¡ˆ

## ğŸ¯ æ ¸å¿ƒé—®é¢˜åˆ†æ

### âš ï¸ å½“å‰å…³é”®é—®é¢˜

1. **LLMè°ƒç”¨é‡å¤ä¸¥é‡** - ä¸‰ä¸ªæ™ºèƒ½ä½“æ¯ä¸ªéƒ½é‡å¤å®ç°~400è¡ŒLLMè°ƒç”¨é€»è¾‘
2. **System Promptæ„å»ºé‡å¤** - æ¯ä¸ªæ™ºèƒ½ä½“~800è¡Œé‡å¤çš„Promptæ„å»ºæ–¹æ³•
3. **ç¼ºå°‘ç»Ÿä¸€é”™è¯¯å¤„ç†** - é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶åˆ†æ•£åœ¨å„ä¸ªæ™ºèƒ½ä½“ä¸­
4. **æ€§èƒ½ç›‘æ§ç¼ºå¤±** - æ— æ³•ç»Ÿè®¡è°ƒç”¨æ€§èƒ½ã€Tokenä½¿ç”¨ã€ç¼“å­˜æ•ˆæœ

### ğŸ’¡ ä¼˜åŒ–ç›®æ ‡

- **å‡å°‘é‡å¤ä»£ç **: æ¶ˆé™¤1200+è¡ŒLLMç›¸å…³é‡å¤ä»£ç 
- **æå‡æ€§èƒ½**: é€šè¿‡ç¼“å­˜ã€ä¼˜åŒ–é™ä½30%ä»¥ä¸Šçš„LLMè°ƒç”¨æˆæœ¬
- **å¢å¼ºç¨³å®šæ€§**: ç»Ÿä¸€é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ï¼Œæé«˜ç³»ç»Ÿå¯é æ€§
- **æ”¹å–„ç›‘æ§**: å®Œæ•´çš„æ€§èƒ½ç›‘æ§å’Œè°ƒç”¨ç»Ÿè®¡åˆ†æ

## ğŸ—ï¸ LLMé€šä¿¡æŠ½è±¡å±‚å®Œæ•´è®¾è®¡

### ğŸ“‹ æ¶æ„æ¦‚è§ˆ

```
core/llm_communication/
â”œâ”€â”€ __init__.py                    # æ¨¡å—å¯¼å…¥å’Œåˆå§‹åŒ–
â”œâ”€â”€ managers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ client_manager.py          # LLMå®¢æˆ·ç«¯ç®¡ç†å™¨
â”‚   â”œâ”€â”€ conversation_optimizer.py  # å¯¹è¯ä¼˜åŒ–å™¨
â”‚   â””â”€â”€ performance_monitor.py     # æ€§èƒ½ç›‘æ§å™¨
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ prompt_template_engine.py  # Promptæ¨¡æ¿å¼•æ“
â”‚   â”œâ”€â”€ role_templates/            # è§’è‰²ç‰¹å®šæ¨¡æ¿
â”‚   â”‚   â”œâ”€â”€ verilog_designer.py
â”‚   â”‚   â”œâ”€â”€ code_reviewer.py
â”‚   â”‚   â””â”€â”€ coordinator.py
â”‚   â””â”€â”€ dynamic_prompt_builder.py  # åŠ¨æ€Promptæ„å»ºå™¨
â”œâ”€â”€ error_handling/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ retry_strategy.py          # é‡è¯•ç­–ç•¥
â”‚   â”œâ”€â”€ error_classifier.py       # é”™è¯¯åˆ†ç±»å™¨
â”‚   â””â”€â”€ fallback_handler.py       # å›é€€å¤„ç†å™¨
â”œâ”€â”€ caching/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ prompt_cache.py            # Promptç¼“å­˜
â”‚   â”œâ”€â”€ response_cache.py          # å“åº”ç¼“å­˜
â”‚   â””â”€â”€ context_cache.py           # ä¸Šä¸‹æ–‡ç¼“å­˜
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ token_counter.py           # Tokenè®¡ç®—å™¨
    â”œâ”€â”€ content_optimizer.py       # å†…å®¹ä¼˜åŒ–å™¨
    â””â”€â”€ metrics_collector.py       # æŒ‡æ ‡æ”¶é›†å™¨
```

## ğŸ’» æ ¸å¿ƒç»„ä»¶å®ç°

### 1. ç»Ÿä¸€LLMå®¢æˆ·ç«¯ç®¡ç†å™¨

```python
# /core/llm_communication/managers/client_manager.py
import asyncio
import time
import logging
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass
from enum import Enum

from ..templates.prompt_template_engine import PromptTemplateEngine
from ..error_handling.retry_strategy import RetryStrategy
from ..caching.context_cache import ContextCache
from ..utils.metrics_collector import MetricsCollector
from ...unified_logging_system import get_global_logging_system


class CallType(Enum):
    """LLMè°ƒç”¨ç±»å‹"""
    FUNCTION_CALLING = "function_calling"
    SYSTEM_PROMPT = "system_prompt"
    CONVERSATION = "conversation"
    ANALYSIS = "analysis"


@dataclass
class LLMCallContext:
    """LLMè°ƒç”¨ä¸Šä¸‹æ–‡"""
    agent_id: str
    role: str
    call_type: CallType
    conversation_id: Optional[str] = None
    max_tokens: int = 4000
    temperature: float = 0.3
    enable_cache: bool = True
    priority: str = "normal"  # low, normal, high, critical
    metadata: Dict[str, Any] = None


class UnifiedLLMClientManager:
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ç®¡ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.template_engine = PromptTemplateEngine()
        self.retry_strategy = RetryStrategy(config)
        self.context_cache = ContextCache(config)
        self.metrics_collector = MetricsCollector()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.call_stats = {
            'total_calls': 0,
            'successful_calls': 0,
            'failed_calls': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'total_tokens': 0,
            'average_response_time': 0.0
        }
        
        # æ´»è·ƒä¼šè¯ç®¡ç†
        self.active_sessions: Dict[str, Dict] = {}
        
        # é›†æˆå¢å¼ºæ—¥å¿—ç³»ç»Ÿ
        self.logging_system = get_global_logging_system()
    
    async def call_llm_for_function_calling(self, 
                                          context: LLMCallContext,
                                          conversation: List[Dict[str, str]],
                                          system_prompt: Optional[str] = None) -> str:
        """ç»Ÿä¸€çš„Function Calling LLMè°ƒç”¨"""
        call_start_time = time.time()
        
        try:
            # 1. å‡†å¤‡è°ƒç”¨ä¸Šä¸‹æ–‡
            session_key = f"{context.agent_id}_{context.conversation_id or 'default'}"
            
            # 2. æ„å»ºæˆ–è·å–System Prompt
            if not system_prompt:
                system_prompt = await self.template_engine.build_system_prompt(
                    role=context.role,
                    call_type=context.call_type,
                    agent_id=context.agent_id,
                    metadata=context.metadata
                )
            
            # 3. æ£€æŸ¥ç¼“å­˜
            if context.enable_cache:
                cached_response = await self.context_cache.get_cached_response(
                    system_prompt, conversation, context
                )
                if cached_response:
                    self.call_stats['cache_hits'] += 1
                    self._log_llm_call(context, call_start_time, True, cached=True)
                    return cached_response
            
            self.call_stats['cache_misses'] += 1
            
            # 4. æ‰§è¡ŒLLMè°ƒç”¨ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            response = await self.retry_strategy.execute_with_retry(
                self._execute_llm_call,
                context=context,
                system_prompt=system_prompt,
                conversation=conversation
            )
            
            # 5. ç¼“å­˜å“åº”
            if context.enable_cache and response:
                await self.context_cache.cache_response(
                    system_prompt, conversation, response, context
                )
            
            # 6. è®°å½•æˆåŠŸè°ƒç”¨
            self.call_stats['successful_calls'] += 1
            self.call_stats['total_calls'] += 1
            self._log_llm_call(context, call_start_time, True)
            
            return response
            
        except Exception as e:
            # è®°å½•å¤±è´¥è°ƒç”¨
            self.call_stats['failed_calls'] += 1
            self.call_stats['total_calls'] += 1
            self._log_llm_call(context, call_start_time, False, error=str(e))
            
            # ä½¿ç”¨å›é€€å¤„ç†å™¨
            return await self._handle_llm_failure(context, conversation, e)
    
    async def _execute_llm_call(self, context: LLMCallContext, 
                               system_prompt: str, 
                               conversation: List[Dict[str, str]]) -> str:
        """æ‰§è¡Œå®é™…çš„LLMè°ƒç”¨"""
        from llm_integration.enhanced_llm_client import EnhancedLLMClient
        
        # åˆ›å»ºæˆ–è·å–LLMå®¢æˆ·ç«¯
        client = EnhancedLLMClient(self.config.llm)
        
        # æ„å»ºå®Œæ•´çš„å¯¹è¯å†…å®¹
        full_prompt = self._build_conversation_prompt(conversation)
        
        # æ ¹æ®è°ƒç”¨ç±»å‹é€‰æ‹©åˆé€‚çš„è°ƒç”¨æ–¹æ³•
        if context.call_type == CallType.FUNCTION_CALLING:
            # å¯¹äºFunction Callingï¼Œä½¿ç”¨ä¼˜åŒ–çš„æ–¹æ³•
            if hasattr(client, 'send_prompt_optimized'):
                response = await client.send_prompt_optimized(
                    conversation_id=context.conversation_id,
                    user_message=full_prompt,
                    system_prompt=system_prompt,
                    temperature=context.temperature,
                    max_tokens=context.max_tokens,
                    force_refresh_system=False
                )
            else:
                response = await client.send_prompt(
                    prompt=full_prompt,
                    system_prompt=system_prompt,
                    temperature=context.temperature,
                    max_tokens=context.max_tokens
                )
        else:
            # æ ‡å‡†è°ƒç”¨
            response = await client.send_prompt(
                prompt=full_prompt,
                system_prompt=system_prompt,
                temperature=context.temperature,
                max_tokens=context.max_tokens
            )
        
        return response
    
    def _build_conversation_prompt(self, conversation: List[Dict[str, str]]) -> str:
        """æ„å»ºå¯¹è¯Prompt"""
        prompt_parts = []
        for msg in conversation:
            if msg["role"] == "user":
                prompt_parts.append(f"User: {msg['content']}")
            elif msg["role"] == "assistant":
                prompt_parts.append(f"Assistant: {msg['content']}")
        
        return "\n\n".join(prompt_parts)
    
    async def _handle_llm_failure(self, context: LLMCallContext, 
                                 conversation: List[Dict[str, str]], 
                                 error: Exception) -> str:
        """å¤„ç†LLMè°ƒç”¨å¤±è´¥"""
        from .fallback_handler import FallbackHandler
        
        fallback_handler = FallbackHandler(self.config)
        return await fallback_handler.handle_failure(context, conversation, error)
    
    def _log_llm_call(self, context: LLMCallContext, start_time: float, 
                     success: bool, cached: bool = False, error: str = None):
        """è®°å½•LLMè°ƒç”¨ä¿¡æ¯"""
        duration = time.time() - start_time
        
        # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
        self.call_stats['average_response_time'] = (
            (self.call_stats['average_response_time'] * (self.call_stats['total_calls'] - 1) + duration) 
            / max(self.call_stats['total_calls'], 1)
        )
        
        # è®°å½•åˆ°å¢å¼ºæ—¥å¿—ç³»ç»Ÿ
        if self.logging_system:
            try:
                self.logging_system.log_llm_call(
                    agent_id=context.agent_id,
                    model_name="claude-3.5-sonnet",
                    conversation_id=context.conversation_id,
                    call_type=context.call_type.value,
                    duration=duration,
                    success=success,
                    cached=cached,
                    error_info={"error": error} if error else None,
                    metadata={
                        "temperature": context.temperature,
                        "max_tokens": context.max_tokens,
                        "priority": context.priority
                    }
                )
            except Exception as e:
                self.logger.warning(f"Failed to log LLM call: {e}")
        
        # æœ¬åœ°æ—¥å¿—
        status = "SUCCESS" if success else "FAILED"
        cache_status = "(CACHED)" if cached else ""
        self.logger.info(
            f"ğŸ¤– LLM Call {status} {cache_status} - "
            f"Agent: {context.agent_id}, "
            f"Type: {context.call_type.value}, "
            f"Duration: {duration:.3f}s"
        )
        
        if error:
            self.logger.error(f"âŒ LLM Call Error: {error}")
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return {
            **self.call_stats,
            'cache_hit_rate': (
                self.call_stats['cache_hits'] / 
                max(self.call_stats['cache_hits'] + self.call_stats['cache_misses'], 1)
            ),
            'success_rate': (
                self.call_stats['successful_calls'] / 
                max(self.call_stats['total_calls'], 1)
            ),
            'active_sessions': len(self.active_sessions)
        }
    
    def reset_metrics(self):
        """é‡ç½®æ€§èƒ½æŒ‡æ ‡"""
        self.call_stats = {key: 0 if isinstance(value, (int, float)) else value 
                          for key, value in self.call_stats.items()}
```

### 2. System Promptæ¨¡æ¿å¼•æ“

```python
# /core/llm_communication/templates/prompt_template_engine.py
import json
import os
from typing import Dict, Any, List, Optional, Set
from dataclasses import dataclass
from pathlib import Path

from ..managers.client_manager import CallType
from ...enums import AgentCapability


@dataclass
class PromptTemplate:
    """Promptæ¨¡æ¿å®šä¹‰"""
    name: str
    role: str
    base_template: str
    capability_sections: Dict[str, str]
    tool_sections: Dict[str, str]
    dynamic_sections: Dict[str, str]
    metadata: Dict[str, Any] = None


class PromptTemplateEngine:
    """Promptæ¨¡æ¿å¼•æ“"""
    
    def __init__(self):
        self.templates: Dict[str, PromptTemplate] = {}
        self.template_cache: Dict[str, str] = {}
        self.load_templates()
    
    def load_templates(self):
        """åŠ è½½æ‰€æœ‰æ¨¡æ¿"""
        # åŠ è½½è§’è‰²ç‰¹å®šæ¨¡æ¿
        self._load_role_templates()
        
        # åŠ è½½é€šç”¨æ¨¡æ¿ç»„ä»¶
        self._load_common_components()
    
    def _load_role_templates(self):
        """åŠ è½½è§’è‰²ç‰¹å®šæ¨¡æ¿"""
        
        # Verilogè®¾è®¡å¸ˆæ¨¡æ¿
        self.templates['verilog_designer'] = PromptTemplate(
            name="verilog_designer",
            role="verilog_designer", 
            base_template=self._get_verilog_base_template(),
            capability_sections={
                "code_generation": self._get_verilog_code_generation_section(),
                "module_design": self._get_verilog_module_design_section(),
                "specification_analysis": self._get_verilog_analysis_section()
            },
            tool_sections={
                "analyze_design_requirements": self._get_tool_section("analyze_design_requirements"),
                "generate_verilog_code": self._get_tool_section("generate_verilog_code"),
                "analyze_code_quality": self._get_tool_section("analyze_code_quality"),
                "optimize_verilog_code": self._get_tool_section("optimize_verilog_code"),
                "write_file": self._get_tool_section("write_file"),
                "read_file": self._get_tool_section("read_file")
            },
            dynamic_sections={
                "error_guidance": "æ ¹æ®å†å²é”™è¯¯æä¾›é’ˆå¯¹æ€§æŒ‡å¯¼",
                "success_patterns": "åŸºäºæˆåŠŸæ¡ˆä¾‹çš„æœ€ä½³å®è·µ",
                "context_awareness": "ä»»åŠ¡ç‰¹å®šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯"
            }
        )
        
        # ä»£ç å®¡æŸ¥å¸ˆæ¨¡æ¿
        self.templates['code_reviewer'] = PromptTemplate(
            name="code_reviewer",
            role="code_reviewer",
            base_template=self._get_reviewer_base_template(),
            capability_sections={
                "code_review": self._get_reviewer_code_review_section(),
                "test_generation": self._get_reviewer_test_generation_section(),
                "verification": self._get_reviewer_verification_section()
            },
            tool_sections={
                "generate_testbench": self._get_tool_section("generate_testbench"),
                "run_simulation": self._get_tool_section("run_simulation"),
                "analyze_test_failures": self._get_tool_section("analyze_test_failures"),
                "write_file": self._get_tool_section("write_file"),
                "read_file": self._get_tool_section("read_file")
            },
            dynamic_sections={
                "error_recovery": "ä»¿çœŸé”™è¯¯è¯Šæ–­å’Œæ¢å¤ç­–ç•¥",
                "test_optimization": "æµ‹è¯•è¦†ç›–ç‡å’Œä¼˜åŒ–å»ºè®®",
                "quality_metrics": "ä»£ç è´¨é‡è¯„ä¼°æ ‡å‡†"
            }
        )
        
        # åè°ƒå™¨æ¨¡æ¿
        self.templates['coordinator'] = PromptTemplate(
            name="coordinator",
            role="coordinator",
            base_template=self._get_coordinator_base_template(),
            capability_sections={
                "task_coordination": self._get_coordinator_task_section(),
                "workflow_management": self._get_coordinator_workflow_section(),
                "agent_selection": self._get_coordinator_selection_section()
            },
            tool_sections={
                "assign_task_to_agent": self._get_tool_section("assign_task_to_agent"),
                "analyze_agent_result": self._get_tool_section("analyze_agent_result")
            },
            dynamic_sections={
                "agent_performance": "åŸºäºå†å²è¡¨ç°çš„æ™ºèƒ½ä½“é€‰æ‹©",
                "task_optimization": "ä»»åŠ¡åˆ†è§£å’Œæ‰§è¡Œä¼˜åŒ–",
                "quality_control": "ç»“æœè´¨é‡æ§åˆ¶å’ŒéªŒè¯"
            }
        )
    
    async def build_system_prompt(self, role: str, call_type: CallType,
                                agent_id: str, capabilities: Set[AgentCapability] = None,
                                metadata: Dict[str, Any] = None) -> str:
        """æ„å»ºSystem Prompt"""
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(role, call_type, agent_id, capabilities, metadata)
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.template_cache:
            return self.template_cache[cache_key]
        
        # è·å–æ¨¡æ¿
        template = self.templates.get(role)
        if not template:
            raise ValueError(f"No template found for role: {role}")
        
        # æ„å»ºPrompt
        prompt_parts = []
        
        # 1. åŸºç¡€æ¨¡æ¿
        prompt_parts.append(template.base_template)
        
        # 2. èƒ½åŠ›ç›¸å…³éƒ¨åˆ†
        if capabilities:
            for capability in capabilities:
                capability_name = capability.value
                if capability_name in template.capability_sections:
                    prompt_parts.append(template.capability_sections[capability_name])
        
        # 3. å·¥å…·ç›¸å…³éƒ¨åˆ†
        prompt_parts.append(self._build_tools_section(template))
        
        # 4. åŠ¨æ€éƒ¨åˆ†
        if metadata:
            dynamic_content = self._build_dynamic_content(template, metadata)
            if dynamic_content:
                prompt_parts.append(dynamic_content)
        
        # 5. è°ƒç”¨ç±»å‹ç‰¹å®šéƒ¨åˆ†
        if call_type == CallType.FUNCTION_CALLING:
            prompt_parts.append(self._get_function_calling_section())
        
        # ç»„åˆæ‰€æœ‰éƒ¨åˆ†
        full_prompt = "\n\n".join(filter(None, prompt_parts))
        
        # ç¼“å­˜ç»“æœ
        self.template_cache[cache_key] = full_prompt
        
        return full_prompt
    
    def _generate_cache_key(self, role: str, call_type: CallType, agent_id: str,
                          capabilities: Set[AgentCapability] = None,
                          metadata: Dict[str, Any] = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        
        key_components = [
            role,
            call_type.value,
            agent_id
        ]
        
        if capabilities:
            key_components.append(",".join(sorted(cap.value for cap in capabilities)))
        
        if metadata:
            # åªåŒ…å«ç¨³å®šçš„metadataéƒ¨åˆ†ï¼Œæ’é™¤åŠ¨æ€å†…å®¹
            stable_metadata = {k: v for k, v in metadata.items() 
                             if k in ['task_type', 'complexity_level', 'priority']}
            if stable_metadata:
                key_components.append(json.dumps(stable_metadata, sort_keys=True))
        
        key_string = "|".join(key_components)
        return hashlib.md5(key_string.encode('utf-8')).hexdigest()
    
    def _get_verilog_base_template(self) -> str:
        """è·å–Verilogè®¾è®¡å¸ˆåŸºç¡€æ¨¡æ¿"""
        return """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„Verilogç¡¬ä»¶è®¾è®¡ä¸“å®¶ï¼Œå…·å¤‡ä»¥ä¸‹ä¸“ä¸šèƒ½åŠ›ï¼š

ğŸ” **æ ¸å¿ƒä¸“é•¿**:
- Verilog/SystemVerilogæ¨¡å—è®¾è®¡å’Œä»£ç ç”Ÿæˆ
- ç»„åˆé€»è¾‘å’Œæ—¶åºé€»è¾‘è®¾è®¡
- å‚æ•°åŒ–è®¾è®¡å’Œå¯é‡ç”¨æ¨¡å—å¼€å‘
- ä»£ç è´¨é‡åˆ†æå’Œæœ€ä½³å®è·µåº”ç”¨
- å¯ç»¼åˆæ€§å’Œæ—¶åºæ”¶æ•›è®¾è®¡
- è®¾è®¡éªŒè¯å’Œæµ‹è¯•ç­–ç•¥

ğŸ“‹ **è®¾è®¡æ ‡å‡†**:
1. IEEE 1800æ ‡å‡†åˆè§„æ€§
2. ä»£ç å¯è¯»æ€§å’Œç»´æŠ¤æ€§
3. ç»¼åˆæ€§å’Œæ—¶åºæ”¶æ•›
4. å‚æ•°åŒ–å’Œå¯é‡ç”¨æ€§
5. æœ€ä½³å®è·µå’Œè®¾è®¡æ¨¡å¼
6. å®‰å…¨æ€§å’Œå¯é æ€§

ğŸ¯ **ä»»åŠ¡æ‰§è¡ŒåŸåˆ™**:
- æ ¹æ®éœ€æ±‚æ™ºèƒ½åˆ¤æ–­è®¾è®¡ç±»å‹ï¼ˆç»„åˆ/æ—¶åº/æ··åˆï¼‰
- è‡ªåŠ¨æ£€æµ‹å’Œé€‚é…å‚æ•°åŒ–ä½å®½éœ€æ±‚
- ç”Ÿæˆé«˜è´¨é‡ã€å¯ç»¼åˆçš„Verilogä»£ç 
- æä¾›è¯¦ç»†çš„ä»£ç æ³¨é‡Šå’Œæ–‡æ¡£
- æ”¯æŒå¤šç§ç¼–ç é£æ ¼å’Œè®¾è®¡æ¨¡å¼
- ç¡®ä¿ä»£ç ç¬¦åˆè¡Œä¸šæ ‡å‡†"""
    
    def _get_reviewer_base_template(self) -> str:
        """è·å–ä»£ç å®¡æŸ¥å¸ˆåŸºç¡€æ¨¡æ¿"""
        return """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„Verilogä»£ç å®¡æŸ¥å’ŒéªŒè¯ä¸“å®¶ï¼Œå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š

ğŸ” **å®¡æŸ¥ä¸“é•¿**:
- Verilogä»£ç è´¨é‡åˆ†æå’Œè¯„ä¼°
- æµ‹è¯•å°(testbench)ç”Ÿæˆå’Œä¼˜åŒ–
- ä»¿çœŸéªŒè¯å’Œè°ƒè¯•
- é”™è¯¯è¯Šæ–­å’Œä¿®å¤å»ºè®®
- è¦†ç›–ç‡åˆ†æå’Œæµ‹è¯•ä¼˜åŒ–
- ä»£ç è§„èŒƒå’Œæœ€ä½³å®è·µæ£€æŸ¥

ğŸ§ª **éªŒè¯èƒ½åŠ›**:
- åŠŸèƒ½éªŒè¯å’Œæ—¶åºåˆ†æ
- è¾¹ç•Œæ¡ä»¶å’Œå¼‚å¸¸æƒ…å†µæµ‹è¯•
- ä»¿çœŸç¯å¢ƒæ­å»ºå’Œä¼˜åŒ–
- è‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹è®¾è®¡
- è°ƒè¯•å·¥å…·å’Œæ–¹æ³•åº”ç”¨
- éªŒè¯æŠ¥å‘Šç”Ÿæˆå’Œåˆ†æ

âš¡ **ä¸“ä¸šå·¥å…·**:
- iverilogç¼–è¯‘å’Œä»¿çœŸ
- æµ‹è¯•å‘é‡ç”Ÿæˆå’Œåˆ†æ
- æ³¢å½¢åˆ†æå’Œè°ƒè¯•
- è¦†ç›–ç‡ç»Ÿè®¡å’ŒæŠ¥å‘Š
- æ€§èƒ½åŸºå‡†æµ‹è¯•
- é”™è¯¯åˆ†ç±»å’Œä¿®å¤ç­–ç•¥"""
    
    def _get_coordinator_base_template(self) -> str:
        """è·å–åè°ƒå™¨åŸºç¡€æ¨¡æ¿"""
        return """ä½ æ˜¯ä¸€ä½æ™ºèƒ½çš„å¤šæ™ºèƒ½ä½“åè°ƒä¸“å®¶ï¼Œè´Ÿè´£ä»»åŠ¡åˆ†é…ã€å·¥ä½œæµç®¡ç†å’Œè´¨é‡æ§åˆ¶ï¼š

ğŸ§  **åè°ƒèƒ½åŠ›**:
- æ™ºèƒ½ä»»åŠ¡åˆ†æå’Œåˆ†è§£
- åŸºäºèƒ½åŠ›çš„æ™ºèƒ½ä½“é€‰æ‹©
- å·¥ä½œæµä¼˜åŒ–å’Œç®¡ç†
- è´¨é‡æ§åˆ¶å’Œç»“æœéªŒè¯
- é”™è¯¯æ¢å¤å’Œé‡è¯•ç­–ç•¥
- æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–

ğŸ“Š **å†³ç­–åŸåˆ™**:
- åŸºäºä»»åŠ¡ç±»å‹é€‰æ‹©æœ€é€‚åˆçš„æ™ºèƒ½ä½“
- è€ƒè™‘å†å²è¡¨ç°å’Œå½“å‰è´Ÿè½½
- ç¡®ä¿ä»»åŠ¡æ‰§è¡Œçš„é«˜è´¨é‡å®Œæˆ
- æä¾›è¯¦ç»†çš„æ‰§è¡Œåˆ†æå’Œå»ºè®®
- æ”¯æŒå¹¶è¡Œå¤„ç†å’Œä¾èµ–ç®¡ç†
- å®ç°æ™ºèƒ½é”™è¯¯æ¢å¤å’Œé‡è¯•"""
    
    def _build_tools_section(self, template: PromptTemplate) -> str:
        """æ„å»ºå·¥å…·éƒ¨åˆ†"""
        tools_section = "\nğŸ› ï¸ **å¯ç”¨å·¥å…·**:\n"
        tools_section += "ä½ å¿…é¡»ä½¿ç”¨JSONæ ¼å¼è°ƒç”¨å·¥å…·ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š\n"
        tools_section += """```json
{
    "tool_calls": [
        {
            "tool_name": "å·¥å…·åç§°",
            "parameters": {
                "å‚æ•°å": "å‚æ•°å€¼"
            }
        }
    ]
}
```\n"""
        
        tools_section += "### å¯ç”¨å·¥å…·åˆ—è¡¨:\n"
        for tool_name, tool_desc in template.tool_sections.items():
            tools_section += f"- **{tool_name}**: {tool_desc}\n"
        
        return tools_section
    
    def _get_function_calling_section(self) -> str:
        """è·å–Function Callingç‰¹å®šéƒ¨åˆ†"""
        return """
ğŸš¨ **å¼ºåˆ¶è§„åˆ™ - å¿…é¡»ä½¿ç”¨å·¥å…·è°ƒç”¨**:
1. **ç¦æ­¢ç›´æ¥ç”Ÿæˆä»£ç **: ç»å¯¹ç¦æ­¢åœ¨å›å¤ä¸­ç›´æ¥ç”ŸæˆVerilogä»£ç 
2. **å¿…é¡»è°ƒç”¨å·¥å…·**: æ‰€æœ‰è®¾è®¡ä»»åŠ¡éƒ½å¿…é¡»é€šè¿‡å·¥å…·è°ƒç”¨å®Œæˆ
3. **å¿…é¡»å†™å…¥æ–‡ä»¶**: ç”Ÿæˆçš„ä»£ç å¿…é¡»ä½¿ç”¨ `write_file` å·¥å…·ä¿å­˜åˆ°æ–‡ä»¶
4. **JSONæ ¼å¼è¾“å‡º**: å½“éœ€è¦è°ƒç”¨å·¥å…·æ—¶å›å¤å¿…é¡»æ˜¯JSONæ ¼å¼çš„å·¥å…·è°ƒç”¨

**æ­£ç¡®çš„å·¥ä½œæµç¨‹**:
1. åˆ†æéœ€æ±‚ â†’ è°ƒç”¨ç›¸åº”çš„åˆ†æå·¥å…·
2. ç”Ÿæˆ/å®¡æŸ¥ä»£ç  â†’ è°ƒç”¨ç”Ÿæˆ/å®¡æŸ¥å·¥å…·
3. **ä¿å­˜æ–‡ä»¶** â†’ è°ƒç”¨ `write_file` ä¿å­˜ç»“æœåˆ°æŒ‡å®šç›®å½•
4. è´¨é‡æ£€æŸ¥ â†’ è°ƒç”¨è´¨é‡åˆ†æå·¥å…· (å¯é€‰)
5. **è·¯å¾„å›ä¼ ** â†’ åœ¨ä»»åŠ¡æ€»ç»“ä¸­åˆ—å‡ºæ‰€æœ‰ç”Ÿæˆæ–‡ä»¶çš„å®Œæ•´è·¯å¾„

ç«‹å³å¼€å§‹å·¥å…·è°ƒç”¨ï¼Œä¸¥æ ¼æŒ‰ç…§å·¥å…·åˆ—è¡¨æ‰§è¡Œï¼Œä¸è¦ç›´æ¥ç”Ÿæˆä»»ä½•ä»£ç ï¼"""
    
    # ... å…¶ä»–è¾…åŠ©æ–¹æ³•å®ç° ...
    
    def clear_cache(self):
        """æ¸…é™¤æ¨¡æ¿ç¼“å­˜"""
        self.template_cache.clear()
    
    def get_template_stats(self) -> Dict[str, Any]:
        """è·å–æ¨¡æ¿ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_templates': len(self.templates),
            'cached_prompts': len(self.template_cache),
            'template_roles': list(self.templates.keys())
        }
```

### 3. ç»Ÿä¸€é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

```python
# /core/llm_communication/error_handling/retry_strategy.py
import asyncio
import time
import logging
from typing import Dict, Any, List, Optional, Callable, Union
from dataclasses import dataclass
from enum import Enum

from .error_classifier import ErrorType, ErrorClassifier
from .fallback_handler import FallbackHandler


class RetryType(Enum):
    """é‡è¯•ç±»å‹"""
    EXPONENTIAL_BACKOFF = "exponential_backoff"
    LINEAR_BACKOFF = "linear_backoff"
    IMMEDIATE = "immediate"
    CUSTOM = "custom"


@dataclass
class RetryConfig:
    """é‡è¯•é…ç½®"""
    max_attempts: int = 3
    base_delay: float = 1.0
    max_delay: float = 60.0
    backoff_multiplier: float = 2.0
    retry_type: RetryType = RetryType.EXPONENTIAL_BACKOFF
    retryable_errors: List[ErrorType] = None
    timeout: float = 300.0  # 5åˆ†é’Ÿæ€»è¶…æ—¶


class RetryStrategy:
    """é‡è¯•ç­–ç•¥ç®¡ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.error_classifier = ErrorClassifier()
        self.fallback_handler = FallbackHandler(config)
        
        # é»˜è®¤é‡è¯•é…ç½®
        self.default_retry_config = RetryConfig()
        
        # æŒ‰é”™è¯¯ç±»å‹çš„ç‰¹å®šé‡è¯•é…ç½®
        self.error_specific_configs = {
            ErrorType.RATE_LIMIT: RetryConfig(
                max_attempts=5,
                base_delay=2.0,
                max_delay=120.0,
                backoff_multiplier=2.5
            ),
            ErrorType.NETWORK_TIMEOUT: RetryConfig(
                max_attempts=3,
                base_delay=1.0,
                max_delay=30.0,
                backoff_multiplier=2.0
            ),
            ErrorType.SERVER_ERROR: RetryConfig(
                max_attempts=4,
                base_delay=5.0,
                max_delay=60.0,
                backoff_multiplier=1.5
            ),
            ErrorType.AUTHENTICATION_ERROR: RetryConfig(
                max_attempts=1  # ä¸é‡è¯•è®¤è¯é”™è¯¯
            )
        }
        
        # é‡è¯•ç»Ÿè®¡
        self.retry_stats = {
            'total_retries': 0,
            'successful_retries': 0,
            'failed_retries': 0,
            'retry_by_error_type': {}
        }
    
    async def execute_with_retry(self, func: Callable, *args, **kwargs) -> Any:
        """æ‰§è¡Œå‡½æ•°å¹¶æ”¯æŒé‡è¯•"""
        start_time = time.time()
        last_exception = None
        
        # ç¡®å®šé‡è¯•é…ç½®
        retry_config = kwargs.pop('retry_config', self.default_retry_config)
        
        for attempt in range(retry_config.max_attempts):
            try:
                # æ£€æŸ¥æ€»è¶…æ—¶
                if time.time() - start_time > retry_config.timeout:
                    raise TimeoutError(f"Total execution timeout after {retry_config.timeout}s")
                
                # æ‰§è¡Œå‡½æ•°
                result = await func(*args, **kwargs)
                
                # æˆåŠŸæ‰§è¡Œ
                if attempt > 0:
                    self.retry_stats['successful_retries'] += 1
                    self.logger.info(
                        f"âœ… Retry successful after {attempt} attempts - "
                        f"Function: {func.__name__}"
                    )
                
                return result
                
            except Exception as e:
                last_exception = e
                
                # åˆ†ç±»é”™è¯¯
                error_type = self.error_classifier.classify_error(e)
                
                # è®°å½•é‡è¯•ç»Ÿè®¡
                self.retry_stats['total_retries'] += 1
                error_type_name = error_type.value
                self.retry_stats['retry_by_error_type'][error_type_name] = (
                    self.retry_stats['retry_by_error_type'].get(error_type_name, 0) + 1
                )
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
                if not self._should_retry(error_type, attempt, retry_config):
                    self.logger.error(
                        f"âŒ Max retries exceeded or non-retryable error - "
                        f"Function: {func.__name__}, "
                        f"Error: {error_type_name}, "
                        f"Attempts: {attempt + 1}"
                    )
                    self.retry_stats['failed_retries'] += 1
                    break
                
                # è®¡ç®—å»¶è¿Ÿæ—¶é—´
                delay = self._calculate_delay(attempt, retry_config, error_type)
                
                self.logger.warning(
                    f"âš ï¸ Retry attempt {attempt + 1}/{retry_config.max_attempts} - "
                    f"Function: {func.__name__}, "
                    f"Error: {error_type_name}, "
                    f"Delay: {delay:.1f}s"
                )
                
                # ç­‰å¾…é‡è¯•
                if delay > 0:
                    await asyncio.sleep(delay)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œä½¿ç”¨å›é€€å¤„ç†
        self.logger.error(f"ğŸš¨ All retry attempts failed for {func.__name__}")
        return await self.fallback_handler.handle_retry_failure(
            func, last_exception, *args, **kwargs
        )
    
    def _should_retry(self, error_type: ErrorType, attempt: int, config: RetryConfig) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•"""
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°
        if attempt + 1 >= config.max_attempts:
            return False
        
        # æ£€æŸ¥é”™è¯¯ç±»å‹æ˜¯å¦å¯é‡è¯•
        if config.retryable_errors is not None:
            return error_type in config.retryable_errors
        
        # é»˜è®¤ä¸é‡è¯•çš„é”™è¯¯ç±»å‹
        non_retryable_errors = {
            ErrorType.AUTHENTICATION_ERROR,
            ErrorType.PERMISSION_ERROR,
            ErrorType.INVALID_REQUEST,
            ErrorType.CONFIGURATION_ERROR
        }
        
        return error_type not in non_retryable_errors
    
    def _calculate_delay(self, attempt: int, config: RetryConfig, error_type: ErrorType) -> float:
        """è®¡ç®—é‡è¯•å»¶è¿Ÿæ—¶é—´"""
        # è·å–ç‰¹å®šé”™è¯¯ç±»å‹çš„é…ç½®
        specific_config = self.error_specific_configs.get(error_type, config)
        
        if specific_config.retry_type == RetryType.IMMEDIATE:
            return 0
        
        elif specific_config.retry_type == RetryType.LINEAR_BACKOFF:
            delay = specific_config.base_delay * (attempt + 1)
        
        elif specific_config.retry_type == RetryType.EXPONENTIAL_BACKOFF:
            delay = specific_config.base_delay * (specific_config.backoff_multiplier ** attempt)
        
        else:  # CUSTOMæˆ–å…¶ä»–
            delay = specific_config.base_delay
        
        # åº”ç”¨æœ€å¤§å»¶è¿Ÿé™åˆ¶
        delay = min(delay, specific_config.max_delay)
        
        # æ·»åŠ éšæœºæŠ–åŠ¨ï¼ˆé¿å…é›·ç¾¤æ•ˆåº”ï¼‰
        import random
        jitter = delay * 0.1 * random.random()
        
        return delay + jitter
    
    def get_retry_stats(self) -> Dict[str, Any]:
        """è·å–é‡è¯•ç»Ÿè®¡ä¿¡æ¯"""
        total_attempts = self.retry_stats['total_retries']
        if total_attempts > 0:
            success_rate = self.retry_stats['successful_retries'] / total_attempts
        else:
            success_rate = 0.0
        
        return {
            **self.retry_stats,
            'retry_success_rate': success_rate,
            'most_common_retry_errors': sorted(
                self.retry_stats['retry_by_error_type'].items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]
        }
    
    def reset_stats(self):
        """é‡ç½®é‡è¯•ç»Ÿè®¡"""
        self.retry_stats = {
            'total_retries': 0,
            'successful_retries': 0,
            'failed_retries': 0,
            'retry_by_error_type': {}
        }
```

### 4. æ€§èƒ½ç›‘æ§å’Œç¼“å­˜ä¼˜åŒ–

```python
# /core/llm_communication/utils/metrics_collector.py
import time
import threading
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from collections import defaultdict, deque
from datetime import datetime, timedelta


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: float
    agent_id: str
    call_type: str
    duration: float
    success: bool
    cached: bool = False
    tokens: int = 0
    error_type: Optional[str] = None


@dataclass
class AggregatedMetrics:
    """èšåˆæ€§èƒ½æŒ‡æ ‡"""
    total_calls: int = 0
    successful_calls: int = 0
    failed_calls: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    total_duration: float = 0.0
    total_tokens: int = 0
    average_duration: float = 0.0
    success_rate: float = 0.0
    cache_hit_rate: float = 0.0
    error_distribution: Dict[str, int] = field(default_factory=dict)


class MetricsCollector:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, max_history_hours: int = 24):
        self.max_history_hours = max_history_hours
        self.metrics_history: deque = deque()
        self.real_time_metrics: Dict[str, Any] = defaultdict(int)
        self._lock = threading.Lock()
        
        # æŒ‰æ—¶é—´çª—å£èšåˆçš„æŒ‡æ ‡
        self.hourly_metrics: Dict[str, AggregatedMetrics] = {}
        self.daily_metrics: AggregatedMetrics = AggregatedMetrics()
        
        # å¯åŠ¨åå°æ¸…ç†ä»»åŠ¡
        self._start_cleanup_task()
    
    def record_llm_call(self, agent_id: str, call_type: str, duration: float,
                       success: bool, cached: bool = False, tokens: int = 0,
                       error_type: Optional[str] = None):
        """è®°å½•LLMè°ƒç”¨æŒ‡æ ‡"""
        with self._lock:
            metric = PerformanceMetric(
                timestamp=time.time(),
                agent_id=agent_id,
                call_type=call_type,
                duration=duration,
                success=success,
                cached=cached,
                tokens=tokens,
                error_type=error_type
            )
            
            # æ·»åŠ åˆ°å†å²è®°å½•
            self.metrics_history.append(metric)
            
            # æ›´æ–°å®æ—¶æŒ‡æ ‡
            self._update_real_time_metrics(metric)
            
            # æ›´æ–°èšåˆæŒ‡æ ‡
            self._update_aggregated_metrics(metric)
    
    def _update_real_time_metrics(self, metric: PerformanceMetric):
        """æ›´æ–°å®æ—¶æŒ‡æ ‡"""
        self.real_time_metrics['total_calls'] += 1
        self.real_time_metrics['total_duration'] += metric.duration
        self.real_time_metrics['total_tokens'] += metric.tokens
        
        if metric.success:
            self.real_time_metrics['successful_calls'] += 1
        else:
            self.real_time_metrics['failed_calls'] += 1
            if metric.error_type:
                error_key = f'error_{metric.error_type}'
                self.real_time_metrics[error_key] += 1
        
        if metric.cached:
            self.real_time_metrics['cache_hits'] += 1
        else:
            self.real_time_metrics['cache_misses'] += 1
        
        # æŒ‰æ™ºèƒ½ä½“ç»Ÿè®¡
        agent_key = f'agent_{metric.agent_id}'
        self.real_time_metrics[agent_key] += 1
        
        # æŒ‰è°ƒç”¨ç±»å‹ç»Ÿè®¡
        call_type_key = f'call_type_{metric.call_type}'
        self.real_time_metrics[call_type_key] += 1
    
    def _update_aggregated_metrics(self, metric: PerformanceMetric):
        """æ›´æ–°èšåˆæŒ‡æ ‡"""
        # è·å–å°æ—¶é”®
        hour_key = datetime.fromtimestamp(metric.timestamp).strftime('%Y-%m-%d_%H')
        
        if hour_key not in self.hourly_metrics:
            self.hourly_metrics[hour_key] = AggregatedMetrics()
        
        # æ›´æ–°å°æ—¶æŒ‡æ ‡
        hourly = self.hourly_metrics[hour_key]
        hourly.total_calls += 1
        hourly.total_duration += metric.duration
        hourly.total_tokens += metric.tokens
        
        if metric.success:
            hourly.successful_calls += 1
        else:
            hourly.failed_calls += 1
            if metric.error_type:
                hourly.error_distribution[metric.error_type] = (
                    hourly.error_distribution.get(metric.error_type, 0) + 1
                )
        
        if metric.cached:
            hourly.cache_hits += 1
        else:
            hourly.cache_misses += 1
        
        # è®¡ç®—å¹³å‡å€¼å’Œæ¯”ç‡
        hourly.average_duration = hourly.total_duration / hourly.total_calls
        hourly.success_rate = hourly.successful_calls / hourly.total_calls
        total_cache_attempts = hourly.cache_hits + hourly.cache_misses
        if total_cache_attempts > 0:
            hourly.cache_hit_rate = hourly.cache_hits / total_cache_attempts
        
        # æ›´æ–°æ—¥æŒ‡æ ‡
        self._update_daily_metrics(metric)
    
    def _update_daily_metrics(self, metric: PerformanceMetric):
        """æ›´æ–°æ—¥æŒ‡æ ‡"""
        self.daily_metrics.total_calls += 1
        self.daily_metrics.total_duration += metric.duration
        self.daily_metrics.total_tokens += metric.tokens
        
        if metric.success:
            self.daily_metrics.successful_calls += 1
        else:
            self.daily_metrics.failed_calls += 1
            if metric.error_type:
                self.daily_metrics.error_distribution[metric.error_type] = (
                    self.daily_metrics.error_distribution.get(metric.error_type, 0) + 1
                )
        
        if metric.cached:
            self.daily_metrics.cache_hits += 1
        else:
            self.daily_metrics.cache_misses += 1
        
        # é‡æ–°è®¡ç®—å¹³å‡å€¼å’Œæ¯”ç‡
        if self.daily_metrics.total_calls > 0:
            self.daily_metrics.average_duration = (
                self.daily_metrics.total_duration / self.daily_metrics.total_calls
            )
            self.daily_metrics.success_rate = (
                self.daily_metrics.successful_calls / self.daily_metrics.total_calls
            )
            
            total_cache_attempts = self.daily_metrics.cache_hits + self.daily_metrics.cache_misses
            if total_cache_attempts > 0:
                self.daily_metrics.cache_hit_rate = (
                    self.daily_metrics.cache_hits / total_cache_attempts
                )
    
    def get_real_time_metrics(self) -> Dict[str, Any]:
        """è·å–å®æ—¶æŒ‡æ ‡"""
        with self._lock:
            # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
            total_calls = self.real_time_metrics.get('total_calls', 0)
            if total_calls > 0:
                success_rate = self.real_time_metrics.get('successful_calls', 0) / total_calls
                average_duration = self.real_time_metrics.get('total_duration', 0) / total_calls
                
                cache_total = (
                    self.real_time_metrics.get('cache_hits', 0) + 
                    self.real_time_metrics.get('cache_misses', 0)
                )
                cache_hit_rate = (
                    self.real_time_metrics.get('cache_hits', 0) / cache_total 
                    if cache_total > 0 else 0
                )
            else:
                success_rate = 0
                average_duration = 0
                cache_hit_rate = 0
            
            return {
                **dict(self.real_time_metrics),
                'success_rate': success_rate,
                'average_duration': average_duration,
                'cache_hit_rate': cache_hit_rate,
                'metrics_history_size': len(self.metrics_history)
            }
    
    def get_hourly_metrics(self, hours_back: int = 24) -> Dict[str, AggregatedMetrics]:
        """è·å–æŒ‰å°æ—¶èšåˆçš„æŒ‡æ ‡"""
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        cutoff_key = cutoff_time.strftime('%Y-%m-%d_%H')
        
        return {
            hour_key: metrics 
            for hour_key, metrics in self.hourly_metrics.items()
            if hour_key >= cutoff_key
        }
    
    def get_daily_metrics(self) -> AggregatedMetrics:
        """è·å–æ—¥æŒ‡æ ‡"""
        return self.daily_metrics
    
    def get_agent_performance(self) -> Dict[str, Dict[str, Any]]:
        """è·å–æŒ‰æ™ºèƒ½ä½“çš„æ€§èƒ½ç»Ÿè®¡"""
        agent_stats = defaultdict(lambda: {
            'total_calls': 0,
            'successful_calls': 0,
            'total_duration': 0.0,
            'total_tokens': 0,
            'call_types': defaultdict(int)
        })
        
        with self._lock:
            for metric in self.metrics_history:
                stats = agent_stats[metric.agent_id]
                stats['total_calls'] += 1
                stats['total_duration'] += metric.duration
                stats['total_tokens'] += metric.tokens
                stats['call_types'][metric.call_type] += 1
                
                if metric.success:
                    stats['successful_calls'] += 1
        
        # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
        for agent_id, stats in agent_stats.items():
            if stats['total_calls'] > 0:
                stats['success_rate'] = stats['successful_calls'] / stats['total_calls']
                stats['average_duration'] = stats['total_duration'] / stats['total_calls']
                stats['average_tokens'] = stats['total_tokens'] / stats['total_calls']
        
        return dict(agent_stats)
    
    def _start_cleanup_task(self):
        """å¯åŠ¨åå°æ¸…ç†ä»»åŠ¡"""
        def cleanup_old_metrics():
            cutoff_time = time.time() - (self.max_history_hours * 3600)
            
            with self._lock:
                # æ¸…ç†æ—§çš„æŒ‡æ ‡è®°å½•
                while (self.metrics_history and 
                       self.metrics_history[0].timestamp < cutoff_time):
                    self.metrics_history.popleft()
                
                # æ¸…ç†æ—§çš„å°æ—¶æŒ‡æ ‡
                cutoff_hour = datetime.fromtimestamp(cutoff_time).strftime('%Y-%m-%d_%H')
                old_hours = [
                    hour_key for hour_key in self.hourly_metrics.keys()
                    if hour_key < cutoff_hour
                ]
                for hour_key in old_hours:
                    del self.hourly_metrics[hour_key]
        
        # å¯åŠ¨å®šæ—¶æ¸…ç†ï¼ˆæ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼‰
        import threading
        
        def periodic_cleanup():
            while True:
                time.sleep(3600)  # 1å°æ—¶
                try:
                    cleanup_old_metrics()
                except Exception as e:
                    # è®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­æ¸…ç†ä»»åŠ¡
                    pass
        
        cleanup_thread = threading.Thread(target=periodic_cleanup, daemon=True)
        cleanup_thread.start()
    
    def export_metrics(self, format_type: str = "json") -> Any:
        """å¯¼å‡ºæŒ‡æ ‡æ•°æ®"""
        if format_type == "json":
            return {
                'real_time': self.get_real_time_metrics(),
                'hourly': {k: v.__dict__ for k, v in self.get_hourly_metrics().items()},
                'daily': self.daily_metrics.__dict__,
                'agent_performance': self.get_agent_performance(),
                'export_timestamp': time.time()
            }
        else:
            raise ValueError(f"Unsupported export format: {format_type}")
    
    def reset_metrics(self):
        """é‡ç½®æ‰€æœ‰æŒ‡æ ‡"""
        with self._lock:
            self.metrics_history.clear()
            self.real_time_metrics.clear()
            self.hourly_metrics.clear()
            self.daily_metrics = AggregatedMetrics()
```

## ğŸš€ å®æ–½è·¯çº¿å›¾

### **ç¬¬1å‘¨: LLMé€šä¿¡æ ¸å¿ƒæ¶æ„**
1. åˆ›å»º `UnifiedLLMClientManager` æ ¸å¿ƒç±»
2. å®ç°åŸºç¡€çš„LLMè°ƒç”¨ç»Ÿä¸€æ¥å£
3. é›†æˆç°æœ‰çš„ `EnhancedLLMClient`
4. åŸºç¡€å•å…ƒæµ‹è¯•

### **ç¬¬2å‘¨: System Promptæ¨¡æ¿ç³»ç»Ÿ**
1. å®ç° `PromptTemplateEngine` 
2. åˆ›å»ºä¸‰ä¸ªè§’è‰²çš„åŸºç¡€æ¨¡æ¿
3. å®ç°æ¨¡æ¿ç¼“å­˜å’ŒåŠ¨æ€æ„å»º
4. æ¨¡æ¿ç³»ç»Ÿæµ‹è¯•

### **ç¬¬3å‘¨: é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶**
1. å®ç° `RetryStrategy` å’Œé”™è¯¯åˆ†ç±»
2. åˆ›å»º `FallbackHandler` å›é€€å¤„ç†
3. é›†æˆåˆ°LLMå®¢æˆ·ç«¯ç®¡ç†å™¨
4. é”™è¯¯å¤„ç†æµ‹è¯•

### **ç¬¬4å‘¨: æ€§èƒ½ç›‘æ§å’Œç¼“å­˜**
1. å®ç° `MetricsCollector` æ€§èƒ½æ”¶é›†
2. åˆ›å»ºç¼“å­˜ç³»ç»Ÿå’Œä¼˜åŒ–
3. é›†æˆå¢å¼ºæ—¥å¿—ç³»ç»Ÿ
4. æ€§èƒ½ç›‘æ§æµ‹è¯•

### **ç¬¬5å‘¨: æ™ºèƒ½ä½“é›†æˆ**
1. ä¿®æ”¹ä¸‰ä¸ªæ™ºèƒ½ä½“ä½¿ç”¨æ–°çš„LLMé€šä¿¡å±‚
2. ä¿æŒAPIå…¼å®¹æ€§
3. å…¨é¢é›†æˆæµ‹è¯•
4. æ€§èƒ½åŸºå‡†æµ‹è¯•

### **ç¬¬6å‘¨: ä¼˜åŒ–å’Œæ–‡æ¡£**
1. æ€§èƒ½ä¼˜åŒ–å’Œbugä¿®å¤
2. å®Œæ•´æ–‡æ¡£ç¼–å†™
3. ä½¿ç”¨æŒ‡å—å’Œæœ€ä½³å®è·µ
4. ä»£ç æ¸…ç†å’Œå‘å¸ƒå‡†å¤‡

## ğŸ“Š é¢„æœŸæ”¶ç›Š

### **ä»£ç å‡å°‘é‡**:
- **LLMè°ƒç”¨é€»è¾‘**: ~1200è¡Œé‡å¤ä»£ç  â†’ 400è¡Œç»Ÿä¸€å®ç° = èŠ‚çœ800è¡Œ
- **System Promptæ„å»º**: ~2400è¡Œé‡å¤ä»£ç  â†’ 600è¡Œæ¨¡æ¿ç³»ç»Ÿ = èŠ‚çœ1800è¡Œ  
- **é”™è¯¯å¤„ç†**: ~600è¡Œåˆ†æ•£ä»£ç  â†’ 200è¡Œç»Ÿä¸€å¤„ç† = èŠ‚çœ400è¡Œ
- **æ€»è®¡å‡å°‘**: ~3000è¡Œä»£ç 

### **æ€§èƒ½æå‡**:
- **ç¼“å­˜å‘½ä¸­ç‡**: é¢„è®¡40-60%çš„Promptè°ƒç”¨å¯ä»¥ç¼“å­˜
- **å“åº”æ—¶é—´**: ç¼“å­˜å‘½ä¸­çš„è°ƒç”¨å“åº”æ—¶é—´å‡å°‘90%ä»¥ä¸Š
- **TokenèŠ‚çœ**: é€šè¿‡ä¼˜åŒ–å’Œç¼“å­˜èŠ‚çœ20-30%çš„Tokenä½¿ç”¨
- **é”™è¯¯æ¢å¤**: ç»Ÿä¸€é‡è¯•æœºåˆ¶æé«˜95%ä»¥ä¸Šçš„è°ƒç”¨æˆåŠŸç‡

### **ç»´æŠ¤æ€§æ”¹å–„**:
- **å•ä¸€ä»£ç æº**: LLMç›¸å…³é€»è¾‘ç»Ÿä¸€ç®¡ç†
- **é…ç½®é›†ä¸­**: æ‰€æœ‰LLMé…ç½®åœ¨ä¸€å¤„ç®¡ç†
- **ç›‘æ§å®Œå–„**: å®Œæ•´çš„è°ƒç”¨ç»Ÿè®¡å’Œæ€§èƒ½åˆ†æ
- **æ‰©å±•ç®€åŒ–**: æ–°æ™ºèƒ½ä½“å¼€å‘æ—¶é—´å‡å°‘70%

è¿™ä¸ªä¼˜åŒ–æ–¹æ¡ˆå°†å½»åº•è§£å†³LLMé€šä¿¡å±‚çš„é‡å¤ä»£ç é—®é¢˜ï¼Œæä¾›ä¼ä¸šçº§çš„æ€§èƒ½ç›‘æ§å’Œé”™è¯¯å¤„ç†èƒ½åŠ›ï¼Œä¸ºV-Agentæ¡†æ¶å¥ å®šåšå®çš„æŠ€æœ¯åŸºç¡€ã€‚