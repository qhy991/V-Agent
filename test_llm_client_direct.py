#!/usr/bin/env python3
"""
ç›´æ¥æµ‹è¯•LLMå®¢æˆ·ç«¯
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config.config import FrameworkConfig
from llm_integration.enhanced_llm_client import EnhancedLLMClient

async def test_llm_client():
    """ç›´æ¥æµ‹è¯•LLMå®¢æˆ·ç«¯"""
    print("ğŸ” ç›´æ¥æµ‹è¯•LLMå®¢æˆ·ç«¯...")
    
    try:
        # 1. åŠ è½½é…ç½®
        print("ğŸ“‹ åŠ è½½é…ç½®...")
        config = FrameworkConfig.from_env()
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"ğŸ¤– Provider: {config.llm.provider}")
        print(f"ğŸ¤– Model: {config.llm.model_name}")
        print(f"ğŸ¤– API Key: {'å·²è®¾ç½®' if config.llm.api_key else 'æœªè®¾ç½®'}")
        if config.llm.api_key:
            print(f"ğŸ¤– API Keyé•¿åº¦: {len(config.llm.api_key)}")
            print(f"ğŸ¤– API Keyå‰ç¼€: {config.llm.api_key[:10]}...")
        print(f"ğŸ¤– API Base URL: {config.llm.api_base_url}")
        
        # 2. åˆ›å»ºLLMå®¢æˆ·ç«¯
        print("\nğŸ”§ åˆ›å»ºLLMå®¢æˆ·ç«¯...")
        llm_client = EnhancedLLMClient(config.llm)
        print("âœ… LLMå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        
        # 3. æµ‹è¯•ç®€å•è¯·æ±‚
        print("\nğŸš€ æµ‹è¯•ç®€å•è¯·æ±‚...")
        test_prompt = "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼Œç”¨ä¸€å¥è¯å›ç­”ã€‚"
        
        response = await llm_client.send_prompt(
            prompt=test_prompt,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚",
            temperature=0.7,
            max_tokens=100
        )
        
        print(f"âœ… è¯·æ±‚æˆåŠŸ!")
        print(f"ğŸ“ å“åº”: {response}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(test_llm_client())
    sys.exit(0 if success else 1) 